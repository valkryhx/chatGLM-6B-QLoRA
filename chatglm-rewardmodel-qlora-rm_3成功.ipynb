{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-12T09:26:35.078561Z","iopub.execute_input":"2023-08-12T09:26:35.078912Z","iopub.status.idle":"2023-08-12T09:26:35.087969Z","shell.execute_reply.started":"2023-08-12T09:26:35.078882Z","shell.execute_reply":"2023-08-12T09:26:35.085371Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/valkryhx/chatGLM-6B-QLoRA","metadata":{"execution":{"iopub.status.busy":"2023-08-12T09:26:35.114778Z","iopub.execute_input":"2023-08-12T09:26:35.115479Z","iopub.status.idle":"2023-08-12T09:26:36.138420Z","shell.execute_reply.started":"2023-08-12T09:26:35.115447Z","shell.execute_reply":"2023-08-12T09:26:36.137130Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"fatal: destination path 'chatGLM-6B-QLoRA' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"#cat /kaggle/working/chatGLM-6B-QLoRA/output-rm-1k-0812-v1/checkpoint-40/adapter_config.json","metadata":{"execution":{"iopub.status.busy":"2023-08-12T05:52:59.477242Z","iopub.execute_input":"2023-08-12T05:52:59.477619Z","iopub.status.idle":"2023-08-12T05:52:59.482401Z","shell.execute_reply.started":"2023-08-12T05:52:59.477585Z","shell.execute_reply":"2023-08-12T05:52:59.481200Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"%cd chatGLM-6B-QLoRA\n!git pull --all --force \n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-08-12T09:26:36.141011Z","iopub.execute_input":"2023-08-12T09:26:36.141450Z","iopub.status.idle":"2023-08-12T09:27:26.486398Z","shell.execute_reply.started":"2023-08-12T09:26:36.141407Z","shell.execute_reply":"2023-08-12T09:27:26.485050Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/chatGLM-6B-QLoRA\nFetching origin\nAlready up to date.\nCollecting peft==0.4.0 (from -r requirements.txt (line 1))\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.30.2)\nCollecting datasets==2.12.0 (from -r requirements.txt (line 3))\n  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm==4.65.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.65.0)\nCollecting loguru==0.7.0 (from -r requirements.txt (line 5))\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fire==0.5.0 (from -r requirements.txt (line 6))\n  Downloading fire-0.5.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting bitsandbytes==0.39.0 (from -r requirements.txt (line 7))\n  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting wandb==0.15.3 (from -r requirements.txt (line 8))\n  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting cpm_kernels==1.0.11 (from -r requirements.txt (line 9))\n  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.20.3)\nRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.1.99)\nCollecting deepspeed==0.9.5 (from -r requirements.txt (line 12))\n  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting evaluate==0.4.0 (from -r requirements.txt (line 13))\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting trl==0.4.7 (from -r requirements.txt (line 14))\n  Downloading trl-0.4.7-py3-none-any.whl (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (2.0.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (0.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (11.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (2.3.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.1.31)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.20.3)\nCollecting hjson (from deepspeed==0.9.5->-r requirements.txt (line 12))\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.11.1)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (9.0.0)\nRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.10.10)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (4.0.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2->-r requirements.txt (line 2)) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0->-r requirements.txt (line 1)) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2023.5.7)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2023.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.3.0)\nBuilding wheels for collected packages: fire, deepspeed\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=8a15741f38b579118694311ea8e2f2388d51e7f7141c0d688802b14e7cc3a8f8\n  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844550 sha256=7a389a19348cf1446a63a1f43c5051f64225c34fd29cbb08d004291926c1a1fa\n  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\nSuccessfully built fire deepspeed\nInstalling collected packages: hjson, cpm_kernels, bitsandbytes, loguru, fire, wandb, deepspeed, peft, datasets, trl, evaluate\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.5\n    Uninstalling wandb-0.15.5:\n      Successfully uninstalled wandb-0.15.5\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed bitsandbytes-0.39.0 cpm_kernels-1.0.11 datasets-2.12.0 deepspeed-0.9.5 evaluate-0.4.0 fire-0.5.0 hjson-3.1.0 loguru-0.7.0 peft-0.4.0 trl-0.4.7 wandb-0.15.3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"cat /kaggle/working/chatGLM-6B-QLoRA/output-rm-1k-0811-v3/config.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!CUDA_VISIBLE_DEVICES=0 python  rm_trl.py \\\n--model_name 'THUDM/chatglm2-6b' \\\n--resume_from_checkpoint /kaggle/working/chatGLM-6B-QLoRA/reward_model_0809_v1/checkpoint-50 \\\n--num_train_epochs 2 \\\n--gradient_accumulation_steps 1 \\\n--per_device_train_batch_size 4 \\\n--per_device_eval_batch_size  4 \\\n--max_length 512 \\\n--output_dir ./reward_model_0810_v2 \\\n--train_subset 80 \\\n--eval_subset 20 \\\n--local_rank 0  \\\n--bf16 False","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:36:48.134969Z","iopub.execute_input":"2023-08-10T15:36:48.135458Z","iopub.status.idle":"2023-08-10T15:40:03.300811Z","shell.execute_reply.started":"2023-08-10T15:36:48.135418Z","shell.execute_reply":"2023-08-10T15:40:03.299460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 上面的warning There were missing keys in the checkpoint model loaded: 'transformer.embedding.word_embeddings.weight', 'transformer.rotary_pos_emb.inv_freq', 'transformer.encoder.layers.0.input_layernorm.weight', ....\n# 是 trainingArguments中设置 load_best_model_at_end = True 后出现的\n# 说明 trainer的确在最后把最好的那个adapters的参数做了load（只不过load时strict=True要求严格匹配了）\n# 这说明保存在output_dir中的pytorch_model.bin和vhead.bin参数都是最佳的 我们来验证下","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:20:02.990258Z","iopub.execute_input":"2023-08-11T13:20:02.990655Z","iopub.status.idle":"2023-08-11T13:20:04.059182Z","shell.execute_reply.started":"2023-08-11T13:20:02.990619Z","shell.execute_reply":"2023-08-11T13:20:04.057829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"验证成功  跑的是py中的test_load_best()\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# deepspeed 单机多卡从头训练 reward model","metadata":{}},{"cell_type":"code","source":"!git pull --all --force \n!deepspeed --include localhost:0,1  rewardmodel_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-rm-1k-0811-v2 \\\n  --num_train_samples 200 \\\n  --num_eval_samples 100 \\\n  --train_data_path ./data/rm_data  \\\n  --eval_data_path  ./data/rm_data    \\\n  --data_type sharegpt  \\\n  --max_length 800 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 4 \\\n  --per_device_eval_batch_size 4  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  2e-5 \\\n  --num_train_epochs  2  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json\n","metadata":{"execution":{"iopub.status.busy":"2023-08-11T12:40:22.920460Z","iopub.execute_input":"2023-08-11T12:40:22.920924Z","iopub.status.idle":"2023-08-11T13:06:52.328475Z","shell.execute_reply.started":"2023-08-11T12:40:22.920884Z","shell.execute_reply":"2023-08-11T13:06:52.327240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--deepspeed ds_zero2_config.json","metadata":{}},{"cell_type":"markdown","source":"# 单机多卡deepspeed 从ckpt继续训练 ","metadata":{}},{"cell_type":"code","source":"!git pull --all --force \n!deepspeed --include localhost:0,1  rewardmodel_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-rm-1k-0811-v3 \\\n  --resume_from_checkpoint output-rm-1k-0811-v2 \\\n  --num_train_samples 200 \\\n  --num_eval_samples 100 \\\n  --train_data_path ./data/rm_data  \\\n  --eval_data_path  ./data/rm_data    \\\n  --data_type sharegpt  \\\n  --max_length 800 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 4 \\\n  --per_device_eval_batch_size 4  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  2e-5 \\\n  --num_train_epochs  2  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"execution":{"iopub.status.busy":"2023-08-11T14:16:06.760852Z","iopub.execute_input":"2023-08-11T14:16:06.761319Z","iopub.status.idle":"2023-08-11T14:42:34.756018Z","shell.execute_reply.started":"2023-08-11T14:16:06.761278Z","shell.execute_reply":"2023-08-11T14:42:34.754676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls -lrt /kaggle/working/chatGLM-6B-QLoRA/output-rm-1k-0811-v3","metadata":{"execution":{"iopub.status.busy":"2023-08-11T14:43:55.710087Z","iopub.execute_input":"2023-08-11T14:43:55.710535Z","iopub.status.idle":"2023-08-11T14:43:56.997109Z","shell.execute_reply.started":"2023-08-11T14:43:55.710495Z","shell.execute_reply":"2023-08-11T14:43:56.995800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 单卡训练 reward model","metadata":{}},{"cell_type":"code","source":"!git pull --all --force \n!CUDA_VISIBLE_DEVICES=0 python  rewardmodel_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-rm-1k-0811-v1 \\\n  --num_train_samples 20 \\\n  --num_eval_samples 20 \\\n  --train_data_path ./data/rm_data  \\\n  --eval_data_path  ./data/rm_data    \\\n  --data_type sharegpt  \\\n  --max_length 800 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 4 \\\n  --per_device_eval_batch_size 4  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  2e-5 \\\n  --num_train_epochs  10  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force \n!CUDA_VISIBLE_DEVICES=0 python  rm_3.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-rm-1k-0812-v1 \\\n  --num_train_samples 20 \\\n  --num_eval_samples 20 \\\n  --train_data_path ./data/rm_data  \\\n  --eval_data_path  ./data/rm_data    \\\n  --data_type sharegpt  \\\n  --max_length 400 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 1 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  2e-5 \\\n  --num_train_epochs  4  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True ","metadata":{"execution":{"iopub.status.busy":"2023-08-11T18:27:26.248812Z","iopub.execute_input":"2023-08-11T18:27:26.249305Z","iopub.status.idle":"2023-08-11T18:36:36.195541Z","shell.execute_reply.started":"2023-08-11T18:27:26.249258Z","shell.execute_reply":"2023-08-11T18:36:36.193892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rm -rf output-rm-1k-0812-v1","metadata":{"execution":{"iopub.status.busy":"2023-08-11T18:27:14.344853Z","iopub.execute_input":"2023-08-11T18:27:14.345318Z","iopub.status.idle":"2023-08-11T18:27:15.559789Z","shell.execute_reply.started":"2023-08-11T18:27:14.345279Z","shell.execute_reply":"2023-08-11T18:27:15.558472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"参考 https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/src/glmtuner/tuner/core/adapter.py#L83\n实现 resume——from --checkpoint\n\n参考","metadata":{}},{"cell_type":"code","source":"https://huggingface.co/docs/trl/models#trl.AutoModelForCausalLMWithValueHead\n    https://github.com/yongzhuo/chatglm-maths/blob/main/chatglm_maths/t10_lora_trl_train_ppo.py#L175\n        https://huggingface.co/docs/trl/v0.5.0/en/customization#use-the-cuda-cache-optimizer\n            https://huggingface.co/docs/trl/trainer#trl.RewardTrainer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# python 【rm_3.py】 单卡训练 从头开始","metadata":{}},{"cell_type":"code","source":"!git pull --all --force \n!CUDA_VISIBLE_DEVICES=0 python  rm_3.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-rm-1k-0812-v1 \\\n  --num_train_samples 20 \\\n  --num_eval_samples 20 \\\n  --train_data_path ./data/rm_data  \\\n  --eval_data_path  ./data/rm_data    \\\n  --data_type sharegpt  \\\n  --max_length 400 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 1 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  2e-5 \\\n  --num_train_epochs  4  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# deepspeed train 【rm_3.py】恢复训练 \n#  batach_size 2  gradient accumulation steps 4","metadata":{}},{"cell_type":"code","source":"!git pull --all --force \n!deepspeed --include localhost:0,1   rm_3.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-rm-1k-0812-v2 \\\n  --resume_from_checkpoint output-rm-1k-0812-v1/checkpoint-60 \\\n  --num_train_samples 500 \\\n  --num_eval_samples 200 \\\n  --train_data_path ./data/rm_data  \\\n  --eval_data_path  ./data/rm_data    \\\n  --data_type sharegpt  \\\n  --max_length 800 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 2  \\\n  --gradient_accumulation_steps 4 \\\n  --learning_rate  2e-5 \\\n  --num_train_epochs  4  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True ","metadata":{"execution":{"iopub.status.busy":"2023-08-12T09:56:17.542178Z","iopub.execute_input":"2023-08-12T09:56:17.543246Z","iopub.status.idle":"2023-08-12T12:00:03.776289Z","shell.execute_reply.started":"2023-08-12T09:56:17.543199Z","shell.execute_reply":"2023-08-12T12:00:03.774957Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Fetching origin\nAlready up to date.\n[2023-08-12 09:56:26,904] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-12 09:56:52,112] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n[2023-08-12 09:56:52,132] [INFO] [runner.py:555:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None rm_3.py --train_args_json luzi.json --model_name_or_path THUDM/chatglm2-6b --output_dir output-rm-1k-0812-v2 --resume_from_checkpoint output-rm-1k-0812-v1/checkpoint-60 --num_train_samples 500 --num_eval_samples 200 --train_data_path ./data/rm_data --eval_data_path ./data/rm_data --data_type sharegpt --max_length 800 --lora_rank 64 --lora_dropout 0.05 --compute_dtype fp16 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 4 --save_total_limit 2 --load_in_4bit True\n[2023-08-12 09:56:54,091] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-12 09:57:00,501] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n[2023-08-12 09:57:00,501] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n[2023-08-12 09:57:00,501] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n[2023-08-12 09:57:00,501] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n[2023-08-12 09:57:00,501] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n[2023-08-12 09:57:00,501] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n[2023-08-12 09:57:00,501] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n[2023-08-12 09:57:00,501] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n[2023-08-12 09:57:00,501] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n[2023-08-12 09:57:00,501] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n[2023-08-12 09:57:00,501] [INFO] [launch.py:163:main] dist_world_size=2\n[2023-08-12 09:57:00,501] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-12 09:57:10,768] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-08-12 09:57:10,768] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n\u001b[32m2023-08-12 09:57:16.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1333\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n\u001b[32m2023-08-12 09:57:16.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1333\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n\u001b[32m2023-08-12 09:57:16.847\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1379\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-12 09:57:16.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m605\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n\u001b[32m2023-08-12 09:57:16.855\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1379\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-12 09:57:16.856\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m605\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 50.44it/s]\n\u001b[32m2023-08-12 09:57:17.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m612\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-12 09:57:17.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m616\u001b[0m - \u001b[1m在取样之后 data len =500\u001b[0m\n\u001b[32m2023-08-12 09:57:17.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m620\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n\u001b[32m2023-08-12 09:57:17.133\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m605\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 359.69it/s]\n\u001b[32m2023-08-12 09:57:17.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m612\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-12 09:57:17.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m616\u001b[0m - \u001b[1m在取样之后 data len =500\u001b[0m\n\u001b[32m2023-08-12 09:57:17.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m620\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n\u001b[32m2023-08-12 09:57:17.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m605\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 635.12it/s]\n\u001b[32m2023-08-12 09:57:17.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m612\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-12 09:57:17.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m616\u001b[0m - \u001b[1m在取样之后 data len =200\u001b[0m\n\u001b[32m2023-08-12 09:57:17.297\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m620\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\nnumber_train_samples=500\nnumber_of_eval_samples=200\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 693.73it/s]\n\u001b[32m2023-08-12 09:57:17.341\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m612\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-12 09:57:17.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m616\u001b[0m - \u001b[1m在取样之后 data len =200\u001b[0m\n\u001b[32m2023-08-12 09:57:17.343\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m620\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\nnumber_train_samples=500\nnumber_of_eval_samples=200\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:46<00:00, 15.28s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:46<00:00, 15.28s/it]\nmemory footprint of model: 3.6520424485206604 GB\nmemory footprint of model: 3.6520424485206604 GB\n\u001b[32m2023-08-12 09:59:05.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1490\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\n\u001b[32m2023-08-12 09:59:05.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1490\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\nbelow trainable paramters only contains peft lora params.\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nbelow trainable paramters only contains peft lora params.\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\n\u001b[32m2023-08-12 10:00:57.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1531\u001b[0m - \u001b[1madapter_weights.keys()=dict_keys(['base_model.model.transformer.encoder.layers.0.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.0.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.0.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.0.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.0.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.0.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.0.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.0.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.1.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.1.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.1.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.1.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.1.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.1.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.1.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.1.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.2.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.2.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.2.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.2.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.2.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.2.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.2.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.2.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.3.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.3.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.3.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.3.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.3.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.3.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.3.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.3.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.4.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.4.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.4.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.4.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.4.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.4.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.4.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.4.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.5.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.5.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.5.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.5.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.5.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.5.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.5.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.5.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.6.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.6.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.6.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.6.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.6.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.6.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.6.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.6.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.7.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.7.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.7.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.7.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.7.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.7.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.7.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.7.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.8.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.8.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.8.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.8.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.8.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.8.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.8.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.8.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.9.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.9.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.9.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.9.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.9.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.9.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.9.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.9.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.10.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.10.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.10.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.10.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.10.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.10.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.10.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.10.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.11.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.11.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.11.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.11.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.11.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.11.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.11.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.11.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.12.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.12.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.12.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.12.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.12.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.12.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.12.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.12.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.13.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.13.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.13.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.13.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.13.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.13.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.13.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.13.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.14.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.14.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.14.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.14.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.14.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.14.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.14.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.14.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.15.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.15.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.15.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.15.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.15.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.15.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.15.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.15.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.16.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.16.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.16.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.16.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.16.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.16.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.16.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.16.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.17.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.17.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.17.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.17.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.17.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.17.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.17.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.17.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.18.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.18.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.18.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.18.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.18.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.18.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.18.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.18.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.19.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.19.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.19.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.19.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.19.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.19.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.19.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.19.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.20.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.20.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.20.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.20.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.20.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.20.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.20.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.20.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.21.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.21.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.21.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.21.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.21.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.21.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.21.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.21.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.22.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.22.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.22.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.22.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.22.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.22.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.22.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.22.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.23.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.23.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.23.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.23.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.23.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.23.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.23.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.23.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.24.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.24.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.24.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.24.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.24.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.24.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.24.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.24.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.25.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.25.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.25.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.25.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.25.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.25.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.25.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.25.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.26.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.26.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.26.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.26.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.26.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.26.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.26.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.26.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.27.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.27.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.27.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.27.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.27.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.27.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.27.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.27.mlp.dense_4h_to_h.lora_B.weight'])\u001b[0m\n\u001b[32m2023-08-12 10:00:57.246\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1531\u001b[0m - \u001b[1madapter_weights.keys()=dict_keys(['base_model.model.transformer.encoder.layers.0.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.0.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.0.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.0.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.0.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.0.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.0.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.0.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.1.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.1.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.1.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.1.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.1.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.1.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.1.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.1.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.2.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.2.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.2.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.2.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.2.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.2.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.2.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.2.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.3.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.3.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.3.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.3.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.3.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.3.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.3.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.3.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.4.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.4.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.4.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.4.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.4.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.4.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.4.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.4.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.5.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.5.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.5.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.5.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.5.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.5.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.5.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.5.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.6.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.6.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.6.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.6.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.6.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.6.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.6.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.6.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.7.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.7.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.7.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.7.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.7.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.7.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.7.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.7.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.8.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.8.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.8.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.8.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.8.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.8.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.8.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.8.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.9.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.9.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.9.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.9.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.9.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.9.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.9.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.9.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.10.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.10.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.10.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.10.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.10.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.10.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.10.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.10.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.11.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.11.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.11.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.11.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.11.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.11.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.11.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.11.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.12.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.12.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.12.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.12.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.12.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.12.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.12.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.12.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.13.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.13.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.13.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.13.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.13.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.13.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.13.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.13.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.14.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.14.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.14.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.14.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.14.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.14.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.14.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.14.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.15.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.15.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.15.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.15.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.15.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.15.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.15.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.15.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.16.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.16.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.16.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.16.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.16.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.16.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.16.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.16.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.17.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.17.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.17.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.17.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.17.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.17.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.17.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.17.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.18.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.18.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.18.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.18.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.18.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.18.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.18.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.18.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.19.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.19.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.19.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.19.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.19.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.19.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.19.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.19.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.20.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.20.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.20.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.20.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.20.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.20.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.20.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.20.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.21.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.21.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.21.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.21.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.21.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.21.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.21.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.21.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.22.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.22.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.22.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.22.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.22.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.22.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.22.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.22.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.23.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.23.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.23.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.23.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.23.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.23.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.23.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.23.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.24.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.24.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.24.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.24.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.24.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.24.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.24.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.24.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.25.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.25.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.25.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.25.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.25.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.25.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.25.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.25.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.26.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.26.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.26.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.26.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.26.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.26.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.26.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.26.mlp.dense_4h_to_h.lora_B.weight', 'base_model.model.transformer.encoder.layers.27.self_attention.query_key_value.lora_A.weight', 'base_model.model.transformer.encoder.layers.27.self_attention.query_key_value.lora_B.weight', 'base_model.model.transformer.encoder.layers.27.self_attention.dense.lora_A.weight', 'base_model.model.transformer.encoder.layers.27.self_attention.dense.lora_B.weight', 'base_model.model.transformer.encoder.layers.27.mlp.dense_h_to_4h.lora_A.weight', 'base_model.model.transformer.encoder.layers.27.mlp.dense_h_to_4h.lora_B.weight', 'base_model.model.transformer.encoder.layers.27.mlp.dense_4h_to_h.lora_A.weight', 'base_model.model.transformer.encoder.layers.27.mlp.dense_4h_to_h.lora_B.weight'])\u001b[0m\nbefore load model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.lora_A.default.weight=Parameter containing:\ntensor([[-0.0138,  0.0060,  0.0104,  ..., -0.0021,  0.0138, -0.0025],\n        [ 0.0086, -0.0063,  0.0156,  ..., -0.0151, -0.0130,  0.0095],\n        [-0.0082, -0.0100,  0.0104,  ..., -0.0017,  0.0141,  0.0023],\n        ...,\n        [-0.0139,  0.0006, -0.0147,  ..., -0.0025, -0.0010, -0.0067],\n        [-0.0144,  0.0014, -0.0067,  ...,  0.0120, -0.0013, -0.0006],\n        [-0.0054, -0.0127, -0.0020,  ..., -0.0086,  0.0132, -0.0045]],\n       device='cuda:1', requires_grad=True)\nbefore load model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.lora_A.default.weight=Parameter containing:\ntensor([[-0.0138,  0.0060,  0.0104,  ..., -0.0021,  0.0138, -0.0025],\n        [ 0.0086, -0.0063,  0.0156,  ..., -0.0151, -0.0130,  0.0095],\n        [-0.0082, -0.0100,  0.0104,  ..., -0.0017,  0.0141,  0.0023],\n        ...,\n        [-0.0139,  0.0006, -0.0147,  ..., -0.0025, -0.0010, -0.0067],\n        [-0.0144,  0.0014, -0.0067,  ...,  0.0120, -0.0013, -0.0006],\n        [-0.0054, -0.0127, -0.0020,  ..., -0.0086,  0.0132, -0.0045]],\n       device='cuda:0', requires_grad=True)\nbefore load model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.weight=Parameter containing:\nParameter(Params4bit([[115],\n            [214],\n            [ 49],\n            ...,\n            [100],\n            [152],\n            [ 85]], device='cuda:1', dtype=torch.uint8))\nbefore load model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.weight=Parameter containing:\nParameter(Params4bit([[115],\n            [214],\n            [ 49],\n            ...,\n            [100],\n            [152],\n            [ 85]], device='cuda:0', dtype=torch.uint8))\nbefore load model.base_model.model.transformer.encoder.layers[27].self_attention.dense.weight=Parameter containing:\nParameter(Params4bit([[ 87],\n            [150],\n            [180],\n            ...,\n            [150],\n            [ 58],\n            [ 89]], device='cuda:0', dtype=torch.uint8))\nbefore load model.base_model.model.transformer.encoder.layers[27].self_attention.dense.weight=Parameter containing:\nParameter(Params4bit([[ 87],\n            [150],\n            [180],\n            ...,\n            [150],\n            [ 58],\n            [ 89]], device='cuda:1', dtype=torch.uint8))\nadapters_weigth:base_model.model.transformer.encoder.layers.27.self_attention.query_key_value.lora_A.weight=tensor([[-0.0142,  0.0064,  0.0101,  ..., -0.0019,  0.0137, -0.0028],\n        [ 0.0082, -0.0060,  0.0153,  ..., -0.0147, -0.0130,  0.0095],\n        [-0.0079, -0.0104,  0.0108,  ..., -0.0019,  0.0142,  0.0025],\n        ...,\n        [-0.0141,  0.0006, -0.0144,  ..., -0.0028, -0.0011, -0.0066],\n        [-0.0147,  0.0015, -0.0071,  ...,  0.0121, -0.0012, -0.0008],\n        [-0.0060, -0.0124, -0.0024,  ..., -0.0086,  0.0134, -0.0046]])\nadapters_weigth:base_model.model.transformer.encoder.layers.27.self_attention.query_key_value.lora_A.weight=tensor([[-0.0142,  0.0064,  0.0101,  ..., -0.0019,  0.0137, -0.0028],\n        [ 0.0082, -0.0060,  0.0153,  ..., -0.0147, -0.0130,  0.0095],\n        [-0.0079, -0.0104,  0.0108,  ..., -0.0019,  0.0142,  0.0025],\n        ...,\n        [-0.0141,  0.0006, -0.0144,  ..., -0.0028, -0.0011, -0.0066],\n        [-0.0147,  0.0015, -0.0071,  ...,  0.0121, -0.0012, -0.0008],\n        [-0.0060, -0.0124, -0.0024,  ..., -0.0086,  0.0134, -0.0046]])\nAfter load model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.lora_A.default.weight=Parameter containing:\ntensor([[-0.0142,  0.0064,  0.0101,  ..., -0.0019,  0.0137, -0.0028],\n        [ 0.0082, -0.0060,  0.0153,  ..., -0.0147, -0.0130,  0.0095],\n        [-0.0079, -0.0104,  0.0108,  ..., -0.0019,  0.0142,  0.0025],\n        ...,\n        [-0.0141,  0.0006, -0.0144,  ..., -0.0028, -0.0011, -0.0066],\n        [-0.0147,  0.0015, -0.0071,  ...,  0.0121, -0.0012, -0.0008],\n        [-0.0060, -0.0124, -0.0024,  ..., -0.0086,  0.0134, -0.0046]],\n       device='cuda:1', requires_grad=True)After load model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.lora_A.default.weight=Parameter containing:\ntensor([[-0.0142,  0.0064,  0.0101,  ..., -0.0019,  0.0137, -0.0028],\n        [ 0.0082, -0.0060,  0.0153,  ..., -0.0147, -0.0130,  0.0095],\n        [-0.0079, -0.0104,  0.0108,  ..., -0.0019,  0.0142,  0.0025],\n        ...,\n        [-0.0141,  0.0006, -0.0144,  ..., -0.0028, -0.0011, -0.0066],\n        [-0.0147,  0.0015, -0.0071,  ...,  0.0121, -0.0012, -0.0008],\n        [-0.0060, -0.0124, -0.0024,  ..., -0.0086,  0.0134, -0.0046]],\n       device='cuda:0', requires_grad=True)\n\n\u001b[32m2023-08-12 10:00:57.832\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1546\u001b[0m - \u001b[31m\u001b[1mlora model complete\u001b[0m\n\u001b[32m2023-08-12 10:00:57.832\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1546\u001b[0m - \u001b[31m\u001b[1mlora model complete\u001b[0m\nbefore load model.v_head.summary.weight=Parameter containing:\ntensor([[ 0.0090,  0.0039,  0.0154,  ..., -0.0065,  0.0088,  0.0089]],\n       device='cuda:1', requires_grad=True)\nbefore load model.v_head.summary.weight=Parameter containing:\ntensor([[ 0.0090,  0.0039,  0.0154,  ..., -0.0065,  0.0088,  0.0089]],\n       device='cuda:0', requires_grad=True)\nbefore load model.v_head.summary.bias=Parameter containing:\ntensor([0.0048], device='cuda:0', requires_grad=True)\nbefore load model.v_head.summary.bias=Parameter containing:\ntensor([0.0048], device='cuda:1', requires_grad=True)\n\u001b[32m2023-08-12 10:00:57.864\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1555\u001b[0m - \u001b[31m\u001b[1mv_head_weights={'summary.weight': tensor([[ 0.0082,  0.0043,  0.0157,  ..., -0.0061,  0.0088,  0.0089]]), 'summary.bias': tensor([0.0048])}\u001b[0m\n\u001b[32m2023-08-12 10:00:57.866\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1555\u001b[0m - \u001b[31m\u001b[1mv_head_weights={'summary.weight': tensor([[ 0.0082,  0.0043,  0.0157,  ..., -0.0061,  0.0088,  0.0089]]), 'summary.bias': tensor([0.0048])}\u001b[0m\nafter load model.pretrained_model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.lora_A.default.weight=Parameter containing:\ntensor([[-0.0142,  0.0064,  0.0101,  ..., -0.0019,  0.0137, -0.0028],\n        [ 0.0082, -0.0060,  0.0153,  ..., -0.0147, -0.0130,  0.0095],\n        [-0.0079, -0.0104,  0.0108,  ..., -0.0019,  0.0142,  0.0025],\n        ...,\n        [-0.0141,  0.0006, -0.0144,  ..., -0.0028, -0.0011, -0.0066],\n        [-0.0147,  0.0015, -0.0071,  ...,  0.0121, -0.0012, -0.0008],\n        [-0.0060, -0.0124, -0.0024,  ..., -0.0086,  0.0134, -0.0046]],\n       device='cuda:0', requires_grad=True)\nafter load model.pretrained_model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.weight=Parameter containing:\nParameter(Params4bit([[115],\n            [214],\n            [ 49],\n            ...,\n            [100],\n            [152],\n            [ 85]], device='cuda:0', dtype=torch.uint8))\nafter load model.pretrained_model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.lora_A.default.weight=Parameter containing:\ntensor([[-0.0142,  0.0064,  0.0101,  ..., -0.0019,  0.0137, -0.0028],\n        [ 0.0082, -0.0060,  0.0153,  ..., -0.0147, -0.0130,  0.0095],\n        [-0.0079, -0.0104,  0.0108,  ..., -0.0019,  0.0142,  0.0025],\n        ...,\n        [-0.0141,  0.0006, -0.0144,  ..., -0.0028, -0.0011, -0.0066],\n        [-0.0147,  0.0015, -0.0071,  ...,  0.0121, -0.0012, -0.0008],\n        [-0.0060, -0.0124, -0.0024,  ..., -0.0086,  0.0134, -0.0046]],\n       device='cuda:1', requires_grad=True)\nafter laod model.pretrained_model.base_model.model.transformer.encoder.layers[27].self_attention.dense.weight=Parameter containing:\nParameter(Params4bit([[ 87],\n            [150],\n            [180],\n            ...,\n            [150],\n            [ 58],\n            [ 89]], device='cuda:0', dtype=torch.uint8))\nafter load model.pretrained_model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.weight=Parameter containing:\nParameter(Params4bit([[115],\n            [214],\n            [ 49],\n            ...,\n            [100],\n            [152],\n            [ 85]], device='cuda:1', dtype=torch.uint8))\nafter laod model.pretrained_model.base_model.model.transformer.encoder.layers[27].self_attention.dense.weight=Parameter containing:\nParameter(Params4bit([[ 87],\n            [150],\n            [180],\n            ...,\n            [150],\n            [ 58],\n            [ 89]], device='cuda:1', dtype=torch.uint8))\nafter load model.v_head.summary.weight=Parameter containing:\ntensor([[ 0.0082,  0.0043,  0.0157,  ..., -0.0061,  0.0088,  0.0089]],\n       device='cuda:0', requires_grad=True)\nafter load model.v_head.summary.weight=Parameter containing:\ntensor([[ 0.0082,  0.0043,  0.0157,  ..., -0.0061,  0.0088,  0.0089]],\n       device='cuda:1', requires_grad=True)\nafter load model.v_head.summary.bias=Parameter containing:\ntensor([0.0048], device='cuda:0', requires_grad=True)\nafter load model.v_head.summary.bias=Parameter containing:\ntensor([0.0048], device='cuda:1', requires_grad=True)\n\u001b[32m2023-08-12 10:00:57.874\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1572\u001b[0m - \u001b[31m\u001b[1mreward model with vhead complete\u001b[0m\n\u001b[32m2023-08-12 10:00:57.874\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1572\u001b[0m - \u001b[31m\u001b[1mreward model with vhead complete\u001b[0m\nbelow trainable params include v_head\nbelow trainable params include v_head\ntrainable params: 118591489 || all params: 3506903041 || trainable%: 3.3817\nFinished loading model/lora_adapter/value_head and tokenizer\ntrainable params: 118591489 || all params: 3506903041 || trainable%: 3.3817\nFinished loading model/lora_adapter/value_head and tokenizer\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=[],\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=1,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-rm-1k-0812-v2,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=4.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-rm-1k-0812-v2,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=2,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=[],\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-rm-1k-0812-v2,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=4.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-rm-1k-0812-v2,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=2,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\n{'loss': 0.3429, 'learning_rate': 1.2e-05, 'epoch': 0.32}                       \n  8%|███▏                                    | 10/124 [06:20<1:11:21, 37.56s/it]\n  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n  4%|█▊                                          | 2/50 [00:04<01:36,  2.02s/it]\u001b[A\n  6%|██▋                                         | 3/50 [00:08<02:15,  2.88s/it]\u001b[A\n  8%|███▌                                        | 4/50 [00:12<02:32,  3.32s/it]\u001b[A\n 10%|████▍                                       | 5/50 [00:16<02:41,  3.59s/it]\u001b[A\n 12%|█████▎                                      | 6/50 [00:20<02:45,  3.75s/it]\u001b[A\n 14%|██████▏                                     | 7/50 [00:24<02:45,  3.86s/it]\u001b[A\n 16%|███████                                     | 8/50 [00:28<02:45,  3.94s/it]\u001b[A\n 18%|███████▉                                    | 9/50 [00:32<02:43,  3.99s/it]\u001b[A\n 20%|████████▌                                  | 10/50 [00:36<02:40,  4.02s/it]\u001b[A\n 22%|█████████▍                                 | 11/50 [00:40<02:37,  4.04s/it]\u001b[A\n 24%|██████████▎                                | 12/50 [00:44<02:34,  4.06s/it]\u001b[A\n 26%|███████████▏                               | 13/50 [00:48<02:30,  4.06s/it]\u001b[A\n 28%|████████████                               | 14/50 [00:53<02:27,  4.08s/it]\u001b[A\n 30%|████████████▉                              | 15/50 [00:57<02:22,  4.08s/it]\u001b[A\n 32%|█████████████▊                             | 16/50 [01:01<02:18,  4.09s/it]\u001b[A\n 34%|██████████████▌                            | 17/50 [01:05<02:14,  4.08s/it]\u001b[A\n 36%|███████████████▍                           | 18/50 [01:09<02:10,  4.09s/it]\u001b[A\n 38%|████████████████▎                          | 19/50 [01:13<02:06,  4.10s/it]\u001b[A\n 40%|█████████████████▏                         | 20/50 [01:17<02:02,  4.10s/it]\u001b[A\n 42%|██████████████████                         | 21/50 [01:21<01:58,  4.10s/it]\u001b[A\n 44%|██████████████████▉                        | 22/50 [01:25<01:54,  4.10s/it]\u001b[A\n 46%|███████████████████▊                       | 23/50 [01:29<01:50,  4.08s/it]\u001b[A\n 48%|████████████████████▋                      | 24/50 [01:33<01:46,  4.08s/it]\u001b[A\n 50%|█████████████████████▌                     | 25/50 [01:38<01:41,  4.08s/it]\u001b[A\n 52%|██████████████████████▎                    | 26/50 [01:42<01:37,  4.08s/it]\u001b[A\n 54%|███████████████████████▏                   | 27/50 [01:46<01:33,  4.07s/it]\u001b[A\n 56%|████████████████████████                   | 28/50 [01:50<01:29,  4.07s/it]\u001b[A\n 58%|████████████████████████▉                  | 29/50 [01:54<01:25,  4.06s/it]\u001b[A\n 60%|█████████████████████████▊                 | 30/50 [01:58<01:21,  4.06s/it]\u001b[A\n 62%|██████████████████████████▋                | 31/50 [02:02<01:17,  4.07s/it]\u001b[A\n 64%|███████████████████████████▌               | 32/50 [02:06<01:13,  4.08s/it]\u001b[A\n 66%|████████████████████████████▍              | 33/50 [02:10<01:09,  4.07s/it]\u001b[A\n 68%|█████████████████████████████▏             | 34/50 [02:14<01:05,  4.08s/it]\u001b[A\n 70%|██████████████████████████████             | 35/50 [02:18<01:00,  4.07s/it]\u001b[A\n 72%|██████████████████████████████▉            | 36/50 [02:22<00:56,  4.07s/it]\u001b[A\n 74%|███████████████████████████████▊           | 37/50 [02:26<00:52,  4.06s/it]\u001b[A\n 76%|████████████████████████████████▋          | 38/50 [02:30<00:48,  4.08s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 39/50 [02:35<00:44,  4.07s/it]\u001b[A\n 80%|██████████████████████████████████▍        | 40/50 [02:39<00:40,  4.07s/it]\u001b[A\n 82%|███████████████████████████████████▎       | 41/50 [02:43<00:36,  4.07s/it]\u001b[A\n 84%|████████████████████████████████████       | 42/50 [02:47<00:32,  4.07s/it]\u001b[A\n 86%|████████████████████████████████████▉      | 43/50 [02:51<00:28,  4.07s/it]\u001b[A\n 88%|█████████████████████████████████████▊     | 44/50 [02:55<00:24,  4.07s/it]\u001b[A\n 90%|██████████████████████████████████████▋    | 45/50 [02:59<00:20,  4.07s/it]\u001b[A\n 92%|███████████████████████████████████████▌   | 46/50 [03:03<00:16,  4.06s/it]\u001b[A\n 94%|████████████████████████████████████████▍  | 47/50 [03:07<00:12,  4.07s/it]\u001b[A\n 96%|█████████████████████████████████████████▎ | 48/50 [03:11<00:08,  4.07s/it]\u001b[A\n 98%|██████████████████████████████████████████▏| 49/50 [03:15<00:04,  4.06s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.2923555076122284, 'eval_accuracy': 0.845, 'eval_runtime': 204.5982, 'eval_samples_per_second': 0.978, 'eval_steps_per_second': 0.244, 'epoch': 0.32}\n  8%|███▏                                    | 10/124 [09:44<1:11:21, 37.56s/it]\n100%|███████████████████████████████████████████| 50/50 [03:21<00:00,  4.06s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-12 10:10:44.277\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v2/checkpoint-10\u001b[0m\n\u001b[32m2023-08-12 10:10:44.278\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-12 10:10:44.286\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.3293, 'learning_rate': 1.894736842105263e-05, 'epoch': 0.64}         \n 16%|██████▍                                 | 20/124 [16:03<1:09:23, 40.04s/it]\n  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n  4%|█▊                                          | 2/50 [00:04<01:36,  2.01s/it]\u001b[A\n  6%|██▋                                         | 3/50 [00:08<02:14,  2.86s/it]\u001b[A\n  8%|███▌                                        | 4/50 [00:12<02:32,  3.31s/it]\u001b[A\n 10%|████▍                                       | 5/50 [00:16<02:40,  3.58s/it]\u001b[A\n 12%|█████▎                                      | 6/50 [00:20<02:44,  3.74s/it]\u001b[A\n 14%|██████▏                                     | 7/50 [00:24<02:45,  3.85s/it]\u001b[A\n 16%|███████                                     | 8/50 [00:28<02:45,  3.93s/it]\u001b[A\n 18%|███████▉                                    | 9/50 [00:32<02:42,  3.96s/it]\u001b[A\n 20%|████████▌                                  | 10/50 [00:36<02:39,  4.00s/it]\u001b[A\n 22%|█████████▍                                 | 11/50 [00:40<02:36,  4.02s/it]\u001b[A\n 24%|██████████▎                                | 12/50 [00:44<02:33,  4.04s/it]\u001b[A\n 26%|███████████▏                               | 13/50 [00:48<02:29,  4.05s/it]\u001b[A\n 28%|████████████                               | 14/50 [00:52<02:26,  4.07s/it]\u001b[A\n 30%|████████████▉                              | 15/50 [00:56<02:22,  4.06s/it]\u001b[A\n 32%|█████████████▊                             | 16/50 [01:01<02:18,  4.07s/it]\u001b[A\n 34%|██████████████▌                            | 17/50 [01:05<02:13,  4.06s/it]\u001b[A\n 36%|███████████████▍                           | 18/50 [01:09<02:10,  4.06s/it]\u001b[A\n 38%|████████████████▎                          | 19/50 [01:13<02:06,  4.08s/it]\u001b[A\n 40%|█████████████████▏                         | 20/50 [01:17<02:02,  4.08s/it]\u001b[A\n 42%|██████████████████                         | 21/50 [01:21<01:58,  4.08s/it]\u001b[A\n 44%|██████████████████▉                        | 22/50 [01:25<01:54,  4.08s/it]\u001b[A\n 46%|███████████████████▊                       | 23/50 [01:29<01:49,  4.07s/it]\u001b[A\n 48%|████████████████████▋                      | 24/50 [01:33<01:45,  4.07s/it]\u001b[A\n 50%|█████████████████████▌                     | 25/50 [01:37<01:41,  4.07s/it]\u001b[A\n 52%|██████████████████████▎                    | 26/50 [01:41<01:37,  4.08s/it]\u001b[A\n 54%|███████████████████████▏                   | 27/50 [01:45<01:33,  4.07s/it]\u001b[A\n 56%|████████████████████████                   | 28/50 [01:49<01:29,  4.08s/it]\u001b[A\n 58%|████████████████████████▉                  | 29/50 [01:54<01:25,  4.07s/it]\u001b[A\n 60%|█████████████████████████▊                 | 30/50 [01:58<01:21,  4.08s/it]\u001b[A\n 62%|██████████████████████████▋                | 31/50 [02:02<01:17,  4.09s/it]\u001b[A\n 64%|███████████████████████████▌               | 32/50 [02:06<01:13,  4.10s/it]\u001b[A\n 66%|████████████████████████████▍              | 33/50 [02:10<01:09,  4.08s/it]\u001b[A\n 68%|█████████████████████████████▏             | 34/50 [02:14<01:05,  4.09s/it]\u001b[A\n 70%|██████████████████████████████             | 35/50 [02:18<01:01,  4.07s/it]\u001b[A\n 72%|██████████████████████████████▉            | 36/50 [02:22<00:57,  4.08s/it]\u001b[A\n 74%|███████████████████████████████▊           | 37/50 [02:26<00:52,  4.07s/it]\u001b[A\n 76%|████████████████████████████████▋          | 38/50 [02:30<00:49,  4.09s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 39/50 [02:34<00:44,  4.09s/it]\u001b[A\n 80%|██████████████████████████████████▍        | 40/50 [02:38<00:40,  4.09s/it]\u001b[A\n 82%|███████████████████████████████████▎       | 41/50 [02:43<00:36,  4.09s/it]\u001b[A\n 84%|████████████████████████████████████       | 42/50 [02:47<00:32,  4.10s/it]\u001b[A\n 86%|████████████████████████████████████▉      | 43/50 [02:51<00:28,  4.09s/it]\u001b[A\n 88%|█████████████████████████████████████▊     | 44/50 [02:55<00:24,  4.10s/it]\u001b[A\n 90%|██████████████████████████████████████▋    | 45/50 [02:59<00:20,  4.09s/it]\u001b[A\n 92%|███████████████████████████████████████▌   | 46/50 [03:03<00:16,  4.09s/it]\u001b[A\n 94%|████████████████████████████████████████▍  | 47/50 [03:07<00:12,  4.08s/it]\u001b[A\n 96%|█████████████████████████████████████████▎ | 48/50 [03:11<00:08,  4.10s/it]\u001b[A\n 98%|██████████████████████████████████████████▏| 49/50 [03:15<00:04,  4.09s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.22687171399593353, 'eval_accuracy': 0.875, 'eval_runtime': 204.0324, 'eval_samples_per_second': 0.98, 'eval_steps_per_second': 0.245, 'epoch': 0.64}\n 16%|██████▍                                 | 20/124 [19:27<1:09:23, 40.04s/it]\n100%|███████████████████████████████████████████| 50/50 [03:21<00:00,  4.10s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-12 10:20:27.154\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v2/checkpoint-20\u001b[0m\n\u001b[32m2023-08-12 10:20:27.154\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-12 10:20:27.155\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.2482, 'learning_rate': 1.719298245614035e-05, 'epoch': 0.96}         \n 24%|█████████▋                              | 30/124 [25:44<1:02:39, 40.00s/it]\n  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n  4%|█▊                                          | 2/50 [00:04<01:37,  2.02s/it]\u001b[A\n  6%|██▋                                         | 3/50 [00:08<02:15,  2.88s/it]\u001b[A\n  8%|███▌                                        | 4/50 [00:12<02:32,  3.31s/it]\u001b[A\n 10%|████▍                                       | 5/50 [00:16<02:41,  3.59s/it]\u001b[A\n 12%|█████▎                                      | 6/50 [00:20<02:44,  3.75s/it]\u001b[A\n 14%|██████▏                                     | 7/50 [00:24<02:45,  3.86s/it]\u001b[A\n 16%|███████                                     | 8/50 [00:28<02:45,  3.93s/it]\u001b[A\n 18%|███████▉                                    | 9/50 [00:32<02:42,  3.97s/it]\u001b[A\n 20%|████████▌                                  | 10/50 [00:36<02:40,  4.01s/it]\u001b[A\n 22%|█████████▍                                 | 11/50 [00:40<02:37,  4.03s/it]\u001b[A\n 24%|██████████▎                                | 12/50 [00:44<02:33,  4.05s/it]\u001b[A\n 26%|███████████▏                               | 13/50 [00:48<02:30,  4.06s/it]\u001b[A\n 28%|████████████                               | 14/50 [00:53<02:26,  4.07s/it]\u001b[A\n 30%|████████████▉                              | 15/50 [00:57<02:22,  4.07s/it]\u001b[A\n 32%|█████████████▊                             | 16/50 [01:01<02:18,  4.08s/it]\u001b[A\n 34%|██████████████▌                            | 17/50 [01:05<02:14,  4.06s/it]\u001b[A\n 36%|███████████████▍                           | 18/50 [01:09<02:10,  4.07s/it]\u001b[A\n 38%|████████████████▎                          | 19/50 [01:13<02:06,  4.08s/it]\u001b[A\n 40%|█████████████████▏                         | 20/50 [01:17<02:02,  4.08s/it]\u001b[A\n 42%|██████████████████                         | 21/50 [01:21<01:58,  4.09s/it]\u001b[A\n 44%|██████████████████▉                        | 22/50 [01:25<01:54,  4.08s/it]\u001b[A\n 46%|███████████████████▊                       | 23/50 [01:29<01:49,  4.07s/it]\u001b[A\n 48%|████████████████████▋                      | 24/50 [01:33<01:45,  4.07s/it]\u001b[A\n 50%|█████████████████████▌                     | 25/50 [01:37<01:41,  4.07s/it]\u001b[A\n 52%|██████████████████████▎                    | 26/50 [01:41<01:37,  4.07s/it]\u001b[A\n 54%|███████████████████████▏                   | 27/50 [01:46<01:33,  4.07s/it]\u001b[A\n 56%|████████████████████████                   | 28/50 [01:50<01:29,  4.07s/it]\u001b[A\n 58%|████████████████████████▉                  | 29/50 [01:54<01:25,  4.06s/it]\u001b[A\n 60%|█████████████████████████▊                 | 30/50 [01:58<01:21,  4.08s/it]\u001b[A\n 62%|██████████████████████████▋                | 31/50 [02:02<01:17,  4.08s/it]\u001b[A\n 64%|███████████████████████████▌               | 32/50 [02:06<01:13,  4.09s/it]\u001b[A\n 66%|████████████████████████████▍              | 33/50 [02:10<01:09,  4.08s/it]\u001b[A\n 68%|█████████████████████████████▏             | 34/50 [02:14<01:05,  4.08s/it]\u001b[A\n 70%|██████████████████████████████             | 35/50 [02:18<01:01,  4.08s/it]\u001b[A\n 72%|██████████████████████████████▉            | 36/50 [02:22<00:57,  4.07s/it]\u001b[A\n 74%|███████████████████████████████▊           | 37/50 [02:26<00:52,  4.06s/it]\u001b[A\n 76%|████████████████████████████████▋          | 38/50 [02:30<00:48,  4.08s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 39/50 [02:34<00:44,  4.08s/it]\u001b[A\n 80%|██████████████████████████████████▍        | 40/50 [02:39<00:40,  4.08s/it]\u001b[A\n 82%|███████████████████████████████████▎       | 41/50 [02:43<00:36,  4.08s/it]\u001b[A\n 84%|████████████████████████████████████       | 42/50 [02:47<00:32,  4.09s/it]\u001b[A\n 86%|████████████████████████████████████▉      | 43/50 [02:51<00:28,  4.08s/it]\u001b[A\n 88%|█████████████████████████████████████▊     | 44/50 [02:55<00:24,  4.08s/it]\u001b[A\n 90%|██████████████████████████████████████▋    | 45/50 [02:59<00:20,  4.07s/it]\u001b[A\n 92%|███████████████████████████████████████▌   | 46/50 [03:03<00:16,  4.08s/it]\u001b[A\n 94%|████████████████████████████████████████▍  | 47/50 [03:07<00:12,  4.09s/it]\u001b[A\n 96%|█████████████████████████████████████████▎ | 48/50 [03:11<00:08,  4.09s/it]\u001b[A\n 98%|██████████████████████████████████████████▏| 49/50 [03:15<00:04,  4.08s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.14328382909297943, 'eval_accuracy': 0.92, 'eval_runtime': 203.9312, 'eval_samples_per_second': 0.981, 'eval_steps_per_second': 0.245, 'epoch': 0.96}\n 24%|█████████▋                              | 30/124 [29:08<1:02:39, 40.00s/it]\n100%|███████████████████████████████████████████| 50/50 [03:21<00:00,  4.07s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-12 10:30:07.914\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v2/checkpoint-30\u001b[0m\n\u001b[32m2023-08-12 10:30:07.915\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-12 10:30:07.916\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.1755, 'learning_rate': 1.56140350877193e-05, 'epoch': 1.28}          \n 32%|█████████████▌                            | 40/124 [35:25<56:03, 40.05s/it]\n  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n  4%|█▊                                          | 2/50 [00:04<01:37,  2.02s/it]\u001b[A\n  6%|██▋                                         | 3/50 [00:08<02:14,  2.87s/it]\u001b[A\n  8%|███▌                                        | 4/50 [00:12<02:33,  3.33s/it]\u001b[A\n 10%|████▍                                       | 5/50 [00:16<02:41,  3.60s/it]\u001b[A\n 12%|█████▎                                      | 6/50 [00:20<02:45,  3.75s/it]\u001b[A\n 14%|██████▏                                     | 7/50 [00:24<02:46,  3.86s/it]\u001b[A\n 16%|███████                                     | 8/50 [00:28<02:45,  3.94s/it]\u001b[A\n 18%|███████▉                                    | 9/50 [00:32<02:43,  3.98s/it]\u001b[A\n 20%|████████▌                                  | 10/50 [00:36<02:40,  4.02s/it]\u001b[A\n 22%|█████████▍                                 | 11/50 [00:40<02:37,  4.04s/it]\u001b[A\n 24%|██████████▎                                | 12/50 [00:44<02:34,  4.06s/it]\u001b[A\n 26%|███████████▏                               | 13/50 [00:49<02:30,  4.07s/it]\u001b[A\n 28%|████████████                               | 14/50 [00:53<02:27,  4.09s/it]\u001b[A\n 30%|████████████▉                              | 15/50 [00:57<02:22,  4.08s/it]\u001b[A\n 32%|█████████████▊                             | 16/50 [01:01<02:18,  4.07s/it]\u001b[A\n 34%|██████████████▌                            | 17/50 [01:05<02:14,  4.06s/it]\u001b[A\n 36%|███████████████▍                           | 18/50 [01:09<02:10,  4.06s/it]\u001b[A\n 38%|████████████████▎                          | 19/50 [01:13<02:06,  4.08s/it]\u001b[A\n 40%|█████████████████▏                         | 20/50 [01:17<02:02,  4.08s/it]\u001b[A\n 42%|██████████████████                         | 21/50 [01:21<01:58,  4.09s/it]\u001b[A\n 44%|██████████████████▉                        | 22/50 [01:25<01:54,  4.08s/it]\u001b[A\n 46%|███████████████████▊                       | 23/50 [01:29<01:49,  4.07s/it]\u001b[A\n 48%|████████████████████▋                      | 24/50 [01:33<01:45,  4.06s/it]\u001b[A\n 50%|█████████████████████▌                     | 25/50 [01:37<01:41,  4.07s/it]\u001b[A\n 52%|██████████████████████▎                    | 26/50 [01:42<01:37,  4.08s/it]\u001b[A\n 54%|███████████████████████▏                   | 27/50 [01:46<01:33,  4.07s/it]\u001b[A\n 56%|████████████████████████                   | 28/50 [01:50<01:29,  4.07s/it]\u001b[A\n 58%|████████████████████████▉                  | 29/50 [01:54<01:25,  4.06s/it]\u001b[A\n 60%|█████████████████████████▊                 | 30/50 [01:58<01:21,  4.07s/it]\u001b[A\n 62%|██████████████████████████▋                | 31/50 [02:02<01:17,  4.09s/it]\u001b[A\n 64%|███████████████████████████▌               | 32/50 [02:06<01:13,  4.09s/it]\u001b[A\n 66%|████████████████████████████▍              | 33/50 [02:10<01:09,  4.08s/it]\u001b[A\n 68%|█████████████████████████████▏             | 34/50 [02:14<01:05,  4.08s/it]\u001b[A\n 70%|██████████████████████████████             | 35/50 [02:18<01:01,  4.07s/it]\u001b[A\n 72%|██████████████████████████████▉            | 36/50 [02:22<00:56,  4.07s/it]\u001b[A\n 74%|███████████████████████████████▊           | 37/50 [02:26<00:52,  4.06s/it]\u001b[A\n 76%|████████████████████████████████▋          | 38/50 [02:30<00:49,  4.09s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 39/50 [02:35<00:44,  4.07s/it]\u001b[A\n 80%|██████████████████████████████████▍        | 40/50 [02:39<00:40,  4.07s/it]\u001b[A\n 82%|███████████████████████████████████▎       | 41/50 [02:43<00:36,  4.07s/it]\u001b[A\n 84%|████████████████████████████████████       | 42/50 [02:47<00:32,  4.08s/it]\u001b[A\n 86%|████████████████████████████████████▉      | 43/50 [02:51<00:28,  4.08s/it]\u001b[A\n 88%|█████████████████████████████████████▊     | 44/50 [02:55<00:24,  4.08s/it]\u001b[A\n 90%|██████████████████████████████████████▋    | 45/50 [02:59<00:20,  4.07s/it]\u001b[A\n 92%|███████████████████████████████████████▌   | 46/50 [03:03<00:16,  4.07s/it]\u001b[A\n 94%|████████████████████████████████████████▍  | 47/50 [03:07<00:12,  4.07s/it]\u001b[A\n 96%|█████████████████████████████████████████▎ | 48/50 [03:11<00:08,  4.08s/it]\u001b[A\n 98%|██████████████████████████████████████████▏| 49/50 [03:15<00:04,  4.06s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.10393962264060974, 'eval_accuracy': 0.94, 'eval_runtime': 203.9067, 'eval_samples_per_second': 0.981, 'eval_steps_per_second': 0.245, 'epoch': 1.28}\n 32%|█████████████▌                            | 40/124 [38:49<56:03, 40.05s/it]\n100%|███████████████████████████████████████████| 50/50 [03:21<00:00,  4.07s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-12 10:39:49.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v2/checkpoint-40\u001b[0m\n\u001b[32m2023-08-12 10:39:49.207\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-12 10:39:49.208\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.1199, 'learning_rate': 1.385964912280702e-05, 'epoch': 1.6}          \n 40%|████████████████▉                         | 50/124 [45:06<49:17, 39.97s/it]\n  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n  4%|█▊                                          | 2/50 [00:04<01:37,  2.03s/it]\u001b[A\n  6%|██▋                                         | 3/50 [00:08<02:15,  2.89s/it]\u001b[A\n  8%|███▌                                        | 4/50 [00:12<02:32,  3.32s/it]\u001b[A\n 10%|████▍                                       | 5/50 [00:16<02:41,  3.59s/it]\u001b[A\n 12%|█████▎                                      | 6/50 [00:20<02:45,  3.75s/it]\u001b[A\n 14%|██████▏                                     | 7/50 [00:24<02:46,  3.86s/it]\u001b[A\n 16%|███████                                     | 8/50 [00:28<02:45,  3.94s/it]\u001b[A\n 18%|███████▉                                    | 9/50 [00:32<02:42,  3.97s/it]\u001b[A\n 20%|████████▌                                  | 10/50 [00:36<02:40,  4.00s/it]\u001b[A\n 22%|█████████▍                                 | 11/50 [00:40<02:37,  4.03s/it]\u001b[A\n 24%|██████████▎                                | 12/50 [00:44<02:33,  4.04s/it]\u001b[A\n 26%|███████████▏                               | 13/50 [00:48<02:29,  4.05s/it]\u001b[A\n 28%|████████████                               | 14/50 [00:53<02:26,  4.07s/it]\u001b[A\n 30%|████████████▉                              | 15/50 [00:57<02:22,  4.07s/it]\u001b[A\n 32%|█████████████▊                             | 16/50 [01:01<02:18,  4.08s/it]\u001b[A\n 34%|██████████████▌                            | 17/50 [01:05<02:14,  4.07s/it]\u001b[A\n 36%|███████████████▍                           | 18/50 [01:09<02:10,  4.07s/it]\u001b[A\n 38%|████████████████▎                          | 19/50 [01:13<02:06,  4.08s/it]\u001b[A\n 40%|█████████████████▏                         | 20/50 [01:17<02:02,  4.08s/it]\u001b[A\n 42%|██████████████████                         | 21/50 [01:21<01:58,  4.08s/it]\u001b[A\n 44%|██████████████████▉                        | 22/50 [01:25<01:54,  4.08s/it]\u001b[A\n 46%|███████████████████▊                       | 23/50 [01:29<01:49,  4.07s/it]\u001b[A\n 48%|████████████████████▋                      | 24/50 [01:33<01:45,  4.06s/it]\u001b[A\n 50%|█████████████████████▌                     | 25/50 [01:37<01:41,  4.07s/it]\u001b[A\n 52%|██████████████████████▎                    | 26/50 [01:41<01:37,  4.07s/it]\u001b[A\n 54%|███████████████████████▏                   | 27/50 [01:46<01:33,  4.08s/it]\u001b[A\n 56%|████████████████████████                   | 28/50 [01:50<01:29,  4.07s/it]\u001b[A\n 58%|████████████████████████▉                  | 29/50 [01:54<01:25,  4.06s/it]\u001b[A\n 60%|█████████████████████████▊                 | 30/50 [01:58<01:21,  4.07s/it]\u001b[A\n 62%|██████████████████████████▋                | 31/50 [02:02<01:17,  4.08s/it]\u001b[A\n 64%|███████████████████████████▌               | 32/50 [02:06<01:13,  4.08s/it]\u001b[A\n 66%|████████████████████████████▍              | 33/50 [02:10<01:09,  4.07s/it]\u001b[A\n 68%|█████████████████████████████▏             | 34/50 [02:14<01:05,  4.07s/it]\u001b[A\n 70%|██████████████████████████████             | 35/50 [02:18<01:00,  4.06s/it]\u001b[A\n 72%|██████████████████████████████▉            | 36/50 [02:22<00:56,  4.07s/it]\u001b[A\n 74%|███████████████████████████████▊           | 37/50 [02:26<00:52,  4.06s/it]\u001b[A\n 76%|████████████████████████████████▋          | 38/50 [02:30<00:48,  4.08s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 39/50 [02:34<00:44,  4.07s/it]\u001b[A\n 80%|██████████████████████████████████▍        | 40/50 [02:38<00:40,  4.07s/it]\u001b[A\n 82%|███████████████████████████████████▎       | 41/50 [02:42<00:36,  4.07s/it]\u001b[A\n 84%|████████████████████████████████████       | 42/50 [02:47<00:32,  4.07s/it]\u001b[A\n 86%|████████████████████████████████████▉      | 43/50 [02:51<00:28,  4.06s/it]\u001b[A\n 88%|█████████████████████████████████████▊     | 44/50 [02:55<00:24,  4.07s/it]\u001b[A\n 90%|██████████████████████████████████████▋    | 45/50 [02:59<00:20,  4.06s/it]\u001b[A\n 92%|███████████████████████████████████████▌   | 46/50 [03:03<00:16,  4.06s/it]\u001b[A\n 94%|████████████████████████████████████████▍  | 47/50 [03:07<00:12,  4.06s/it]\u001b[A\n 96%|█████████████████████████████████████████▎ | 48/50 [03:11<00:08,  4.06s/it]\u001b[A\n 98%|██████████████████████████████████████████▏| 49/50 [03:15<00:04,  4.07s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.07617460936307907, 'eval_accuracy': 0.95, 'eval_runtime': 203.6203, 'eval_samples_per_second': 0.982, 'eval_steps_per_second': 0.246, 'epoch': 1.6}\n 40%|████████████████▉                         | 50/124 [48:29<49:17, 39.97s/it]\n100%|███████████████████████████████████████████| 50/50 [03:21<00:00,  4.06s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-12 10:49:29.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v2/checkpoint-50\u001b[0m\n\u001b[32m2023-08-12 10:49:29.487\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-12 10:49:29.489\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.1664, 'learning_rate': 1.2105263157894737e-05, 'epoch': 1.92}        \n 48%|████████████████████▎                     | 60/124 [54:47<42:46, 40.10s/it]\n  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n  4%|█▊                                          | 2/50 [00:04<01:37,  2.04s/it]\u001b[A\n  6%|██▋                                         | 3/50 [00:08<02:15,  2.88s/it]\u001b[A\n  8%|███▌                                        | 4/50 [00:12<02:33,  3.33s/it]\u001b[A\n 10%|████▍                                       | 5/50 [00:16<02:41,  3.58s/it]\u001b[A\n 12%|█████▎                                      | 6/50 [00:20<02:45,  3.75s/it]\u001b[A\n 14%|██████▏                                     | 7/50 [00:24<02:45,  3.86s/it]\u001b[A\n 16%|███████                                     | 8/50 [00:28<02:45,  3.93s/it]\u001b[A\n 18%|███████▉                                    | 9/50 [00:32<02:42,  3.97s/it]\u001b[A\n 20%|████████▌                                  | 10/50 [00:36<02:39,  4.00s/it]\u001b[A\n 22%|█████████▍                                 | 11/50 [00:40<02:37,  4.03s/it]\u001b[A\n 24%|██████████▎                                | 12/50 [00:44<02:33,  4.04s/it]\u001b[A\n 26%|███████████▏                               | 13/50 [00:48<02:29,  4.05s/it]\u001b[A\n 28%|████████████                               | 14/50 [00:53<02:26,  4.07s/it]\u001b[A\n 30%|████████████▉                              | 15/50 [00:57<02:22,  4.06s/it]\u001b[A\n 32%|█████████████▊                             | 16/50 [01:01<02:18,  4.07s/it]\u001b[A\n 34%|██████████████▌                            | 17/50 [01:05<02:14,  4.06s/it]\u001b[A\n 36%|███████████████▍                           | 18/50 [01:09<02:09,  4.06s/it]\u001b[A\n 38%|████████████████▎                          | 19/50 [01:13<02:06,  4.07s/it]\u001b[A\n 40%|█████████████████▏                         | 20/50 [01:17<02:02,  4.07s/it]\u001b[A\n 42%|██████████████████                         | 21/50 [01:21<01:58,  4.08s/it]\u001b[A\n 44%|██████████████████▉                        | 22/50 [01:25<01:54,  4.07s/it]\u001b[A\n 46%|███████████████████▊                       | 23/50 [01:29<01:49,  4.07s/it]\u001b[A\n 48%|████████████████████▋                      | 24/50 [01:33<01:45,  4.08s/it]\u001b[A\n 50%|█████████████████████▌                     | 25/50 [01:37<01:41,  4.08s/it]\u001b[A\n 52%|██████████████████████▎                    | 26/50 [01:41<01:37,  4.08s/it]\u001b[A\n 54%|███████████████████████▏                   | 27/50 [01:45<01:33,  4.07s/it]\u001b[A\n 56%|████████████████████████                   | 28/50 [01:50<01:29,  4.07s/it]\u001b[A\n 58%|████████████████████████▉                  | 29/50 [01:54<01:25,  4.06s/it]\u001b[A\n 60%|█████████████████████████▊                 | 30/50 [01:58<01:21,  4.08s/it]\u001b[A\n 62%|██████████████████████████▋                | 31/50 [02:02<01:17,  4.09s/it]\u001b[A\n 64%|███████████████████████████▌               | 32/50 [02:06<01:13,  4.09s/it]\u001b[A\n 66%|████████████████████████████▍              | 33/50 [02:10<01:09,  4.08s/it]\u001b[A\n 68%|█████████████████████████████▏             | 34/50 [02:14<01:05,  4.07s/it]\u001b[A\n 70%|██████████████████████████████             | 35/50 [02:18<01:01,  4.07s/it]\u001b[A\n 72%|██████████████████████████████▉            | 36/50 [02:22<00:57,  4.07s/it]\u001b[A\n 74%|███████████████████████████████▊           | 37/50 [02:26<00:52,  4.07s/it]\u001b[A\n 76%|████████████████████████████████▋          | 38/50 [02:30<00:48,  4.08s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 39/50 [02:34<00:44,  4.08s/it]\u001b[A\n 80%|██████████████████████████████████▍        | 40/50 [02:38<00:40,  4.07s/it]\u001b[A\n 82%|███████████████████████████████████▎       | 41/50 [02:43<00:36,  4.08s/it]\u001b[A\n 84%|████████████████████████████████████       | 42/50 [02:47<00:32,  4.09s/it]\u001b[A\n 86%|████████████████████████████████████▉      | 43/50 [02:51<00:28,  4.09s/it]\u001b[A\n 88%|█████████████████████████████████████▊     | 44/50 [02:55<00:24,  4.10s/it]\u001b[A\n 90%|██████████████████████████████████████▋    | 45/50 [02:59<00:20,  4.09s/it]\u001b[A\n 92%|███████████████████████████████████████▌   | 46/50 [03:03<00:16,  4.09s/it]\u001b[A\n 94%|████████████████████████████████████████▍  | 47/50 [03:07<00:12,  4.08s/it]\u001b[A\n 96%|█████████████████████████████████████████▎ | 48/50 [03:11<00:08,  4.10s/it]\u001b[A\n 98%|██████████████████████████████████████████▏| 49/50 [03:15<00:04,  4.09s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.05917114391922951, 'eval_accuracy': 0.95, 'eval_runtime': 203.9738, 'eval_samples_per_second': 0.981, 'eval_steps_per_second': 0.245, 'epoch': 1.92}\n 48%|████████████████████▎                     | 60/124 [58:11<42:46, 40.10s/it]\n100%|███████████████████████████████████████████| 50/50 [03:21<00:00,  4.09s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-12 10:59:11.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v2/checkpoint-60\u001b[0m\n\u001b[32m2023-08-12 10:59:11.066\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-12 10:59:11.067\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.1063, 'learning_rate': 1.0350877192982459e-05, 'epoch': 2.24}        \n 56%|██████████████████████▌                 | 70/124 [1:04:27<36:01, 40.02s/it]\n  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n  4%|█▊                                          | 2/50 [00:04<01:37,  2.04s/it]\u001b[A\n  6%|██▋                                         | 3/50 [00:08<02:15,  2.89s/it]\u001b[A\n  8%|███▌                                        | 4/50 [00:12<02:33,  3.34s/it]\u001b[A\n 10%|████▍                                       | 5/50 [00:16<02:41,  3.60s/it]\u001b[A\n 12%|█████▎                                      | 6/50 [00:20<02:44,  3.74s/it]\u001b[A\n 14%|██████▏                                     | 7/50 [00:24<02:46,  3.86s/it]\u001b[A\n 16%|███████                                     | 8/50 [00:28<02:45,  3.93s/it]\u001b[A\n 18%|███████▉                                    | 9/50 [00:32<02:43,  3.98s/it]\u001b[A\n 20%|████████▌                                  | 10/50 [00:36<02:40,  4.01s/it]\u001b[A\n 22%|█████████▍                                 | 11/50 [00:40<02:37,  4.03s/it]\u001b[A\n 24%|██████████▎                                | 12/50 [00:44<02:33,  4.04s/it]\u001b[A\n 26%|███████████▏                               | 13/50 [00:48<02:30,  4.05s/it]\u001b[A\n 28%|████████████                               | 14/50 [00:53<02:26,  4.07s/it]\u001b[A\n 30%|████████████▉                              | 15/50 [00:57<02:21,  4.06s/it]\u001b[A\n 32%|█████████████▊                             | 16/50 [01:01<02:18,  4.07s/it]\u001b[A\n 34%|██████████████▌                            | 17/50 [01:05<02:14,  4.06s/it]\u001b[A\n 36%|███████████████▍                           | 18/50 [01:09<02:10,  4.06s/it]\u001b[A\n 38%|████████████████▎                          | 19/50 [01:13<02:06,  4.07s/it]\u001b[A\n 40%|█████████████████▏                         | 20/50 [01:17<02:02,  4.07s/it]\u001b[A\n 42%|██████████████████                         | 21/50 [01:21<01:58,  4.07s/it]\u001b[A\n 44%|██████████████████▉                        | 22/50 [01:25<01:54,  4.07s/it]\u001b[A\n 46%|███████████████████▊                       | 23/50 [01:29<01:49,  4.07s/it]\u001b[A\n 48%|████████████████████▋                      | 24/50 [01:33<01:45,  4.07s/it]\u001b[A\n 50%|█████████████████████▌                     | 25/50 [01:37<01:41,  4.07s/it]\u001b[A\n 52%|██████████████████████▎                    | 26/50 [01:41<01:37,  4.06s/it]\u001b[A\n 54%|███████████████████████▏                   | 27/50 [01:45<01:33,  4.07s/it]\u001b[A\n 56%|████████████████████████                   | 28/50 [01:49<01:29,  4.06s/it]\u001b[A\n 58%|████████████████████████▉                  | 29/50 [01:54<01:25,  4.05s/it]\u001b[A\n 60%|█████████████████████████▊                 | 30/50 [01:58<01:21,  4.06s/it]\u001b[A\n 62%|██████████████████████████▋                | 31/50 [02:02<01:17,  4.07s/it]\u001b[A\n 64%|███████████████████████████▌               | 32/50 [02:06<01:13,  4.08s/it]\u001b[A\n 66%|████████████████████████████▍              | 33/50 [02:10<01:09,  4.07s/it]\u001b[A\n 68%|█████████████████████████████▏             | 34/50 [02:14<01:05,  4.06s/it]\u001b[A\n 70%|██████████████████████████████             | 35/50 [02:18<01:01,  4.07s/it]\u001b[A\n 72%|██████████████████████████████▉            | 36/50 [02:22<00:56,  4.06s/it]\u001b[A\n 74%|███████████████████████████████▊           | 37/50 [02:26<00:52,  4.06s/it]\u001b[A\n 76%|████████████████████████████████▋          | 38/50 [02:30<00:48,  4.08s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 39/50 [02:34<00:44,  4.07s/it]\u001b[A\n 80%|██████████████████████████████████▍        | 40/50 [02:38<00:40,  4.06s/it]\u001b[A\n 82%|███████████████████████████████████▎       | 41/50 [02:42<00:36,  4.07s/it]\u001b[A\n 84%|████████████████████████████████████       | 42/50 [02:46<00:32,  4.08s/it]\u001b[A\n 86%|████████████████████████████████████▉      | 43/50 [02:50<00:28,  4.06s/it]\u001b[A\n 88%|█████████████████████████████████████▊     | 44/50 [02:55<00:24,  4.06s/it]\u001b[A\n 90%|██████████████████████████████████████▋    | 45/50 [02:59<00:20,  4.06s/it]\u001b[A\n 92%|███████████████████████████████████████▌   | 46/50 [03:03<00:16,  4.06s/it]\u001b[A\n 94%|████████████████████████████████████████▍  | 47/50 [03:07<00:12,  4.06s/it]\u001b[A\n 96%|█████████████████████████████████████████▎ | 48/50 [03:11<00:08,  4.07s/it]\u001b[A\n 98%|██████████████████████████████████████████▏| 49/50 [03:15<00:04,  4.07s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.04972020164132118, 'eval_accuracy': 0.95, 'eval_runtime': 203.518, 'eval_samples_per_second': 0.983, 'eval_steps_per_second': 0.246, 'epoch': 2.24}\n 56%|██████████████████████▌                 | 70/124 [1:07:51<36:01, 40.02s/it]\n100%|███████████████████████████████████████████| 50/50 [03:21<00:00,  4.06s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-12 11:08:51.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v2/checkpoint-70\u001b[0m\n\u001b[32m2023-08-12 11:08:51.168\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-12 11:08:51.169\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.0784, 'learning_rate': 8.596491228070176e-06, 'epoch': 2.56}         \n 65%|█████████████████████████▊              | 80/124 [1:14:09<29:21, 40.04s/it]\n  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n  4%|█▊                                          | 2/50 [00:04<01:37,  2.03s/it]\u001b[A\n  6%|██▋                                         | 3/50 [00:08<02:15,  2.87s/it]\u001b[A\n  8%|███▌                                        | 4/50 [00:12<02:32,  3.32s/it]\u001b[A\n 10%|████▍                                       | 5/50 [00:16<02:41,  3.59s/it]\u001b[A\n 12%|█████▎                                      | 6/50 [00:20<02:44,  3.74s/it]\u001b[A\n 14%|██████▏                                     | 7/50 [00:24<02:45,  3.85s/it]\u001b[A\n 16%|███████                                     | 8/50 [00:28<02:45,  3.93s/it]\u001b[A\n 18%|███████▉                                    | 9/50 [00:32<02:42,  3.97s/it]\u001b[A\n 20%|████████▌                                  | 10/50 [00:36<02:40,  4.00s/it]\u001b[A\n 22%|█████████▍                                 | 11/50 [00:40<02:36,  4.03s/it]\u001b[A\n 24%|██████████▎                                | 12/50 [00:44<02:33,  4.03s/it]\u001b[A\n 26%|███████████▏                               | 13/50 [00:48<02:29,  4.05s/it]\u001b[A\n 28%|████████████                               | 14/50 [00:52<02:26,  4.07s/it]\u001b[A\n 30%|████████████▉                              | 15/50 [00:56<02:21,  4.06s/it]\u001b[A\n 32%|█████████████▊                             | 16/50 [01:01<02:18,  4.06s/it]\u001b[A\n 34%|██████████████▌                            | 17/50 [01:05<02:13,  4.06s/it]\u001b[A\n 36%|███████████████▍                           | 18/50 [01:09<02:10,  4.07s/it]\u001b[A\n 38%|████████████████▎                          | 19/50 [01:13<02:06,  4.07s/it]\u001b[A\n 40%|█████████████████▏                         | 20/50 [01:17<02:02,  4.08s/it]\u001b[A\n 42%|██████████████████                         | 21/50 [01:21<01:58,  4.09s/it]\u001b[A\n 44%|██████████████████▉                        | 22/50 [01:25<01:54,  4.08s/it]\u001b[A\n 46%|███████████████████▊                       | 23/50 [01:29<01:49,  4.07s/it]\u001b[A\n 48%|████████████████████▋                      | 24/50 [01:33<01:45,  4.06s/it]\u001b[A\n 50%|█████████████████████▌                     | 25/50 [01:37<01:41,  4.07s/it]\u001b[A\n 52%|██████████████████████▎                    | 26/50 [01:41<01:37,  4.08s/it]\u001b[A\n 54%|███████████████████████▏                   | 27/50 [01:45<01:33,  4.08s/it]\u001b[A\n 56%|████████████████████████                   | 28/50 [01:49<01:29,  4.07s/it]\u001b[A\n 58%|████████████████████████▉                  | 29/50 [01:53<01:25,  4.06s/it]\u001b[A\n 60%|█████████████████████████▊                 | 30/50 [01:58<01:21,  4.07s/it]\u001b[A\n 62%|██████████████████████████▋                | 31/50 [02:02<01:17,  4.09s/it]\u001b[A\n 64%|███████████████████████████▌               | 32/50 [02:06<01:13,  4.09s/it]\u001b[A\n 66%|████████████████████████████▍              | 33/50 [02:10<01:09,  4.08s/it]\u001b[A\n 68%|█████████████████████████████▏             | 34/50 [02:14<01:05,  4.07s/it]\u001b[A\n 70%|██████████████████████████████             | 35/50 [02:18<01:01,  4.07s/it]\u001b[A\n 72%|██████████████████████████████▉            | 36/50 [02:22<00:56,  4.07s/it]\u001b[A\n 74%|███████████████████████████████▊           | 37/50 [02:26<00:52,  4.06s/it]\u001b[A\n 76%|████████████████████████████████▋          | 38/50 [02:30<00:48,  4.08s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 39/50 [02:34<00:44,  4.08s/it]\u001b[A\n 80%|██████████████████████████████████▍        | 40/50 [02:38<00:40,  4.08s/it]\u001b[A\n 82%|███████████████████████████████████▎       | 41/50 [02:42<00:36,  4.08s/it]\u001b[A\n 84%|████████████████████████████████████       | 42/50 [02:47<00:32,  4.09s/it]\u001b[A\n 86%|████████████████████████████████████▉      | 43/50 [02:51<00:28,  4.07s/it]\u001b[A\n 88%|█████████████████████████████████████▊     | 44/50 [02:55<00:24,  4.08s/it]\u001b[A\n 90%|██████████████████████████████████████▋    | 45/50 [02:59<00:20,  4.07s/it]\u001b[A\n 92%|███████████████████████████████████████▌   | 46/50 [03:03<00:16,  4.07s/it]\u001b[A\n 94%|████████████████████████████████████████▍  | 47/50 [03:07<00:12,  4.07s/it]\u001b[A\n 96%|█████████████████████████████████████████▎ | 48/50 [03:11<00:08,  4.09s/it]\u001b[A\n 98%|██████████████████████████████████████████▏| 49/50 [03:15<00:04,  4.07s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.044467125087976456, 'eval_accuracy': 0.95, 'eval_runtime': 203.7437, 'eval_samples_per_second': 0.982, 'eval_steps_per_second': 0.245, 'epoch': 2.56}\n 65%|█████████████████████████▊              | 80/124 [1:17:32<29:21, 40.04s/it]\n100%|███████████████████████████████████████████| 50/50 [03:21<00:00,  4.07s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-12 11:18:32.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v2/checkpoint-80\u001b[0m\n\u001b[32m2023-08-12 11:18:32.533\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-12 11:18:32.534\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.055, 'learning_rate': 6.842105263157896e-06, 'epoch': 2.88}          \n 73%|█████████████████████████████           | 90/124 [1:23:49<22:38, 39.97s/it]\n  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n  4%|█▊                                          | 2/50 [00:04<01:37,  2.03s/it]\u001b[A\n  6%|██▋                                         | 3/50 [00:08<02:15,  2.89s/it]\u001b[A\n  8%|███▌                                        | 4/50 [00:12<02:33,  3.34s/it]\u001b[A\n 10%|████▍                                       | 5/50 [00:16<02:41,  3.59s/it]\u001b[A\n 12%|█████▎                                      | 6/50 [00:20<02:45,  3.75s/it]\u001b[A\n 14%|██████▏                                     | 7/50 [00:24<02:46,  3.86s/it]\u001b[A\n 16%|███████                                     | 8/50 [00:28<02:45,  3.94s/it]\u001b[A\n 18%|███████▉                                    | 9/50 [00:32<02:43,  3.98s/it]\u001b[A\n 20%|████████▌                                  | 10/50 [00:36<02:40,  4.02s/it]\u001b[A\n 22%|█████████▍                                 | 11/50 [00:40<02:37,  4.04s/it]\u001b[A\n 24%|██████████▎                                | 12/50 [00:44<02:33,  4.05s/it]\u001b[A\n 26%|███████████▏                               | 13/50 [00:49<02:30,  4.07s/it]\u001b[A\n 28%|████████████                               | 14/50 [00:53<02:27,  4.09s/it]\u001b[A\n 30%|████████████▉                              | 15/50 [00:57<02:22,  4.07s/it]\u001b[A\n 32%|█████████████▊                             | 16/50 [01:01<02:19,  4.09s/it]\u001b[A\n 34%|██████████████▌                            | 17/50 [01:05<02:14,  4.07s/it]\u001b[A\n 36%|███████████████▍                           | 18/50 [01:09<02:10,  4.08s/it]\u001b[A\n 38%|████████████████▎                          | 19/50 [01:13<02:06,  4.09s/it]\u001b[A\n 40%|█████████████████▏                         | 20/50 [01:17<02:02,  4.08s/it]\u001b[A\n 42%|██████████████████                         | 21/50 [01:21<01:58,  4.09s/it]\u001b[A\n 44%|██████████████████▉                        | 22/50 [01:25<01:54,  4.09s/it]\u001b[A\n 46%|███████████████████▊                       | 23/50 [01:29<01:50,  4.08s/it]\u001b[A\n 48%|████████████████████▋                      | 24/50 [01:33<01:46,  4.08s/it]\u001b[A\n 50%|█████████████████████▌                     | 25/50 [01:38<01:42,  4.08s/it]\u001b[A\n 52%|██████████████████████▎                    | 26/50 [01:42<01:37,  4.08s/it]\u001b[A\n 54%|███████████████████████▏                   | 27/50 [01:46<01:33,  4.08s/it]\u001b[A\n 56%|████████████████████████                   | 28/50 [01:50<01:29,  4.08s/it]\u001b[A\n 58%|████████████████████████▉                  | 29/50 [01:54<01:25,  4.07s/it]\u001b[A\n 60%|█████████████████████████▊                 | 30/50 [01:58<01:21,  4.08s/it]\u001b[A\n 62%|██████████████████████████▋                | 31/50 [02:02<01:17,  4.09s/it]\u001b[A\n 64%|███████████████████████████▌               | 32/50 [02:06<01:13,  4.09s/it]\u001b[A\n 66%|████████████████████████████▍              | 33/50 [02:10<01:09,  4.08s/it]\u001b[A\n 68%|█████████████████████████████▏             | 34/50 [02:14<01:05,  4.08s/it]\u001b[A\n 70%|██████████████████████████████             | 35/50 [02:18<01:01,  4.08s/it]\u001b[A\n 72%|██████████████████████████████▉            | 36/50 [02:22<00:57,  4.07s/it]\u001b[A\n 74%|███████████████████████████████▊           | 37/50 [02:26<00:52,  4.07s/it]\u001b[A\n 76%|████████████████████████████████▋          | 38/50 [02:31<00:49,  4.09s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 39/50 [02:35<00:44,  4.08s/it]\u001b[A\n 80%|██████████████████████████████████▍        | 40/50 [02:39<00:40,  4.09s/it]\u001b[A\n 82%|███████████████████████████████████▎       | 41/50 [02:43<00:36,  4.09s/it]\u001b[A\n 84%|████████████████████████████████████       | 42/50 [02:47<00:32,  4.10s/it]\u001b[A\n 86%|████████████████████████████████████▉      | 43/50 [02:51<00:28,  4.09s/it]\u001b[A\n 88%|█████████████████████████████████████▊     | 44/50 [02:55<00:24,  4.11s/it]\u001b[A\n 90%|██████████████████████████████████████▋    | 45/50 [02:59<00:20,  4.09s/it]\u001b[A\n 92%|███████████████████████████████████████▌   | 46/50 [03:03<00:16,  4.10s/it]\u001b[A\n 94%|████████████████████████████████████████▍  | 47/50 [03:07<00:12,  4.09s/it]\u001b[A\n 96%|█████████████████████████████████████████▎ | 48/50 [03:12<00:08,  4.10s/it]\u001b[A\n 98%|██████████████████████████████████████████▏| 49/50 [03:16<00:04,  4.09s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.04062463715672493, 'eval_accuracy': 0.95, 'eval_runtime': 204.3324, 'eval_samples_per_second': 0.979, 'eval_steps_per_second': 0.245, 'epoch': 2.88}\n 73%|█████████████████████████████           | 90/124 [1:27:13<22:38, 39.97s/it]\n100%|███████████████████████████████████████████| 50/50 [03:22<00:00,  4.08s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-12 11:28:13.476\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v2/checkpoint-90\u001b[0m\n\u001b[32m2023-08-12 11:28:13.477\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-12 11:28:13.478\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.0782, 'learning_rate': 5.087719298245615e-06, 'epoch': 3.2}          \n 81%|███████████████████████████████▍       | 100/124 [1:33:30<15:59, 39.98s/it]\n  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n  4%|█▊                                          | 2/50 [00:04<01:37,  2.04s/it]\u001b[A\n  6%|██▋                                         | 3/50 [00:08<02:16,  2.90s/it]\u001b[A\n  8%|███▌                                        | 4/50 [00:12<02:33,  3.34s/it]\u001b[A\n 10%|████▍                                       | 5/50 [00:16<02:42,  3.61s/it]\u001b[A\n 12%|█████▎                                      | 6/50 [00:20<02:45,  3.77s/it]\u001b[A\n 14%|██████▏                                     | 7/50 [00:24<02:46,  3.88s/it]\u001b[A\n 16%|███████                                     | 8/50 [00:28<02:46,  3.95s/it]\u001b[A\n 18%|███████▉                                    | 9/50 [00:32<02:43,  3.99s/it]\u001b[A\n 20%|████████▌                                  | 10/50 [00:36<02:40,  4.01s/it]\u001b[A\n 22%|█████████▍                                 | 11/50 [00:40<02:37,  4.04s/it]\u001b[A\n 24%|██████████▎                                | 12/50 [00:44<02:33,  4.04s/it]\u001b[A\n 26%|███████████▏                               | 13/50 [00:49<02:30,  4.06s/it]\u001b[A\n 28%|████████████                               | 14/50 [00:53<02:27,  4.08s/it]\u001b[A\n 30%|████████████▉                              | 15/50 [00:57<02:22,  4.07s/it]\u001b[A\n 32%|█████████████▊                             | 16/50 [01:01<02:19,  4.09s/it]\u001b[A\n 34%|██████████████▌                            | 17/50 [01:05<02:14,  4.07s/it]\u001b[A\n 36%|███████████████▍                           | 18/50 [01:09<02:10,  4.07s/it]\u001b[A\n 38%|████████████████▎                          | 19/50 [01:13<02:06,  4.08s/it]\u001b[A\n 40%|█████████████████▏                         | 20/50 [01:17<02:02,  4.08s/it]\u001b[A\n 42%|██████████████████                         | 21/50 [01:21<01:58,  4.08s/it]\u001b[A\n 44%|██████████████████▉                        | 22/50 [01:25<01:54,  4.07s/it]\u001b[A\n 46%|███████████████████▊                       | 23/50 [01:29<01:49,  4.07s/it]\u001b[A\n 48%|████████████████████▋                      | 24/50 [01:33<01:45,  4.06s/it]\u001b[A\n 50%|█████████████████████▌                     | 25/50 [01:37<01:41,  4.07s/it]\u001b[A\n 52%|██████████████████████▎                    | 26/50 [01:42<01:37,  4.06s/it]\u001b[A\n 54%|███████████████████████▏                   | 27/50 [01:46<01:33,  4.07s/it]\u001b[A\n 56%|████████████████████████                   | 28/50 [01:50<01:29,  4.06s/it]\u001b[A\n 58%|████████████████████████▉                  | 29/50 [01:54<01:25,  4.05s/it]\u001b[A\n 60%|█████████████████████████▊                 | 30/50 [01:58<01:21,  4.05s/it]\u001b[A\n 62%|██████████████████████████▋                | 31/50 [02:02<01:17,  4.07s/it]\u001b[A\n 64%|███████████████████████████▌               | 32/50 [02:06<01:13,  4.07s/it]\u001b[A\n 66%|████████████████████████████▍              | 33/50 [02:10<01:09,  4.06s/it]\u001b[A\n 68%|█████████████████████████████▏             | 34/50 [02:14<01:05,  4.06s/it]\u001b[A\n 70%|██████████████████████████████             | 35/50 [02:18<01:00,  4.05s/it]\u001b[A\n 72%|██████████████████████████████▉            | 36/50 [02:22<00:56,  4.05s/it]\u001b[A\n 74%|███████████████████████████████▊           | 37/50 [02:26<00:52,  4.04s/it]\u001b[A\n 76%|████████████████████████████████▋          | 38/50 [02:30<00:48,  4.07s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 39/50 [02:34<00:44,  4.06s/it]\u001b[A\n 80%|██████████████████████████████████▍        | 40/50 [02:38<00:40,  4.07s/it]\u001b[A\n 82%|███████████████████████████████████▎       | 41/50 [02:42<00:36,  4.06s/it]\u001b[A\n 84%|████████████████████████████████████       | 42/50 [02:47<00:32,  4.07s/it]\u001b[A\n 86%|████████████████████████████████████▉      | 43/50 [02:51<00:28,  4.07s/it]\u001b[A\n 88%|█████████████████████████████████████▊     | 44/50 [02:55<00:24,  4.07s/it]\u001b[A\n 90%|██████████████████████████████████████▋    | 45/50 [02:59<00:20,  4.06s/it]\u001b[A\n 92%|███████████████████████████████████████▌   | 46/50 [03:03<00:16,  4.07s/it]\u001b[A\n 94%|████████████████████████████████████████▍  | 47/50 [03:07<00:12,  4.06s/it]\u001b[A\n 96%|█████████████████████████████████████████▎ | 48/50 [03:11<00:08,  4.07s/it]\u001b[A\n 98%|██████████████████████████████████████████▏| 49/50 [03:15<00:04,  4.06s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.038880106061697006, 'eval_accuracy': 0.95, 'eval_runtime': 203.6353, 'eval_samples_per_second': 0.982, 'eval_steps_per_second': 0.246, 'epoch': 3.2}\n 81%|███████████████████████████████▍       | 100/124 [1:36:53<15:59, 39.98s/it]\n100%|███████████████████████████████████████████| 50/50 [03:21<00:00,  4.06s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-12 11:37:53.441\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v2/checkpoint-100\u001b[0m\n\u001b[32m2023-08-12 11:37:53.441\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-12 11:37:53.443\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.04, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.52}          \n 89%|██████████████████████████████████▌    | 110/124 [1:43:10<09:20, 40.00s/it]\n  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n  4%|█▊                                          | 2/50 [00:04<01:36,  2.02s/it]\u001b[A\n  6%|██▋                                         | 3/50 [00:08<02:15,  2.89s/it]\u001b[A\n  8%|███▌                                        | 4/50 [00:12<02:33,  3.33s/it]\u001b[A\n 10%|████▍                                       | 5/50 [00:16<02:41,  3.60s/it]\u001b[A\n 12%|█████▎                                      | 6/50 [00:20<02:45,  3.75s/it]\u001b[A\n 14%|██████▏                                     | 7/50 [00:24<02:46,  3.86s/it]\u001b[A\n 16%|███████                                     | 8/50 [00:28<02:45,  3.94s/it]\u001b[A\n 18%|███████▉                                    | 9/50 [00:32<02:42,  3.97s/it]\u001b[A\n 20%|████████▌                                  | 10/50 [00:36<02:39,  4.00s/it]\u001b[A\n 22%|█████████▍                                 | 11/50 [00:40<02:37,  4.03s/it]\u001b[A\n 24%|██████████▎                                | 12/50 [00:44<02:33,  4.04s/it]\u001b[A\n 26%|███████████▏                               | 13/50 [00:48<02:29,  4.05s/it]\u001b[A\n 28%|████████████                               | 14/50 [00:53<02:26,  4.07s/it]\u001b[A\n 30%|████████████▉                              | 15/50 [00:57<02:22,  4.07s/it]\u001b[A\n 32%|█████████████▊                             | 16/50 [01:01<02:18,  4.07s/it]\u001b[A\n 34%|██████████████▌                            | 17/50 [01:05<02:14,  4.07s/it]\u001b[A\n 36%|███████████████▍                           | 18/50 [01:09<02:10,  4.07s/it]\u001b[A\n 38%|████████████████▎                          | 19/50 [01:13<02:06,  4.08s/it]\u001b[A\n 40%|█████████████████▏                         | 20/50 [01:17<02:02,  4.08s/it]\u001b[A\n 42%|██████████████████                         | 21/50 [01:21<01:58,  4.08s/it]\u001b[A\n 44%|██████████████████▉                        | 22/50 [01:25<01:54,  4.08s/it]\u001b[A\n 46%|███████████████████▊                       | 23/50 [01:29<01:49,  4.07s/it]\u001b[A\n 48%|████████████████████▋                      | 24/50 [01:33<01:45,  4.07s/it]\u001b[A\n 50%|█████████████████████▌                     | 25/50 [01:37<01:41,  4.07s/it]\u001b[A\n 52%|██████████████████████▎                    | 26/50 [01:41<01:37,  4.08s/it]\u001b[A\n 54%|███████████████████████▏                   | 27/50 [01:46<01:34,  4.09s/it]\u001b[A\n 56%|████████████████████████                   | 28/50 [01:50<01:29,  4.07s/it]\u001b[A\n 58%|████████████████████████▉                  | 29/50 [01:54<01:25,  4.07s/it]\u001b[A\n 60%|█████████████████████████▊                 | 30/50 [01:58<01:21,  4.08s/it]\u001b[A\n 62%|██████████████████████████▋                | 31/50 [02:02<01:17,  4.09s/it]\u001b[A\n 64%|███████████████████████████▌               | 32/50 [02:06<01:13,  4.09s/it]\u001b[A\n 66%|████████████████████████████▍              | 33/50 [02:10<01:09,  4.09s/it]\u001b[A\n 68%|█████████████████████████████▏             | 34/50 [02:14<01:05,  4.08s/it]\u001b[A\n 70%|██████████████████████████████             | 35/50 [02:18<01:01,  4.08s/it]\u001b[A\n 72%|██████████████████████████████▉            | 36/50 [02:22<00:57,  4.07s/it]\u001b[A\n 74%|███████████████████████████████▊           | 37/50 [02:26<00:52,  4.07s/it]\u001b[A\n 76%|████████████████████████████████▋          | 38/50 [02:30<00:49,  4.10s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 39/50 [02:35<00:45,  4.09s/it]\u001b[A\n 80%|██████████████████████████████████▍        | 40/50 [02:39<00:40,  4.09s/it]\u001b[A\n 82%|███████████████████████████████████▎       | 41/50 [02:43<00:36,  4.09s/it]\u001b[A\n 84%|████████████████████████████████████       | 42/50 [02:47<00:32,  4.10s/it]\u001b[A\n 86%|████████████████████████████████████▉      | 43/50 [02:51<00:28,  4.08s/it]\u001b[A\n 88%|█████████████████████████████████████▊     | 44/50 [02:55<00:24,  4.09s/it]\u001b[A\n 90%|██████████████████████████████████████▋    | 45/50 [02:59<00:20,  4.08s/it]\u001b[A\n 92%|███████████████████████████████████████▌   | 46/50 [03:03<00:16,  4.09s/it]\u001b[A\n 94%|████████████████████████████████████████▍  | 47/50 [03:07<00:12,  4.09s/it]\u001b[A\n 96%|█████████████████████████████████████████▎ | 48/50 [03:11<00:08,  4.10s/it]\u001b[A\n 98%|██████████████████████████████████████████▏| 49/50 [03:15<00:04,  4.09s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.03803635761141777, 'eval_accuracy': 0.95, 'eval_runtime': 204.1509, 'eval_samples_per_second': 0.98, 'eval_steps_per_second': 0.245, 'epoch': 3.52}\n 89%|██████████████████████████████████▌    | 110/124 [1:46:34<09:20, 40.00s/it]\n100%|███████████████████████████████████████████| 50/50 [03:21<00:00,  4.09s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-12 11:47:34.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v2/checkpoint-110\u001b[0m\n\u001b[32m2023-08-12 11:47:34.636\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-12 11:47:34.637\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.0561, 'learning_rate': 1.5789473684210526e-06, 'epoch': 3.84}        \n 97%|█████████████████████████████████████▋ | 120/124 [1:52:51<02:40, 40.01s/it]\n  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n  4%|█▊                                          | 2/50 [00:04<01:37,  2.03s/it]\u001b[A\n  6%|██▋                                         | 3/50 [00:08<02:15,  2.89s/it]\u001b[A\n  8%|███▌                                        | 4/50 [00:12<02:33,  3.34s/it]\u001b[A\n 10%|████▍                                       | 5/50 [00:16<02:42,  3.60s/it]\u001b[A\n 12%|█████▎                                      | 6/50 [00:20<02:45,  3.77s/it]\u001b[A\n 14%|██████▏                                     | 7/50 [00:24<02:46,  3.87s/it]\u001b[A\n 16%|███████                                     | 8/50 [00:28<02:45,  3.94s/it]\u001b[A\n 18%|███████▉                                    | 9/50 [00:32<02:43,  3.98s/it]\u001b[A\n 20%|████████▌                                  | 10/50 [00:36<02:40,  4.01s/it]\u001b[A\n 22%|█████████▍                                 | 11/50 [00:40<02:37,  4.03s/it]\u001b[A\n 24%|██████████▎                                | 12/50 [00:44<02:33,  4.04s/it]\u001b[A\n 26%|███████████▏                               | 13/50 [00:49<02:30,  4.06s/it]\u001b[A\n 28%|████████████                               | 14/50 [00:53<02:26,  4.08s/it]\u001b[A\n 30%|████████████▉                              | 15/50 [00:57<02:22,  4.07s/it]\u001b[A\n 32%|█████████████▊                             | 16/50 [01:01<02:18,  4.07s/it]\u001b[A\n 34%|██████████████▌                            | 17/50 [01:05<02:14,  4.07s/it]\u001b[A\n 36%|███████████████▍                           | 18/50 [01:09<02:10,  4.07s/it]\u001b[A\n 38%|████████████████▎                          | 19/50 [01:13<02:06,  4.08s/it]\u001b[A\n 40%|█████████████████▏                         | 20/50 [01:17<02:02,  4.08s/it]\u001b[A\n 42%|██████████████████                         | 21/50 [01:21<01:58,  4.08s/it]\u001b[A\n 44%|██████████████████▉                        | 22/50 [01:25<01:54,  4.08s/it]\u001b[A\n 46%|███████████████████▊                       | 23/50 [01:29<01:49,  4.07s/it]\u001b[A\n 48%|████████████████████▋                      | 24/50 [01:33<01:45,  4.07s/it]\u001b[A\n 50%|█████████████████████▌                     | 25/50 [01:37<01:41,  4.06s/it]\u001b[A\n 52%|██████████████████████▎                    | 26/50 [01:42<01:37,  4.07s/it]\u001b[A\n 54%|███████████████████████▏                   | 27/50 [01:46<01:33,  4.07s/it]\u001b[A\n 56%|████████████████████████                   | 28/50 [01:50<01:29,  4.07s/it]\u001b[A\n 58%|████████████████████████▉                  | 29/50 [01:54<01:25,  4.06s/it]\u001b[A\n 60%|█████████████████████████▊                 | 30/50 [01:58<01:21,  4.07s/it]\u001b[A\n 62%|██████████████████████████▋                | 31/50 [02:02<01:17,  4.07s/it]\u001b[A\n 64%|███████████████████████████▌               | 32/50 [02:06<01:13,  4.07s/it]\u001b[A\n 66%|████████████████████████████▍              | 33/50 [02:10<01:09,  4.07s/it]\u001b[A\n 68%|█████████████████████████████▏             | 34/50 [02:14<01:05,  4.07s/it]\u001b[A\n 70%|██████████████████████████████             | 35/50 [02:18<01:00,  4.06s/it]\u001b[A\n 72%|██████████████████████████████▉            | 36/50 [02:22<00:56,  4.06s/it]\u001b[A\n 74%|███████████████████████████████▊           | 37/50 [02:26<00:52,  4.05s/it]\u001b[A\n 76%|████████████████████████████████▋          | 38/50 [02:30<00:48,  4.07s/it]\u001b[A\n 78%|█████████████████████████████████▌         | 39/50 [02:34<00:44,  4.06s/it]\u001b[A\n 80%|██████████████████████████████████▍        | 40/50 [02:38<00:40,  4.06s/it]\u001b[A\n 82%|███████████████████████████████████▎       | 41/50 [02:42<00:36,  4.06s/it]\u001b[A\n 84%|████████████████████████████████████       | 42/50 [02:47<00:32,  4.07s/it]\u001b[A\n 86%|████████████████████████████████████▉      | 43/50 [02:51<00:28,  4.06s/it]\u001b[A\n 88%|█████████████████████████████████████▊     | 44/50 [02:55<00:24,  4.06s/it]\u001b[A\n 90%|██████████████████████████████████████▋    | 45/50 [02:59<00:20,  4.06s/it]\u001b[A\n 92%|███████████████████████████████████████▌   | 46/50 [03:03<00:16,  4.06s/it]\u001b[A\n 94%|████████████████████████████████████████▍  | 47/50 [03:07<00:12,  4.06s/it]\u001b[A\n 96%|█████████████████████████████████████████▎ | 48/50 [03:11<00:08,  4.07s/it]\u001b[A\n 98%|██████████████████████████████████████████▏| 49/50 [03:15<00:04,  4.06s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.03643597662448883, 'eval_accuracy': 0.95, 'eval_runtime': 203.5972, 'eval_samples_per_second': 0.982, 'eval_steps_per_second': 0.246, 'epoch': 3.84}\n 97%|█████████████████████████████████████▋ | 120/124 [1:56:15<02:40, 40.01s/it]\n100%|███████████████████████████████████████████| 50/50 [03:21<00:00,  4.06s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-12 11:57:15.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v2/checkpoint-120\u001b[0m\n\u001b[32m2023-08-12 11:57:15.211\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-12 11:57:15.213\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n100%|███████████████████████████████████████| 124/124 [1:58:48<00:00, 59.28s/it]\u001b[32m2023-08-12 11:59:47.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_load_best_model\u001b[0m:\u001b[36m880\u001b[0m - \u001b[1mLoading best model from output-rm-1k-0812-v2/checkpoint-120 (score[here is best eval_loss]: 0.03643597662448883).\u001b[0m\n\u001b[32m2023-08-12 11:59:47.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_load_best_model\u001b[0m:\u001b[36m880\u001b[0m - \u001b[1mLoading best model from output-rm-1k-0812-v2/checkpoint-120 (score[here is best eval_loss]: 0.03643597662448883).\u001b[0m\n{'train_runtime': 7129.234, 'train_samples_per_second': 0.281, 'train_steps_per_second': 0.017, 'train_loss': 0.1453736132431415, 'epoch': 3.97}\n100%|███████████████████████████████████████| 124/124 [1:58:48<00:00, 57.49s/it]\n\u001b[32m2023-08-12 11:59:48.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v2\u001b[0m\n\u001b[32m2023-08-12 11:59:48.350\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-12 11:59:48.351\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n[2023-08-12 11:59:59,401] [INFO] [launch.py:347:main] Process 395 exits successfully.\n[2023-08-12 11:59:59,401] [INFO] [launch.py:347:main] Process 394 exits successfully.\n","output_type":"stream"}]}]}