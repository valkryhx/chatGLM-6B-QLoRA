{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-22T05:58:12.091394Z","iopub.execute_input":"2023-09-22T05:58:12.092219Z","iopub.status.idle":"2023-09-22T05:58:12.486100Z","shell.execute_reply.started":"2023-09-22T05:58:12.092178Z","shell.execute_reply":"2023-09-22T05:58:12.485007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone https://github.com/valkryhx/chatGLM-6B-QLoRA","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:29:10.284063Z","iopub.execute_input":"2023-09-22T07:29:10.284443Z","iopub.status.idle":"2023-09-22T07:29:11.251362Z","shell.execute_reply.started":"2023-09-22T07:29:10.284411Z","shell.execute_reply":"2023-09-22T07:29:11.250240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd chatGLM-6B-QLoRA/\n!git pull --all --force \n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:29:13.997115Z","iopub.execute_input":"2023-09-22T07:29:13.997550Z","iopub.status.idle":"2023-09-22T07:30:07.191268Z","shell.execute_reply.started":"2023-09-22T07:29:13.997505Z","shell.execute_reply":"2023-09-22T07:30:07.189940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 测试  新的 train_lora.py","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!deepspeed --include localhost:1  train_lora.py \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b  \\\n  --output_dir output0908_v2 \\\n  --resume_from_checkpoint /kaggle/working/chatGLM-6B-QLoRA/output0908/checkpoint-200 \\\n  --num_eval_samples 1 \\\n  --train_data_path data/alpaca_yunguan_0814 \\\n  --eval_data_path  data/alpaca_yunguan_0814 \\\n  --max_input_length 256 \\\n  --max_output_length 256 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 4 \\\n  --learning_rate  1e-6 \\\n  --num_train_epochs  70  \\\n  --save_total_limit 2 \\\n  --use_qlora True \\\n--deepspeed ds_zero2_config.json","metadata":{"execution":{"iopub.status.busy":"2023-09-08T13:28:25.268781Z","iopub.execute_input":"2023-09-08T13:28:25.269247Z","iopub.status.idle":"2023-09-08T13:50:47.810929Z","shell.execute_reply.started":"2023-09-08T13:28:25.269209Z","shell.execute_reply":"2023-09-08T13:50:47.809449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 测试  新的 sft_lora_multi_turn.py","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!deepspeed --include localhost:0,1  sft_lora_multi_turn.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-sharegpt-2k-sft-0908 \\\n  --num_train_samples -1 \\\n  --num_eval_samples 2 \\\n  --train_data_path ./data/sharegpt_data  \\\n  --eval_data_path  ./data/sharegpt_data    \\\n  --data_type sharegpt  \\\n  --max_length 1000 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 2  \\\n  --gradient_accumulation_steps 2 \\\n  --learning_rate  1.8e-5 \\\n  --num_train_epochs  40  \\\n  --save_total_limit 2 \\\n  --use_qlora True \\\n--deepspeed ds_zero2_config.json","metadata":{"execution":{"iopub.status.busy":"2023-09-08T14:14:39.906518Z","iopub.execute_input":"2023-09-08T14:14:39.906967Z","iopub.status.idle":"2023-09-08T14:51:19.100275Z","shell.execute_reply.started":"2023-09-08T14:14:39.906930Z","shell.execute_reply":"2023-09-08T14:51:19.098423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 构造和测试langchain prompt 数据 已经将构造的数据上传到github项目","metadata":{}},{"cell_type":"code","source":"import json \nimport os\n\ntemplate = \"\"\"现提供如下信息：\nhistory={history}\ncontext={context}\n请使用上述信息回答：{question}\n\"\"\"\n\nparent_dir = \"/kaggle/working/chatGLM-6B-QLoRA/data/alpaca_yunguan_0814\"\npaths = ['yunguan_aug_part_3.json',\n         'yunguan_aug_part_2.json',\n        \"yunguan_aug_part_1.json\",\n        \"refuse_phone.json\"]\nlist_all =[]\nfor name in paths :\n    with open(os.path.join(parent_dir , name) ,\"r\",encoding=\"utf-8\") as fr:\n        list_all.extend([json.loads(item) for item in  fr.readlines() ])\n\nprint(len(list_all))\n\n\ndef get_q_a(idx):\n    q = list_all[idx][\"instruction\"]\n    a = list_all[idx][\"output\"]\n    return \"\\n\\n问：{q}\\n\\n答：{a}\".format(q=q,a=a)\n\nimport random\nres = []\nidx_list_all = range(len(list_all))\nfor chosen_i in range(len(list_all)) :\n    his_idx = random.choices(idx_list_all,k=2)\n    #print(his_idx)\n    context_idx = random.choices(idx_list_all,k=2) \n    #print(context_idx)\n    context_idx.extend([chosen_i]) # 两个不相关检索答案+ 正确检索答案 ,比如 [1,2,100] 其中100是正确的q和a 等会idx=100的q作为question\n    random.shuffle(context_idx) # 打乱上下文的顺序 [1,100,2]\n    history = \"\".join([get_q_a(idx) for idx in his_idx])\n    context = \"\".join([get_q_a(idx) for idx in context_idx])\n    question = list_all[chosen_i][\"instruction\"]\n    answer = list_all[chosen_i][\"output\"]\n    res.append({\"instruction\":template.format(history=history,context=context,question=question)  ,\"input\":\"\" , \"output\":answer })\n\n# print(res[-10]['instruction'])\n# print(res[-10]['output'])\n\nmax_q ,max_a, max_total = 0,0,0\n\nfor item in res:\n    max_q = max(max_q, len(item[\"instruction\"]))\n    max_a = max(max_a, len(item['output']))\n    max_total = max(max_total , len(item[\"instruction\"]) + len(item[\"output\"]) )\n\nwith open(f\"alpaca_langchain_prompt_yunguan_0922_max_q_{max_q}_max_a_{max_a}_max_total_{max_total}.json\",\"w\",encoding=\"utf-8\") as fw:\n    for item in res :\n        fw.write(json.dumps(item,ensure_ascii=False)+\"\\n\")\nprint(f\"alpaca_langchain_prompt_yunguan_0922_max_q_{max_q}_max_a_{max_a}_max_total_{max_total}.json   done\")","metadata":{"execution":{"iopub.status.busy":"2023-09-22T06:21:46.328155Z","iopub.execute_input":"2023-09-22T06:21:46.328877Z","iopub.status.idle":"2023-09-22T06:21:46.486933Z","shell.execute_reply.started":"2023-09-22T06:21:46.328840Z","shell.execute_reply":"2023-09-22T06:21:46.485787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#cat alpaca_langchain_prompt_yunguan_0919.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 用alpaca格式的langchain prompt新数据跑","metadata":{}},{"cell_type":"code","source":"!git pull --all --force","metadata":{"execution":{"iopub.status.busy":"2023-09-22T06:38:04.705489Z","iopub.execute_input":"2023-09-22T06:38:04.705986Z","iopub.status.idle":"2023-09-22T06:38:06.423451Z","shell.execute_reply.started":"2023-09-22T06:38:04.705928Z","shell.execute_reply":"2023-09-22T06:38:06.422132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!deepspeed --include localhost:0,1  train_lora.py \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b  \\\n  --output_dir output_langchain_yunguan_0922 \\\n  --num_eval_samples 100 \\\n  --train_data_path data/alpaca_langchain_yunguan \\\n  --eval_data_path  data/alpaca_langchain_yunguan \\\n  --max_input_length 900 \\\n  --max_output_length 256 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 4 \\\n  --learning_rate  1e-6 \\\n  --num_train_epochs  70  \\\n  --save_total_limit 2 \\\n  --use_qlora True \\\n--deepspeed ds_zero2_config.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!deepspeed --include localhost:0,1  train_lora.py \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b  \\\n  --output_dir output_langchain_yunguan_0922_v2 \\\n  --resume_from_checkpoint /kaggle/working/chatGLM-6B-QLoRA/output_langchain_yunguan_0922/checkpoint-200 \\\n  --num_eval_samples 100 \\\n  --train_data_path data/alpaca_langchain_yunguan \\\n  --eval_data_path  data/alpaca_langchain_yunguan \\\n  --max_input_length 900 \\\n  --max_output_length 256 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 4 \\\n  --learning_rate  1e-6 \\\n  --num_train_epochs  70  \\\n  --save_total_limit 2 \\\n  --use_qlora True \\\n--deepspeed ds_zero2_config.json","metadata":{"execution":{"iopub.status.busy":"2023-09-22T07:31:11.853843Z","iopub.execute_input":"2023-09-22T07:31:11.854243Z","iopub.status.idle":"2023-09-22T07:39:09.665682Z","shell.execute_reply.started":"2023-09-22T07:31:11.854210Z","shell.execute_reply":"2023-09-22T07:39:09.664072Z"},"trusted":true},"execution_count":null,"outputs":[]}]}