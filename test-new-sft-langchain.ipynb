{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-11T14:43:33.277744Z","iopub.execute_input":"2023-10-11T14:43:33.278123Z","iopub.status.idle":"2023-10-11T14:43:33.607777Z","shell.execute_reply.started":"2023-10-11T14:43:33.278082Z","shell.execute_reply":"2023-10-11T14:43:33.606816Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone https://github.com/valkryhx/chatGLM-6B-QLoRA","metadata":{"execution":{"iopub.status.busy":"2023-10-11T14:43:33.609875Z","iopub.execute_input":"2023-10-11T14:43:33.610671Z","iopub.status.idle":"2023-10-11T14:43:34.634866Z","shell.execute_reply.started":"2023-10-11T14:43:33.610636Z","shell.execute_reply":"2023-10-11T14:43:34.633581Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working\nfatal: destination path 'chatGLM-6B-QLoRA' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd chatGLM-6B-QLoRA/\n!git pull --all --force \n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-10-11T14:43:34.636878Z","iopub.execute_input":"2023-10-11T14:43:34.637220Z","iopub.status.idle":"2023-10-11T14:44:23.680255Z","shell.execute_reply.started":"2023-10-11T14:43:34.637185Z","shell.execute_reply":"2023-10-11T14:44:23.678955Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/chatGLM-6B-QLoRA\nFetching origin\nremote: Enumerating objects: 34, done.\u001b[K\nremote: Counting objects: 100% (34/34), done.\u001b[K\nremote: Compressing objects: 100% (30/30), done.\u001b[K\nremote: Total 30 (delta 19), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (30/30), 18.16 KiB | 170.00 KiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   c5fc957..c2a7ed2  main       -> origin/main\nUpdating c5fc957..c2a7ed2\nFast-forward\n sft_multi_turn_qlora_chatglm2.py                   |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n test-new-sft-langchain.ipynb                       |   2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n train_lora_baichuan7b.py                           | 498 \u001b[32m+++++++++++++++++++++\u001b[m\n ...50\\200\\203\\344\\270\\216\\347\\273\\217\\351\\252\\214\" |  82 \u001b[32m++++\u001b[m\n 4 files changed, 582 insertions(+), 2 deletions(-)\n create mode 100644 train_lora_baichuan7b.py\nCollecting peft==0.5.0 (from -r requirements.txt (line 2))\n  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting transformers==4.30.2 (from -r requirements.txt (line 3))\n  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting datasets==2.12.0 (from -r requirements.txt (line 4))\n  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tqdm==4.65.0 (from -r requirements.txt (line 5))\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting loguru==0.7.0 (from -r requirements.txt (line 6))\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fire==0.5.0 (from -r requirements.txt (line 7))\n  Downloading fire-0.5.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting bitsandbytes==0.41.0 (from -r requirements.txt (line 9))\n  Downloading bitsandbytes-0.41.0-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting wandb==0.15.3 (from -r requirements.txt (line 10))\n  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting cpm_kernels==1.0.11 (from -r requirements.txt (line 11))\n  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting accelerate==0.21.0 (from -r requirements.txt (line 13))\n  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.1.99)\nCollecting deepspeed==0.9.5 (from -r requirements.txt (line 15))\n  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting evaluate==0.4.0 (from -r requirements.txt (line 16))\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting trl==0.7.1 (from -r requirements.txt (line 18))\n  Downloading trl-0.7.1-py3-none-any.whl (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.0/118.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (2.0.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0->-r requirements.txt (line 2)) (0.3.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 3)) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 3)) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 3)) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 3)) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 3)) (0.13.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 4)) (11.0.0)\nCollecting dill<0.3.7,>=0.3.0 (from datasets==2.12.0->-r requirements.txt (line 4))\n  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 4)) (2.0.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 4)) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 4)) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 4)) (2023.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 4)) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 4)) (0.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 7)) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 7)) (2.3.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (3.1.31)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (1.30.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (68.0.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 10)) (3.20.3)\nCollecting hjson (from deepspeed==0.9.5->-r requirements.txt (line 15))\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 15)) (1.11.1)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 15)) (9.0.0)\nRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 15)) (1.10.9)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 4)) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 4)) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 4)) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 4)) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 4)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 4)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 4)) (1.3.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 10)) (4.0.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2->-r requirements.txt (line 3)) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.5.0->-r requirements.txt (line 2)) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 3)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 3)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 3)) (2023.7.22)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (3.1.2)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.12.0->-r requirements.txt (line 4))\n  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 4)) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 4)) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 4)) (2023.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 10)) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.5.0->-r requirements.txt (line 2)) (1.3.0)\nBuilding wheels for collected packages: fire, deepspeed\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=a0d61b91b060127df4e7e41732ebc144d271823f0eedfe8e6bc02022cace74a8\n  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844530 sha256=8b36d956011004ec9287cd6c75d7e8c18a69ceecfd63b0e98b8c6031085a6ad0\n  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\nSuccessfully built fire deepspeed\nInstalling collected packages: hjson, cpm_kernels, bitsandbytes, tqdm, loguru, fire, dill, multiprocess, wandb, transformers, deepspeed, accelerate, peft, datasets, trl, evaluate\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.66.1\n    Uninstalling tqdm-4.66.1:\n      Successfully uninstalled tqdm-4.66.1\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.7\n    Uninstalling dill-0.3.7:\n      Successfully uninstalled dill-0.3.7\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.15\n    Uninstalling multiprocess-0.70.15:\n      Successfully uninstalled multiprocess-0.70.15\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.9\n    Uninstalling wandb-0.15.9:\n      Successfully uninstalled wandb-0.15.9\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.33.0\n    Uninstalling transformers-4.33.0:\n      Successfully uninstalled transformers-4.33.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.22.0\n    Uninstalling accelerate-0.22.0:\n      Successfully uninstalled accelerate-0.22.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.621.222118 requires jupyter-server~=1.16, but you have jupyter-server 2.6.0 which is incompatible.\nfitter 1.6.0 requires pandas<3.0.0,>=2.0.3, but you have pandas 2.0.2 which is incompatible.\nfitter 1.6.0 requires tqdm<5.0.0,>=4.65.1, but you have tqdm 4.65.0 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npathos 0.3.1 requires dill>=0.3.7, but you have dill 0.3.6 which is incompatible.\npathos 0.3.1 requires multiprocess>=0.70.15, but you have multiprocess 0.70.14 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.2 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.21.0 bitsandbytes-0.41.0 cpm_kernels-1.0.11 datasets-2.12.0 deepspeed-0.9.5 dill-0.3.6 evaluate-0.4.0 fire-0.5.0 hjson-3.1.0 loguru-0.7.0 multiprocess-0.70.14 peft-0.5.0 tqdm-4.65.0 transformers-4.30.2 trl-0.7.1 wandb-0.15.3\n","output_type":"stream"}]},{"cell_type":"code","source":"ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 测试baichuan2-7b-chat的sft","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!deepspeed --include localhost:0,1  train_lora_baichuan7b.py \\\n  --train_args_json luzi.json \\\n  --model_name_or_path  baichuan-inc/Baichuan2-7B-Chat \\\n  --output_dir sft_baichuan2_7b_1010 \\\n  --num_eval_samples 1 \\\n  --train_data_path data/alpaca_yunguan_0814 \\\n  --eval_data_path  data/alpaca_yunguan_0814 \\\n  --max_length 512 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  1e-6 \\\n  --num_train_epochs  70  \\\n  --save_total_limit 2 \\\n  --use_qlora True \\\n--deepspeed ds_zero2_config.json","metadata":{"execution":{"iopub.status.busy":"2023-10-11T15:24:45.883606Z","iopub.execute_input":"2023-10-11T15:24:45.884009Z","iopub.status.idle":"2023-10-11T15:34:27.944831Z","shell.execute_reply.started":"2023-10-11T15:24:45.883975Z","shell.execute_reply":"2023-10-11T15:34:27.943552Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Fetching origin\nremote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (3/3), 796 bytes | 199.00 KiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   81622f1..fbd78f5  main       -> origin/main\nUpdating 81622f1..fbd78f5\nFast-forward\n train_lora_baichuan7b.py | 4 \u001b[32m+++\u001b[m\u001b[31m-\u001b[m\n 1 file changed, 3 insertions(+), 1 deletion(-)\n[2023-10-11 15:24:50,145] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2023-10-11 15:24:55,855] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n[2023-10-11 15:24:55,856] [INFO] [runner.py:555:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train_lora_baichuan7b.py --train_args_json luzi.json --model_name_or_path baichuan-inc/Baichuan2-7B-Chat --output_dir sft_baichuan2_7b_1010 --num_eval_samples 1 --train_data_path data/alpaca_yunguan_0814 --eval_data_path data/alpaca_yunguan_0814 --max_length 512 --lora_rank 64 --lora_dropout 0.05 --compute_dtype fp16 --per_device_train_batch_size 2 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --learning_rate 1e-6 --num_train_epochs 70 --save_total_limit 2 --use_qlora True --deepspeed ds_zero2_config.json\n[2023-10-11 15:24:57,424] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2023-10-11 15:25:02,424] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n[2023-10-11 15:25:02,424] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n[2023-10-11 15:25:02,424] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n[2023-10-11 15:25:02,424] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n[2023-10-11 15:25:02,424] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n[2023-10-11 15:25:02,424] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n[2023-10-11 15:25:02,425] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n[2023-10-11 15:25:02,425] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n[2023-10-11 15:25:02,425] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n[2023-10-11 15:25:02,425] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n[2023-10-11 15:25:02,425] [INFO] [launch.py:163:main] dist_world_size=2\n[2023-10-11 15:25:02,425] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2023-10-11 15:25:06,707] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-11 15:25:06,754] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\ninto lora trainer !!!\ninto lora trainer !!!\nq_config=BitsAndBytesConfig()\nq_config=BitsAndBytesConfig()\n[2023-10-11 15:34:20,800] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 639\n[2023-10-11 15:34:20,803] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 640\n[2023-10-11 15:34:21,540] [ERROR] [launch.py:321:sigkill_handler] ['/opt/conda/bin/python3.10', '-u', 'train_lora_baichuan7b.py', '--local_rank=1', '--train_args_json', 'luzi.json', '--model_name_or_path', 'baichuan-inc/Baichuan2-7B-Chat', '--output_dir', 'sft_baichuan2_7b_1010', '--num_eval_samples', '1', '--train_data_path', 'data/alpaca_yunguan_0814', '--eval_data_path', 'data/alpaca_yunguan_0814', '--max_length', '512', '--lora_rank', '64', '--lora_dropout', '0.05', '--compute_dtype', 'fp16', '--per_device_train_batch_size', '2', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '1', '--learning_rate', '1e-6', '--num_train_epochs', '70', '--save_total_limit', '2', '--use_qlora', 'True', '--deepspeed', 'ds_zero2_config.json'] exits with return code = -9\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 测试  新的 train_lora.py","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!deepspeed --include localhost:1  train_lora.py \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b  \\\n  --output_dir output0908_v2 \\\n  --resume_from_checkpoint /kaggle/working/chatGLM-6B-QLoRA/output0908/checkpoint-200 \\\n  --num_eval_samples 1 \\\n  --train_data_path data/alpaca_yunguan_0814 \\\n  --eval_data_path  data/alpaca_yunguan_0814 \\\n  --max_length 1000 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 4 \\\n  --learning_rate  1e-6 \\\n  --num_train_epochs  70  \\\n  --save_total_limit 2 \\\n  --use_qlora True \\\n--deepspeed ds_zero2_config.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 测试  新的 sft_lora_multi_turn.py","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n!deepspeed --include localhost:0,1  sft_lora_multi_turn.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-sharegpt-2k-sft-0908 \\\n  --num_train_samples -1 \\\n  --num_eval_samples 2 \\\n  --train_data_path ./data/sharegpt_data  \\\n  --eval_data_path  ./data/sharegpt_data    \\\n  --data_type sharegpt  \\\n  --max_length 1000 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 2  \\\n  --gradient_accumulation_steps 2 \\\n  --learning_rate  1.8e-5 \\\n  --num_train_epochs  40  \\\n  --save_total_limit 2 \\\n  --use_qlora True \\\n--deepspeed ds_zero2_config.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 构造和测试langchain prompt 数据 已经将构造的数据上传到github项目","metadata":{}},{"cell_type":"code","source":"import json \nimport os\n\ntemplate = \"\"\"现提供如下信息：\nhistory={history}\ncontext={context}\n请使用上述信息回答：{question}\n\"\"\"\n\nparent_dir = \"/kaggle/working/chatGLM-6B-QLoRA/data/alpaca_yunguan_0814\"\npaths = ['yunguan_aug_part_3.json',\n         'yunguan_aug_part_2.json',\n        \"yunguan_aug_part_1.json\",\n        \"refuse_phone.json\"]\nlist_all =[]\nfor name in paths :\n    with open(os.path.join(parent_dir , name) ,\"r\",encoding=\"utf-8\") as fr:\n        list_all.extend([json.loads(item) for item in  fr.readlines() ])\n\nprint(len(list_all))\n\n\ndef get_q_a(idx):\n    q = list_all[idx][\"instruction\"]\n    a = list_all[idx][\"output\"]\n    return \"\\n\\n问：{q}\\n\\n答：{a}\".format(q=q,a=a)\n\nimport random\nres = []\nidx_list_all = range(len(list_all))\nfor chosen_i in range(len(list_all)) :\n    his_idx = random.choices(idx_list_all,k=2)\n    #print(his_idx)\n    context_idx = random.choices(idx_list_all,k=2) \n    #print(context_idx)\n    context_idx.extend([chosen_i]) # 两个不相关检索答案+ 正确检索答案 ,比如 [1,2,100] 其中100是正确的q和a 等会idx=100的q作为question\n    random.shuffle(context_idx) # 打乱上下文的顺序 [1,100,2]\n    history = \"\".join([get_q_a(idx) for idx in his_idx])\n    context = \"\".join([get_q_a(idx) for idx in context_idx])\n    question = list_all[chosen_i][\"instruction\"]\n    answer = list_all[chosen_i][\"output\"]\n    res.append({\"instruction\":template.format(history=history,context=context,question=question)  ,\"input\":\"\" , \"output\":answer })\n\n# print(res[-10]['instruction'])\n# print(res[-10]['output'])\n\nmax_q ,max_a, max_total = 0,0,0\n\nfor item in res:\n    max_q = max(max_q, len(item[\"instruction\"]))\n    max_a = max(max_a, len(item['output']))\n    max_total = max(max_total , len(item[\"instruction\"]) + len(item[\"output\"]) )\n\nwith open(f\"alpaca_langchain_prompt_yunguan_0922_max_q_{max_q}_max_a_{max_a}_max_total_{max_total}.json\",\"w\",encoding=\"utf-8\") as fw:\n    for item in res :\n        fw.write(json.dumps(item,ensure_ascii=False)+\"\\n\")\nprint(f\"alpaca_langchain_prompt_yunguan_0922_max_q_{max_q}_max_a_{max_a}_max_total_{max_total}.json   done\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#cat alpaca_langchain_prompt_yunguan_0919.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 用alpaca格式的langchain prompt新数据跑","metadata":{}},{"cell_type":"code","source":"!git pull --all --force","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!deepspeed --include localhost:0,1  train_lora.py \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b  \\\n  --output_dir output_langchain_yunguan_0922 \\\n  --num_eval_samples 100 \\\n  --train_data_path data/alpaca_langchain_yunguan \\\n  --eval_data_path  data/alpaca_langchain_yunguan \\\n  --max_length 1200 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 4 \\\n  --learning_rate  1e-6 \\\n  --num_train_epochs  70  \\\n  --save_total_limit 2 \\\n  --use_qlora True \\\n--deepspeed ds_zero2_config.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# qlora后测试langchain 格式的问答","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel, PeftConfig\n\n\n# 有效peft_model_path = '/kaggle/working/chatGLM-6B-QLoRA/output-sft-qlora-ds-yunguan0712-aug-v3'\n\npeft_model_path= \"/kaggle/working/chatGLM-6B-QLoRA/output_langchain_yunguan_0922/checkpoint-200\"\nconfig = PeftConfig.from_pretrained(peft_model_path)\nq_config = BitsAndBytesConfig(load_in_4bit=True,\n                              bnb_4bit_quant_type='nf4',\n                              bnb_4bit_use_double_quant=True,\n                              bnb_4bit_compute_dtype=torch.float16)\n# base_model加载时保持和train时完全一致的参数配置\nbase_model = AutoModel.from_pretrained(config.base_model_name_or_path,\n                                       quantization_config=q_config,\n                                       trust_remote_code=True,\n                                       device_map='auto')\n\n# input_text = '请你作为烽火SSE智能助手，帮我回答下面的问题。新型全电发票目前系统无法识别，要如何录入？'\n# print(f'输入：\\n{input_text}')\n# tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n\n# response, history = base_model.chat(tokenizer=tokenizer,history=[], query=input_text)\n# print(f'微调前：\\n{response}')\n\n# model = PeftModel.from_pretrained(base_model, peft_model_path)\n# response, history = model.chat(tokenizer=tokenizer, history=[] ,query=input_text)\n# print(f'微调后: \\n{response}')\nmodel = PeftModel.from_pretrained(base_model, peft_model_path)\nmodel.eval()\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\nprint(\"adapter loaded!\")\n","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:47:38.743104Z","iopub.execute_input":"2023-09-24T07:47:38.743556Z","iopub.status.idle":"2023-09-24T07:50:44.188805Z","shell.execute_reply.started":"2023-09-24T07:47:38.743516Z","shell.execute_reply":"2023-09-24T07:50:44.186611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"line = input()\nwhile line:\n    if line.strip()==\"#QUIT\": # 输入#QUIT退出对话循环\n        break\n    #TEMPLATE = \"作为烽火SSE助手，请帮忙回答如下问题：\" #计算两个矩阵的乘积：\"\n    #line = PROMPT_TEMPLATE.format_map({'instruction': line})\n    if line.strip() in [\"我有一个新问题\" ,\"#CLEAR\"]:\n        history=[]\n        line = input().replace(\"\\\\n\",\"\\n\")\n    \n    #print(f\"【formatted line】={line}\")\n    response, history = model.chat(tokenizer, line, history=[],top_p= 0.95,\n                                  temperature=0.1,num_beams=7,max_length=1024,repetition_penalty=1.1,do_sample=False) # 量化后 num_beams只能=1 否则报错\n    print(\"ChatGLM2-6B：\\n\", response.replace(\"\\\\n\",'\\n'))\n    print(\"\\n------------------------------------------------\\n用户：\")\n    line = input()\n    \n    print(\"\\n\\n\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:25.609608Z","iopub.execute_input":"2023-09-24T08:01:25.610120Z","iopub.status.idle":"2023-09-24T08:07:42.225620Z","shell.execute_reply.started":"2023-09-24T08:01:25.610071Z","shell.execute_reply":"2023-09-24T08:07:42.224400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color=red>根据上面的测试 ，很关键的一个参数是num_beams，目前设置 num_beams=7 ,这个值越大，可能的答案路径就越多，最终会从多个答案路径中选择总和概率最大的，测试发现num_beams越大，model表现的越能利用提供的历史对话和检索信息，根据这些信息回答问题的能力就越强。</font>","metadata":{}},{"cell_type":"markdown","source":"# 使用不sft的基座模型测试下  感觉不开启do_sample=True 效果就很波动？？？有时候表现可以 有时候又不行\n# 并且 基座模型无法开启num_beams =2,3,4 ，只要不是1 运行就报错","metadata":{}},{"cell_type":"code","source":"\n\nimport torch\nfrom transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel, PeftConfig\n\n\n# 有效peft_model_path = '/kaggle/working/chatGLM-6B-QLoRA/output-sft-qlora-ds-yunguan0712-aug-v3'\n\n#peft_model_path= \"/kaggle/working/chatGLM-6B-QLoRA/output_langchain_yunguan_0922/checkpoint-200\"\nconfig = PeftConfig.from_pretrained(peft_model_path)\n#q_config = BitsAndBytesConfig(load_in_4bit=True,\n#                               bnb_4bit_quant_type='nf4',\n#                               bnb_4bit_use_double_quant=True,\n#                               bnb_4bit_compute_dtype=torch.float16)\n# base_model加载时保持和train时完全一致的参数配置\nmodel = AutoModel.from_pretrained(config.base_model_name_or_path,\n                                       #quantization_config=q_config,\n                                       trust_remote_code=True,\n                                       device_map='auto')\n\n# input_text = '请你作为烽火SSE智能助手，帮我回答下面的问题。新型全电发票目前系统无法识别，要如何录入？'\n# print(f'输入：\\n{input_text}')\n# tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n\n# response, history = base_model.chat(tokenizer=tokenizer,history=[], query=input_text)\n# print(f'微调前：\\n{response}')\n\n# model = PeftModel.from_pretrained(base_model, peft_model_path)\n# response, history = model.chat(tokenizer=tokenizer, history=[] ,query=input_text)\n# print(f'微调后: \\n{response}')\n#model = PeftModel.from_pretrained(base_model, peft_model_path)\nmodel.eval()\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\nprint(\"adapter loaded!\")","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:07:51.486819Z","iopub.execute_input":"2023-09-24T08:07:51.487238Z","iopub.status.idle":"2023-09-24T08:09:07.563458Z","shell.execute_reply.started":"2023-09-24T08:07:51.487204Z","shell.execute_reply":"2023-09-24T08:09:07.562235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"line = input()\nwhile line:\n    if line.strip()==\"#QUIT\": # 输入#QUIT退出对话循环\n        break\n    #TEMPLATE = \"作为烽火SSE助手，请帮忙回答如下问题：\" #计算两个矩阵的乘积：\"\n    #line = PROMPT_TEMPLATE.format_map({'instruction': line})\n    if line.strip() in [\"我有一个新问题\" ,\"#CLEAR\"]:\n        history=[]\n        line = input().replace(\"\\\\n\",\"\\n\")\n    \n    #print(f\"【formatted line】={line}\")\n    response, history = model.chat(tokenizer, line, history=[],top_p= 0.95,\n                                  temperature=0.05,num_beams=1,max_length=8192,repetition_penalty=1.1,do_sample=True) # 量化后 num_beams只能=1 否则报错\n    print(\"ChatGLM2-6B：\\n\", response.replace(\"\\\\n\",'\\n'))\n    print(\"\\n------------------------------------------------\\n用户：\")\n    line = input()\n    \n    print(\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:19:56.624257Z","iopub.execute_input":"2023-09-24T08:19:56.625477Z","iopub.status.idle":"2023-09-24T08:20:46.918291Z","shell.execute_reply.started":"2023-09-24T08:19:56.625435Z","shell.execute_reply":"2023-09-24T08:20:46.917052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 下载","metadata":{}},{"cell_type":"code","source":"cp -r /kaggle/working/chatGLM-6B-QLoRA/output_langchain_yunguan_0922/checkpoint-200  /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:39:15.867696Z","iopub.execute_input":"2023-09-24T07:39:15.868147Z","iopub.status.idle":"2023-09-24T07:39:17.584343Z","shell.execute_reply.started":"2023-09-24T07:39:15.868106Z","shell.execute_reply":"2023-09-24T07:39:17.582963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"  \nimport os\nos.chdir(r'/kaggle/working')\n!tar -cvf   langchain_ckpt_200.tar.gz  /kaggle/working/checkpoint-200\n#!tar -czf Landscapes.tar.gz images_out/Landscapes\n#!tar -czf    basemodel_chatglm2_6b_ckt_0715.tar.gz   /kaggle/working/basemodel_chatglm2_6b_ckt_0715\nfrom IPython.display import FileLink\nFileLink(r'langchain_ckpt_200.tar.gz')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:45:00.368860Z","iopub.execute_input":"2023-09-24T07:45:00.369494Z","iopub.status.idle":"2023-09-24T07:45:03.041597Z","shell.execute_reply.started":"2023-09-24T07:45:00.369451Z","shell.execute_reply":"2023-09-24T07:45:03.040385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls -lrth","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:46:19.562201Z","iopub.execute_input":"2023-09-24T07:46:19.563517Z","iopub.status.idle":"2023-09-24T07:46:20.567582Z","shell.execute_reply.started":"2023-09-24T07:46:19.563472Z","shell.execute_reply":"2023-09-24T07:46:20.566296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:42:59.778094Z","iopub.execute_input":"2023-09-24T07:42:59.778485Z","iopub.status.idle":"2023-09-24T07:43:00.758183Z","shell.execute_reply.started":"2023-09-24T07:42:59.778453Z","shell.execute_reply":"2023-09-24T07:43:00.756999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(r'/kaggle/working')\n\n#!tar -czf Landscapes.tar.gz images_out/Landscapes\n#!tar -czf    basemodel_chatglm2_6b_ckt_0715.tar.gz   /kaggle/working/basemodel_chatglm2_6b_ckt_0715\nfrom IPython.display import FileLink\nFileLink(r'langchain_ckpt_200.tar.gz')","metadata":{"execution":{"iopub.status.busy":"2023-09-24T07:42:49.677966Z","iopub.execute_input":"2023-09-24T07:42:49.678384Z","iopub.status.idle":"2023-09-24T07:42:49.686722Z","shell.execute_reply.started":"2023-09-24T07:42:49.678306Z","shell.execute_reply":"2023-09-24T07:42:49.685597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!deepspeed --include localhost:0,1  train_lora.py \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b  \\\n  --output_dir output_langchain_yunguan_0922_v2 \\\n  --resume_from_checkpoint /kaggle/working/chatGLM-6B-QLoRA/output_langchain_yunguan_0922/checkpoint-200 \\\n  --num_eval_samples 100 \\\n  --train_data_path data/alpaca_langchain_yunguan \\\n  --eval_data_path  data/alpaca_langchain_yunguan \\\n  --max_input_length 900 \\\n  --max_output_length 256 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 4 \\\n  --learning_rate  1e-6 \\\n  --num_train_epochs  70  \\\n  --save_total_limit 2 \\\n  --use_qlora True \\\n--deepspeed ds_zero2_config.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}