{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 注意 用shareGPT格式 的medical 1k 和 conversations 1k 训练了 15个小时\n# lr=2e-5 观察到1.8e-5 能加快loss减小","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/valkryhx/chatGLM-6B-QLoRA","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working/chatGLM-6B-QLoRA\n!git pull --all --force\n!ls\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-08-06T08:14:41.082115Z","iopub.execute_input":"2023-08-06T08:14:41.082467Z","iopub.status.idle":"2023-08-06T08:15:31.460268Z","shell.execute_reply.started":"2023-08-06T08:14:41.082436Z","shell.execute_reply":"2023-08-06T08:15:31.458897Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/working/chatGLM-6B-QLoRA\nFetching origin\nAlready up to date.\nREADME.md\nchatGLM_6B_QLoRA.json\nconvert_dataset.py\ndata\nds_zero2_config.json\nds_zero3_config.json\ninference_qlora.py\ninference_test.py\nluzi.json\nmerge_lora_and_quantize.py\noutput-history-sft-0805-v1\noutput-history-sft-0806-v1\noutput-sft-medi-alpaca-0804-v1\noutput-sft-medi-alpaca-0804-v2\noutput-sharegpt-2k-sft-0804-v2\noutput-sharegpt-2k-sft-0804-v3\noutput-sharegpt-2k-sft-0804-v4\noutput-sharegpt-2k-sft-0805-v1\noutput-sharegpt-sft-0804-v1\npics\npretrain_chatglm2_v5.ipynb\npretrain_chatglm2_v7.ipynb\npretrain_chatglm2_v8.ipynb\npretrain_qlora_chatglm2.py\nprocess_anli2samples.py\nqa_aug.json\nqlora_param.json\nremote_scripts\nrequirements.txt\nsft_multi_turn_qlora_chatglm2.py\nsharegpt_sft_success_0802.ipynb\nsharegpt_to_history.py\nstate.db\ntrain_ds_zero2_test.py——错误的\ntrain_normal_lora.py\ntrain_qlora.py\ntrain_qlora_deepspeed_zero.py\n二次预训练pt_sft_继续lora训练_qlora_glm_成功0730.ipynb\n参考与经验\nCollecting peft==0.4.0 (from -r requirements.txt (line 1))\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.30.2)\nCollecting datasets==2.12.0 (from -r requirements.txt (line 3))\n  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm==4.65.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.65.0)\nCollecting loguru==0.7.0 (from -r requirements.txt (line 5))\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fire==0.5.0 (from -r requirements.txt (line 6))\n  Downloading fire-0.5.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting bitsandbytes==0.39.0 (from -r requirements.txt (line 7))\n  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting wandb==0.15.3 (from -r requirements.txt (line 8))\n  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting cpm_kernels==1.0.11 (from -r requirements.txt (line 9))\n  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.20.3)\nRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.1.99)\nCollecting deepspeed==0.9.5 (from -r requirements.txt (line 12))\n  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (2.0.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (0.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (11.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (2.3.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.1.31)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.20.3)\nCollecting hjson (from deepspeed==0.9.5->-r requirements.txt (line 12))\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.11.1)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (9.0.0)\nRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.10.10)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (4.0.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2->-r requirements.txt (line 2)) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0->-r requirements.txt (line 1)) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2023.5.7)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2023.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.3.0)\nBuilding wheels for collected packages: fire, deepspeed\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=09a030142b63e494b611d9b8eb292e89a2db0f79bda9658e4b4942e2347b5602\n  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844547 sha256=c7abd2b928e04d2fa1edc18e5de47666b86728b95f799c0c4769f57ac7970ba9\n  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\nSuccessfully built fire deepspeed\nInstalling collected packages: hjson, cpm_kernels, bitsandbytes, loguru, fire, wandb, deepspeed, peft, datasets\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.5\n    Uninstalling wandb-0.15.5:\n      Successfully uninstalled wandb-0.15.5\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed bitsandbytes-0.39.0 cpm_kernels-1.0.11 datasets-2.12.0 deepspeed-0.9.5 fire-0.5.0 hjson-3.1.0 loguru-0.7.0 peft-0.4.0 wandb-0.15.3\n","output_type":"stream"}]},{"cell_type":"code","source":"# --resume_from_checkpoint ./output-pt-anli-21-v2 \\","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force \n#!pip install  -U git+https://github.com/huggingface/peft.git   # 20230717 peft==0.4.0正式发布了 不用调版本了推理完后再训练需要重新升级到0.4.0dev 所以有这句\n!deepspeed --num_gpus=2  train_qlora_deepspeed_zero.py \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-sft-medi-alpaca-0804-v2 \\\n  --num_eval_samples 2 \\\n  --train_data_path ./data/medi_alpaca_glm_prompt \\\n  --eval_data_path  ./data/medi_alpaca_glm_prompt \\\n  --max_input_length 200 \\\n  --max_output_length 1600 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 2  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  1e-5 \\\n  --num_train_epochs  40  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/valkryhx/chatGLM-6B-QLoRA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n#!pip install peft==0.4.0\n#!pip install  -U git+https://github.com/huggingface/peft.git\n%cd /kaggle/working/chatGLM-6B-QLoRA \n!ls\n!pip install -r requirements.txt\n#!pip install deepspeed==0.9.5  这个也是需要的 但是目前kaggle 的runtime自带了","metadata":{"execution":{"iopub.status.busy":"2023-08-06T07:51:35.075873Z","iopub.execute_input":"2023-08-06T07:51:35.076805Z","iopub.status.idle":"2023-08-06T07:52:15.385822Z","shell.execute_reply.started":"2023-08-06T07:51:35.076769Z","shell.execute_reply":"2023-08-06T07:52:15.384289Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"fatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n/kaggle/working/chatGLM-6B-QLoRA\nREADME.md\nchatGLM_6B_QLoRA.json\nconvert_dataset.py\ndata\nds_zero2_config.json\nds_zero3_config.json\ninference_qlora.py\ninference_test.py\nluzi.json\nmerge_lora_and_quantize.py\noutput-history-sft-0805-v1\noutput-history-sft-0806-v1\noutput-sft-medi-alpaca-0804-v1\noutput-sft-medi-alpaca-0804-v2\noutput-sharegpt-2k-sft-0804-v2\noutput-sharegpt-2k-sft-0804-v3\noutput-sharegpt-2k-sft-0804-v4\noutput-sharegpt-2k-sft-0805-v1\noutput-sharegpt-sft-0804-v1\npics\npretrain_chatglm2_v5.ipynb\npretrain_chatglm2_v7.ipynb\npretrain_chatglm2_v8.ipynb\npretrain_qlora_chatglm2.py\nprocess_anli2samples.py\nqa_aug.json\nqlora_param.json\nremote_scripts\nrequirements.txt\nsft_multi_turn_qlora_chatglm2.py\nsharegpt_sft_success_0802.ipynb\nsharegpt_to_history.py\nstate.db\ntrain_ds_zero2_test.py——错误的\ntrain_normal_lora.py\ntrain_qlora.py\ntrain_qlora_deepspeed_zero.py\n二次预训练pt_sft_继续lora训练_qlora_glm_成功0730.ipynb\n参考与经验\nCollecting peft==0.4.0 (from -r requirements.txt (line 1))\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.30.2)\nCollecting datasets==2.12.0 (from -r requirements.txt (line 3))\n  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm==4.65.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.65.0)\nCollecting loguru==0.7.0 (from -r requirements.txt (line 5))\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fire==0.5.0 (from -r requirements.txt (line 6))\n  Downloading fire-0.5.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting bitsandbytes==0.39.0 (from -r requirements.txt (line 7))\n  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hCollecting wandb==0.15.3 (from -r requirements.txt (line 8))\n  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting cpm_kernels==1.0.11 (from -r requirements.txt (line 9))\n  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.20.3)\nRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.1.99)\nCollecting deepspeed==0.9.5 (from -r requirements.txt (line 12))\n  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (2.0.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (0.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (11.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (2.3.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.1.31)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.20.3)\nCollecting hjson (from deepspeed==0.9.5->-r requirements.txt (line 12))\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.11.1)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (9.0.0)\nRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.10.10)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (4.0.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2->-r requirements.txt (line 2)) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0->-r requirements.txt (line 1)) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2023.5.7)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2023.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.3.0)\nBuilding wheels for collected packages: fire, deepspeed\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=df2035ec3b9252f1c7c156b8566f96e3eeb3d7036df49635efdc2148451be866\n  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844543 sha256=7262625a86067137f170d2d71edf2e83dafa031b757df79de736f00ec303513d\n  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\nSuccessfully built fire deepspeed\nInstalling collected packages: hjson, cpm_kernels, bitsandbytes, loguru, fire, wandb, deepspeed, peft, datasets\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.5\n    Uninstalling wandb-0.15.5:\n      Successfully uninstalled wandb-0.15.5\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed bitsandbytes-0.39.0 cpm_kernels-1.0.11 datasets-2.12.0 deepspeed-0.9.5 fire-0.5.0 hjson-3.1.0 loguru-0.7.0 peft-0.4.0 wandb-0.15.3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"用单一数据 42条来跑 发现loss下降的很快 说明流程没问题？","metadata":{}},{"cell_type":"code","source":"# ./data/sharegpt_multi_turn_data 目录不能有空白json文件\n!git pull --all --force \n#!pip install  -U git+https://github.com/huggingface/peft.git   # 20230717 peft==0.4.0正式发布了 不用调版本了推理完后再训练需要重新升级到0.4.0dev 所以有这句\n!deepspeed --include localhost:0,1  sft_multi_turn_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-sharegpt-sft-0804-v1 \\\n  --num_train_samples -1 \\\n  --num_eval_samples 2 \\\n  --train_data_path ./data/sharegpt_test  \\\n  --eval_data_path  ./data/sharegpt_test    \\\n  --data_type sharegpt  \\\n  --max_length 1800 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 2  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  1e-5 \\\n  --num_train_epochs  40  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 用完整的medi + shargpt 对话共2K 数据跑 lr =1.8e-5能正常下降 跑了8个小时下降到0.12","metadata":{}},{"cell_type":"code","source":"# ./data/sharegpt_multi_turn_data 目录不能有空白json文件\n!git pull --all --force \n#!pip install  -U git+https://github.com/huggingface/peft.git   # 20230717 peft==0.4.0正式发布了 不用调版本了推理完后再训练需要重新升级到0.4.0dev 所以有这句\n!deepspeed --include localhost:0,1  sft_multi_turn_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-sharegpt-2k-sft-0804-v4 \\\n  --num_train_samples -1 \\\n  --num_eval_samples 2 \\\n  --resume_from_checkpoint ./output-sharegpt-2k-sft-0804-v3/checkpoint-220 \\\n  --train_data_path ./data/sharegpt_data  \\\n  --eval_data_path  ./data/sharegpt_data    \\\n  --data_type sharegpt  \\\n  --max_length 1800 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 2  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  2e-5 \\\n  --num_train_epochs  40  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 在loss=0.12的基础上继续","metadata":{}},{"cell_type":"code","source":"# ./data/sharegpt_multi_turn_data 目录不能有空白json文件\n!git pull --all --force \n#!pip install  -U git+https://github.com/huggingface/peft.git   # 20230717 peft==0.4.0正式发布了 不用调版本了推理完后再训练需要重新升级到0.4.0dev 所以有这句\n!deepspeed --include localhost:0,1  sft_multi_turn_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-sharegpt-2k-sft-0805-v1 \\\n  --num_train_samples -1 \\\n  --num_eval_samples 2 \\\n  --resume_from_checkpoint ./output-sharegpt-2k-sft-0804-v4/checkpoint-3980 \\\n  --train_data_path ./data/sharegpt_data  \\\n  --eval_data_path  ./data/sharegpt_data    \\\n  --data_type sharegpt  \\\n  --max_length 1800 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 2  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  1.8e-5 \\\n  --num_train_epochs  40  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. import libs","metadata":{}},{"cell_type":"code","source":"# 推理 \nimport os\nimport argparse\nfrom typing import List, Dict, Optional\nfrom accelerate import init_empty_weights  # load an empty model,just structure , no real weight.\nimport bitsandbytes as bnb\nimport torch\nfrom glob import glob\nfrom loguru import logger\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    HfArgumentParser,\n    set_seed,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig \n)\nfrom peft import (\n    TaskType,\n    LoraConfig,\n    #AdaLoraConfig ,  #  提出自2020年 感觉和lora区别不大 而且和qlora有冲突 这里代码没有用到 \n                     #例子https://www.zhihu.com/question/596950521/answer/3109759716\n    get_peft_model,\n    set_peft_model_state_dict,\n    prepare_model_for_kbit_training\n)\nfrom peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\nfrom transformers.deepspeed import HfDeepSpeedConfig\nimport deepspeed\nimport json\nfrom itertools import chain","metadata":{"execution":{"iopub.status.busy":"2023-08-06T07:54:48.209778Z","iopub.execute_input":"2023-08-06T07:54:48.210208Z","iopub.status.idle":"2023-08-06T07:55:02.015982Z","shell.execute_reply.started":"2023-08-06T07:54:48.210173Z","shell.execute_reply":"2023-08-06T07:55:02.015010Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[2023-08-06 07:54:50,042] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\nCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. 加载base model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel, PeftConfig\n\n\n# 有效peft_model_path = '/kaggle/working/chatGLM-6B-QLoRA/output-sft-qlora-ds-yunguan0712-aug-v3'\n\npeft_model_path= \"/kaggle/working/chatGLM-6B-QLoRA/output-sharegpt-2k-sft-0805-v1/checkpoint-250\"\nconfig = PeftConfig.from_pretrained(peft_model_path)\nq_config = BitsAndBytesConfig(load_in_4bit=True,\n                              bnb_4bit_quant_type='nf4',\n                              bnb_4bit_use_double_quant=True,\n                              bnb_4bit_compute_dtype=torch.float16)\n# base_model加载时保持和train时完全一致的参数配置\nmodel = AutoModel.from_pretrained(config.base_model_name_or_path,\n                                       quantization_config=q_config,\n                                       trust_remote_code=True,\n                                       device_map='auto')\n\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n# input_text = '请你作为烽火SSE智能助手，帮我回答下面的问题。新型全电发票目前系统无法识别，要如何录入？'\n# print(f'输入：\\n{input_text}')\n#tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n\n# response, history = base_model.chat(tokenizer=tokenizer,history=[], query=input_text)\n# print(f'微调前：\\n{response}')\n\n# model = PeftModel.from_pretrained(base_model, peft_model_path)\n# response, history = model.chat(tokenizer=tokenizer, history=[] ,query=input_text)\n# print(f'微调后: \\n{response}')","metadata":{"execution":{"iopub.status.busy":"2023-08-06T07:57:43.838139Z","iopub.execute_input":"2023-08-06T07:57:43.839818Z","iopub.status.idle":"2023-08-06T08:00:43.444065Z","shell.execute_reply.started":"2023-08-06T07:57:43.839774Z","shell.execute_reply":"2023-08-06T08:00:43.443003Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb1d3782649643e89f2f1448f254c643"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)iguration_chatglm.py:   0%|          | 0.00/2.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8648f81e4e754967a588be7c3bbdad34"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/modeling_chatglm.py:   0%|          | 0.00/50.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0e0cd356128458ab9582efecec73ec3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)main/quantization.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10b78b9861c84b7dadc600ac1ed843e3"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- modeling_chatglm.py\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e40cfc4803d4ca6b10f0e7f6aa34dd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44f7c588252149ac962231479a434d18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca2f54307e3e4c0f95f31d05532d2684"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eaf3824d07d49bdb299f81d76770c58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6790b9cbd310453087b02dc279e33024"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e048da8a33004bc3a3c67645da7dec88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5397e63708a4ecc96caa29f96b5761c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d6836bc5d3741eba835f1d7f3d89b64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3621552b7cc4615802fd56733c1607c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e27fa2b9d19404d8859a11e027e8f57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efc120459c1946dea95858127d6716ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)enization_chatglm.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc7874515ffa4f9184ff704e8483dd8b"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6f5e87d495a491ab2812cb04dcb5337"}},"metadata":{}}]},{"cell_type":"markdown","source":"# 2.1 base model chat","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\nhistory=[]\nprint(\"用户：你好，智能小助手\\n\")\nold_response = \"\"\nresponse, history  =  model.chat(tokenizer, \"你好\", history=history, max_length=8192, \n                                           top_p=0.9, temperature=0.75,\n                                           num_beams=5)\n    \nprint(response)\n#print(1111,old_response)作为烽火智能助手，我遇到了以下故障现象，某地市某网络单盘占用槽位显示异常现场实际占用两个槽位。请给出类似案例以供参考。\n#print(\"ChatGLM2-6B：\\n\",response)\n#print(\"\\n------------------------------------------------\\n用户：\")\n#raise ValueError(123)\nline = input()\npast_key_values=None\nhistory =[]\n#prompt_format = \"[Round {}]\\n\\n问：{}\\n\\n答：\"\nwhile line:\n    if line.strip()==\"#QUIT\": # 输入#QUIT退出对话循环\n        break\n    #inst = \"我有关于烽火SSE的问题，请你作为烽火SSE智能助手，帮我回答下面的问题。\" #计算两个矩阵的乘积：\"\n    #line = PROMPT_TEMPLATE.format_map({'instruction': line})\n    if line.strip()==\"#CLEAR\" :  #清空历史对话\n        history=[]\n    #format_line = prompt_format.format(1,line)\n    #print(f\"【formatted line】={format_line}\")\n    #old_response = \"\"\n    beams =2\n    #print(f\"num_beams={beams}\")\n    \n    response, history =  model.chat(tokenizer=tokenizer, \n                                               history=history ,\n                                               max_length=8192,\n                                               query=line,\n                                               top_p=0.95,temperature=0.85,\n                                               num_beams=1,\n                                               past_key_values=None)\n        #print(past_key_values) 不要打印 太长了\n        #print(response[len(old_response):], end=\"\")\n        #old_response = response\n    #print(end=\"\\r\")\n    #print(f\"\\nold response=\\n{old_response}\")\n    #print(end=\"\\r\") # 量化后 num_beams只能=1 否则报错\n    print(\"ChatGLM2-6B：\\n\", response.replace(\"\\\\n\",'\\n'))\n    print(\"\\n------------------------------------------------\\n用户：\")\n    line = input()","metadata":{"execution":{"iopub.status.busy":"2023-08-06T08:02:53.552252Z","iopub.execute_input":"2023-08-06T08:02:53.552698Z","iopub.status.idle":"2023-08-06T08:11:27.946869Z","shell.execute_reply.started":"2023-08-06T08:02:53.552664Z","shell.execute_reply":"2023-08-06T08:11:27.945655Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"用户：你好，智能小助手\n\n你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" α1抗胰蛋白酶缺乏性肝病的预防措施有哪些？\n"},{"name":"stdout","text":"ChatGLM2-6B：\n α1抗胰蛋白酶缺乏性肝病是一种常染色体隐性遗传的肝病，主要受累的细胞为肝脏细胞。预防该病的措施包括以下几点：\n\n1. 避免婚前检查，因为婚前检查对于α1抗胰蛋白酶缺乏性肝病的预防意义不大。\n\n2. 提倡遗传咨询，对于有遗传倾向的家庭，可以接受遗传咨询，以便及早发现问题。\n\n3. 提倡健康的生活方式，如健康饮食、适量运动、戒烟限酒等，这些措施可以有效降低患病的风险。\n\n4. 提倡母婴保健，孕妇在怀孕期间应该接受产前检查，以便及早发现和治疗胎儿的问题。对于新生儿，应该进行新生儿筛查和预防措施，如出生后72小时内给予足量人血白蛋白。\n\n5. 注意感染传播，如病毒性肝炎、肝病等，这些疾病与α1抗胰蛋白酶缺乏性肝病有一定的关联。因此，在预防措施中应该注意避免感染传播。\n\n6. 对患者进行定期的随访和监测，以便及早发现问题并采取相应措施。\n\n总之，α1抗胰蛋白酶缺乏性肝病的预防措施需要从多个方面进行，包括避免婚前检查、提倡遗传咨询、提倡健康的生活方式、提倡母婴保健、注意感染传播以及对患者进行定期的随访和监测等。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 年龄相关性白内障的临床表现有些什么？\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 年龄相关性白内障是一种常见的眼部疾病，多随着年龄的增长而发病。其临床表现主要包括以下几个方面：\n\n1. 视力下降：随着年龄的增长，晶状体的弹性会降低，导致晶状体变浑浊，看远处的物体时，需要通过睫状肌的收缩来调节焦距，这样会导致视力的下降。\n\n2. 眼前有阴影：当晶状体变浑浊时，部分光线会被折射或反射，形成眼前有阴影的感觉。\n\n3. 视力模糊：随着晶状体逐渐变浑浊，看近处的物体时需要更长时间的调节，导致视力模糊。\n\n4. 眼睛干涩：随着年龄的增长，泪液分泌会减少，眼睛会感到干涩。\n\n5. 眼睛疼痛：晶状体变得浑浊时，眼睛容易感到疼痛或不适。\n\n6. 夜盲：年龄相关性白内障可能导致夜间视力下降，特别是在照明条件差的情况下。\n\n7. 虹膜睫状体炎：年龄相关性白内障有时会伴随虹膜睫状体炎，这是一种眼部疾病，会导致眼睛疼痛、红肿和视力下降。\n\n总之，年龄相关性白内障的临床表现主要包括视力下降、眼前有阴影、视力模糊、眼睛干涩、疼痛和夜盲等。如果您有这些症状，建议及时就医，以接受正确的治疗。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 分裂症认知功能障碍的推荐药有些什么\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 分裂症认知功能障碍是指分裂症患者在认知和记忆方面的问题，例如出现记忆力下降、注意力不集中、视幻觉等。针对分裂症认知功能障碍的推荐药物包括以下几种：\n\n1. 抗精神病药物：对于有幻觉、妄想等症状的患者，抗精神病药物是首选药物，如奥氮平、利培酮等。这些药物可以减少患者的精神症状，改善其认知功能。\n\n2. 抗抑郁药：分裂症患者常常出现抑郁症状，因此抗抑郁药也是常用的药物，如SSRI类药物，如氟西汀、帕罗西汀等。\n\n3. 抗精神病药物的配合使用：有些患者在服用抗精神病药物的同时，还可能出现认知功能障碍，这时候可以考虑配合使用记忆力和注意力的抗痴呆药物，如乙酰胆碱酯酶抑制剂、多奈哌齐等。\n\n4. 神经保护药物：神经保护药物可以保护神经元，减少神经元的损伤，如谷氨酸受体拮抗剂、γ-氨基丁酸受体拮抗剂等。\n\n需要注意的是，分裂症认知功能障碍的治疗需要根据患者的具体情况进行个体化治疗，最好在医生的指导下进行治疗。同时，治疗期间需要密切观察患者的症状变化，并及时调整治疗方案。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 健阳片成分或处方\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 健阳片是一种中药，其成分和处方可能因品牌和制剂而异。但是，根据常见的健阳片配方，其主要成分包括：\n\n1. 熟地黄：熟地黄具有滋阴补肾、益精血、强筋骨的功效，是健阳片中的主要成分之一。\n\n2. 当归：当归具有活血化瘀、调经止痛的功效，对健阳片中的补肾壮阳作用有一定帮助。\n\n3. 枸杞子：枸杞子具有滋阴补肾、明目、养肝益肝的功效，对健阳片中的补肾壮阳作用有一定帮助。\n\n4. 肉桂：肉桂具有补肾壮阳、温通经脉的功效，对健阳片中的补肾壮阳作用有一定帮助。\n\n5. 甘草：甘草具有清热解毒、润肺止咳的功效，对健阳片中的补肾壮阳作用有一定帮助。\n\n6. 苦涩剂：苦涩剂可以调节男性荷尔蒙水平，对健阳片中的补肾壮阳作用有一定帮助。\n\n需要注意的是，中药的使用和剂量需要根据具体病情和医生建议进行，最好在医生的指导下使用。同时，中药可能会有一定的副作用和不良反应，应在医生的指导下使用。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 输卵管粘连会复发吗\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 输卵管粘连是一种常见的女性不孕不育症状，其特点是输卵管变得僵硬、蠕动不灵活，导致精子与卵子结合困难。如果输卵管粘连得到有效治疗，通常不会复发。\n\n然而，如果治疗不彻底或患者不遵守治疗方案，输卵管粘连可能会再次恶化。例如，患者可能会出现不规律的月经、排卵障碍、痛经等症状，甚至会导致不孕不育。\n\n因此，对于输卵管粘连患者，建议及时就医并接受医生的治疗建议。同时，患者应积极配合医生的治疗，遵守治疗方案，定期进行检查和复查，以便及早发现问题并采取相应措施。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 术后限制深静脉栓塞的辅助治疗有？\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 术后限制深静脉栓塞的辅助治疗包括以下几个方面：\n\n1. 抗凝血治疗：抗凝血药物可以减少深静脉栓塞的发生，常用的抗凝血药物包括肝素、华法林、阿哌沙班等。\n\n2. 溶栓药物：溶栓药物可以溶解血栓，减轻深静脉栓塞的严重程度，常用的溶栓药物包括阿司匹林、替硝唑等。\n\n3. 物理治疗：物理治疗可以改善静脉血液循环，减轻深静脉栓塞的症状，如按摩、低频脉冲电磁波治疗等。\n\n4. 手术治疗：手术治疗可以彻底清除深静脉内的血栓，解除对静脉的压迫，改善静脉血液循环，减轻症状，如静脉栓塞清除术、结扎术等。\n\n5. 改变生活方式：患者应保持良好的作息习惯，避免长时间卧床，定期活动身体，保持大便通畅，避免下肢静脉曲张的发生。\n\n6. 营养支持：患者应多吃富含维生素C和纤维素的食物，如水果、蔬菜、全谷类等，增加血管的弹性。\n\n需要注意的是，术后限制深静脉栓塞需要综合治疗，包括药物治疗、物理治疗、手术治疗等，应根据患者的具体情况选择合适的治疗方法，并定期进行检查和复查，以免影响治疗效果。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" #QUIT\n"}]},{"cell_type":"markdown","source":"# 上面的base model 和下面的 peftmodel确实输出不一样","metadata":{}},{"cell_type":"markdown","source":"# 3. 加载 adapter","metadata":{}},{"cell_type":"code","source":"model = PeftModel.from_pretrained(model, peft_model_path)\n\n#下面这三行貌似可以不用\n#peft_model_dict = model.state_dict()\n#peft_model_dict.update(torch.load(f\"{peft_model_path}/adapter_model.bin\"))\n#model.load_state_dict(peft_model_dict,strict=False)\n\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-08-06T07:35:23.964449Z","iopub.execute_input":"2023-08-06T07:35:23.965241Z","iopub.status.idle":"2023-08-06T07:36:18.098943Z","shell.execute_reply.started":"2023-08-06T07:35:23.965206Z","shell.execute_reply":"2023-08-06T07:36:18.097899Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): ChatGLMForConditionalGeneration(\n      (transformer): ChatGLMModel(\n        (embedding): Embedding(\n          (word_embeddings): Embedding(65024, 4096)\n        )\n        (rotary_pos_emb): RotaryEmbedding()\n        (encoder): GLMTransformer(\n          (layers): ModuleList(\n            (0-27): 28 x GLMBlock(\n              (input_layernorm): RMSNorm()\n              (self_attention): SelfAttention(\n                (query_key_value): Linear4bit(\n                  in_features=4096, out_features=4608, bias=True\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4608, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n                (core_attention): CoreAttention(\n                  (attention_dropout): Dropout(p=0.0, inplace=False)\n                )\n                (dense): Linear4bit(\n                  in_features=4096, out_features=4096, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4096, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n              )\n              (post_attention_layernorm): RMSNorm()\n              (mlp): MLP(\n                (dense_h_to_4h): Linear4bit(\n                  in_features=4096, out_features=27392, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=27392, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n                (dense_4h_to_h): Linear4bit(\n                  in_features=13696, out_features=4096, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=13696, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4096, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n              )\n            )\n          )\n          (final_layernorm): RMSNorm()\n        )\n        (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# 4. infer stream_chat","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n\nprint(\"用户：你好，智能小助手\\n\")\nold_response = \"\"\nfor response, history in model.stream_chat(tokenizer, \"你好\", [], max_length=1024, \n                                           top_p=0.7, temperature=0.95,\n                                           num_beams=5):\n    print(response[len(old_response):], end=\"\")\n    old_response = response\nprint(end=\"\\r\")\n#print(1111,old_response)作为烽火智能助手，我遇到了以下故障现象，某地市某网络单盘占用槽位显示异常现场实际占用两个槽位。请给出类似案例以供参考。\n#print(\"ChatGLM2-6B：\\n\",response)\n#print(\"\\n------------------------------------------------\\n用户：\")\n#raise ValueError(123)\nline = input()\npast_key_values=None\nhistory =[]\n#prompt_format = \"[Round {}]\\n\\n问：{}\\n\\n答：\"\nwhile line:\n    if line.strip()==\"#QUIT\": # 输入#QUIT退出对话循环\n        break\n    #inst = \"我有关于烽火SSE的问题，请你作为烽火SSE智能助手，帮我回答下面的问题。\" #计算两个矩阵的乘积：\"\n    #line = PROMPT_TEMPLATE.format_map({'instruction': line})\n    if line.strip()==\"#CLEAR\" :  #清空历史对话\n        history=[]\n    #format_line = prompt_format.format(1,line)\n    #print(f\"【formatted line】={format_line}\")\n    #old_response = \"\"\n    beams =2\n    print(f\"num_beams={beams}\")\n    \n    for response, history  in model.stream_chat(tokenizer=tokenizer, \n                                               history=history ,\n                                               max_length=8192,\n                                               query=line,\n                                               top_p=0.95,temperature=0.65,\n                                               num_beams=beams,\n                                               return_past_key_values=False,\n                                               past_key_values=None):\n        #print(past_key_values) 不要打印 太长了\n        print(response[len(old_response):], end=\"\")\n        old_response = response\n    print(end=\"\\r\")\n    #print(f\"\\nold response=\\n{old_response}\")\n    #print(end=\"\\r\") # 量化后 num_beams只能=1 否则报错\n    #print(\"ChatGLM2-6B：\\n\", old_response.replace(\"\\\\n\",'\\n'))\n    print(\"\\n------------------------------------------------\\n用户：\")\n    line = input()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4.1 infer chat 确实能看到回答中包含了语料中的知识 但是又不完全一样","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\nhistory=[]\nprint(\"用户：你好，智能小助手\\n\")\nold_response = \"\"\nresponse, history  =  model.chat(tokenizer, \"你好\", history=history, max_length=8192, \n                                           top_p=0.9, temperature=0.75,\n                                           num_beams=1)\n    \nprint(response)\n#print(1111,old_response)作为烽火智能助手，我遇到了以下故障现象，某地市某网络单盘占用槽位显示异常现场实际占用两个槽位。请给出类似案例以供参考。\n#print(\"ChatGLM2-6B：\\n\",response)\n#print(\"\\n------------------------------------------------\\n用户：\")\n#raise ValueError(123)\nline = input()\npast_key_values=None\nhistory =[]\n#prompt_format = \"[Round {}]\\n\\n问：{}\\n\\n答：\"\nwhile line:\n    if line.strip()==\"#QUIT\": # 输入#QUIT退出对话循环\n        break\n    #inst = \"我有关于烽火SSE的问题，请你作为烽火SSE智能助手，帮我回答下面的问题。\" #计算两个矩阵的乘积：\"\n    #line = PROMPT_TEMPLATE.format_map({'instruction': line})\n    if line.strip()==\"#CLEAR\" :  #清空历史对话\n        history=[]\n    #format_line = prompt_format.format(1,line)\n    #print(f\"【formatted line】={format_line}\")\n    #old_response = \"\"\n    beams =1\n    #print(f\"num_beams={beams}\")\n    \n    response, history =  model.chat(tokenizer=tokenizer, \n                                               history=history ,\n                                               max_length=8192,\n                                               query=line,\n                                               top_p=0.95,temperature=0.85,\n                                               num_beams=1,\n                                               past_key_values=None)\n        #print(past_key_values) 不要打印 太长了\n        #print(response[len(old_response):], end=\"\")\n        #old_response = response\n    #print(end=\"\\r\")\n    #print(f\"\\nold response=\\n{old_response}\")\n    #print(end=\"\\r\") # 量化后 num_beams只能=1 否则报错\n    print(\"ChatGLM2-6B：\\n\", response.replace(\"\\\\n\",'\\n'))\n    print(\"\\n------------------------------------------------\\n用户：\")\n    line = input()","metadata":{"execution":{"iopub.status.busy":"2023-08-06T07:38:29.244095Z","iopub.execute_input":"2023-08-06T07:38:29.248593Z","iopub.status.idle":"2023-08-06T07:47:14.554669Z","shell.execute_reply.started":"2023-08-06T07:38:29.248535Z","shell.execute_reply":"2023-08-06T07:47:14.553642Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"用户：你好，智能小助手\n\n你好！今天我能为您提供什么帮助？\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" α1抗胰蛋白酶缺乏性肝病的预防措施有哪些？\n"},{"name":"stdout","text":"ChatGLM2-6B：\n α1抗胰蛋白酶缺乏性肝病是一种遗传性疾病，为常染色体隐性遗传，主要特点为肝脏损伤程度轻，但足月结扎过子的母乳喂养儿发生率较高。预防此病的方法包括：\n\n1. 婚姻登记：禁止近亲结婚。\n\n2. 产前诊断：孕妇产前诊断排除α1抗胰蛋白酶缺乏症。\n\n3. 母乳喂养：提倡母乳喂养。\n\n4. 禁烟酒：吸烟和饮酒可加重α1抗胰蛋白酶缺乏性肺气肿。\n\n5. 避免毒素：禁用阿司匹林、镇痛药、抗凝血药等。\n\n6. 增加运动：宜多作低强度、长期的有氧运动，如散步、慢跑、游泳、健身等。\n\n7. 维持正常体重：肥胖减轻机体内负担，预防肝内脂肪化。\n\n8. 治疗：出现黄疸、肝功异常时，应结合其它检查，在医生的指导下进行治疗。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 年龄相关性白内障的临床表现有些什么？\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 年龄相关性白内障临床表现有：\n\n1. 散在性。即使有自觉症状，往往也只有在晚期出现短暂而轻的自觉症状，一般视力常在正常半数或接近正常。\n\n2. 单眼性。由于虹膜向瞳孔靠拢，因此患眼患眼内高反光。\n\n3. 色觉异常。年龄性黄斑病变常伴有色觉异常，如黄色或紫色混浊或色彩不鲜明。合并有其他玻璃体病变时则出现其他异常。\n\n4. 细胞一蛋白质异常。虹膜后间隙细胞和蛋白质的密度增加，导致视网膜色素上皮脱离。\n\n5. 玻璃体混浊。大多数患者的玻璃体有不同程度的混浊，常见于40岁以上患者。\n\n6. 牵涉性瞳孔缩小。由于虹膜向瞳孔靠拢，瞳孔常缩小。\n\n7. 视网膜色素上皮脱离。常伴有黄斑病变。\n\n8. 失明。在晚期发展的全盲或接近全盲。\n\n需要注意的是，以上症状并不是每个患者都具备，个体差异很大，部分患者无症状。而临床表现也可以分为膜性化和晶体病变两种，根据是否伴发黄斑病变，分为膜性化型和晶体型。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 分裂症认知功能障碍的推荐药有些什么\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 文拉法辛\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 健阳片成分或处方\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 蜈蚣粉、熟地黄、甘草、蜂皇浆。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 糖尿病人为什么伤口不容易愈合\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 糖尿病人的伤口不容易愈合，有促进愈合的药物帮助会更好，我们可以补充多种维生素，特别是维生素C，维生素E，或胶原蛋白，每次2片，一日3次，这些维生素都具有促进愈合的作用，另外，糖尿病人伤口不易愈合还可以服用中药人参皂苷RH2改善伤口愈合，提高免疫力，防止细菌的感染。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 术后限制深静脉栓塞的辅助治疗有？\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 被动运动。抗凝溶栓。抗凝血药物。抗血小板药物。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 输卵管粘连会复发吗\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 输卵管粘连会复发的，因为输卵管粘连后的患者，发生异位妊娠的机会明显增加，而异位妊娠是引起输卵管粘连的常见原因，所以输卵管粘连后的患者，发生异位妊娠的风险明显增加，而且输卵管粘连后的患者，出现异位妊娠后，病发率较高，所以输卵管粘连后的患者，需要积极的治疗，包括清宫手术和输卵管造影等。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 面痛可能是什么疾病的症状\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 慢性蝶窦炎；慢性筛窦炎；老年人脑血栓形成；大头瘟；流行性乙型脑炎；钩端螺旋体病；异物伤；肾结石；消化不良；慢性胰腺炎；急性肾炎；急性胃肠炎；食物中毒；维生素A缺乏；维生素C缺乏；维生素E缺乏；癌症；糖尿病；心绞痛\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 喉梗阻的病因是什么\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 肺吸虫；气管外套管脱出或移位\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 新生儿喉喘鸣的治疗方式是什么\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 对症治疗；抗心律失常药物治疗；手术治疗\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" #QUIT\n"}]},{"cell_type":"markdown","source":"# history 格式数据微调","metadata":{}},{"cell_type":"code","source":"# ./data/sharegpt_multi_turn_data 目录不能有空白json文件\n!git pull --all --force \n#!pip install  -U git+https://github.com/huggingface/peft.git   # 20230717 peft==0.4.0正式发布了 不用调版本了推理完后再训练需要重新升级到0.4.0dev 所以有这句\n!deepspeed --include localhost:0,1  sft_multi_turn_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-history-sft-0805-v1 \\\n  --num_train_samples -1 \\\n  --num_eval_samples 2 \\\n  --train_data_path ./data/history \\\n  --eval_data_path  ./data/history    \\\n  --data_type history  \\\n  --max_length 1800 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 2  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  1.8e-5 \\\n  --num_train_epochs  40  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 上面8小时跑到loss=0.8  由于kaggle 一个session最多跑12小时 主动中断后下面继续 \n# 跑了8小时loss到0.01-0.3波动  推理效果不如上面那个loss到0.07的\n# 又跑了一个epcoh 500steps loss到了0.03  lr=1.8e-5  似乎lr为常数 不下降 效果就可以","metadata":{}},{"cell_type":"code","source":"ls output-history-sft-0806-v2/checkpoint-500","metadata":{"execution":{"iopub.status.busy":"2023-08-06T10:11:10.641776Z","iopub.execute_input":"2023-08-06T10:11:10.642167Z","iopub.status.idle":"2023-08-06T10:11:11.637375Z","shell.execute_reply.started":"2023-08-06T10:11:10.642126Z","shell.execute_reply":"2023-08-06T10:11:11.636133Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"README.md            optimizer.pt     scheduler.pt\nadapter_config.json  rng_state_0.pth  trainer_state.json\nadapter_model.bin    rng_state_1.pth  training_args.bin\n","output_type":"stream"}]},{"cell_type":"code","source":"# ./data/sharegpt_multi_turn_data 目录不能有空白json文件\n!git pull --all --force \n#!pip install  -U git+https://github.com/huggingface/peft.git   # 20230717 peft==0.4.0正式发布了 不用调版本了推理完后再训练需要重新升级到0.4.0dev 所以有这句\n!deepspeed --include localhost:0,1  sft_multi_turn_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-history-sft-0806-v2 \\\n  --resume_from_checkpoint /kaggle/working/chatGLM-6B-QLoRA/output-history-sft-0806-v1/checkpoint-2370 \\\n  --num_train_samples -1 \\\n  --num_eval_samples 2 \\\n  --train_data_path ./data/history \\\n  --eval_data_path  ./data/history    \\\n  --data_type history  \\\n  --max_length 1800 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 2  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  1.8e-5 \\\n  --num_train_epochs  40  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  history 数据sft后的推理","metadata":{}},{"cell_type":"markdown","source":"## 1. import libs","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/chatGLM-6B-QLoRA\n!git pull --all --force\n!ls\n!pip install -r requirements.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 推理 \nimport os\nimport argparse\nfrom typing import List, Dict, Optional\nfrom accelerate import init_empty_weights  # load an empty model,just structure , no real weight.\nimport bitsandbytes as bnb\nimport torch\nfrom glob import glob\nfrom loguru import logger\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    HfArgumentParser,\n    set_seed,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig \n)\nfrom peft import (\n    TaskType,\n    LoraConfig,\n    #AdaLoraConfig ,  #  提出自2020年 感觉和lora区别不大 而且和qlora有冲突 这里代码没有用到 \n                     #例子https://www.zhihu.com/question/596950521/answer/3109759716\n    get_peft_model,\n    set_peft_model_state_dict,\n    prepare_model_for_kbit_training\n)\nfrom peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\nfrom transformers.deepspeed import HfDeepSpeedConfig\nimport deepspeed\nimport json\nfrom itertools import chain","metadata":{"execution":{"iopub.status.busy":"2023-08-06T10:12:42.725796Z","iopub.execute_input":"2023-08-06T10:12:42.726235Z","iopub.status.idle":"2023-08-06T10:13:06.790052Z","shell.execute_reply.started":"2023-08-06T10:12:42.726200Z","shell.execute_reply":"2023-08-06T10:13:06.788853Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[2023-08-06 10:12:45,240] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\nCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel, PeftConfig\n\n\n# 有效peft_model_path = '/kaggle/working/chatGLM-6B-QLoRA/output-sft-qlora-ds-yunguan0712-aug-v3'\n\npeft_model_path= \"./output-history-sft-0806-v2/checkpoint-500\"\nconfig = PeftConfig.from_pretrained(peft_model_path)\nq_config = BitsAndBytesConfig(load_in_4bit=True,\n                              bnb_4bit_quant_type='nf4',\n                              bnb_4bit_use_double_quant=True,\n                              bnb_4bit_compute_dtype=torch.float16)\n# base_model加载时保持和train时完全一致的参数配置\nmodel = AutoModel.from_pretrained(config.base_model_name_or_path,\n                                       quantization_config=q_config,\n                                       trust_remote_code=True,\n                                       device_map='auto')\n\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-06T10:13:11.772194Z","iopub.execute_input":"2023-08-06T10:13:11.773075Z","iopub.status.idle":"2023-08-06T10:15:00.071642Z","shell.execute_reply.started":"2023-08-06T10:13:11.773038Z","shell.execute_reply":"2023-08-06T10:15:00.070479Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"284b4f9101b044698250673cb185cbf4"}},"metadata":{}}]},{"cell_type":"markdown","source":"# base model chat beams只能等于1 否则会报错 probability=inf or nan","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\nhistory=[]\nprint(\"用户：你好，智能小助手\\n\")\nold_response = \"\"\nresponse, history  =  model.chat(tokenizer, \"你好\", history=history, max_length=8192, \n                                           top_p=0.9, temperature=0.75,\n                                           num_beams=5)\n    \nprint(response)\n#print(1111,old_response)作为烽火智能助手，我遇到了以下故障现象，某地市某网络单盘占用槽位显示异常现场实际占用两个槽位。请给出类似案例以供参考。\n#print(\"ChatGLM2-6B：\\n\",response)\n#print(\"\\n------------------------------------------------\\n用户：\")\n#raise ValueError(123)\nline = input()\npast_key_values=None\nhistory =[]\n#prompt_format = \"[Round {}]\\n\\n问：{}\\n\\n答：\"\nwhile line:\n    if line.strip()==\"#QUIT\": # 输入#QUIT退出对话循环\n        break\n    #inst = \"我有关于烽火SSE的问题，请你作为烽火SSE智能助手，帮我回答下面的问题。\" #计算两个矩阵的乘积：\"\n    #line = PROMPT_TEMPLATE.format_map({'instruction': line})\n    if line.strip()==\"#CLEAR\" :  #清空历史对话\n        history=[]\n    #format_line = prompt_format.format(1,line)\n    #print(f\"【formatted line】={format_line}\")\n    #old_response = \"\"\n    beams =1\n    #print(f\"num_beams={beams}\")\n    \n    response, history =  model.chat(tokenizer=tokenizer, \n                                               history=history ,\n                                               max_length=1024,\n                                               query=line,\n                                               top_p=0.95,temperature=0.65,\n                                               num_beams=beams,\n                                               past_key_values=None)\n        #print(past_key_values) 不要打印 太长了\n        #print(response[len(old_response):], end=\"\")\n        #old_response = response\n    #print(end=\"\\r\")\n    #print(f\"\\nold response=\\n{old_response}\")\n    #print(end=\"\\r\") # 量化后 num_beams只能=1 否则报错\n    print(\"ChatGLM2-6B：\\n\", response.replace(\"\\\\n\",'\\n'))\n    print(\"\\n------------------------------------------------\\n用户：\")\n    line = input()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 加载adapter","metadata":{}},{"cell_type":"code","source":"model = PeftModel.from_pretrained(model, peft_model_path)\n\n#下面这三行貌似可以不用\n#peft_model_dict = model.state_dict()\n#peft_model_dict.update(torch.load(f\"{peft_model_path}/adapter_model.bin\"))\n#model.load_state_dict(peft_model_dict,strict=False)\n\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-08-06T10:15:18.989409Z","iopub.execute_input":"2023-08-06T10:15:18.989875Z","iopub.status.idle":"2023-08-06T10:16:21.332972Z","shell.execute_reply.started":"2023-08-06T10:15:18.989841Z","shell.execute_reply":"2023-08-06T10:16:21.331612Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): ChatGLMForConditionalGeneration(\n      (transformer): ChatGLMModel(\n        (embedding): Embedding(\n          (word_embeddings): Embedding(65024, 4096)\n        )\n        (rotary_pos_emb): RotaryEmbedding()\n        (encoder): GLMTransformer(\n          (layers): ModuleList(\n            (0-27): 28 x GLMBlock(\n              (input_layernorm): RMSNorm()\n              (self_attention): SelfAttention(\n                (query_key_value): Linear4bit(\n                  in_features=4096, out_features=4608, bias=True\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4608, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n                (core_attention): CoreAttention(\n                  (attention_dropout): Dropout(p=0.0, inplace=False)\n                )\n                (dense): Linear4bit(\n                  in_features=4096, out_features=4096, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4096, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n              )\n              (post_attention_layernorm): RMSNorm()\n              (mlp): MLP(\n                (dense_h_to_4h): Linear4bit(\n                  in_features=4096, out_features=27392, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=27392, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n                (dense_4h_to_h): Linear4bit(\n                  in_features=13696, out_features=4096, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=13696, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4096, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n              )\n            )\n          )\n          (final_layernorm): RMSNorm()\n        )\n        (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# 多次续训的history 数据的sft model  需要选一下 top_p 和 temperature 效果才能出来 确实也有","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\nhistory=[]\nprint(\"用户：你好，智能小助手\\n\")\nold_response = \"\"\nresponse, history  =  model.chat(tokenizer, \"你好\", history=history, max_length=256, \n                                           top_p=0.9, temperature=0.55,\n                                           num_beams=1)\n    \nprint(response)\n#print(1111,old_response)作为烽火智能助手，我遇到了以下故障现象，某地市某网络单盘占用槽位显示异常现场实际占用两个槽位。请给出类似案例以供参考。\n#print(\"ChatGLM2-6B：\\n\",response)\n#print(\"\\n------------------------------------------------\\n用户：\")\n#raise ValueError(123)\nline = input()\npast_key_values=None\nhistory =[]\n#prompt_format = \"[Round {}]\\n\\n问：{}\\n\\n答：\"\nwhile line:\n    if line.strip()==\"#QUIT\": # 输入#QUIT退出对话循环\n        break\n    #inst = \"我有关于烽火SSE的问题，请你作为烽火SSE智能助手，帮我回答下面的问题。\" #计算两个矩阵的乘积：\"\n    #line = PROMPT_TEMPLATE.format_map({'instruction': line})\n    if line.strip()==\"#CLEAR\" :  #清空历史对话\n        history=[]\n    #format_line = prompt_format.format(1,line)\n    #print(f\"【formatted line】={format_line}\")\n    #old_response = \"\"\n    beams =1\n    #print(f\"num_beams={beams}\")\n    \n    response, history =  model.chat(tokenizer=tokenizer, \n                                               history=history ,\n                                               max_length=8192,\n                                               query=line,\n                                               top_p=0.8,temperature=0.75,\n                                               num_beams=1,\n                                               past_key_values=None)\n        #print(past_key_values) 不要打印 太长了\n        #print(response[len(old_response):], end=\"\")\n        #old_response = response\n    #print(end=\"\\r\")\n    #print(f\"\\nold response=\\n{old_response}\")\n    #print(end=\"\\r\") # 量化后 num_beams只能=1 否则报错\n    print(\"ChatGLM2-6B：\\n\", response.replace(\"\\\\n\",'\\n'))\n    print(\"\\n------------------------------------------------\\n用户：\")\n    line = input()","metadata":{"execution":{"iopub.status.busy":"2023-08-06T10:25:23.941926Z","iopub.execute_input":"2023-08-06T10:25:23.942322Z","iopub.status.idle":"2023-08-06T10:30:39.497096Z","shell.execute_reply.started":"2023-08-06T10:25:23.942290Z","shell.execute_reply":"2023-08-06T10:30:39.495382Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"用户：你好，智能小助手\n\n答：你好！今天我能为您提供什么帮助？\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" α1抗胰蛋白酶缺乏性肝病的预防措施有哪些？\n"},{"name":"stdout","text":"ChatGLM2-6B：\n α1抗胰蛋白酶缺乏性肝病日常预防措施1.禁烟、限制饮酒：吸烟及饮酒可加重α1-抗胰蛋白酶缺乏性肝病症状，因此应禁止吸烟，不饮酒。2.防治并发症：有并发症的可短期住院治疗，包括：有创性手术、大量肝切除术、门脉高压治疗、胃肠减容术、腹腔镜治疗等。3.肝移植：肝移植是治疗Pizz终末期肝硬化的有效方法，应作为最后的选择。4.对症治疗：有症状者可应用保肝、降酶及支持治疗。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 年龄相关性白内障的临床表现有些什么？\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 年龄相关性白内障临床表现1.散光瞳孔：由于瞳孔后移，晶状体等大，因此出现散光瞳孔。2.远立体视：远立体视伴发白内障。3.虹膜睫状体炎：虹膜睫状体炎伴发白内障。4.糖尿病性白内障：糖尿病性白内障伴发散光。5.核黄素缺乏性白内障：多见于小儿，因缺乏维生素B12或叶酸引起。6.代谢性白内障：如甲状腺功能亢进、甲减、糖尿病、营养不良等引起。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 分裂症认知功能障碍的推荐药有些什么\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 抗精神病药对于分裂症认知功能障碍的治疗，可选用阿立哌唑或奥氮平。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 术后限制深静脉栓塞的辅助治疗有？\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 被动运动；抗凝溶栓\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 输卵管粘连会复发吗\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 根据所提供的信息来看，单纯输卵管在解剖学上不需要考虑其可逆性，因此我们暂不认为单纯输卵管粘连可逆。但是，在临床实践中输卵管粘连可逆，因此需要针对具体病情采取有效的治疗措施。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 面痛可能是什么疾病的症状\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 1、慢性蝶窦炎：慢性蝶窦炎的症状有头痛、面部痛、恶心和意识障碍。2、慢性筛窦炎：慢性筛窦炎的症状有头痛、面部痛和恶心。3、老年人脑血栓形成：老年人脑血栓形成的症状有头痛、恶心、呕吐、意识障碍和局部的癫痫样发作。4、大头瘟：大头瘟的症状有发热、头痛、鼻塞、咳嗽和口腔溃疡。5、新型冠状病毒感染：新型冠状病毒感染的症状有发热、干咳、疲劳和肌肉或关节疼痛。6、脑膜炎：脑膜炎的症状有发热、头痛、恶心、呕吐和意识障碍。7、脑血管意外：脑血管意外的症状有突然的头痛、恶心、呕吐、意识障碍和瘫痪。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 喉梗阻的病因是什么\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 1、气管阻塞：气管阻塞是引起喉梗阻最常见的原因，主要由于外呼吸道狭窄或阻塞，如鼻窦炎、扁桃体炎、息肉、肿瘤或异物等。2、声门重叠：声门重叠是次常见的病因，常见于男性。3、肺或胸廓疾病：肺或胸廓疾病，如肺炎、胸膜炎、心包炎、纵隔肿瘤或淋巴结压迫。4、食道：食道阻塞可导致严重的呼吸衰竭。5、四腔管：四腔管阻塞可导致声音嘶哑。6、肛门：肛门阻塞可导致大便失禁。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 新生儿喉喘鸣的治疗方式是什么\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 对症治疗；药物治疗\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 喉梗阻有哪些病因\n"},{"name":"stdout","text":"ChatGLM2-6B：\n 1.气管阻塞：气管阻塞是引起喉梗阻最常见的原因，主要由于外呼吸道狭窄或阻塞，如鼻窦炎、扁桃体炎、息肉、肿瘤或异物等。2.声门重叠：声门重叠是次常见的病因，常见于男性。3.肺或胸廓疾病：肺或胸廓疾病，如肺炎、胸膜炎、心包炎、纵隔肿瘤或淋巴结压迫。4.食道：食道阻塞可导致严重的呼吸衰竭。5.四腔管：四腔管阻塞可导致声音嘶哑。6.肛门：肛门阻塞可导致大便失禁。7.感染：病毒感染或细菌感染引起。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" #QUIT\n"}]},{"cell_type":"code","source":"α1抗胰蛋白酶缺乏性肝病的预防措施有哪些？\n年龄相关性白内障的临床表现有些什么？\n分裂症认知功能障碍的推荐药有些什么\n术后限制深静脉栓塞的辅助治疗有？\n输卵管粘连会复发吗\n面痛可能是什么疾病的症状\n喉梗阻的病因是什么\n新生儿喉喘鸣的治疗方式是什么","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}