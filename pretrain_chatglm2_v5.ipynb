{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-31T10:55:01.938162Z","iopub.execute_input":"2023-07-31T10:55:01.938630Z","iopub.status.idle":"2023-07-31T10:55:01.952222Z","shell.execute_reply.started":"2023-07-31T10:55:01.938599Z","shell.execute_reply":"2023-07-31T10:55:01.951032Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/test-txt/test0730.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install peft==0.4.0\n!pip install transformers==4.30.2\n!pip install datasets==2.12.0\n!pip install tqdm==4.65.0\n!pip install loguru==0.7.0\n!pip install fire==0.5.0\n!pip install bitsandbytes==0.39.0\n#wandb==0.15.3\n!pip install cpm_kernels==1.0.11\n!pip install accelerate==0.20.3\n!pip install sentencepiece==0.1.99\n!pip install deepspeed==0.9.5","metadata":{"execution":{"iopub.status.busy":"2023-07-31T10:57:12.732537Z","iopub.execute_input":"2023-07-31T10:57:12.733489Z","iopub.status.idle":"2023-07-31T10:59:46.398286Z","shell.execute_reply.started":"2023-07-31T10:57:12.733431Z","shell.execute_reply":"2023-07-31T10:59:46.397023Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting peft==0.4.0\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (4.30.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (0.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0) (2023.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\nInstalling collected packages: peft\nSuccessfully installed peft-0.4.0\nRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.30.2) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2) (2023.5.7)\nCollecting datasets==2.12.0\n  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (1.23.5)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (11.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (0.18.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (6.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.12.0) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.12.0) (1.16.0)\nInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed datasets-2.12.0\nRequirement already satisfied: tqdm==4.65.0 in /opt/conda/lib/python3.10/site-packages (4.65.0)\nCollecting loguru==0.7.0\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: loguru\nSuccessfully installed loguru-0.7.0\nCollecting fire==0.5.0\n  Downloading fire-0.5.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0) (2.3.0)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=f5aba8e123e4a43d4bb3428c896e73e2fab8f26adac52e66a7795a9fe971021c\n  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\nSuccessfully built fire\nInstalling collected packages: fire\nSuccessfully installed fire-0.5.0\nCollecting bitsandbytes==0.39.0\n  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.39.0\nCollecting cpm_kernels==1.0.11\n  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: cpm_kernels\nSuccessfully installed cpm_kernels-1.0.11\nRequirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.10/site-packages (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3) (6.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.20.3) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate==0.20.3) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate==0.20.3) (1.3.0)\nRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (0.1.99)\nCollecting deepspeed==0.9.5\n  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting hjson (from deepspeed==0.9.5)\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (1.11.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (9.0.0)\nRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (1.10.10)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (2.0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (4.65.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->deepspeed==0.9.5) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2.0.0->deepspeed==0.9.5) (4.6.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.9.5) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.9.5) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.9.5) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.9.5) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed==0.9.5) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed==0.9.5) (1.3.0)\nBuilding wheels for collected packages: deepspeed\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844545 sha256=aa0024ae51353786470bad1056913f081263470e9ee57594d014fe8e77abba55\n  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\nSuccessfully built deepspeed\nInstalling collected packages: hjson, deepspeed\nSuccessfully installed deepspeed-0.9.5 hjson-3.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport argparse\nfrom typing import List, Dict, Optional\nfrom accelerate import init_empty_weights  # load an empty model,just structure , no real weight.\nimport bitsandbytes as bnb\nimport torch\nfrom glob import glob\nfrom loguru import logger\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    HfArgumentParser,\n    set_seed,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig \n)\nfrom peft import (\n    TaskType,\n    LoraConfig,\n    #AdaLoraConfig ,  #  提出自2020年 感觉和lora区别不大 而且和qlora有冲突 这里代码没有用到 \n                     #例子https://www.zhihu.com/question/596950521/answer/3109759716\n    get_peft_model,\n    set_peft_model_state_dict,\n    prepare_model_for_kbit_training\n)\nfrom peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\nfrom transformers.deepspeed import HfDeepSpeedConfig\nimport deepspeed\nimport json\nfrom itertools import chain","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:01:01.133479Z","iopub.execute_input":"2023-07-31T11:01:01.134211Z","iopub.status.idle":"2023-07-31T11:01:01.141541Z","shell.execute_reply.started":"2023-07-31T11:01:01.134177Z","shell.execute_reply":"2023-07-31T11:01:01.140581Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:01:04.683574Z","iopub.execute_input":"2023-07-31T11:01:04.683950Z","iopub.status.idle":"2023-07-31T11:01:05.419050Z","shell.execute_reply.started":"2023-07-31T11:01:04.683918Z","shell.execute_reply":"2023-07-31T11:01:05.417751Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7568a9df55a43aaaca87a0831abf9c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)enization_chatglm.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ebe911647a440048e8cc468e9a17473"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa9ccadfce594329b4c9a365f8be2753"}},"metadata":{}}]},{"cell_type":"code","source":"string_list =\"我<unk></s>\"\n#string_list =\"11+2+3+4\"\n\n\ntokenized_res = tokenizer.encode(string_list,add_special_tokens=True,padding=True)\nlogger.info(tokenized_res)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:01:08.228148Z","iopub.execute_input":"2023-07-31T11:01:08.228532Z","iopub.status.idle":"2023-07-31T11:01:08.236131Z","shell.execute_reply.started":"2023-07-31T11:01:08.228483Z","shell.execute_reply":"2023-07-31T11:01:08.235151Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"\u001b[32m2023-07-31 11:01:08.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1m[64790, 64792, 34211, 31002, 3013, 6029, 30917, 30994]\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"数字开头的话 前面会加上三个 \"\" 空白\n而非数字开头 前面加2个空白 怪的很","metadata":{}},{"cell_type":"code","source":"tokenizer.batch_decode(tokenized_res,add_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:01:13.383444Z","iopub.execute_input":"2023-07-31T11:01:13.383858Z","iopub.status.idle":"2023-07-31T11:01:13.392887Z","shell.execute_reply.started":"2023-07-31T11:01:13.383825Z","shell.execute_reply":"2023-07-31T11:01:13.391775Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['', '', '我', '<', 'unk', '></', 's', '>']"},"metadata":{}}]},{"cell_type":"markdown","source":"tokenizer.pad_token_id =0  pad_token=<unk>\n    eos_token_id=2 eos_token=</s>","metadata":{"execution":{"iopub.status.busy":"2023-07-31T01:41:23.430793Z","iopub.execute_input":"2023-07-31T01:41:23.431223Z","iopub.status.idle":"2023-07-31T01:41:23.438775Z","shell.execute_reply.started":"2023-07-31T01:41:23.431160Z","shell.execute_reply":"2023-07-31T01:41:23.437720Z"}}},{"cell_type":"code","source":"logger.info(f\"tokenizer.pad_token_id={tokenizer.pad_token_id},tokenizer.pad_token={tokenizer.pad_token}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:01:16.539592Z","iopub.execute_input":"2023-07-31T11:01:16.539950Z","iopub.status.idle":"2023-07-31T11:01:16.546241Z","shell.execute_reply.started":"2023-07-31T11:01:16.539923Z","shell.execute_reply":"2023-07-31T11:01:16.545268Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"\u001b[32m2023-07-31 11:01:16.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mtokenizer.pad_token_id=0,tokenizer.pad_token=<unk>\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"logger.info(f\"tokenizer.pad_token_id={tokenizer.eos_token_id},tokenizer.pad_token={tokenizer.eos_token}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:01:19.594261Z","iopub.execute_input":"2023-07-31T11:01:19.594679Z","iopub.status.idle":"2023-07-31T11:01:19.601358Z","shell.execute_reply.started":"2023-07-31T11:01:19.594643Z","shell.execute_reply":"2023-07-31T11:01:19.600406Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"\u001b[32m2023-07-31 11:01:19.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mtokenizer.pad_token_id=2,tokenizer.pad_token=</s>\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"logger.info(f\"tokenizer.pad_token_id={tokenizer.bos_token_id},tokenizer.pad_token={tokenizer.bos_token}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:01:22.906285Z","iopub.execute_input":"2023-07-31T11:01:22.906669Z","iopub.status.idle":"2023-07-31T11:01:22.914228Z","shell.execute_reply.started":"2023-07-31T11:01:22.906638Z","shell.execute_reply":"2023-07-31T11:01:22.913187Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Using bos_token, but it is not set yet.\n\u001b[32m2023-07-31 11:01:22.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mtokenizer.pad_token_id=None,tokenizer.pad_token=None\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_dataset(data_path, tokenizer, global_args=None,max_samples=None):\n    \"\"\"读取本地包含json/jsonl文件的目录，将目录中所有文件作为dataset，并tokenize，shuffle，返回datasets.dataset\"\"\"\n    \n    if not (data_path is not None and os.path.exists(data_path)):\n        raise ValueError(\"data_path requires a directory pointing to  .txt files\")\n    \"\"\"https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py#L383\"\"\"\n    data_files_list = glob(f'{data_path}/**/*.txt', recursive=True) \n    logger.info(f\"data files: {', '.join(data_files_list)}\")\n          \n    extension = \"text\"\n    # sasmple_by=\"document\" 就是整个txt全部读取了\n    raw_datasets = load_dataset(\n        extension,\n        data_files=data_files_list,\n        sample_by=\"document\"\n    )\n#     logger.info(f\"在取样之前 data len ={len(data['train'])}\")\n#     if max_samples is not None and max_samples > 0:\n#             max_samples = min(len(data['train']), max_samples)  # \n#             data['train'] =  data['train'].select(range(max_samples))\n#     logger.info(f\"在取样之后 data len ={len(data['train'])}\")\n    \n#     column_names = data['train'].column_names  # remove_columns=column_names  ,remove all at once\n#     \"\"\"tokenize_func 中是单样本处理的写法 所以这里的batched只能设置为False\"\"\"\n#     logger.info(\"preprocessing dataset...\")\n#     dataset = data['train'].map(lambda example: tokenize_func(example, tokenizer, global_args),\n#                                 batched=False, \n#                                 remove_columns=column_names)\n#     dataset = dataset.shuffle(seed=global_args.seed)\n#     dataset = dataset.flatten_indices()\n    return raw_datasets","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:04:50.165158Z","iopub.execute_input":"2023-07-31T14:04:50.165619Z","iopub.status.idle":"2023-07-31T14:04:50.176925Z","shell.execute_reply.started":"2023-07-31T14:04:50.165573Z","shell.execute_reply":"2023-07-31T14:04:50.175681Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(examples,tokenizer):\n    # 这个函数必须返回dict \n    ## 不然raw_datasets.map(tokenize_function, ...)调用会提示报错\n    ## 要么就直接这么写  return tokenizer(examples[\"text\"])\n    return {\"input_ids\" :tokenizer(examples[\"text\"]).input_ids}","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:04:54.090024Z","iopub.execute_input":"2023-07-31T14:04:54.090461Z","iopub.status.idle":"2023-07-31T14:04:54.097487Z","shell.execute_reply.started":"2023-07-31T14:04:54.090425Z","shell.execute_reply":"2023-07-31T14:04:54.096326Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"block_size = 10","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:04:58.099095Z","iopub.execute_input":"2023-07-31T14:04:58.099530Z","iopub.status.idle":"2023-07-31T14:04:58.105562Z","shell.execute_reply.started":"2023-07-31T14:04:58.099469Z","shell.execute_reply":"2023-07-31T14:04:58.104376Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"def group_texts(examples,block_size):\n        # Concatenate all texts.\n        #concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n        total_length = len(examples[list(examples.keys())[0]])\n        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n        # customize this part to your needs.\n        print(f\"total_length={total_length}\")\n        \n        #ADD 20230731 原先shiming是去掉了最后的尾巴 但是我觉得可以保留 毕竟语料不多 每一部分都要用上\n        #if total_length >= block_size:\n        #    total_length = (total_length // block_size) * block_size\n        \n        # Split by chunks of max_len.\n        result = {\n            k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n            for k, t in examples.items()\n        }\n        #result[\"labels\"] = result[\"input_ids\"].copy()\n        return result","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:05:02.006246Z","iopub.execute_input":"2023-07-31T14:05:02.006682Z","iopub.status.idle":"2023-07-31T14:05:02.016789Z","shell.execute_reply.started":"2023-07-31T14:05:02.006645Z","shell.execute_reply":"2023-07-31T14:05:02.015579Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:05:05.266353Z","iopub.execute_input":"2023-07-31T14:05:05.267753Z","iopub.status.idle":"2023-07-31T14:05:05.531457Z","shell.execute_reply.started":"2023-07-31T14:05:05.267700Z","shell.execute_reply":"2023-07-31T14:05:05.530339Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"pad_token_id = tokenizer.pad_token_id\nprint(f\"pad_token_id = {pad_token_id}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:05:07.949958Z","iopub.execute_input":"2023-07-31T14:05:07.950378Z","iopub.status.idle":"2023-07-31T14:05:07.956106Z","shell.execute_reply.started":"2023-07-31T14:05:07.950341Z","shell.execute_reply":"2023-07-31T14:05:07.954937Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"pad_token_id = 0\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\ndef get_chained_lm_datasets(lm_datasets,\n                            pad_token_id: int ,\n                            global_args_max_length: int = 2048,\n                            num_samples=-1,\n                            tokenizer=None):\n    \"做padding和根据global.max_length截取\"\n    input_ids_list = list(chain(*lm_datasets['input_ids']))\n    # padding  with max_len_of_samples\n    max_len_ids = max([len(input_ids) for input_ids in input_ids_list] ) \n    print(f\"样本中ids 最大长度 max_len_ids={max_len_ids}\")\n    print(f\"全局训练参数的global_args_max_length={global_args_max_length}\")\n    print(f\"cutoff to len = {min(global_args_max_length,max_len_ids)}\")\n    \n    #截断\n    new_input_ids=[]\n    for ids in input_ids_list :\n        print(f\"len_sample_ids={len(ids)}\")\n        pad_len = max_len_ids - len(ids)\n        ids += [pad_token_id]* pad_len\n        # 截取 根据命令行输入的max len和 ids_list中最大长度 两值做比较取较小的来截取前面N个\n        ids =ids[: min(global_args_max_length,max_len_ids)]\n        new_input_ids.append(ids)\n    # 抽样\n    random_chosen_samples=new_input_ids\n    print(f\"验证查看 shuffle之前 第一个样本\\n{tokenizer.decode(random_chosen_samples[0])}\")\n    print(f\"验证查看 shuffle之前 最后一个样本\\n{tokenizer.decode(random_chosen_samples[-1])}\")\n    random.shuffle(new_input_ids)\n    if num_samples > -1: # num_samples  一般只是debug的时候取少量样本测试 ；\n        #random.sample 不重复抽样\n        random_chosen_samples = random.sample(new_input_ids,k=num_samples)\n    print(f\"样本数量={len(random_chosen_samples)}\")\n    #return {\"input_ids\":random_chosen_samples  ,\"labels\":random_chosen_samples.copy()}\n    return [{\"input_ids\": item ,\"labels\":item.copy() } for item in random_chosen_samples]","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:05:13.171433Z","iopub.execute_input":"2023-07-31T14:05:13.171883Z","iopub.status.idle":"2023-07-31T14:05:13.183137Z","shell.execute_reply.started":"2023-07-31T14:05:13.171848Z","shell.execute_reply":"2023-07-31T14:05:13.181919Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:\n    if not isinstance(features[0], Mapping):\n        features = [vars(f) for f in features]\n    first = features[0]\n    batch = {}\n\n    # Special handling for labels.\n    # Ensure that tensor is created with the correct type\n    if \"label\" in first and first[\"label\"] is not None:\n        label = first[\"label\"].item() if isinstance(first[\"label\"], torch.Tensor) else first[\"label\"]\n        dtype = torch.long if isinstance(label, int) else torch.float\n        batch[\"labels\"] = torch.tensor([f[\"label\"] for f in features], dtype=dtype)\n    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n        if isinstance(first[\"label_ids\"], torch.Tensor):\n            batch[\"labels\"] = torch.stack([f[\"label_ids\"] for f in features])\n        else:\n            dtype = torch.long if type(first[\"label_ids\"][0]) is int else torch.float\n            batch[\"labels\"] = torch.tensor([f[\"label_ids\"] for f in features], dtype=dtype)\n\n    # Handling of all other possible keys.\n    # Again, we will use the first element to figure out which key/values are not None for this model.\n    try:\n        for k, v in first.items():\n            if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n                if isinstance(v, torch.Tensor):\n                    batch[k] = torch.stack([f[k] for f in features])\n                elif isinstance(v, np.ndarray):\n                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))\n                else:\n                    batch[k] = torch.tensor([f[k] for f in features])\n    except ValueError:  # quick fix by simply take the first example\n        for k, v in first.items():\n            if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n                if isinstance(v, torch.Tensor):\n                    batch[k] = torch.stack([features[0][k]] * len(features))\n                elif isinstance(v, np.ndarray):\n                    batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))\n                else:\n                    batch[k] = torch.tensor([features[0][k]] * len(features))\n    #print(\"打印fault_tolerance_data_collator处理后的信息\")\n    #print(f\"batch.keys()={batch.keys()},len_batch_key_labels={len(batch['labels'])}    ,len_batch_key_k={len(batch[k])}\")\n    return batch","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:38:11.820321Z","iopub.execute_input":"2023-07-31T11:38:11.820722Z","iopub.status.idle":"2023-07-31T11:38:11.837468Z","shell.execute_reply.started":"2023-07-31T11:38:11.820689Z","shell.execute_reply":"2023-07-31T11:38:11.836108Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"a=[1,2,3]\nrandom.sample(a,k=2)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:38:15.694975Z","iopub.execute_input":"2023-07-31T11:38:15.695350Z","iopub.status.idle":"2023-07-31T11:38:15.703949Z","shell.execute_reply.started":"2023-07-31T11:38:15.695318Z","shell.execute_reply":"2023-07-31T11:38:15.701863Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"[3, 2]"},"metadata":{}}]},{"cell_type":"code","source":"# 是两段文本\nraw_datasets = get_dataset(data_path=\"/kaggle/working/test\",tokenizer=tokenizer,max_samples=10000)\nprint(raw_datasets)\nprint(raw_datasets['train']['text'])","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:05:20.252637Z","iopub.execute_input":"2023-07-31T14:05:20.253040Z","iopub.status.idle":"2023-07-31T14:05:20.522374Z","shell.execute_reply.started":"2023-07-31T14:05:20.253008Z","shell.execute_reply":"2023-07-31T14:05:20.521203Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stderr","text":"\u001b[32m2023-07-31 14:05:20.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_dataset\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mdata files: /kaggle/working/test/test/test0730.txt, /kaggle/working/test/test/test0730_2.txt\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f3259d560b944d99dd4c9bf6f50affa"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 2\n    })\n})\n[\"1+2+3+466666 66666 hgsg sgs hshshs hsfh shsh sh dh fhh\\n4+3+2=1                     afghu wfsvjoer n gw jgohuth4odjf ajaddpjfwon sdjnfslf  \\n我很优秀  你呢？我不知道啊，哈哈。。。 vsoro nvlknlbh h rwlj flvnskd gb gd \\n吃放睡觉打豆豆哦，【】[];'lvsvd a  gg  \\\\n brh5p bph owgwk rwf s59jbg\\nhd g  ghr5 eg r hsghrwgrwhhwh hg  wfsvjoer\\ng shth h j efwe gb\\n\\nef g h hsr \\n\\n\\nge whreh \\n hgs ghhth\", \"1+2+3+466666 66666 hgsg sgs hshshs hsfh shsh sh dh fhh\\n4+3+2=1                     afghu wfsvjoer n gw jgohuth4odjf ajaddpjfwon sdjnfslf  \\n我很优秀  你呢？我不知道啊，哈哈。。。 vsoro nvlknlbh h rwlj flvnskd gb gd \\n吃放睡觉打豆豆哦，【】[];'lvsvd a  gg  \\\\n brh5p bph owgwk rwf s59jbg\\nhd g  ghr5 eg r hsghrwgrwhhwh hg  wfsvjoer\\ng shth h j efwe gb\\n\\nef g h hsr \\n\\n\\nge whreh \\n hgs ghhth\"]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized_datasets = raw_datasets.map(\n                lambda examples:tokenize_function(examples,tokenizer),\n                batched=True,\n                num_proc=2,#data_args.preprocessing_num_workers,\n                remove_columns=['text'],#column_names,\n                #load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on dataset\",\n            )\nprint(tokenized_datasets)\nprint(tokenized_datasets['train']['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:05:28.258397Z","iopub.execute_input":"2023-07-31T14:05:28.259431Z","iopub.status.idle":"2023-07-31T14:05:28.286670Z","shell.execute_reply.started":"2023-07-31T14:05:28.259395Z","shell.execute_reply":"2023-07-31T14:05:28.285590Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids'],\n        num_rows: 2\n    })\n})\n[[64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972, 30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978, 30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917, 299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578, 13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296, 10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317, 30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953, 2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926, 5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514, 42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920, 16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933, 30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058, 54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995, 4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265, 30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927, 30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540, 13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299, 10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265, 30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299, 467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317, 299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366, 263, 30919, 30910, 13, 299, 6196, 317, 12578, 398], [64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972, 30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978, 30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917, 299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578, 13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296, 10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317, 30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953, 2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926, 5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514, 42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920, 16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933, 30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058, 54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995, 4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265, 30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927, 30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540, 13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299, 10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265, 30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299, 467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317, 299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366, 263, 30919, 30910, 13, 299, 6196, 317, 12578, 398]]\n","output_type":"stream"}]},{"cell_type":"code","source":"#raw_data['train']['text']","metadata":{"execution":{"iopub.status.busy":"2023-07-31T04:30:50.188040Z","iopub.execute_input":"2023-07-31T04:30:50.188504Z","iopub.status.idle":"2023-07-31T04:30:50.193872Z","shell.execute_reply.started":"2023-07-31T04:30:50.188465Z","shell.execute_reply":"2023-07-31T04:30:50.192680Z"},"trusted":true},"execution_count":243,"outputs":[]},{"cell_type":"code","source":"block_size=50","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:06:26.542621Z","iopub.execute_input":"2023-07-31T14:06:26.543257Z","iopub.status.idle":"2023-07-31T14:06:26.548926Z","shell.execute_reply.started":"2023-07-31T14:06:26.543209Z","shell.execute_reply":"2023-07-31T14:06:26.547903Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"lm_datasets = tokenized_datasets['train'].map(\n                lambda examples :group_texts(examples ,block_size) ,\n                batched=False,\n                num_proc=2,#data_args.preprocessing_num_workers,\n                #load_from_cache_file=not data_args.overwrite_cache,\n                desc=f\"Grouping texts in chunks of {block_size}\",\n                #remove_columns =['attention_mask', 'position_ids',]\n            )\nprint(lm_datasets)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:06:29.816465Z","iopub.execute_input":"2023-07-31T14:06:29.816973Z","iopub.status.idle":"2023-07-31T14:06:30.218954Z","shell.execute_reply.started":"2023-07-31T14:06:29.816931Z","shell.execute_reply":"2023-07-31T14:06:30.216996Z"},"trusted":true},"execution_count":129,"outputs":[{"output_type":"display_data","data":{"text/plain":"Grouping texts in chunks of 50 (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"total_length=219total_length=219\n\nDataset({\n    features: ['input_ids'],\n    num_rows: 2\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_train = get_chained_lm_datasets(lm_datasets,\n                        pad_token_id=tokenizer.pad_token_id,\n                        global_args_max_length=50000,\n                         num_samples=-1,\n                        tokenizer = tokenizer)\nprint(dataset_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:06:33.495448Z","iopub.execute_input":"2023-07-31T14:06:33.496730Z","iopub.status.idle":"2023-07-31T14:06:33.509114Z","shell.execute_reply.started":"2023-07-31T14:06:33.496685Z","shell.execute_reply":"2023-07-31T14:06:33.507814Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stdout","text":"样本中ids 最大长度 max_len_ids=50\n全局训练参数的global_args_max_length=50000\ncutoff to len = 50\nlen_sample_ids=50\nlen_sample_ids=50\nlen_sample_ids=50\nlen_sample_ids=50\nlen_sample_ids=19\nlen_sample_ids=50\nlen_sample_ids=50\nlen_sample_ids=50\nlen_sample_ids=50\nlen_sample_ids=19\nshuffle之前 第一个样本 =1+2+3+466666 66666 hgsg sgs hshshs hsfh shsh sh dh fhh\n4+3+2=1                    \nshuffle之前 最后一个样本 =h hsr \n\n\nge whreh \n hgs ghhth\n样本数量=10\n[{'input_ids': [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366, 263, 30919, 30910, 13, 299, 6196, 317, 12578, 398, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366, 263, 30919, 30910, 13, 299, 6196, 317, 12578, 398, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'input_ids': [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317, 30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953, 2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926, 5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514, 42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920], 'labels': [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317, 30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953, 2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926, 5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514, 42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920]}, {'input_ids': [64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972, 30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978, 30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917, 299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578, 13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296], 'labels': [64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972, 30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978, 30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917, 299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578, 13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296]}, {'input_ids': [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366, 263, 30919, 30910, 13, 299, 6196, 317, 12578, 398, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366, 263, 30919, 30910, 13, 299, 6196, 317, 12578, 398, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'input_ids': [64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972, 30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978, 30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917, 299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578, 13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296], 'labels': [64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972, 30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978, 30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917, 299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578, 13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296]}, {'input_ids': [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933, 30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058, 54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995, 4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265, 30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927], 'labels': [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933, 30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058, 54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995, 4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265, 30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927]}, {'input_ids': [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540, 13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299, 10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265, 30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299, 467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317], 'labels': [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540, 13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299, 10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265, 30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299, 467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317]}, {'input_ids': [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317, 30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953, 2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926, 5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514, 42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920], 'labels': [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317, 30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953, 2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926, 5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514, 42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920]}, {'input_ids': [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933, 30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058, 54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995, 4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265, 30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927], 'labels': [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933, 30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058, 54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995, 4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265, 30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927]}, {'input_ids': [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540, 13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299, 10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265, 30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299, 467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317], 'labels': [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540, 13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299, 10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265, 30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299, 467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317]}]\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_eval = get_chained_lm_datasets(lm_datasets,\n                        pad_token_id=tokenizer.pad_token_id,\n                        global_args_max_length=5,\n                                 num_samples=2)\nprint(dataset_eval)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:32:39.085970Z","iopub.execute_input":"2023-07-31T11:32:39.087034Z","iopub.status.idle":"2023-07-31T11:32:39.094084Z","shell.execute_reply.started":"2023-07-31T11:32:39.086998Z","shell.execute_reply":"2023-07-31T11:32:39.092447Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"样本中ids 最大长度 max_len_ids=219\n全局训练参数的global_args_max_length=5\ncutoff to len = 5\n样本数量=2\n[{'input_ids': [64790, 64792, 30910, 30939, 31011], 'labels': [64790, 64792, 30910, 30939, 31011]}, {'input_ids': [64790, 64792, 30910, 30939, 31011], 'labels': [64790, 64792, 30910, 30939, 31011]}]\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_datset_for_pretrain(data_path, tokenizer, block_size=10,global_args_max_length=0,max_samples=0):\n    \"\"\"读取本地包含json/jsonl文件的目录，将目录中所有文件作为dataset，并tokenize，shuffle，返回datasets.dataset\"\"\"\n    print(123)\n    if not (data_path is not None and os.path.exists(data_path)):\n        raise ValueError(\"data_path requires a directory pointing to  .txt files\")\n    \"\"\"https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py#L383\"\"\"\n    data_files_list = glob(f'{data_path}/**/*.txt', recursive=True) \n    logger.info(f\"data files: {', '.join(data_files_list)}\")\n          \n    extension = \"text\"\n    # 读取全部txt文档 有N篇文档 那raw_datasets这个list就包含N个篇章级别的文本结果\n    #sasmple_by=\"document\" 就是将一个txt当做一个sample全部读取\n    raw_datasets = load_dataset(\n        extension,\n        data_files=data_files_list,\n        sample_by=\"document\"\n    )\n    # 对全部N个文档str 做分词 分词结果也是N个长list 每个list都含有input_ids这个\n    tokenized_datasets = raw_datasets.map(\n                lambda examples : tokenize_function(examples ,tokenizer),\n                batched=True,\n                num_proc=2,#data_args.preprocessing_num_workers,\n                remove_columns=['text'],#column_names,\n                #load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on dataset\",\n            )\n    # N个长list在这一步被按照block_size切成小段 原先是[长1],[长2] 现在仍然为2元素 但是每个里面都是小段[[短1-1],[短1-2]] ,[[短2-1],[短2-2]]\n    lm_datasets = tokenized_datasets['train'].map(\n                lambda examples :group_texts(examples , block_size),\n                batched=False,\n                num_proc=2,#data_args.preprocessing_num_workers,\n                #load_from_cache_file=not data_args.overwrite_cache,\n                #desc=f\"Grouping texts in chunks of {block_size}\",\n                #remove_columns =['attention_mask', 'position_ids',]\n            )\n    dataset_final = get_chained_lm_datasets(lm_datasets,\n                        pad_token_id=tokenizer.pad_token_id,\n                        global_args_max_length=global_args_max_length,\n                        num_samples=max_samples,\n                        tokenizer = tokenizer)\n#     logger.info(f\"在取样之前 data len ={len(data['train'])}\")\n#     if max_samples is not None and max_samples > 0:\n#             max_samples = min(len(data['train']), max_samples)  # \n#             data['train'] =  data['train'].select(range(max_samples))\n#     logger.info(f\"在取样之后 data len ={len(data['train'])}\")\n    \n#     column_names = data['train'].column_names  # remove_columns=column_names  ,remove all at once\n#     \"\"\"tokenize_func 中是单样本处理的写法 所以这里的batched只能设置为False\"\"\"\n#     logger.info(\"preprocessing dataset...\")\n#     dataset = data['train'].map(lambda example: tokenize_func(example, tokenizer, global_args),\n#                                 batched=False, \n#                                 remove_columns=column_names)\n#     dataset = dataset.shuffle(seed=global_args.seed)\n#     dataset = dataset.flatten_indices()\n    return dataset_final","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:08:33.681211Z","iopub.execute_input":"2023-07-31T11:08:33.681608Z","iopub.status.idle":"2023-07-31T11:08:33.694435Z","shell.execute_reply.started":"2023-07-31T11:08:33.681575Z","shell.execute_reply":"2023-07-31T11:08:33.693559Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train_ds = get_datset_for_pretrain(data_path=\"/kaggle/working/test\",\n                                  tokenizer =  AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True),\n                                  block_size = 10,\n                                  global_args_max_length=6,\n                                  max_samples=-1)\nprint(train_ds)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:08:57.615461Z","iopub.execute_input":"2023-07-31T11:08:57.615871Z","iopub.status.idle":"2023-07-31T11:08:59.048973Z","shell.execute_reply.started":"2023-07-31T11:08:57.615840Z","shell.execute_reply":"2023-07-31T11:08:59.047973Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"\u001b[32m2023-07-31 11:08:57.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset_for_pretrain\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mdata files: /kaggle/working/test/test/test0730.txt, /kaggle/working/test/test/test0730_2.txt\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"123\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8741e7b022f441c9a974ea570cfb23dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"total_length=219total_length=219\n\n样本中ids 最大长度 max_len_ids=10\n全局训练参数的global_args_max_length=6\ncutoff to len = 6\n样本数量=42\n[{'input_ids': [13, 15939, 317, 265, 30927, 5323], 'labels': [13, 15939, 317, 265, 30927, 5323]}, {'input_ids': [30929, 30934, 416, 30929, 30926, 268], 'labels': [30929, 30934, 416, 30929, 30926, 268]}, {'input_ids': [30974, 30916, 822, 30919, 30970, 30925], 'labels': [30974, 30916, 822, 30919, 30970, 30925]}, {'input_ids': [2065, 30925, 30965, 30926, 23347, 268], 'labels': [2065, 30925, 30965, 30926, 23347, 268]}, {'input_ids': [10789, 30927, 11408, 271, 9563, 30933], 'labels': [10789, 30927, 11408, 271, 9563, 30933]}, {'input_ids': [30929, 9563, 30933, 5282, 266, 13], 'labels': [30929, 9563, 30933, 5282, 266, 13]}, {'input_ids': [10038, 5323, 30929, 689, 2277, 30919], 'labels': [10038, 5323, 30929, 689, 2277, 30919]}, {'input_ids': [54804, 35228, 54819, 55720, 55720, 56645], 'labels': [54804, 35228, 54819, 55720, 55720, 56645]}, {'input_ids': [2065, 30925, 30965, 30926, 23347, 268], 'labels': [2065, 30925, 30965, 30926, 23347, 268]}, {'input_ids': [30978, 299, 6196, 30927, 268, 6196], 'labels': [30978, 299, 6196, 30927, 268, 6196]}, {'input_ids': [30929, 30934, 416, 30929, 30926, 268], 'labels': [30929, 30934, 416, 30929, 30926, 268]}, {'input_ids': [299, 299, 30917, 30918, 30910, 13], 'labels': [299, 299, 30917, 30918, 30910, 13]}, {'input_ids': [30916, 2823, 30921, 317, 30931, 317], 'labels': [30916, 2823, 30921, 317, 30931, 317]}, {'input_ids': [16462, 26604, 30919, 299, 416, 30929], 'labels': [16462, 26604, 30919, 299, 416, 30929]}, {'input_ids': [30978, 299, 6196, 30927, 268, 6196], 'labels': [30978, 299, 6196, 30927, 268, 6196]}, {'input_ids': [30978, 30978, 30978, 30978, 30978, 30910], 'labels': [30978, 30978, 30978, 30978, 30978, 30910]}, {'input_ids': [64790, 64792, 30910, 30939, 31011, 30943], 'labels': [64790, 64792, 30910, 30939, 31011, 30943]}, {'input_ids': [13, 30972, 31011, 30966, 31011, 30943], 'labels': [13, 30972, 31011, 30966, 31011, 30943]}, {'input_ids': [467, 315, 30926, 734, 317, 30931], 'labels': [467, 315, 30926, 734, 317, 30931]}, {'input_ids': [10038, 5323, 30929, 689, 2277, 30919], 'labels': [10038, 5323, 30929, 689, 2277, 30919]}, {'input_ids': [299, 10240, 30919, 438, 1480, 438], 'labels': [299, 10240, 30919, 438, 1480, 438]}, {'input_ids': [13, 30972, 31011, 30966, 31011, 30943], 'labels': [13, 30972, 31011, 30966, 31011, 30943]}, {'input_ids': [13, 15939, 317, 265, 30927, 5323], 'labels': [13, 15939, 317, 265, 30927, 5323]}, {'input_ids': [30916, 2823, 30921, 317, 30931, 317], 'labels': [30916, 2823, 30921, 317, 30931, 317]}, {'input_ids': [42660, 55674, 31123, 34380, 39507, 5088], 'labels': [42660, 55674, 31123, 34380, 39507, 5088]}, {'input_ids': [467, 315, 30926, 734, 317, 30931], 'labels': [467, 315, 30926, 734, 317, 30931]}, {'input_ids': [30929, 9563, 30933, 5282, 266, 13], 'labels': [30929, 9563, 30933, 5282, 266, 13]}, {'input_ids': [30929, 467, 30927, 1063, 1056, 30972], 'labels': [30929, 467, 30927, 1063, 1056, 30972]}, {'input_ids': [16462, 26604, 30919, 299, 416, 30929], 'labels': [16462, 26604, 30919, 299, 416, 30929]}, {'input_ids': [42660, 55674, 31123, 34380, 39507, 5088], 'labels': [42660, 55674, 31123, 34380, 39507, 5088]}, {'input_ids': [64790, 64792, 30910, 30939, 31011, 30943], 'labels': [64790, 64792, 30910, 30939, 31011, 30943]}, {'input_ids': [54804, 35228, 54819, 55720, 55720, 56645], 'labels': [54804, 35228, 54819, 55720, 55720, 56645]}, {'input_ids': [10789, 30927, 11408, 271, 9563, 30933], 'labels': [10789, 30927, 11408, 271, 9563, 30933]}, {'input_ids': [5239, 30926, 265, 13, 36545, 31994], 'labels': [5239, 30926, 265, 13, 36545, 31994]}, {'input_ids': [4571, 30953, 30920, 30933, 13532, 30921], 'labels': [4571, 30953, 30920, 30933, 13532, 30921]}, {'input_ids': [30978, 30978, 30978, 30978, 30978, 30910], 'labels': [30978, 30978, 30978, 30978, 30978, 30910]}, {'input_ids': [30974, 30916, 822, 30919, 30970, 30925], 'labels': [30974, 30916, 822, 30919, 30970, 30925]}, {'input_ids': [299, 299, 30917, 30918, 30910, 13], 'labels': [299, 299, 30917, 30918, 30910, 13]}, {'input_ids': [5239, 30926, 265, 13, 36545, 31994], 'labels': [5239, 30926, 265, 13, 36545, 31994]}, {'input_ids': [4571, 30953, 30920, 30933, 13532, 30921], 'labels': [4571, 30953, 30920, 30933, 13532, 30921]}, {'input_ids': [299, 10240, 30919, 438, 1480, 438], 'labels': [299, 10240, 30919, 438, 1480, 438]}, {'input_ids': [30929, 467, 30927, 1063, 1056, 30972], 'labels': [30929, 467, 30927, 1063, 1056, 30972]}]\n","output_type":"stream"}]},{"cell_type":"code","source":"eval_ds = get_datset_for_pretrain(data_path=\"/kaggle/working/test\",\n                                  tokenizer =  AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True),\n                                  block_size = 10,\n                                  global_args_max_length=6,\n                                  max_samples=5)\nprint(eval_ds)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:09:06.648999Z","iopub.execute_input":"2023-07-31T11:09:06.650002Z","iopub.status.idle":"2023-07-31T11:09:07.033138Z","shell.execute_reply.started":"2023-07-31T11:09:06.649963Z","shell.execute_reply":"2023-07-31T11:09:07.032082Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"\u001b[32m2023-07-31 11:09:06.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset_for_pretrain\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mdata files: /kaggle/working/test/test/test0730.txt, /kaggle/working/test/test/test0730_2.txt\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"123\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a77d920f19104606960855bd30c560d9"}},"metadata":{}},{"name":"stdout","text":"样本中ids 最大长度 max_len_ids=10\n全局训练参数的global_args_max_length=6\ncutoff to len = 6\n样本数量=5\n[{'input_ids': [42660, 55674, 31123, 34380, 39507, 5088], 'labels': [42660, 55674, 31123, 34380, 39507, 5088]}, {'input_ids': [299, 10240, 30919, 438, 1480, 438], 'labels': [299, 10240, 30919, 438, 1480, 438]}, {'input_ids': [10789, 30927, 11408, 271, 9563, 30933], 'labels': [10789, 30927, 11408, 271, 9563, 30933]}, {'input_ids': [30929, 467, 30927, 1063, 1056, 30972], 'labels': [30929, 467, 30927, 1063, 1056, 30972]}, {'input_ids': [13, 30972, 31011, 30966, 31011, 30943], 'labels': [13, 30972, 31011, 30966, 31011, 30943]}]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport os\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom itertools import chain\nfrom typing import Optional, List, Dict, Any, Mapping\ndef fault_tolerance_data_collator(features: List) -> Dict[str, Any]:\n    if not isinstance(features[0], Mapping):\n        features = [vars(f) for f in features]\n    first = features[0]\n    batch = {}\n\n    # Special handling for labels.\n    # Ensure that tensor is created with the correct type\n    if \"label\" in first and first[\"label\"] is not None:\n        label = first[\"label\"].item() if isinstance(first[\"label\"], torch.Tensor) else first[\"label\"]\n        dtype = torch.long if isinstance(label, int) else torch.float\n        batch[\"labels\"] = torch.tensor([f[\"label\"] for f in features], dtype=dtype)\n    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n        if isinstance(first[\"label_ids\"], torch.Tensor):\n            batch[\"labels\"] = torch.stack([f[\"label_ids\"] for f in features])\n        else:\n            dtype = torch.long if type(first[\"label_ids\"][0]) is int else torch.float\n            batch[\"labels\"] = torch.tensor([f[\"label_ids\"] for f in features], dtype=dtype)\n\n    # Handling of all other possible keys.\n    # Again, we will use the first element to figure out which key/values are not None for this model.\n    try:\n        for k, v in first.items():\n            if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n                if isinstance(v, torch.Tensor):\n                    batch[k] = torch.stack([f[k] for f in features])\n                elif isinstance(v, np.ndarray):\n                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))\n                else:\n                    batch[k] = torch.tensor([f[k] for f in features])\n    except ValueError:  # quick fix by simply take the first example\n        for k, v in first.items():\n            if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n                if isinstance(v, torch.Tensor):\n                    batch[k] = torch.stack([features[0][k]] * len(features))\n                elif isinstance(v, np.ndarray):\n                    batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))\n                else:\n                    batch[k] = torch.tensor([features[0][k]] * len(features))\n    #print(\"打印fault_tolerance_data_collator处理后的信息\")\n    #print(f\"batch.keys()={batch.keys()},len_batch_key_labels={len(batch['labels'])}    ,len_batch_key_k={len(batch[k])}\")\n    return batch","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:14:48.136277Z","iopub.execute_input":"2023-07-31T11:14:48.137287Z","iopub.status.idle":"2023-07-31T11:14:48.156430Z","shell.execute_reply.started":"2023-07-31T11:14:48.137216Z","shell.execute_reply":"2023-07-31T11:14:48.155473Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"fault_tolerance_data_collator(eval_ds)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:14:51.959136Z","iopub.execute_input":"2023-07-31T11:14:51.959516Z","iopub.status.idle":"2023-07-31T11:14:52.021243Z","shell.execute_reply.started":"2023-07-31T11:14:51.959469Z","shell.execute_reply":"2023-07-31T11:14:52.020202Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[42660, 55674, 31123, 34380, 39507,  5088],\n         [  299, 10240, 30919,   438,  1480,   438],\n         [10789, 30927, 11408,   271,  9563, 30933],\n         [30929,   467, 30927,  1063,  1056, 30972],\n         [   13, 30972, 31011, 30966, 31011, 30943]]),\n 'labels': tensor([[42660, 55674, 31123, 34380, 39507,  5088],\n         [  299, 10240, 30919,   438,  1480,   438],\n         [10789, 30927, 11408,   271,  9563, 30933],\n         [30929,   467, 30927,  1063,  1056, 30972],\n         [   13, 30972, 31011, 30966, 31011, 30943]])}"},"metadata":{}}]},{"cell_type":"code","source":"class DataCollatorForChatGLM:\n    \n    # 虽然这个函数中又做了pad和截断 但是实际在pt阶段没有效果 因为之前已经是统一长度而且把尾部短文本用\n    # pad token 补齐过了 这里代码留着是为了后面参考改多轮对话用\n    def __init__(self,\n                 pad_token_id: int,\n                 max_length: int = 2048,\n                 ignore_label_id: int = -100):\n        self.pad_token_id = pad_token_id\n        self.ignore_label_id = ignore_label_id\n        self.max_length = max_length\n\n    def __call__(self, batch_data: List[Dict[str, List]]) -> Dict[str, torch.Tensor]:\n        \"\"\"根据batch最大长度做padding\"\"\"\n        len_list = [len(d['input_ids']) for d in batch_data]\n        batch_max_len = max(len_list)\n        input_ids, labels = [], []\n        for len_of_d, d in sorted(zip(len_list, batch_data), key=lambda x: -x[0]):\n            #padding :虽然预训练之前的步骤做了 长度已经是统一的 但是这个可以继续pad\n            #注意 ids和label使用的pad token不同 \n            # 在预训练阶段 下面的6行实际没有任何作用 因为pad_len 此时为0 可以打印看看\n            pad_len = batch_max_len - len_of_d\n            print(f\"pad_len in data collator = {pad_len}\")\n            ids = d['input_ids'] + [self.pad_token_id] * pad_len\n            label = d['labels'] + [self.ignore_label_id] * pad_len\n            if batch_max_len > self.max_length:\n                ids = ids[: self.max_length]\n                label = label[: self.max_length]\n            input_ids.append(torch.LongTensor(ids))\n            labels.append(torch.LongTensor(label))\n        input_ids = torch.stack(input_ids)\n        labels = torch.stack(labels)\n        return {'input_ids': input_ids, 'labels': labels}\n\ndata_collator = DataCollatorForChatGLM(pad_token_id=tokenizer.pad_token_id,\n                                           max_length=7)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:16:55.699696Z","iopub.execute_input":"2023-07-31T11:16:55.700445Z","iopub.status.idle":"2023-07-31T11:16:55.711548Z","shell.execute_reply.started":"2023-07-31T11:16:55.700407Z","shell.execute_reply":"2023-07-31T11:16:55.710471Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"data_collator(train_ds)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:17:15.706497Z","iopub.execute_input":"2023-07-31T11:17:15.706925Z","iopub.status.idle":"2023-07-31T11:17:15.743203Z","shell.execute_reply.started":"2023-07-31T11:17:15.706894Z","shell.execute_reply":"2023-07-31T11:17:15.741985Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[   13, 15939,   317,   265, 30927,  5323],\n         [30929, 30934,   416, 30929, 30926,   268],\n         [30974, 30916,   822, 30919, 30970, 30925],\n         [ 2065, 30925, 30965, 30926, 23347,   268],\n         [10789, 30927, 11408,   271,  9563, 30933],\n         [30929,  9563, 30933,  5282,   266,    13],\n         [10038,  5323, 30929,   689,  2277, 30919],\n         [54804, 35228, 54819, 55720, 55720, 56645],\n         [ 2065, 30925, 30965, 30926, 23347,   268],\n         [30978,   299,  6196, 30927,   268,  6196],\n         [30929, 30934,   416, 30929, 30926,   268],\n         [  299,   299, 30917, 30918, 30910,    13],\n         [30916,  2823, 30921,   317, 30931,   317],\n         [16462, 26604, 30919,   299,   416, 30929],\n         [30978,   299,  6196, 30927,   268,  6196],\n         [30978, 30978, 30978, 30978, 30978, 30910],\n         [64790, 64792, 30910, 30939, 31011, 30943],\n         [   13, 30972, 31011, 30966, 31011, 30943],\n         [  467,   315, 30926,   734,   317, 30931],\n         [10038,  5323, 30929,   689,  2277, 30919],\n         [  299, 10240, 30919,   438,  1480,   438],\n         [   13, 30972, 31011, 30966, 31011, 30943],\n         [   13, 15939,   317,   265, 30927,  5323],\n         [30916,  2823, 30921,   317, 30931,   317],\n         [42660, 55674, 31123, 34380, 39507,  5088],\n         [  467,   315, 30926,   734,   317, 30931],\n         [30929,  9563, 30933,  5282,   266,    13],\n         [30929,   467, 30927,  1063,  1056, 30972],\n         [16462, 26604, 30919,   299,   416, 30929],\n         [42660, 55674, 31123, 34380, 39507,  5088],\n         [64790, 64792, 30910, 30939, 31011, 30943],\n         [54804, 35228, 54819, 55720, 55720, 56645],\n         [10789, 30927, 11408,   271,  9563, 30933],\n         [ 5239, 30926,   265,    13, 36545, 31994],\n         [ 4571, 30953, 30920, 30933, 13532, 30921],\n         [30978, 30978, 30978, 30978, 30978, 30910],\n         [30974, 30916,   822, 30919, 30970, 30925],\n         [  299,   299, 30917, 30918, 30910,    13],\n         [ 5239, 30926,   265,    13, 36545, 31994],\n         [ 4571, 30953, 30920, 30933, 13532, 30921],\n         [  299, 10240, 30919,   438,  1480,   438],\n         [30929,   467, 30927,  1063,  1056, 30972]]),\n 'labels': tensor([[   13, 15939,   317,   265, 30927,  5323],\n         [30929, 30934,   416, 30929, 30926,   268],\n         [30974, 30916,   822, 30919, 30970, 30925],\n         [ 2065, 30925, 30965, 30926, 23347,   268],\n         [10789, 30927, 11408,   271,  9563, 30933],\n         [30929,  9563, 30933,  5282,   266,    13],\n         [10038,  5323, 30929,   689,  2277, 30919],\n         [54804, 35228, 54819, 55720, 55720, 56645],\n         [ 2065, 30925, 30965, 30926, 23347,   268],\n         [30978,   299,  6196, 30927,   268,  6196],\n         [30929, 30934,   416, 30929, 30926,   268],\n         [  299,   299, 30917, 30918, 30910,    13],\n         [30916,  2823, 30921,   317, 30931,   317],\n         [16462, 26604, 30919,   299,   416, 30929],\n         [30978,   299,  6196, 30927,   268,  6196],\n         [30978, 30978, 30978, 30978, 30978, 30910],\n         [64790, 64792, 30910, 30939, 31011, 30943],\n         [   13, 30972, 31011, 30966, 31011, 30943],\n         [  467,   315, 30926,   734,   317, 30931],\n         [10038,  5323, 30929,   689,  2277, 30919],\n         [  299, 10240, 30919,   438,  1480,   438],\n         [   13, 30972, 31011, 30966, 31011, 30943],\n         [   13, 15939,   317,   265, 30927,  5323],\n         [30916,  2823, 30921,   317, 30931,   317],\n         [42660, 55674, 31123, 34380, 39507,  5088],\n         [  467,   315, 30926,   734,   317, 30931],\n         [30929,  9563, 30933,  5282,   266,    13],\n         [30929,   467, 30927,  1063,  1056, 30972],\n         [16462, 26604, 30919,   299,   416, 30929],\n         [42660, 55674, 31123, 34380, 39507,  5088],\n         [64790, 64792, 30910, 30939, 31011, 30943],\n         [54804, 35228, 54819, 55720, 55720, 56645],\n         [10789, 30927, 11408,   271,  9563, 30933],\n         [ 5239, 30926,   265,    13, 36545, 31994],\n         [ 4571, 30953, 30920, 30933, 13532, 30921],\n         [30978, 30978, 30978, 30978, 30978, 30910],\n         [30974, 30916,   822, 30919, 30970, 30925],\n         [  299,   299, 30917, 30918, 30910,    13],\n         [ 5239, 30926,   265,    13, 36545, 31994],\n         [ 4571, 30953, 30920, 30933, 13532, 30921],\n         [  299, 10240, 30919,   438,  1480,   438],\n         [30929,   467, 30927,  1063,  1056, 30972]])}"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# 注意加上*\na=lm_datasets['input_ids']\nlist(chain(*a))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-31T04:36:49.226020Z","iopub.execute_input":"2023-07-31T04:36:49.226464Z","iopub.status.idle":"2023-07-31T04:36:49.246300Z","shell.execute_reply.started":"2023-07-31T04:36:49.226426Z","shell.execute_reply":"2023-07-31T04:36:49.245138Z"},"trusted":true},"execution_count":250,"outputs":[{"execution_count":250,"output_type":"execute_result","data":{"text/plain":"[[64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972],\n [30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978],\n [30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917],\n [299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578],\n [13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296],\n [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317],\n [30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953],\n [2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926],\n [5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514],\n [42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920],\n [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933],\n [30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058],\n [54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995],\n [4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265],\n [30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927],\n [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540],\n [13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299],\n [10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265],\n [30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299],\n [467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317],\n [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366],\n [64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972],\n [30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978],\n [30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917],\n [299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578],\n [13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296],\n [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317],\n [30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953],\n [2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926],\n [5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514],\n [42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920],\n [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933],\n [30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058],\n [54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995],\n [4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265],\n [30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927],\n [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540],\n [13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299],\n [10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265],\n [30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299],\n [467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317],\n [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366]]"},"metadata":{}}]},{"cell_type":"code","source":"lm_datasets['train']['input_ids']","metadata":{"execution":{"iopub.status.busy":"2023-07-31T03:06:31.270348Z","iopub.execute_input":"2023-07-31T03:06:31.270792Z","iopub.status.idle":"2023-07-31T03:06:31.290551Z","shell.execute_reply.started":"2023-07-31T03:06:31.270759Z","shell.execute_reply":"2023-07-31T03:06:31.289556Z"},"trusted":true},"execution_count":138,"outputs":[{"execution_count":138,"output_type":"execute_result","data":{"text/plain":"[[[64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972],\n  [30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978],\n  [30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917],\n  [299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578],\n  [13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296],\n  [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317],\n  [30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953],\n  [2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926],\n  [5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514],\n  [42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920],\n  [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933],\n  [30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058],\n  [54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995],\n  [4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265],\n  [30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927],\n  [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540],\n  [13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299],\n  [10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265],\n  [30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299],\n  [467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317],\n  [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366]],\n [[64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972],\n  [30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978],\n  [30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917],\n  [299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578],\n  [13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296],\n  [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317],\n  [30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953],\n  [2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926],\n  [5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514],\n  [42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920],\n  [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933],\n  [30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058],\n  [54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995],\n  [4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265],\n  [30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927],\n  [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540],\n  [13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299],\n  [10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265],\n  [30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299],\n  [467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317],\n  [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366]]]"},"metadata":{}}]},{"cell_type":"code","source":"a=[[64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972],\n  [30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978]]\nb=[[30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917],\n  [299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578],\n  [13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296],]\nfrom itertools import chain\nlist(chain(a,b))","metadata":{"execution":{"iopub.status.busy":"2023-07-31T03:39:10.354886Z","iopub.execute_input":"2023-07-31T03:39:10.355335Z","iopub.status.idle":"2023-07-31T03:39:10.367623Z","shell.execute_reply.started":"2023-07-31T03:39:10.355301Z","shell.execute_reply":"2023-07-31T03:39:10.366689Z"},"trusted":true},"execution_count":173,"outputs":[{"execution_count":173,"output_type":"execute_result","data":{"text/plain":"[[64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972],\n [30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978],\n [30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917],\n [299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578],\n [13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296]]"},"metadata":{}}]},{"cell_type":"code","source":"# 可以验证\n#list(chain(lm_datasets['train']['input_ids'][0] ,lm_datasets['train']['input_ids'][1]))","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:10:33.423041Z","iopub.execute_input":"2023-07-31T14:10:33.423431Z","iopub.status.idle":"2023-07-31T14:10:33.428209Z","shell.execute_reply.started":"2023-07-31T14:10:33.423398Z","shell.execute_reply":"2023-07-31T14:10:33.427085Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"#可以验证\n#tokenized_dataset['train']['input_ids']","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:10:41.417770Z","iopub.execute_input":"2023-07-31T14:10:41.418163Z","iopub.status.idle":"2023-07-31T14:10:41.423007Z","shell.execute_reply.started":"2023-07-31T14:10:41.418128Z","shell.execute_reply":"2023-07-31T14:10:41.421539Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"a=[1,2]\na[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-31T07:20:33.711492Z","iopub.execute_input":"2023-07-31T07:20:33.711901Z","iopub.status.idle":"2023-07-31T07:20:33.720058Z","shell.execute_reply.started":"2023-07-31T07:20:33.711867Z","shell.execute_reply":"2023-07-31T07:20:33.718994Z"},"trusted":true},"execution_count":399,"outputs":[{"execution_count":399,"output_type":"execute_result","data":{"text/plain":"[1, 2]"},"metadata":{}}]},{"cell_type":"code","source":"!git clone https://github.com/valkryhx/chatGLM-6B-QLoRA","metadata":{"execution":{"iopub.status.busy":"2023-07-31T07:38:32.577090Z","iopub.execute_input":"2023-07-31T07:38:32.577986Z","iopub.status.idle":"2023-07-31T07:38:36.080069Z","shell.execute_reply.started":"2023-07-31T07:38:32.577951Z","shell.execute_reply":"2023-07-31T07:38:36.078925Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'chatGLM-6B-QLoRA'...\nremote: Enumerating objects: 941, done.\u001b[K\nremote: Counting objects: 100% (161/161), done.\u001b[K\nremote: Compressing objects: 100% (85/85), done.\u001b[K\nremote: Total 941 (delta 123), reused 73 (delta 73), pack-reused 780\u001b[K\nReceiving objects: 100% (941/941), 17.26 MiB | 15.56 MiB/s, done.\nResolving deltas: 100% (592/592), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"cd /kaggle/working/chatGLM-6B-QLoRA","metadata":{"execution":{"iopub.status.busy":"2023-07-31T12:08:07.005651Z","iopub.execute_input":"2023-07-31T12:08:07.006765Z","iopub.status.idle":"2023-07-31T12:08:07.013734Z","shell.execute_reply.started":"2023-07-31T12:08:07.006715Z","shell.execute_reply":"2023-07-31T12:08:07.012676Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"/kaggle/working/chatGLM-6B-QLoRA\n","output_type":"stream"}]},{"cell_type":"code","source":"!git pull --all --force\n#!pip install peft==0.4.0\n#!pip install  -U git+https://github.com/huggingface/peft.git\n%cd /kaggle/working/chatGLM-6B-QLoRA \n!ls\n!pip install -r requirements.txt\n#!pip install deepspeed==0.9.5  这个也是需要的 但是目前kaggle 的runtime自带了","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls data/pretrain/xuanyan","metadata":{"execution":{"iopub.status.busy":"2023-07-31T13:36:04.353352Z","iopub.execute_input":"2023-07-31T13:36:04.353941Z","iopub.status.idle":"2023-07-31T13:36:05.355775Z","shell.execute_reply.started":"2023-07-31T13:36:04.353898Z","shell.execute_reply":"2023-07-31T13:36:05.354598Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stdout","text":"2  xy.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!git pull --all --force \n#!pip install  -U git+https://github.com/huggingface/peft.git   # 20230717 peft==0.4.0正式发布了 不用调版本了推理完后再训练需要重新升级到0.4.0dev 所以有这句\n!deepspeed --include localhost:0,1  pretrain_qlora_chatglm2.py \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-pt \\\n  --num_train_samples -1 \\\n  --num_eval_samples 20 \\\n  --block_size 1024 \\\n  --train_data_path ./data/pretrain/xuanyan/ \\\n  --eval_data_path  ./data/pretrain/xuanyan/ \\\n  --max_length 1024 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 1 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  5e-5 \\\n  --num_train_epochs  40  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"execution":{"iopub.status.busy":"2023-07-31T15:42:55.202053Z","iopub.execute_input":"2023-07-31T15:42:55.202476Z","iopub.status.idle":"2023-07-31T16:10:46.278003Z","shell.execute_reply.started":"2023-07-31T15:42:55.202439Z","shell.execute_reply":"2023-07-31T16:10:46.271671Z"},"trusted":true},"execution_count":149,"outputs":[{"name":"stdout","text":"Fetching origin\nremote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (3/3), 682 bytes | 227.00 KiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   5d7feba..78dd514  main       -> origin/main\nUpdating 5d7feba..78dd514\nFast-forward\n pretrain_qlora_chatglm2.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n 1 file changed, 1 insertion(+), 1 deletion(-)\n[2023-07-31 15:42:59,920] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-07-31 15:43:07,267] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n[2023-07-31 15:43:07,283] [INFO] [runner.py:555:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_qlora_chatglm2.py --train_args_json luzi.json --model_name_or_path THUDM/chatglm2-6b --output_dir output-pt --num_train_samples -1 --num_eval_samples 20 --block_size 1024 --train_data_path ./data/pretrain/xuanyan/ --eval_data_path ./data/pretrain/xuanyan/ --max_length 1024 --lora_rank 64 --lora_dropout 0.05 --compute_dtype fp16 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --learning_rate 5e-5 --num_train_epochs 40 --save_total_limit 2 --load_in_4bit True --deepspeed ds_zero2_config.json\n[2023-07-31 15:43:09,144] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-07-31 15:43:15,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n[2023-07-31 15:43:15,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n[2023-07-31 15:43:15,415] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n[2023-07-31 15:43:15,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n[2023-07-31 15:43:15,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n[2023-07-31 15:43:15,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n[2023-07-31 15:43:15,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n[2023-07-31 15:43:15,415] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n[2023-07-31 15:43:15,415] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n[2023-07-31 15:43:15,416] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n[2023-07-31 15:43:15,416] [INFO] [launch.py:163:main] dist_world_size=2\n[2023-07-31 15:43:15,416] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n[2023-07-31 15:43:18,588] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-07-31 15:43:18,671] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n/kaggle/working/chatGLM-6B-QLoRA/pretrain_qlora_chatglm2.py:51: FutureWarning: set_caching_enabled is deprecated and will be removed in the next major version of datasets. Use datasets.enable_caching() or datasets.disable_caching() instead. This function will be removed in a future version of datasets.\n  set_caching_enabled(False)\nsave !!!!!!\n\u001b[32m2023-07-31 15:43:29.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m419\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n/kaggle/working/chatGLM-6B-QLoRA/pretrain_qlora_chatglm2.py:51: FutureWarning: set_caching_enabled is deprecated and will be removed in the next major version of datasets. Use datasets.enable_caching() or datasets.disable_caching() instead. This function will be removed in a future version of datasets.\n  set_caching_enabled(False)\nsave !!!!!!\n\u001b[32m2023-07-31 15:43:29.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m419\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n\u001b[32m2023-07-31 15:43:29.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m465\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-07-31 15:43:29.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_dataset_for_pretrain\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1mdata files: ./data/pretrain/xuanyan/xy.txt\u001b[0m\n\u001b[32m2023-07-31 15:43:29.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m465\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-07-31 15:43:29.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_dataset_for_pretrain\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1mdata files: ./data/pretrain/xuanyan/xy.txt\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 257.81it/s]\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 258.70it/s]\nMap:   0%|                                         | 0/1 [00:00<?, ? examples/s]\u001b[32m2023-07-31 15:43:29.947\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m201\u001b[0m - \u001b[31m\u001b[1mexamples.keys()=['input_ids']\u001b[0m\nMap:   0%|                                         | 0/1 [00:00<?, ? examples/s]\u001b[32m2023-07-31 15:43:29.952\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m201\u001b[0m - \u001b[31m\u001b[1mexamples.keys()=['input_ids']\u001b[0m\n每个txt文件对应的samples个数 total_length=40558\n\u001b[32m2023-07-31 15:43:29.997\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m206\u001b[0m - \u001b[31m\u001b[1m注意如果出现空白内容txt 则会出现异常短的单个sample 长度为4 类似[64790, 64792, 30910, 13, 0, 0, 0, 0, 0,0]，很难排查 所以数据集目录中不要出现空白内容txt。\u001b[0m\n每个txt文件对应的samples个数 total_length=40558\n\u001b[32m2023-07-31 15:43:30.020\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m206\u001b[0m - \u001b[31m\u001b[1m注意如果出现空白内容txt 则会出现异常短的单个sample 长度为4 类似[64790, 64792, 30910, 13, 0, 0, 0, 0, 0,0]，很难排查 所以数据集目录中不要出现空白内容txt。\u001b[0m\n样本中ids 最大长度 max_len_ids=1024                                                    \n全局训练参数的global_args_max_length=1024\ncutoff to len = 1024\n\u001b[32m2023-07-31 15:43:30.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m244\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 第一个样本】\n中文马克思主义文库 -> 马克思　恩格斯\n共产党宣言\n\n马克思 恩格斯\n\n（1848年）\n\n\n· 1872年德文版序言\n· 1882年俄文版序言\n· 1883年德文版序言\n· 1888年英文版序言\n· 1890年德文版序言\n· 1892年波兰文版序言\n· 1893年意大利文版序言\n\n共产党宣言\n一、资产者和无产者\n二、无产者和共产党人\n三、社会主义的和共产主义的文献\n四、共产党人对各种反对党派的态度\n人名索引\n\n\n　　〔来源〕原载中共中央马克思恩格斯列宁斯大林著作编译局《共产党宣言（纪念马克思诞辰200周年马克思恩格斯著作特辑》（人民出版社，2018-4-1）\n　　中文马克思主义文库2022年11月更新。\n\n\n1872年德文版序言[1]\n\n　　共产主义者同盟[2]这个在当时条件下自然只能是秘密团体的国际工人组织，1847年11月在伦敦举行的代表大会上委托我们两人起草一个准备公布的详细的理论和实践的党纲。结果就产生了这个《宣言》，《宣言》原稿在二月革命[3]前几星期送到伦敦付印。《宣言》最初用德文出版，它用这种文字在德国、英国和美国至少印过12种不同的版本。第一个英译本是由海伦·麦克法林女士翻译的，于1850年在伦敦《红色共和党人》[4]杂志上发表，1871年至少又有三种不同的英译本在美国出版。法译本于1848年六月起义[5]前不久第一次在巴黎印行，最近又有法译本在纽约《社会主义者报》[6]上发表；现在有人在准备新译本。波兰文译本在德文本初版问世后不久就在伦敦出现。俄译本是60年代在日内瓦出版的。丹麦文译本也是在原书问世后不久就出版了。\n\n　　不管最近25年来的情况发生了多大的变化，这个《宣言》中所阐述的一般原理整个说来直到现在还是完全正确的。某些地方本来可以作一些修改。这些原理的实际运用，正如《宣言》中所说的，随时随地都要以当时的历史条件为转移，所以第二章末尾提出的那些革命措施根本没有特别的意义。如果是在今天，这一段在许多方面都会有不同的写法了。由于最近25年来大工业有了巨大发展而工人阶级的政党组织也跟着发展起来，由于首先有了二月革命的实际经验而后来尤其是有了无产阶级第一次掌握政权达两月之久的巴黎公社[7]的实际经验，所以这个纲领现在有些地方已经过时了。特别是公社已经证明：“工人阶级不能简单地掌握现成的国家机器，并运用它来达到自己的目的。”（见《法兰西内战。国际工人协会总委员会宣言》德文版第19页，那里对这个思想作了更详细的阐述。）〔注：见《马克思恩格斯选集》第3版第3卷第95页。——编者注〕其次，很明显，对于社会主义文献所作的批判在今天看来是不完全的，因为这一批判只包括到1847年为止；同样也很明显，关于共产党人对待各种反对党派的态度的论述（第四章）虽然在原则上今天还是正确的，但是就其实际运用来说今天毕竟已经过时，因为政治形势已经完全改变，当时所列举的那些党派大部分已被历史的发展彻底扫除了。\n\n　　但是《宣言》是一个历史文件，我们已没有权利来加以修改。下次再版时也许能加上一篇论述1847年到现在这段时期的导言。这次再版太仓促了，我们来不及做这件工作。\n\n　　卡尔·马克思　弗里德里希·恩格斯\n　　1872年6月24日于伦敦\n\n\n卡·马克思和弗·恩格斯写于1872年6月24日\n载于1872年在莱比锡出版的德文版《共产主义宣言》一书\n\n原文是德文\n选自《马克思恩格斯选集》第3版第1卷第376-377页\n\n\n\n1882年俄文版序言[8]\n\n　　巴枯宁翻译的《共产党宣言》俄文第一版，60年代初〔注：应是1869年。——编者注〕由《钟声》印刷所出版。当时西方认为这件事（《宣言》译成俄文出版）不过是著作界的一件\u001b[0m\n样本中ids 最大长度 max_len_ids=1024\n全局训练参数的global_args_max_length=1024\ncutoff to len = 1024\n\u001b[32m2023-07-31 15:43:30.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m245\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 倒数第二个样本（长度满足block size）】\n政府主义理论的创始人，第二共和国时期是制宪议会议员（1848）。——10、18、19、59、109、118、124、129。\n\n　　普芬德，卡尔（Pfänder，Carl 1819-1876）——德国微型画画家，德国工人运动和国际工人运动的活动家，1845年起侨居伦敦，正义者同盟盟员，伦敦德意志工人共产主义教育协会会员；1849年巴登-普法尔茨起义的参加者，起义失败后流亡英国；共产主义者同盟中央委员会委员，1850年共产主义者同盟分裂后支持马克思和恩格斯；国际总委员会委员（1864-1867和1870-1872）；马克思和恩格斯的朋友和战友。——104。\n\nR\n\n　　茹柯夫斯基，尤利·加拉克季昂诺维奇（Жуковский, Юлий Галактионович 1822-1907）——俄国资产阶级庸俗经济学家和政论家；国家银行行长；曾撰写《卡尔·马克思和他的〈资本论〉一书》一文，攻击马克思主义。——134。\n\nS\n\n　　沙佩尔，卡尔（Schapper，Karl 1812-1870）——德国工人运动和国际工人运动的活动家，正义者同盟的领导者之一，伦敦德意志工人共产主义教育协会创建人之一，共产主义者同盟中央委员会委员；1848-1849年革命的参加者；民主主义者莱茵区域委员会委员，该委员会案件（1849年2月8日）的被告之一；1849年2-5月为科隆工人联合会主席，《新莱茵报》撰稿人；1850年共产主义者同盟分裂时为冒险主义宗派集团的领袖之一；1856年起重新同马克思和恩格斯接近；国际总委员会委员（1865），1865年伦敦代表会议的参加者。——75、95、96、102、107、110、113、114、144。\n\n　　圣西门，昂利（Saint-Simon，Henri 1760-1825）——法国空想社会主义者。——60。\n\n　　施梯伯，威廉（Stieber，Wilhelm 1818-1882）——普鲁士警官，普鲁士政治警察局局长（1852-1860），科隆共产党人案件（1852）的策划者之一和主要原告证人；同卡·维尔穆特合编《19世纪共产主义者的阴谋》一书；普奥战争（1866）和普法战争（1870-1871）时期为军事警察局局长，在法国境内的德国情报机关的首脑。——94、105。\n\n　　叔尔茨，卡尔（Schurz，Carl 1829-1906）——德国政论家，小资产阶级民主主义者，1849年巴登-普法尔茨起义的参加者，起义失败后流亡瑞士，加入秘密组织“革命集中”；1852年迁居美国，站在北部方面参加美国内战，美国共和党领袖之一，曾任美国驻西班牙公使，后为参议员和内政部长（1877-1881）。——111。\n\nW\n\n　　维尔穆特，卡尔·格奥尔格·路德维希（Wermuth，Carl Georg Ludwig 1803-1867）——德国警官，汉诺威警察局长，科隆共产党人案件（1852）的策划者之一和原告证人；同威·施梯伯合编《19世纪共产主义者的阴谋》一书。——94、105。\n\n　　维利希，奥古斯特（Willich，August 1810-1878）——普鲁士军官，1847年起为共产主义者同盟盟员，1849年巴登-普法尔茨起义中为志愿军团首领；1850年共产主义者同盟分裂时同卡·沙佩尔一起组成反对马克思的冒险主义宗派集团；1853年侨居美国，站在北部方面参加美国内战，任将军。——110、111、113、114。\n\n　　魏特林，克里斯蒂安·威廉（Weitling，\u001b[0m\n\u001b[32m2023-07-31 15:43:30.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m244\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 第一个样本】\n中文马克思主义文库 -> 马克思　恩格斯\n共产党宣言\n\n马克思 恩格斯\n\n（1848年）\n\n\n· 1872年德文版序言\n· 1882年俄文版序言\n· 1883年德文版序言\n· 1888年英文版序言\n· 1890年德文版序言\n· 1892年波兰文版序言\n· 1893年意大利文版序言\n\n共产党宣言\n一、资产者和无产者\n二、无产者和共产党人\n三、社会主义的和共产主义的文献\n四、共产党人对各种反对党派的态度\n人名索引\n\n\n　　〔来源〕原载中共中央马克思恩格斯列宁斯大林著作编译局《共产党宣言（纪念马克思诞辰200周年马克思恩格斯著作特辑》（人民出版社，2018-4-1）\n　　中文马克思主义文库2022年11月更新。\n\n\n1872年德文版序言[1]\n\n　　共产主义者同盟[2]这个在当时条件下自然只能是秘密团体的国际工人组织，1847年11月在伦敦举行的代表大会上委托我们两人起草一个准备公布的详细的理论和实践的党纲。结果就产生了这个《宣言》，《宣言》原稿在二月革命[3]前几星期送到伦敦付印。《宣言》最初用德文出版，它用这种文字在德国、英国和美国至少印过12种不同的版本。第一个英译本是由海伦·麦克法林女士翻译的，于1850年在伦敦《红色共和党人》[4]杂志上发表，1871年至少又有三种不同的英译本在美国出版。法译本于1848年六月起义[5]前不久第一次在巴黎印行，最近又有法译本在纽约《社会主义者报》[6]上发表；现在有人在准备新译本。波兰文译本在德文本初版问世后不久就在伦敦出现。俄译本是60年代在日内瓦出版的。丹麦文译本也是在原书问世后不久就出版了。\n\n　　不管最近25年来的情况发生了多大的变化，这个《宣言》中所阐述的一般原理整个说来直到现在还是完全正确的。某些地方本来可以作一些修改。这些原理的实际运用，正如《宣言》中所说的，随时随地都要以当时的历史条件为转移，所以第二章末尾提出的那些革命措施根本没有特别的意义。如果是在今天，这一段在许多方面都会有不同的写法了。由于最近25年来大工业有了巨大发展而工人阶级的政党组织也跟着发展起来，由于首先有了二月革命的实际经验而后来尤其是有了无产阶级第一次掌握政权达两月之久的巴黎公社[7]的实际经验，所以这个纲领现在有些地方已经过时了。特别是公社已经证明：“工人阶级不能简单地掌握现成的国家机器，并运用它来达到自己的目的。”（见《法兰西内战。国际工人协会总委员会宣言》德文版第19页，那里对这个思想作了更详细的阐述。）〔注：见《马克思恩格斯选集》第3版第3卷第95页。——编者注〕其次，很明显，对于社会主义文献所作的批判在今天看来是不完全的，因为这一批判只包括到1847年为止；同样也很明显，关于共产党人对待各种反对党派的态度的论述（第四章）虽然在原则上今天还是正确的，但是就其实际运用来说今天毕竟已经过时，因为政治形势已经完全改变，当时所列举的那些党派大部分已被历史的发展彻底扫除了。\n\n　　但是《宣言》是一个历史文件，我们已没有权利来加以修改。下次再版时也许能加上一篇论述1847年到现在这段时期的导言。这次再版太仓促了，我们来不及做这件工作。\n\n　　卡尔·马克思　弗里德里希·恩格斯\n　　1872年6月24日于伦敦\n\n\n卡·马克思和弗·恩格斯写于1872年6月24日\n载于1872年在莱比锡出版的德文版《共产主义宣言》一书\n\n原文是德文\n选自《马克思恩格斯选集》第3版第1卷第376-377页\n\n\n\n1882年俄文版序言[8]\n\n　　巴枯宁翻译的《共产党宣言》俄文第一版，60年代初〔注：应是1869年。——编者注〕由《钟声》印刷所出版。当时西方认为这件事（《宣言》译成俄文出版）不过是著作界的一件\u001b[0m\n\u001b[32m2023-07-31 15:43:30.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m245\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 倒数第二个样本（长度满足block size）】\n政府主义理论的创始人，第二共和国时期是制宪议会议员（1848）。——10、18、19、59、109、118、124、129。\n\n　　普芬德，卡尔（Pfänder，Carl 1819-1876）——德国微型画画家，德国工人运动和国际工人运动的活动家，1845年起侨居伦敦，正义者同盟盟员，伦敦德意志工人共产主义教育协会会员；1849年巴登-普法尔茨起义的参加者，起义失败后流亡英国；共产主义者同盟中央委员会委员，1850年共产主义者同盟分裂后支持马克思和恩格斯；国际总委员会委员（1864-1867和1870-1872）；马克思和恩格斯的朋友和战友。——104。\n\nR\n\n　　茹柯夫斯基，尤利·加拉克季昂诺维奇（Жуковский, Юлий Галактионович 1822-1907）——俄国资产阶级庸俗经济学家和政论家；国家银行行长；曾撰写《卡尔·马克思和他的〈资本论〉一书》一文，攻击马克思主义。——134。\n\nS\n\n　　沙佩尔，卡尔（Schapper，Karl 1812-1870）——德国工人运动和国际工人运动的活动家，正义者同盟的领导者之一，伦敦德意志工人共产主义教育协会创建人之一，共产主义者同盟中央委员会委员；1848-1849年革命的参加者；民主主义者莱茵区域委员会委员，该委员会案件（1849年2月8日）的被告之一；1849年2-5月为科隆工人联合会主席，《新莱茵报》撰稿人；1850年共产主义者同盟分裂时为冒险主义宗派集团的领袖之一；1856年起重新同马克思和恩格斯接近；国际总委员会委员（1865），1865年伦敦代表会议的参加者。——75、95、96、102、107、110、113、114、144。\n\n　　圣西门，昂利（Saint-Simon，Henri 1760-1825）——法国空想社会主义者。——60。\n\n　　施梯伯，威廉（Stieber，Wilhelm 1818-1882）——普鲁士警官，普鲁士政治警察局局长（1852-1860），科隆共产党人案件（1852）的策划者之一和主要原告证人；同卡·维尔穆特合编《19世纪共产主义者的阴谋》一书；普奥战争（1866）和普法战争（1870-1871）时期为军事警察局局长，在法国境内的德国情报机关的首脑。——94、105。\n\n　　叔尔茨，卡尔（Schurz，Carl 1829-1906）——德国政论家，小资产阶级民主主义者，1849年巴登-普法尔茨起义的参加者，起义失败后流亡瑞士，加入秘密组织“革命集中”；1852年迁居美国，站在北部方面参加美国内战，美国共和党领袖之一，曾任美国驻西班牙公使，后为参议员和内政部长（1877-1881）。——111。\n\nW\n\n　　维尔穆特，卡尔·格奥尔格·路德维希（Wermuth，Carl Georg Ludwig 1803-1867）——德国警官，汉诺威警察局长，科隆共产党人案件（1852）的策划者之一和原告证人；同威·施梯伯合编《19世纪共产主义者的阴谋》一书。——94、105。\n\n　　维利希，奥古斯特（Willich，August 1810-1878）——普鲁士军官，1847年起为共产主义者同盟盟员，1849年巴登-普法尔茨起义中为志愿军团首领；1850年共产主义者同盟分裂时同卡·沙佩尔一起组成反对马克思的冒险主义宗派集团；1853年侨居美国，站在北部方面参加美国内战，任将军。——110、111、113、114。\n\n　　魏特林，克里斯蒂安·威廉（Weitling，\u001b[0m\n\u001b[32m2023-07-31 15:43:30.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 最后一个样本（长度不足blcok size）】\n【tokens】=[23688, 27720, 30910, 30939, 30973, 30940, 30973, 30941, 30939, 30973, 30981, 30939, 53437, 32942, 33796, 31903, 31651, 54561, 31123, 36495, 54631, 43166, 35945, 31123, 32156, 54532, 55857, 56985, 54659, 54913, 54695, 32686, 41005, 32135, 45508, 55685, 54587, 54561, 54659, 33796, 43166, 54530, 35461, 32395, 33796, 32892, 54771, 54612, 54530, 32124, 54631, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 31730, 31123, 39435, 33810, 31721, 33796, 32302, 36905, 30939, 30943, 31201, 30939, 30969, 31201, 30969, 30981, 30941, 30939, 30940, 30940, 31201, 30939, 30940, 30966, 31201, 30939, 30939, 30939, 31201, 30939, 30939, 30966, 31201, 30939, 30939, 30969, 31201, 30939, 30966, 30943, 31155, 13, 13, 31478, 31478, 40069, 56642, 31123, 56642, 54662, 38963, 55211, 31065, 42224, 31301, 30959, 325, 486, 31123, 30960, 2161, 6611, 27720, 30910, 55867, 55186, 54927, 380, 455, 337, 30910, 30939, 30973, 30940, 30969, 30941, 30939, 30973, 30978, 30972, 53437, 32942, 45853, 32123, 45508, 54681, 54903, 54561, 31123, 32156, 54532, 54635, 54650, 31123, 54739, 54662, 34793, 32868, 42574, 54659, 30939, 30973, 30966, 30972, 30941, 30939, 30973, 30966, 30969, 39103, 54637, 54534, 55186, 55867, 54930, 38600, 54659, 30939, 30973, 30972, 30978, 30941, 30939, 30973, 30972, 30981, 52317, 41588, 39136, 41005, 34105, 38508, 31123, 41005, 54631, 43166, 35461, 31905, 54542, 43166, 31819, 38508, 31301, 30939, 30973, 30972, 30973, 54540, 30966, 41630, 31798, 54611, 54575, 56236, 58181, 54771, 54612, 32494, 31301, 30939, 30973, 30972, 30973, 30941, 30939, 30973, 30972, 30969, 31798, 32681, 45856, 56236, 58181, 32159, 32007, 54542, 54693, 55954, 31660, 38508, 54659, 45886, 54982, 55049, 33039, 54910, 31769, 54650, 31123, 32180, 55023, 55419, 55443, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 37533, 31123, 30939, 30973, 30970, 30939, 54540, 56081, 55118, 32463, 31123, 30939, 30973, 30970, 30966, 37017, 54534, 56115, 55678, 34602, 54673, 54635, 54650, 54659, 33929, 54542, 55609, 47734, 33695, 54542, 40493, 36905, 30981, 30970, 31201, 30939, 30940, 30970, 31201, 30939, 30940, 30981, 31201, 30939, 30940, 30969, 31201, 30939, 30943, 30940, 31155, 13, 13, 31017, 13, 13, 31478, 31478, 44466, 55717, 54616, 31123, 54772, 31065, 55404, 55075, 31065, 56236, 55532, 55428, 55075, 31065, 54739, 55717, 54954, 31065, 54954, 31301, 30937, 1082, 639, 30915, 31123, 30977, 14649, 30941, 12392, 773, 30941, 30957, 31008, 264, 446, 3019, 11437, 384, 30910, 30939, 30981, 30981, 30966, 30941, 30939, 30973, 30972, 30943, 53437, 37533, 50639, 47450, 34486, 31123, 31832, 35936, 54538, 34317, 54545, 55443, 38942, 32477, 36905, 30970, 30972, 31155, 13, 13, 31478, 31478, 55287, 55075, 31123, 44554, 16747, 54904, 55404, 56313, 55075, 31123, 44554, 31155, 13, 13, 30986, 13, 13, 31478, 31478, 55640, 54693, 54703, 31123, 55256, 55929, 55097, 57100, 31301, 30977, 338, 15373, 31123, 30938, 7078, 2521, 30910, 30939, 30973, 30966, 30940, 30941, 30939, 30969, 30939, 30969, 53437, 32942, 32718, 31123, 55336, 55609, 49413, 37556, 32991, 34136, 33972, 31301, 30939, 30973, 30970, 30940, 30941, 30939, 30973, 30970, 30939, 31798, 41005, 54631, 43166, 55653, 54650, 54542, 54717, 54783, 31123, 54693, 55954, 32555, 54533, 32722, 31301, 30939, 30973, 30970, 30943, 31300, 54530, 34148, 31905, 31123, 54732, 56024, 55225, 36720, 39069, 54716, 55512, 31123, 50678, 54732, 55034, 30989, 46332, 37553, 52603, 30991, 54617, 31913, 54732, 55153, 55630, 54659, 30939, 30973, 30970, 30966, 54540, 54805, 55737, 32463, 31123, 54585, 56081, 55118, 31730, 31123, 35777, 54530, 41203, 54547, 31924, 34768, 31885, 54659, 33348, 36144, 31687, 31836, 31730, 52790, 31123, 54585, 54541, 35152, 48949, 49286, 31301, 30939, 30973, 30973, 30970, 30941, 30939, 30973, 30973, 30969, 31798, 31784, 37396, 54530, 32037, 54542, 33283, 31123, 55172, 54536, 32834, 32924, 34355, 36905, 30939, 30939, 30972, 31155, 13, 13, 31478, 31478, 51329, 41305, 47358, 31301, 31069, 30737, 6856, 31018, 31000, 6866, 30910, 30939, 30973, 30972, 30970, 30941, 30939, 30973, 30969, 30972, 53437, 46161, 33690, 31301, 30939, 30973, 30973, 30939, 30941, 30939, 30973, 30969, 30972, 31966, 16747, 30978, 31201, 30939, 30978, 31155, 13, 13, 13, 32823, 30910, 57906, 56484, 31201, 55233, 56722, 57087, 30910, 52806, 13, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n【最后一个样本原文】=Christian Wilhelm 1808-1871）——德国工人运动活动家，正义者同盟领导人，职业是裁缝；空想平均共产主义理论家和鼓动家；工人同盟的创始人，《工人共和国报》的出版者；1849年流亡美国，晚年接近国际工人协会。——12、19、97-100、103、111、113、119、132。\n\n　　沃尔弗，弗里德里希·威廉（Wolff，Friedrich Wilhelm 鲁普斯 Lupus 1809-1864）——德国无产阶级革命家和政论家，职业是教员，西里西亚农民的儿子；1834-1839年被关在普鲁士监狱；1846-1847年为布鲁塞尔共产主义通讯委员会委员，共产主义者同盟创始人之一和同盟中央委员会委员（1848年3月起），《新莱茵报》编辑（1848-1849），民主主义者莱茵区域委员会和科隆安全委员会委员；法兰克福国民议会议员，属于极左派；1849年流亡瑞士，1851年迁居英国，1853年起在曼彻斯特当教员；马克思和恩格斯的朋友和战友。——75、105、107、109、120。\n\nX\n\n　　西斯蒙第，让·沙尔·莱奥纳尔·西蒙德·德（Sismondi，Jean-Charles-Léonard Simonde de 1773-1842）——瑞士经济学家和历史学家，政治经济学中浪漫学派的代表人物。——54。\n\n　　席尔，卡尔——见沙佩尔，卡尔。\n\nY\n\n　　雅科比，阿伯拉罕（Jacobi，Abraham 1830-1919）——德国医生，波恩体操联合会创建人和会长（1850-1851），共产主义者同盟盟员和特使，科隆共产党人案件（1852）的被告之一，被陪审法庭宣告无罪，但因被控“侮辱国王陛下”而继续被监禁；1853年流亡英国，后迁居美国，在美国的刊物上宣传马克思主义思想；站在北部方面参加美国内战，后为纽约医学科学院院长（1885-1889），一些医学院的教授和院长，写有医学方面的著作。——114。\n\n　　亚历山大三世（Александр III 1845-1894）——俄国皇帝（1881-1894）。——6、16。\n\n\n感谢 聂孟、卫玄鹤 校对\n\n\n【最后一个样本原文长度】=893\u001b[0m\n\u001b[32m2023-07-31 15:43:30.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m验证查看完毕\u001b[0m\n样本数量=40\n\u001b[32m2023-07-31 15:43:30.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_dataset_for_pretrain\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1mdata files: ./data/pretrain/xuanyan/xy.txt\u001b[0m\n\u001b[32m2023-07-31 15:43:30.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 最后一个样本（长度不足blcok size）】\n【tokens】=[23688, 27720, 30910, 30939, 30973, 30940, 30973, 30941, 30939, 30973, 30981, 30939, 53437, 32942, 33796, 31903, 31651, 54561, 31123, 36495, 54631, 43166, 35945, 31123, 32156, 54532, 55857, 56985, 54659, 54913, 54695, 32686, 41005, 32135, 45508, 55685, 54587, 54561, 54659, 33796, 43166, 54530, 35461, 32395, 33796, 32892, 54771, 54612, 54530, 32124, 54631, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 31730, 31123, 39435, 33810, 31721, 33796, 32302, 36905, 30939, 30943, 31201, 30939, 30969, 31201, 30969, 30981, 30941, 30939, 30940, 30940, 31201, 30939, 30940, 30966, 31201, 30939, 30939, 30939, 31201, 30939, 30939, 30966, 31201, 30939, 30939, 30969, 31201, 30939, 30966, 30943, 31155, 13, 13, 31478, 31478, 40069, 56642, 31123, 56642, 54662, 38963, 55211, 31065, 42224, 31301, 30959, 325, 486, 31123, 30960, 2161, 6611, 27720, 30910, 55867, 55186, 54927, 380, 455, 337, 30910, 30939, 30973, 30940, 30969, 30941, 30939, 30973, 30978, 30972, 53437, 32942, 45853, 32123, 45508, 54681, 54903, 54561, 31123, 32156, 54532, 54635, 54650, 31123, 54739, 54662, 34793, 32868, 42574, 54659, 30939, 30973, 30966, 30972, 30941, 30939, 30973, 30966, 30969, 39103, 54637, 54534, 55186, 55867, 54930, 38600, 54659, 30939, 30973, 30972, 30978, 30941, 30939, 30973, 30972, 30981, 52317, 41588, 39136, 41005, 34105, 38508, 31123, 41005, 54631, 43166, 35461, 31905, 54542, 43166, 31819, 38508, 31301, 30939, 30973, 30972, 30973, 54540, 30966, 41630, 31798, 54611, 54575, 56236, 58181, 54771, 54612, 32494, 31301, 30939, 30973, 30972, 30973, 30941, 30939, 30973, 30972, 30969, 31798, 32681, 45856, 56236, 58181, 32159, 32007, 54542, 54693, 55954, 31660, 38508, 54659, 45886, 54982, 55049, 33039, 54910, 31769, 54650, 31123, 32180, 55023, 55419, 55443, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 37533, 31123, 30939, 30973, 30970, 30939, 54540, 56081, 55118, 32463, 31123, 30939, 30973, 30970, 30966, 37017, 54534, 56115, 55678, 34602, 54673, 54635, 54650, 54659, 33929, 54542, 55609, 47734, 33695, 54542, 40493, 36905, 30981, 30970, 31201, 30939, 30940, 30970, 31201, 30939, 30940, 30981, 31201, 30939, 30940, 30969, 31201, 30939, 30943, 30940, 31155, 13, 13, 31017, 13, 13, 31478, 31478, 44466, 55717, 54616, 31123, 54772, 31065, 55404, 55075, 31065, 56236, 55532, 55428, 55075, 31065, 54739, 55717, 54954, 31065, 54954, 31301, 30937, 1082, 639, 30915, 31123, 30977, 14649, 30941, 12392, 773, 30941, 30957, 31008, 264, 446, 3019, 11437, 384, 30910, 30939, 30981, 30981, 30966, 30941, 30939, 30973, 30972, 30943, 53437, 37533, 50639, 47450, 34486, 31123, 31832, 35936, 54538, 34317, 54545, 55443, 38942, 32477, 36905, 30970, 30972, 31155, 13, 13, 31478, 31478, 55287, 55075, 31123, 44554, 16747, 54904, 55404, 56313, 55075, 31123, 44554, 31155, 13, 13, 30986, 13, 13, 31478, 31478, 55640, 54693, 54703, 31123, 55256, 55929, 55097, 57100, 31301, 30977, 338, 15373, 31123, 30938, 7078, 2521, 30910, 30939, 30973, 30966, 30940, 30941, 30939, 30969, 30939, 30969, 53437, 32942, 32718, 31123, 55336, 55609, 49413, 37556, 32991, 34136, 33972, 31301, 30939, 30973, 30970, 30940, 30941, 30939, 30973, 30970, 30939, 31798, 41005, 54631, 43166, 55653, 54650, 54542, 54717, 54783, 31123, 54693, 55954, 32555, 54533, 32722, 31301, 30939, 30973, 30970, 30943, 31300, 54530, 34148, 31905, 31123, 54732, 56024, 55225, 36720, 39069, 54716, 55512, 31123, 50678, 54732, 55034, 30989, 46332, 37553, 52603, 30991, 54617, 31913, 54732, 55153, 55630, 54659, 30939, 30973, 30970, 30966, 54540, 54805, 55737, 32463, 31123, 54585, 56081, 55118, 31730, 31123, 35777, 54530, 41203, 54547, 31924, 34768, 31885, 54659, 33348, 36144, 31687, 31836, 31730, 52790, 31123, 54585, 54541, 35152, 48949, 49286, 31301, 30939, 30973, 30973, 30970, 30941, 30939, 30973, 30973, 30969, 31798, 31784, 37396, 54530, 32037, 54542, 33283, 31123, 55172, 54536, 32834, 32924, 34355, 36905, 30939, 30939, 30972, 31155, 13, 13, 31478, 31478, 51329, 41305, 47358, 31301, 31069, 30737, 6856, 31018, 31000, 6866, 30910, 30939, 30973, 30972, 30970, 30941, 30939, 30973, 30969, 30972, 53437, 46161, 33690, 31301, 30939, 30973, 30973, 30939, 30941, 30939, 30973, 30969, 30972, 31966, 16747, 30978, 31201, 30939, 30978, 31155, 13, 13, 13, 32823, 30910, 57906, 56484, 31201, 55233, 56722, 57087, 30910, 52806, 13, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n【最后一个样本原文】=Christian Wilhelm 1808-1871）——德国工人运动活动家，正义者同盟领导人，职业是裁缝；空想平均共产主义理论家和鼓动家；工人同盟的创始人，《工人共和国报》的出版者；1849年流亡美国，晚年接近国际工人协会。——12、19、97-100、103、111、113、119、132。\n\n　　沃尔弗，弗里德里希·威廉（Wolff，Friedrich Wilhelm 鲁普斯 Lupus 1809-1864）——德国无产阶级革命家和政论家，职业是教员，西里西亚农民的儿子；1834-1839年被关在普鲁士监狱；1846-1847年为布鲁塞尔共产主义通讯委员会委员，共产主义者同盟创始人之一和同盟中央委员会委员（1848年3月起），《新莱茵报》编辑（1848-1849），民主主义者莱茵区域委员会和科隆安全委员会委员；法兰克福国民议会议员，属于极左派；1849年流亡瑞士，1851年迁居英国，1853年起在曼彻斯特当教员；马克思和恩格斯的朋友和战友。——75、105、107、109、120。\n\nX\n\n　　西斯蒙第，让·沙尔·莱奥纳尔·西蒙德·德（Sismondi，Jean-Charles-Léonard Simonde de 1773-1842）——瑞士经济学家和历史学家，政治经济学中浪漫学派的代表人物。——54。\n\n　　席尔，卡尔——见沙佩尔，卡尔。\n\nY\n\n　　雅科比，阿伯拉罕（Jacobi，Abraham 1830-1919）——德国医生，波恩体操联合会创建人和会长（1850-1851），共产主义者同盟盟员和特使，科隆共产党人案件（1852）的被告之一，被陪审法庭宣告无罪，但因被控“侮辱国王陛下”而继续被监禁；1853年流亡英国，后迁居美国，在美国的刊物上宣传马克思主义思想；站在北部方面参加美国内战，后为纽约医学科学院院长（1885-1889），一些医学院的教授和院长，写有医学方面的著作。——114。\n\n　　亚历山大三世（Александр III 1845-1894）——俄国皇帝（1881-1894）。——6、16。\n\n\n感谢 聂孟、卫玄鹤 校对\n\n\n【最后一个样本原文长度】=893\u001b[0m\n\u001b[32m2023-07-31 15:43:30.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m验证查看完毕\u001b[0m\n样本数量=40\n\u001b[32m2023-07-31 15:43:30.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_dataset_for_pretrain\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1mdata files: ./data/pretrain/xuanyan/xy.txt\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 489.47it/s]\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 563.30it/s]\n\u001b[32m2023-07-31 15:43:30.730\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m201\u001b[0m - \u001b[31m\u001b[1mexamples.keys()=['input_ids']\u001b[0m\nMap:   0%|                                         | 0/1 [00:00<?, ? examples/s]\u001b[32m2023-07-31 15:43:30.745\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m201\u001b[0m - \u001b[31m\u001b[1mexamples.keys()=['input_ids']\u001b[0m\n每个txt文件对应的samples个数 total_length=40558\n\u001b[32m2023-07-31 15:43:30.774\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m206\u001b[0m - \u001b[31m\u001b[1m注意如果出现空白内容txt 则会出现异常短的单个sample 长度为4 类似[64790, 64792, 30910, 13, 0, 0, 0, 0, 0,0]，很难排查 所以数据集目录中不要出现空白内容txt。\u001b[0m\n每个txt文件对应的samples个数 total_length=40558\n\u001b[32m2023-07-31 15:43:30.787\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m206\u001b[0m - \u001b[31m\u001b[1m注意如果出现空白内容txt 则会出现异常短的单个sample 长度为4 类似[64790, 64792, 30910, 13, 0, 0, 0, 0, 0,0]，很难排查 所以数据集目录中不要出现空白内容txt。\u001b[0m\n样本中ids 最大长度 max_len_ids=1024                                                    \n全局训练参数的global_args_max_length=1024\ncutoff to len = 1024\n样本中ids 最大长度 max_len_ids=1024\n全局训练参数的global_args_max_length=1024\ncutoff to len = 1024\n\u001b[32m2023-07-31 15:43:30.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m244\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 第一个样本】\n中文马克思主义文库 -> 马克思　恩格斯\n共产党宣言\n\n马克思 恩格斯\n\n（1848年）\n\n\n· 1872年德文版序言\n· 1882年俄文版序言\n· 1883年德文版序言\n· 1888年英文版序言\n· 1890年德文版序言\n· 1892年波兰文版序言\n· 1893年意大利文版序言\n\n共产党宣言\n一、资产者和无产者\n二、无产者和共产党人\n三、社会主义的和共产主义的文献\n四、共产党人对各种反对党派的态度\n人名索引\n\n\n　　〔来源〕原载中共中央马克思恩格斯列宁斯大林著作编译局《共产党宣言（纪念马克思诞辰200周年马克思恩格斯著作特辑》（人民出版社，2018-4-1）\n　　中文马克思主义文库2022年11月更新。\n\n\n1872年德文版序言[1]\n\n　　共产主义者同盟[2]这个在当时条件下自然只能是秘密团体的国际工人组织，1847年11月在伦敦举行的代表大会上委托我们两人起草一个准备公布的详细的理论和实践的党纲。结果就产生了这个《宣言》，《宣言》原稿在二月革命[3]前几星期送到伦敦付印。《宣言》最初用德文出版，它用这种文字在德国、英国和美国至少印过12种不同的版本。第一个英译本是由海伦·麦克法林女士翻译的，于1850年在伦敦《红色共和党人》[4]杂志上发表，1871年至少又有三种不同的英译本在美国出版。法译本于1848年六月起义[5]前不久第一次在巴黎印行，最近又有法译本在纽约《社会主义者报》[6]上发表；现在有人在准备新译本。波兰文译本在德文本初版问世后不久就在伦敦出现。俄译本是60年代在日内瓦出版的。丹麦文译本也是在原书问世后不久就出版了。\n\n　　不管最近25年来的情况发生了多大的变化，这个《宣言》中所阐述的一般原理整个说来直到现在还是完全正确的。某些地方本来可以作一些修改。这些原理的实际运用，正如《宣言》中所说的，随时随地都要以当时的历史条件为转移，所以第二章末尾提出的那些革命措施根本没有特别的意义。如果是在今天，这一段在许多方面都会有不同的写法了。由于最近25年来大工业有了巨大发展而工人阶级的政党组织也跟着发展起来，由于首先有了二月革命的实际经验而后来尤其是有了无产阶级第一次掌握政权达两月之久的巴黎公社[7]的实际经验，所以这个纲领现在有些地方已经过时了。特别是公社已经证明：“工人阶级不能简单地掌握现成的国家机器，并运用它来达到自己的目的。”（见《法兰西内战。国际工人协会总委员会宣言》德文版第19页，那里对这个思想作了更详细的阐述。）〔注：见《马克思恩格斯选集》第3版第3卷第95页。——编者注〕其次，很明显，对于社会主义文献所作的批判在今天看来是不完全的，因为这一批判只包括到1847年为止；同样也很明显，关于共产党人对待各种反对党派的态度的论述（第四章）虽然在原则上今天还是正确的，但是就其实际运用来说今天毕竟已经过时，因为政治形势已经完全改变，当时所列举的那些党派大部分已被历史的发展彻底扫除了。\n\n　　但是《宣言》是一个历史文件，我们已没有权利来加以修改。下次再版时也许能加上一篇论述1847年到现在这段时期的导言。这次再版太仓促了，我们来不及做这件工作。\n\n　　卡尔·马克思　弗里德里希·恩格斯\n　　1872年6月24日于伦敦\n\n\n卡·马克思和弗·恩格斯写于1872年6月24日\n载于1872年在莱比锡出版的德文版《共产主义宣言》一书\n\n原文是德文\n选自《马克思恩格斯选集》第3版第1卷第376-377页\n\n\n\n1882年俄文版序言[8]\n\n　　巴枯宁翻译的《共产党宣言》俄文第一版，60年代初〔注：应是1869年。——编者注〕由《钟声》印刷所出版。当时西方认为这件事（《宣言》译成俄文出版）不过是著作界的一件\u001b[0m\n\u001b[32m2023-07-31 15:43:30.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m244\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 第一个样本】\n中文马克思主义文库 -> 马克思　恩格斯\n共产党宣言\n\n马克思 恩格斯\n\n（1848年）\n\n\n· 1872年德文版序言\n· 1882年俄文版序言\n· 1883年德文版序言\n· 1888年英文版序言\n· 1890年德文版序言\n· 1892年波兰文版序言\n· 1893年意大利文版序言\n\n共产党宣言\n一、资产者和无产者\n二、无产者和共产党人\n三、社会主义的和共产主义的文献\n四、共产党人对各种反对党派的态度\n人名索引\n\n\n　　〔来源〕原载中共中央马克思恩格斯列宁斯大林著作编译局《共产党宣言（纪念马克思诞辰200周年马克思恩格斯著作特辑》（人民出版社，2018-4-1）\n　　中文马克思主义文库2022年11月更新。\n\n\n1872年德文版序言[1]\n\n　　共产主义者同盟[2]这个在当时条件下自然只能是秘密团体的国际工人组织，1847年11月在伦敦举行的代表大会上委托我们两人起草一个准备公布的详细的理论和实践的党纲。结果就产生了这个《宣言》，《宣言》原稿在二月革命[3]前几星期送到伦敦付印。《宣言》最初用德文出版，它用这种文字在德国、英国和美国至少印过12种不同的版本。第一个英译本是由海伦·麦克法林女士翻译的，于1850年在伦敦《红色共和党人》[4]杂志上发表，1871年至少又有三种不同的英译本在美国出版。法译本于1848年六月起义[5]前不久第一次在巴黎印行，最近又有法译本在纽约《社会主义者报》[6]上发表；现在有人在准备新译本。波兰文译本在德文本初版问世后不久就在伦敦出现。俄译本是60年代在日内瓦出版的。丹麦文译本也是在原书问世后不久就出版了。\n\n　　不管最近25年来的情况发生了多大的变化，这个《宣言》中所阐述的一般原理整个说来直到现在还是完全正确的。某些地方本来可以作一些修改。这些原理的实际运用，正如《宣言》中所说的，随时随地都要以当时的历史条件为转移，所以第二章末尾提出的那些革命措施根本没有特别的意义。如果是在今天，这一段在许多方面都会有不同的写法了。由于最近25年来大工业有了巨大发展而工人阶级的政党组织也跟着发展起来，由于首先有了二月革命的实际经验而后来尤其是有了无产阶级第一次掌握政权达两月之久的巴黎公社[7]的实际经验，所以这个纲领现在有些地方已经过时了。特别是公社已经证明：“工人阶级不能简单地掌握现成的国家机器，并运用它来达到自己的目的。”（见《法兰西内战。国际工人协会总委员会宣言》德文版第19页，那里对这个思想作了更详细的阐述。）〔注：见《马克思恩格斯选集》第3版第3卷第95页。——编者注〕其次，很明显，对于社会主义文献所作的批判在今天看来是不完全的，因为这一批判只包括到1847年为止；同样也很明显，关于共产党人对待各种反对党派的态度的论述（第四章）虽然在原则上今天还是正确的，但是就其实际运用来说今天毕竟已经过时，因为政治形势已经完全改变，当时所列举的那些党派大部分已被历史的发展彻底扫除了。\n\n　　但是《宣言》是一个历史文件，我们已没有权利来加以修改。下次再版时也许能加上一篇论述1847年到现在这段时期的导言。这次再版太仓促了，我们来不及做这件工作。\n\n　　卡尔·马克思　弗里德里希·恩格斯\n　　1872年6月24日于伦敦\n\n\n卡·马克思和弗·恩格斯写于1872年6月24日\n载于1872年在莱比锡出版的德文版《共产主义宣言》一书\n\n原文是德文\n选自《马克思恩格斯选集》第3版第1卷第376-377页\n\n\n\n1882年俄文版序言[8]\n\n　　巴枯宁翻译的《共产党宣言》俄文第一版，60年代初〔注：应是1869年。——编者注〕由《钟声》印刷所出版。当时西方认为这件事（《宣言》译成俄文出版）不过是著作界的一件\u001b[0m\n\u001b[32m2023-07-31 15:43:30.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m245\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 倒数第二个样本（长度满足block size）】\n政府主义理论的创始人，第二共和国时期是制宪议会议员（1848）。——10、18、19、59、109、118、124、129。\n\n　　普芬德，卡尔（Pfänder，Carl 1819-1876）——德国微型画画家，德国工人运动和国际工人运动的活动家，1845年起侨居伦敦，正义者同盟盟员，伦敦德意志工人共产主义教育协会会员；1849年巴登-普法尔茨起义的参加者，起义失败后流亡英国；共产主义者同盟中央委员会委员，1850年共产主义者同盟分裂后支持马克思和恩格斯；国际总委员会委员（1864-1867和1870-1872）；马克思和恩格斯的朋友和战友。——104。\n\nR\n\n　　茹柯夫斯基，尤利·加拉克季昂诺维奇（Жуковский, Юлий Галактионович 1822-1907）——俄国资产阶级庸俗经济学家和政论家；国家银行行长；曾撰写《卡尔·马克思和他的〈资本论〉一书》一文，攻击马克思主义。——134。\n\nS\n\n　　沙佩尔，卡尔（Schapper，Karl 1812-1870）——德国工人运动和国际工人运动的活动家，正义者同盟的领导者之一，伦敦德意志工人共产主义教育协会创建人之一，共产主义者同盟中央委员会委员；1848-1849年革命的参加者；民主主义者莱茵区域委员会委员，该委员会案件（1849年2月8日）的被告之一；1849年2-5月为科隆工人联合会主席，《新莱茵报》撰稿人；1850年共产主义者同盟分裂时为冒险主义宗派集团的领袖之一；1856年起重新同马克思和恩格斯接近；国际总委员会委员（1865），1865年伦敦代表会议的参加者。——75、95、96、102、107、110、113、114、144。\n\n　　圣西门，昂利（Saint-Simon，Henri 1760-1825）——法国空想社会主义者。——60。\n\n　　施梯伯，威廉（Stieber，Wilhelm 1818-1882）——普鲁士警官，普鲁士政治警察局局长（1852-1860），科隆共产党人案件（1852）的策划者之一和主要原告证人；同卡·维尔穆特合编《19世纪共产主义者的阴谋》一书；普奥战争（1866）和普法战争（1870-1871）时期为军事警察局局长，在法国境内的德国情报机关的首脑。——94、105。\n\n　　叔尔茨，卡尔（Schurz，Carl 1829-1906）——德国政论家，小资产阶级民主主义者，1849年巴登-普法尔茨起义的参加者，起义失败后流亡瑞士，加入秘密组织“革命集中”；1852年迁居美国，站在北部方面参加美国内战，美国共和党领袖之一，曾任美国驻西班牙公使，后为参议员和内政部长（1877-1881）。——111。\n\nW\n\n　　维尔穆特，卡尔·格奥尔格·路德维希（Wermuth，Carl Georg Ludwig 1803-1867）——德国警官，汉诺威警察局长，科隆共产党人案件（1852）的策划者之一和原告证人；同威·施梯伯合编《19世纪共产主义者的阴谋》一书。——94、105。\n\n　　维利希，奥古斯特（Willich，August 1810-1878）——普鲁士军官，1847年起为共产主义者同盟盟员，1849年巴登-普法尔茨起义中为志愿军团首领；1850年共产主义者同盟分裂时同卡·沙佩尔一起组成反对马克思的冒险主义宗派集团；1853年侨居美国，站在北部方面参加美国内战，任将军。——110、111、113、114。\n\n　　魏特林，克里斯蒂安·威廉（Weitling，\u001b[0m\n\u001b[32m2023-07-31 15:43:30.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m245\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 倒数第二个样本（长度满足block size）】\n政府主义理论的创始人，第二共和国时期是制宪议会议员（1848）。——10、18、19、59、109、118、124、129。\n\n　　普芬德，卡尔（Pfänder，Carl 1819-1876）——德国微型画画家，德国工人运动和国际工人运动的活动家，1845年起侨居伦敦，正义者同盟盟员，伦敦德意志工人共产主义教育协会会员；1849年巴登-普法尔茨起义的参加者，起义失败后流亡英国；共产主义者同盟中央委员会委员，1850年共产主义者同盟分裂后支持马克思和恩格斯；国际总委员会委员（1864-1867和1870-1872）；马克思和恩格斯的朋友和战友。——104。\n\nR\n\n　　茹柯夫斯基，尤利·加拉克季昂诺维奇（Жуковский, Юлий Галактионович 1822-1907）——俄国资产阶级庸俗经济学家和政论家；国家银行行长；曾撰写《卡尔·马克思和他的〈资本论〉一书》一文，攻击马克思主义。——134。\n\nS\n\n　　沙佩尔，卡尔（Schapper，Karl 1812-1870）——德国工人运动和国际工人运动的活动家，正义者同盟的领导者之一，伦敦德意志工人共产主义教育协会创建人之一，共产主义者同盟中央委员会委员；1848-1849年革命的参加者；民主主义者莱茵区域委员会委员，该委员会案件（1849年2月8日）的被告之一；1849年2-5月为科隆工人联合会主席，《新莱茵报》撰稿人；1850年共产主义者同盟分裂时为冒险主义宗派集团的领袖之一；1856年起重新同马克思和恩格斯接近；国际总委员会委员（1865），1865年伦敦代表会议的参加者。——75、95、96、102、107、110、113、114、144。\n\n　　圣西门，昂利（Saint-Simon，Henri 1760-1825）——法国空想社会主义者。——60。\n\n　　施梯伯，威廉（Stieber，Wilhelm 1818-1882）——普鲁士警官，普鲁士政治警察局局长（1852-1860），科隆共产党人案件（1852）的策划者之一和主要原告证人；同卡·维尔穆特合编《19世纪共产主义者的阴谋》一书；普奥战争（1866）和普法战争（1870-1871）时期为军事警察局局长，在法国境内的德国情报机关的首脑。——94、105。\n\n　　叔尔茨，卡尔（Schurz，Carl 1829-1906）——德国政论家，小资产阶级民主主义者，1849年巴登-普法尔茨起义的参加者，起义失败后流亡瑞士，加入秘密组织“革命集中”；1852年迁居美国，站在北部方面参加美国内战，美国共和党领袖之一，曾任美国驻西班牙公使，后为参议员和内政部长（1877-1881）。——111。\n\nW\n\n　　维尔穆特，卡尔·格奥尔格·路德维希（Wermuth，Carl Georg Ludwig 1803-1867）——德国警官，汉诺威警察局长，科隆共产党人案件（1852）的策划者之一和原告证人；同威·施梯伯合编《19世纪共产主义者的阴谋》一书。——94、105。\n\n　　维利希，奥古斯特（Willich，August 1810-1878）——普鲁士军官，1847年起为共产主义者同盟盟员，1849年巴登-普法尔茨起义中为志愿军团首领；1850年共产主义者同盟分裂时同卡·沙佩尔一起组成反对马克思的冒险主义宗派集团；1853年侨居美国，站在北部方面参加美国内战，任将军。——110、111、113、114。\n\n　　魏特林，克里斯蒂安·威廉（Weitling，\u001b[0m\n\u001b[32m2023-07-31 15:43:30.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 最后一个样本（长度不足blcok size）】\n【tokens】=[23688, 27720, 30910, 30939, 30973, 30940, 30973, 30941, 30939, 30973, 30981, 30939, 53437, 32942, 33796, 31903, 31651, 54561, 31123, 36495, 54631, 43166, 35945, 31123, 32156, 54532, 55857, 56985, 54659, 54913, 54695, 32686, 41005, 32135, 45508, 55685, 54587, 54561, 54659, 33796, 43166, 54530, 35461, 32395, 33796, 32892, 54771, 54612, 54530, 32124, 54631, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 31730, 31123, 39435, 33810, 31721, 33796, 32302, 36905, 30939, 30943, 31201, 30939, 30969, 31201, 30969, 30981, 30941, 30939, 30940, 30940, 31201, 30939, 30940, 30966, 31201, 30939, 30939, 30939, 31201, 30939, 30939, 30966, 31201, 30939, 30939, 30969, 31201, 30939, 30966, 30943, 31155, 13, 13, 31478, 31478, 40069, 56642, 31123, 56642, 54662, 38963, 55211, 31065, 42224, 31301, 30959, 325, 486, 31123, 30960, 2161, 6611, 27720, 30910, 55867, 55186, 54927, 380, 455, 337, 30910, 30939, 30973, 30940, 30969, 30941, 30939, 30973, 30978, 30972, 53437, 32942, 45853, 32123, 45508, 54681, 54903, 54561, 31123, 32156, 54532, 54635, 54650, 31123, 54739, 54662, 34793, 32868, 42574, 54659, 30939, 30973, 30966, 30972, 30941, 30939, 30973, 30966, 30969, 39103, 54637, 54534, 55186, 55867, 54930, 38600, 54659, 30939, 30973, 30972, 30978, 30941, 30939, 30973, 30972, 30981, 52317, 41588, 39136, 41005, 34105, 38508, 31123, 41005, 54631, 43166, 35461, 31905, 54542, 43166, 31819, 38508, 31301, 30939, 30973, 30972, 30973, 54540, 30966, 41630, 31798, 54611, 54575, 56236, 58181, 54771, 54612, 32494, 31301, 30939, 30973, 30972, 30973, 30941, 30939, 30973, 30972, 30969, 31798, 32681, 45856, 56236, 58181, 32159, 32007, 54542, 54693, 55954, 31660, 38508, 54659, 45886, 54982, 55049, 33039, 54910, 31769, 54650, 31123, 32180, 55023, 55419, 55443, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 37533, 31123, 30939, 30973, 30970, 30939, 54540, 56081, 55118, 32463, 31123, 30939, 30973, 30970, 30966, 37017, 54534, 56115, 55678, 34602, 54673, 54635, 54650, 54659, 33929, 54542, 55609, 47734, 33695, 54542, 40493, 36905, 30981, 30970, 31201, 30939, 30940, 30970, 31201, 30939, 30940, 30981, 31201, 30939, 30940, 30969, 31201, 30939, 30943, 30940, 31155, 13, 13, 31017, 13, 13, 31478, 31478, 44466, 55717, 54616, 31123, 54772, 31065, 55404, 55075, 31065, 56236, 55532, 55428, 55075, 31065, 54739, 55717, 54954, 31065, 54954, 31301, 30937, 1082, 639, 30915, 31123, 30977, 14649, 30941, 12392, 773, 30941, 30957, 31008, 264, 446, 3019, 11437, 384, 30910, 30939, 30981, 30981, 30966, 30941, 30939, 30973, 30972, 30943, 53437, 37533, 50639, 47450, 34486, 31123, 31832, 35936, 54538, 34317, 54545, 55443, 38942, 32477, 36905, 30970, 30972, 31155, 13, 13, 31478, 31478, 55287, 55075, 31123, 44554, 16747, 54904, 55404, 56313, 55075, 31123, 44554, 31155, 13, 13, 30986, 13, 13, 31478, 31478, 55640, 54693, 54703, 31123, 55256, 55929, 55097, 57100, 31301, 30977, 338, 15373, 31123, 30938, 7078, 2521, 30910, 30939, 30973, 30966, 30940, 30941, 30939, 30969, 30939, 30969, 53437, 32942, 32718, 31123, 55336, 55609, 49413, 37556, 32991, 34136, 33972, 31301, 30939, 30973, 30970, 30940, 30941, 30939, 30973, 30970, 30939, 31798, 41005, 54631, 43166, 55653, 54650, 54542, 54717, 54783, 31123, 54693, 55954, 32555, 54533, 32722, 31301, 30939, 30973, 30970, 30943, 31300, 54530, 34148, 31905, 31123, 54732, 56024, 55225, 36720, 39069, 54716, 55512, 31123, 50678, 54732, 55034, 30989, 46332, 37553, 52603, 30991, 54617, 31913, 54732, 55153, 55630, 54659, 30939, 30973, 30970, 30966, 54540, 54805, 55737, 32463, 31123, 54585, 56081, 55118, 31730, 31123, 35777, 54530, 41203, 54547, 31924, 34768, 31885, 54659, 33348, 36144, 31687, 31836, 31730, 52790, 31123, 54585, 54541, 35152, 48949, 49286, 31301, 30939, 30973, 30973, 30970, 30941, 30939, 30973, 30973, 30969, 31798, 31784, 37396, 54530, 32037, 54542, 33283, 31123, 55172, 54536, 32834, 32924, 34355, 36905, 30939, 30939, 30972, 31155, 13, 13, 31478, 31478, 51329, 41305, 47358, 31301, 31069, 30737, 6856, 31018, 31000, 6866, 30910, 30939, 30973, 30972, 30970, 30941, 30939, 30973, 30969, 30972, 53437, 46161, 33690, 31301, 30939, 30973, 30973, 30939, 30941, 30939, 30973, 30969, 30972, 31966, 16747, 30978, 31201, 30939, 30978, 31155, 13, 13, 13, 32823, 30910, 57906, 56484, 31201, 55233, 56722, 57087, 30910, 52806, 13, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n【最后一个样本原文】=Christian Wilhelm 1808-1871）——德国工人运动活动家，正义者同盟领导人，职业是裁缝；空想平均共产主义理论家和鼓动家；工人同盟的创始人，《工人共和国报》的出版者；1849年流亡美国，晚年接近国际工人协会。——12、19、97-100、103、111、113、119、132。\n\n　　沃尔弗，弗里德里希·威廉（Wolff，Friedrich Wilhelm 鲁普斯 Lupus 1809-1864）——德国无产阶级革命家和政论家，职业是教员，西里西亚农民的儿子；1834-1839年被关在普鲁士监狱；1846-1847年为布鲁塞尔共产主义通讯委员会委员，共产主义者同盟创始人之一和同盟中央委员会委员（1848年3月起），《新莱茵报》编辑（1848-1849），民主主义者莱茵区域委员会和科隆安全委员会委员；法兰克福国民议会议员，属于极左派；1849年流亡瑞士，1851年迁居英国，1853年起在曼彻斯特当教员；马克思和恩格斯的朋友和战友。——75、105、107、109、120。\n\nX\n\n　　西斯蒙第，让·沙尔·莱奥纳尔·西蒙德·德（Sismondi，Jean-Charles-Léonard Simonde de 1773-1842）——瑞士经济学家和历史学家，政治经济学中浪漫学派的代表人物。——54。\n\n　　席尔，卡尔——见沙佩尔，卡尔。\n\nY\n\n　　雅科比，阿伯拉罕（Jacobi，Abraham 1830-1919）——德国医生，波恩体操联合会创建人和会长（1850-1851），共产主义者同盟盟员和特使，科隆共产党人案件（1852）的被告之一，被陪审法庭宣告无罪，但因被控“侮辱国王陛下”而继续被监禁；1853年流亡英国，后迁居美国，在美国的刊物上宣传马克思主义思想；站在北部方面参加美国内战，后为纽约医学科学院院长（1885-1889），一些医学院的教授和院长，写有医学方面的著作。——114。\n\n　　亚历山大三世（Александр III 1845-1894）——俄国皇帝（1881-1894）。——6、16。\n\n\n感谢 聂孟、卫玄鹤 校对\n\n\n【最后一个样本原文长度】=893\u001b[0m\n\u001b[32m2023-07-31 15:43:30.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m验证查看完毕\u001b[0m\n样本数量=20\n\u001b[32m2023-07-31 15:43:30.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 最后一个样本（长度不足blcok size）】\n【tokens】=[23688, 27720, 30910, 30939, 30973, 30940, 30973, 30941, 30939, 30973, 30981, 30939, 53437, 32942, 33796, 31903, 31651, 54561, 31123, 36495, 54631, 43166, 35945, 31123, 32156, 54532, 55857, 56985, 54659, 54913, 54695, 32686, 41005, 32135, 45508, 55685, 54587, 54561, 54659, 33796, 43166, 54530, 35461, 32395, 33796, 32892, 54771, 54612, 54530, 32124, 54631, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 31730, 31123, 39435, 33810, 31721, 33796, 32302, 36905, 30939, 30943, 31201, 30939, 30969, 31201, 30969, 30981, 30941, 30939, 30940, 30940, 31201, 30939, 30940, 30966, 31201, 30939, 30939, 30939, 31201, 30939, 30939, 30966, 31201, 30939, 30939, 30969, 31201, 30939, 30966, 30943, 31155, 13, 13, 31478, 31478, 40069, 56642, 31123, 56642, 54662, 38963, 55211, 31065, 42224, 31301, 30959, 325, 486, 31123, 30960, 2161, 6611, 27720, 30910, 55867, 55186, 54927, 380, 455, 337, 30910, 30939, 30973, 30940, 30969, 30941, 30939, 30973, 30978, 30972, 53437, 32942, 45853, 32123, 45508, 54681, 54903, 54561, 31123, 32156, 54532, 54635, 54650, 31123, 54739, 54662, 34793, 32868, 42574, 54659, 30939, 30973, 30966, 30972, 30941, 30939, 30973, 30966, 30969, 39103, 54637, 54534, 55186, 55867, 54930, 38600, 54659, 30939, 30973, 30972, 30978, 30941, 30939, 30973, 30972, 30981, 52317, 41588, 39136, 41005, 34105, 38508, 31123, 41005, 54631, 43166, 35461, 31905, 54542, 43166, 31819, 38508, 31301, 30939, 30973, 30972, 30973, 54540, 30966, 41630, 31798, 54611, 54575, 56236, 58181, 54771, 54612, 32494, 31301, 30939, 30973, 30972, 30973, 30941, 30939, 30973, 30972, 30969, 31798, 32681, 45856, 56236, 58181, 32159, 32007, 54542, 54693, 55954, 31660, 38508, 54659, 45886, 54982, 55049, 33039, 54910, 31769, 54650, 31123, 32180, 55023, 55419, 55443, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 37533, 31123, 30939, 30973, 30970, 30939, 54540, 56081, 55118, 32463, 31123, 30939, 30973, 30970, 30966, 37017, 54534, 56115, 55678, 34602, 54673, 54635, 54650, 54659, 33929, 54542, 55609, 47734, 33695, 54542, 40493, 36905, 30981, 30970, 31201, 30939, 30940, 30970, 31201, 30939, 30940, 30981, 31201, 30939, 30940, 30969, 31201, 30939, 30943, 30940, 31155, 13, 13, 31017, 13, 13, 31478, 31478, 44466, 55717, 54616, 31123, 54772, 31065, 55404, 55075, 31065, 56236, 55532, 55428, 55075, 31065, 54739, 55717, 54954, 31065, 54954, 31301, 30937, 1082, 639, 30915, 31123, 30977, 14649, 30941, 12392, 773, 30941, 30957, 31008, 264, 446, 3019, 11437, 384, 30910, 30939, 30981, 30981, 30966, 30941, 30939, 30973, 30972, 30943, 53437, 37533, 50639, 47450, 34486, 31123, 31832, 35936, 54538, 34317, 54545, 55443, 38942, 32477, 36905, 30970, 30972, 31155, 13, 13, 31478, 31478, 55287, 55075, 31123, 44554, 16747, 54904, 55404, 56313, 55075, 31123, 44554, 31155, 13, 13, 30986, 13, 13, 31478, 31478, 55640, 54693, 54703, 31123, 55256, 55929, 55097, 57100, 31301, 30977, 338, 15373, 31123, 30938, 7078, 2521, 30910, 30939, 30973, 30966, 30940, 30941, 30939, 30969, 30939, 30969, 53437, 32942, 32718, 31123, 55336, 55609, 49413, 37556, 32991, 34136, 33972, 31301, 30939, 30973, 30970, 30940, 30941, 30939, 30973, 30970, 30939, 31798, 41005, 54631, 43166, 55653, 54650, 54542, 54717, 54783, 31123, 54693, 55954, 32555, 54533, 32722, 31301, 30939, 30973, 30970, 30943, 31300, 54530, 34148, 31905, 31123, 54732, 56024, 55225, 36720, 39069, 54716, 55512, 31123, 50678, 54732, 55034, 30989, 46332, 37553, 52603, 30991, 54617, 31913, 54732, 55153, 55630, 54659, 30939, 30973, 30970, 30966, 54540, 54805, 55737, 32463, 31123, 54585, 56081, 55118, 31730, 31123, 35777, 54530, 41203, 54547, 31924, 34768, 31885, 54659, 33348, 36144, 31687, 31836, 31730, 52790, 31123, 54585, 54541, 35152, 48949, 49286, 31301, 30939, 30973, 30973, 30970, 30941, 30939, 30973, 30973, 30969, 31798, 31784, 37396, 54530, 32037, 54542, 33283, 31123, 55172, 54536, 32834, 32924, 34355, 36905, 30939, 30939, 30972, 31155, 13, 13, 31478, 31478, 51329, 41305, 47358, 31301, 31069, 30737, 6856, 31018, 31000, 6866, 30910, 30939, 30973, 30972, 30970, 30941, 30939, 30973, 30969, 30972, 53437, 46161, 33690, 31301, 30939, 30973, 30973, 30939, 30941, 30939, 30973, 30969, 30972, 31966, 16747, 30978, 31201, 30939, 30978, 31155, 13, 13, 13, 32823, 30910, 57906, 56484, 31201, 55233, 56722, 57087, 30910, 52806, 13, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n【最后一个样本原文】=Christian Wilhelm 1808-1871）——德国工人运动活动家，正义者同盟领导人，职业是裁缝；空想平均共产主义理论家和鼓动家；工人同盟的创始人，《工人共和国报》的出版者；1849年流亡美国，晚年接近国际工人协会。——12、19、97-100、103、111、113、119、132。\n\n　　沃尔弗，弗里德里希·威廉（Wolff，Friedrich Wilhelm 鲁普斯 Lupus 1809-1864）——德国无产阶级革命家和政论家，职业是教员，西里西亚农民的儿子；1834-1839年被关在普鲁士监狱；1846-1847年为布鲁塞尔共产主义通讯委员会委员，共产主义者同盟创始人之一和同盟中央委员会委员（1848年3月起），《新莱茵报》编辑（1848-1849），民主主义者莱茵区域委员会和科隆安全委员会委员；法兰克福国民议会议员，属于极左派；1849年流亡瑞士，1851年迁居英国，1853年起在曼彻斯特当教员；马克思和恩格斯的朋友和战友。——75、105、107、109、120。\n\nX\n\n　　西斯蒙第，让·沙尔·莱奥纳尔·西蒙德·德（Sismondi，Jean-Charles-Léonard Simonde de 1773-1842）——瑞士经济学家和历史学家，政治经济学中浪漫学派的代表人物。——54。\n\n　　席尔，卡尔——见沙佩尔，卡尔。\n\nY\n\n　　雅科比，阿伯拉罕（Jacobi，Abraham 1830-1919）——德国医生，波恩体操联合会创建人和会长（1850-1851），共产主义者同盟盟员和特使，科隆共产党人案件（1852）的被告之一，被陪审法庭宣告无罪，但因被控“侮辱国王陛下”而继续被监禁；1853年流亡英国，后迁居美国，在美国的刊物上宣传马克思主义思想；站在北部方面参加美国内战，后为纽约医学科学院院长（1885-1889），一些医学院的教授和院长，写有医学方面的著作。——114。\n\n　　亚历山大三世（Александр III 1845-1894）——俄国皇帝（1881-1894）。——6、16。\n\n\n感谢 聂孟、卫玄鹤 校对\n\n\n【最后一个样本原文长度】=893\u001b[0m\n\u001b[32m2023-07-31 15:43:30.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m验证查看完毕\u001b[0m\n样本数量=20\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:44<00:00, 14.89s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:44<00:00, 14.89s/it]\nmemory footprint of model: 3.6520424485206604 GB\n\u001b[32m2023-07-31 15:45:16.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m568\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\nmemory footprint of model: 3.6520424485206604 GB\n\u001b[32m2023-07-31 15:45:16.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m568\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nnumber_train_samples=40\nnumber_of_eval_numbers=20\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=5e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=1,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-pt,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=40.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-pt,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=1,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.0,\nxpu_backend=None,\n)\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nnumber_train_samples=40\nnumber_of_eval_numbers=20\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=5e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-pt,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=40.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-pt,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=1,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.0,\nxpu_backend=None,\n)\n{'loss': 9.3749, 'learning_rate': 4.375e-06, 'epoch': 0.5}                      \n  1%|▌                                         | 10/800 [00:34<42:28,  3.23s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:04,  1.70it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:05,  1.17it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:05,  1.01it/s]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:04<00:05,  1.06s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.11s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.15s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.17s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:09<00:01,  1.19s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 9.158706665039062, 'eval_runtime': 13.4102, 'eval_samples_per_second': 1.491, 'eval_steps_per_second': 0.746, 'epoch': 0.5}\n  1%|▌                                         | 10/800 [00:48<42:28,  3.23s/it]\n100%|███████████████████████████████████████████| 10/10 [00:11<00:00,  1.20s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 8.3408, 'learning_rate': 1e-05, 'epoch': 1.0}                          \n  2%|█                                         | 20/800 [01:23<46:12,  3.55s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.56it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.12it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.04s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.11s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.16s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.20s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.22s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.24s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 8.151468276977539, 'eval_runtime': 12.6858, 'eval_samples_per_second': 1.577, 'eval_steps_per_second': 0.788, 'epoch': 1.0}\n  2%|█                                         | 20/800 [01:36<46:12,  3.55s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.25s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 7.3967, 'learning_rate': 1.6250000000000002e-05, 'epoch': 1.5}         \n  4%|█▌                                        | 30/800 [02:13<47:19,  3.69s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 6.320302486419678, 'eval_runtime': 12.821, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.78, 'epoch': 1.5}\n  4%|█▌                                        | 30/800 [02:26<47:19,  3.69s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 5.3765, 'learning_rate': 2.1875e-05, 'epoch': 2.0}                     \n  5%|██                                        | 40/800 [03:04<47:10,  3.72s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.56it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.11it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.04s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 4.975358963012695, 'eval_runtime': 12.7857, 'eval_samples_per_second': 1.564, 'eval_steps_per_second': 0.782, 'epoch': 2.0}\n  5%|██                                        | 40/800 [03:17<47:10,  3.72s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 4.1276, 'learning_rate': 2.7500000000000004e-05, 'epoch': 2.5}         \n  6%|██▋                                       | 50/800 [03:55<46:57,  3.76s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 3.8995280265808105, 'eval_runtime': 12.8126, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.78, 'epoch': 2.5}\n  6%|██▋                                       | 50/800 [04:07<46:57,  3.76s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 3.6702, 'learning_rate': 3.375000000000001e-05, 'epoch': 3.0}          \n  8%|███▏                                      | 60/800 [04:45<46:05,  3.74s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 3.1182117462158203, 'eval_runtime': 12.8171, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.78, 'epoch': 3.0}\n  8%|███▏                                      | 60/800 [04:58<46:05,  3.74s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 2.9281, 'learning_rate': 4e-05, 'epoch': 3.5}                          \n  9%|███▋                                      | 70/800 [05:36<45:26,  3.74s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.688452959060669, 'eval_runtime': 12.8141, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.78, 'epoch': 3.5}\n  9%|███▋                                      | 70/800 [05:49<45:26,  3.74s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 2.5833, 'learning_rate': 4.6250000000000006e-05, 'epoch': 4.0}         \n 10%|████▏                                     | 80/800 [06:26<44:55,  3.74s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.53it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.3532204627990723, 'eval_runtime': 12.8229, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.78, 'epoch': 4.0}\n 10%|████▏                                     | 80/800 [06:39<44:55,  3.74s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 2.1908, 'learning_rate': 4.972222222222223e-05, 'epoch': 4.5}          \n 11%|████▋                                     | 90/800 [07:17<44:06,  3.73s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.0095479488372803, 'eval_runtime': 12.8107, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.781, 'epoch': 4.5}\n 11%|████▋                                     | 90/800 [07:29<44:06,  3.73s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 2.0675, 'learning_rate': 4.902777777777778e-05, 'epoch': 5.0}          \n 12%|█████▏                                   | 100/800 [08:07<43:43,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.777112603187561, 'eval_runtime': 12.813, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.78, 'epoch': 5.0}\n 12%|█████▏                                   | 100/800 [08:20<43:43,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 1.6654, 'learning_rate': 4.8333333333333334e-05, 'epoch': 5.5}         \n 14%|█████▋                                   | 110/800 [08:58<42:55,  3.73s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.3937040567398071, 'eval_runtime': 12.8147, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.78, 'epoch': 5.5}\n 14%|█████▋                                   | 110/800 [09:10<42:55,  3.73s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 1.4235, 'learning_rate': 4.7638888888888887e-05, 'epoch': 6.0}         \n 15%|██████▏                                  | 120/800 [09:49<42:43,  3.77s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.2182728052139282, 'eval_runtime': 12.835, 'eval_samples_per_second': 1.558, 'eval_steps_per_second': 0.779, 'epoch': 6.0}\n 15%|██████▏                                  | 120/800 [10:02<42:43,  3.77s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 1.1643, 'learning_rate': 4.6944444444444446e-05, 'epoch': 6.5}         \n 16%|██████▋                                  | 130/800 [10:39<41:39,  3.73s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.54it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.10it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.056727409362793, 'eval_runtime': 12.7906, 'eval_samples_per_second': 1.564, 'eval_steps_per_second': 0.782, 'epoch': 6.5}\n 16%|██████▋                                  | 130/800 [10:52<41:39,  3.73s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 1.0491, 'learning_rate': 4.6250000000000006e-05, 'epoch': 7.0}         \n 18%|███████▏                                 | 140/800 [11:30<41:12,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.8522399663925171, 'eval_runtime': 12.8126, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.78, 'epoch': 7.0}\n 18%|███████▏                                 | 140/800 [11:43<41:12,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.25s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.772, 'learning_rate': 4.555555555555556e-05, 'epoch': 7.5}           \n 19%|███████▋                                 | 150/800 [12:20<40:37,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.53it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6952500939369202, 'eval_runtime': 12.8113, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.781, 'epoch': 7.5}\n 19%|███████▋                                 | 150/800 [12:33<40:37,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.7081, 'learning_rate': 4.486111111111111e-05, 'epoch': 8.0}          \n 20%|████████▏                                | 160/800 [13:11<39:47,  3.73s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.53it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.10it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5874060988426208, 'eval_runtime': 12.799, 'eval_samples_per_second': 1.563, 'eval_steps_per_second': 0.781, 'epoch': 8.0}\n 20%|████████▏                                | 160/800 [13:24<39:47,  3.73s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.391, 'learning_rate': 4.4166666666666665e-05, 'epoch': 8.5}          \n 21%|████████▋                                | 170/800 [14:01<39:22,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.08it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.4233517646789551, 'eval_runtime': 12.8297, 'eval_samples_per_second': 1.559, 'eval_steps_per_second': 0.779, 'epoch': 8.5}\n 21%|████████▋                                | 170/800 [14:14<39:22,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.5461, 'learning_rate': 4.3472222222222225e-05, 'epoch': 9.0}         \n 22%|█████████▏                               | 180/800 [14:52<38:45,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.54it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.10it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.3296318054199219, 'eval_runtime': 12.7891, 'eval_samples_per_second': 1.564, 'eval_steps_per_second': 0.782, 'epoch': 9.0}\n 22%|█████████▏                               | 180/800 [15:05<38:45,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!save 123 !!!!!!\n\n{'loss': 0.3341, 'learning_rate': 4.277777777777778e-05, 'epoch': 9.5}          \n 24%|█████████▋                               | 190/800 [15:43<38:03,  3.74s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.52it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.3400401175022125, 'eval_runtime': 12.8221, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.78, 'epoch': 9.5}\n 24%|█████████▋                               | 190/800 [15:55<38:03,  3.74s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.2921, 'learning_rate': 4.208333333333334e-05, 'epoch': 10.0}         \n 25%|██████████▎                              | 200/800 [16:33<37:28,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.2524644732475281, 'eval_runtime': 12.8372, 'eval_samples_per_second': 1.558, 'eval_steps_per_second': 0.779, 'epoch': 10.0}\n 25%|██████████▎                              | 200/800 [16:46<37:28,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.159, 'learning_rate': 4.138888888888889e-05, 'epoch': 10.5}          \n 26%|██████████▊                              | 210/800 [17:24<36:51,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.23511140048503876, 'eval_runtime': 12.8174, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.78, 'epoch': 10.5}\n 26%|██████████▊                              | 210/800 [17:37<36:51,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.2787, 'learning_rate': 4.0694444444444444e-05, 'epoch': 11.0}        \n 28%|███████████▎                             | 220/800 [18:15<36:06,  3.73s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.2081928700208664, 'eval_runtime': 12.8193, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.78, 'epoch': 11.0}\n 28%|███████████▎                             | 220/800 [18:27<36:06,  3.73s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.107, 'learning_rate': 4e-05, 'epoch': 11.5}                          \n 29%|███████████▊                             | 230/800 [19:05<35:35,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.53it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.10it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.24s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.1859593242406845, 'eval_runtime': 12.789, 'eval_samples_per_second': 1.564, 'eval_steps_per_second': 0.782, 'epoch': 11.5}\n 29%|███████████▊                             | 230/800 [19:18<35:35,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.25s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.2416, 'learning_rate': 3.9305555555555556e-05, 'epoch': 12.0}        \n 30%|████████████▎                            | 240/800 [19:56<34:52,  3.74s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.17775551974773407, 'eval_runtime': 12.8272, 'eval_samples_per_second': 1.559, 'eval_steps_per_second': 0.78, 'epoch': 12.0}\n 30%|████████████▎                            | 240/800 [20:09<34:52,  3.74s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.0817, 'learning_rate': 3.8611111111111116e-05, 'epoch': 12.5}        \n 31%|████████████▊                            | 250/800 [20:46<34:16,  3.74s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.54it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.10it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.24s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.17295053601264954, 'eval_runtime': 12.7821, 'eval_samples_per_second': 1.565, 'eval_steps_per_second': 0.782, 'epoch': 12.5}\n 31%|████████████▊                            | 250/800 [20:59<34:16,  3.74s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.1949, 'learning_rate': 3.791666666666667e-05, 'epoch': 13.0}         \n 32%|█████████████▎                           | 260/800 [21:37<33:21,  3.71s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.53it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.15463270246982574, 'eval_runtime': 12.8101, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.781, 'epoch': 13.0}\n 32%|█████████████▎                           | 260/800 [21:50<33:21,  3.71s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.25s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.1764, 'learning_rate': 3.722222222222222e-05, 'epoch': 13.5}         \n 34%|█████████████▊                           | 270/800 [22:27<32:58,  3.73s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.53it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.1672443002462387, 'eval_runtime': 12.8193, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.78, 'epoch': 13.5}\n 34%|█████████████▊                           | 270/800 [22:40<32:58,  3.73s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.0695, 'learning_rate': 3.6527777777777775e-05, 'epoch': 14.0}        \n 35%|██████████████▎                          | 280/800 [23:18<32:27,  3.74s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.14790131151676178, 'eval_runtime': 12.8342, 'eval_samples_per_second': 1.558, 'eval_steps_per_second': 0.779, 'epoch': 14.0}\n 35%|██████████████▎                          | 280/800 [23:31<32:27,  3.74s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n 35%|██████████████▌                          | 283/800 [23:44<50:14,  5.83s/it]^C\n[2023-07-31 16:10:45,747] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4066\n","output_type":"stream"}]}]}