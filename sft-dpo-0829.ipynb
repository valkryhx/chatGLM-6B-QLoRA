{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-29T11:37:41.963503Z","iopub.execute_input":"2023-08-29T11:37:41.963772Z","iopub.status.idle":"2023-08-29T11:37:41.970936Z","shell.execute_reply.started":"2023-08-29T11:37:41.963747Z","shell.execute_reply":"2023-08-29T11:37:41.969993Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone https://github.com/valkryhx/chatGLM-6B-QLoRA\n%cd chatGLM-6B-QLoRA\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-08-29T13:23:29.705352Z","iopub.execute_input":"2023-08-29T13:23:29.706330Z","iopub.status.idle":"2023-08-29T13:23:43.883064Z","shell.execute_reply.started":"2023-08-29T13:23:29.706293Z","shell.execute_reply":"2023-08-29T13:23:43.881834Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working\nfatal: destination path 'chatGLM-6B-QLoRA' already exists and is not an empty directory.\n/kaggle/working/chatGLM-6B-QLoRA\nRequirement already satisfied: peft==0.4.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.4.0)\nRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.30.2)\nRequirement already satisfied: datasets==2.12.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.12.0)\nRequirement already satisfied: tqdm==4.65.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.65.0)\nRequirement already satisfied: loguru==0.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.7.0)\nRequirement already satisfied: fire==0.5.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.5.0)\nRequirement already satisfied: bitsandbytes==0.39.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.39.0)\nRequirement already satisfied: wandb==0.15.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.15.3)\nRequirement already satisfied: cpm_kernels==1.0.11 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (1.0.11)\nRequirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.20.3)\nRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.1.99)\nRequirement already satisfied: deepspeed==0.9.5 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.9.5)\nRequirement already satisfied: evaluate==0.4.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (0.4.0)\nRequirement already satisfied: trl==0.5.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (0.5.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (2.0.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (0.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (11.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (2.3.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.1.31)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.20.3)\nRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (3.1.0)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.11.1)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (9.0.0)\nRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.10.10)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (4.0.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2->-r requirements.txt (line 2)) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0->-r requirements.txt (line 1)) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2023.5.7)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2023.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install bitsandbytes==0.41.1\n!pip install peft==0.5.0 \n!pip install accelerate==0.21.0 \n!pip install trl==0.6.0","metadata":{"execution":{"iopub.status.busy":"2023-08-29T13:24:50.517621Z","iopub.execute_input":"2023-08-29T13:24:50.517989Z","iopub.status.idle":"2023-08-29T13:25:48.203829Z","shell.execute_reply.started":"2023-08-29T13:24:50.517956Z","shell.execute_reply":"2023-08-29T13:25:48.202599Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes==0.41.1\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\n  Attempting uninstall: bitsandbytes\n    Found existing installation: bitsandbytes 0.39.0\n    Uninstalling bitsandbytes-0.39.0:\n      Successfully uninstalled bitsandbytes-0.39.0\nSuccessfully installed bitsandbytes-0.41.1\nCollecting peft==0.5.0\n  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (4.30.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (4.65.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (0.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.5.0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0) (0.13.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.5.0) (2023.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.5.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.5.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.5.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.5.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.5.0) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.5.0) (1.3.0)\nInstalling collected packages: peft\n  Attempting uninstall: peft\n    Found existing installation: peft 0.4.0\n    Uninstalling peft-0.4.0:\n      Successfully uninstalled peft-0.4.0\nSuccessfully installed peft-0.5.0\nCollecting accelerate==0.21.0\n  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (6.0)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.21.0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.20.3\n    Uninstalling accelerate-0.20.3:\n      Successfully uninstalled accelerate-0.20.3\nSuccessfully installed accelerate-0.21.0\nCollecting trl==0.6.0\n  Downloading trl-0.6.0-py3-none-any.whl (110 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.0/110.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.6.0) (2.0.0)\nRequirement already satisfied: transformers>=4.18.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.6.0) (4.30.2)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.6.0) (1.23.5)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.6.0) (0.21.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.6.0) (2.12.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.6.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.6.0) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.6.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.6.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.6.0) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (0.16.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (4.65.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.6.0) (5.9.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (11.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.6.0) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.6.0) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.6.0) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.6.0) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.6.0) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.6.0) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.6.0) (1.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.18.0->trl==0.6.0) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.6.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.6.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.6.0) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.6.0) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.6.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.6.0) (2023.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.6.0) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets->trl==0.6.0) (1.16.0)\nInstalling collected packages: trl\n  Attempting uninstall: trl\n    Found existing installation: trl 0.5.0\n    Uninstalling trl-0.5.0:\n      Successfully uninstalled trl-0.5.0\nSuccessfully installed trl-0.6.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":" * # <font color=red>首先sft  with qlora </font> ","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/chatGLM-6B-QLoRA\n!git pull --all --force \n!deepspeed --include localhost:0,1  train_qlora_deepspeed_zero.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output_yungaun_0827_v1 \\\n  --num_train_samples -1 \\\n  --num_eval_samples 100 \\\n  --train_data_path ./data/augment_staff298_qa70and14  \\\n  --eval_data_path  ./data/augment_staff298_qa70and14    \\\n  --max_input_length 256 \\\n  --max_output_length 400 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 2  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  1e-5 \\\n  --num_train_epochs  20  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:06:19.273595Z","iopub.execute_input":"2023-08-29T12:06:19.274038Z","iopub.status.idle":"2023-08-29T12:21:15.697404Z","shell.execute_reply.started":"2023-08-29T12:06:19.274001Z","shell.execute_reply":"2023-08-29T12:21:15.695824Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/working/chatGLM-6B-QLoRA\nFetching origin\nremote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (3/3), 709 bytes | 354.00 KiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   addf61b..623d4a3  main       -> origin/main\nUpdating addf61b..623d4a3\nFast-forward\n train_qlora_deepspeed_zero.py | 3 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n 1 file changed, 2 insertions(+), 1 deletion(-)\n[2023-08-29 12:06:28,035] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-29 12:06:50,685] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n[2023-08-29 12:06:50,701] [INFO] [runner.py:555:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train_qlora_deepspeed_zero.py --train_args_json luzi.json --model_name_or_path THUDM/chatglm2-6b --output_dir output_yungaun_0827_v1 --num_train_samples -1 --num_eval_samples 100 --train_data_path ./data/augment_staff298_qa70and14 --eval_data_path ./data/augment_staff298_qa70and14 --max_input_length 256 --max_output_length 400 --lora_rank 64 --lora_dropout 0.05 --compute_dtype fp16 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_steps 1 --learning_rate 1e-5 --num_train_epochs 20 --save_total_limit 2 --load_in_4bit True --deepspeed ds_zero2_config.json\n[2023-08-29 12:06:52,533] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-29 12:06:58,649] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n[2023-08-29 12:06:58,649] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n[2023-08-29 12:06:58,649] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n[2023-08-29 12:06:58,649] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n[2023-08-29 12:06:58,649] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n[2023-08-29 12:06:58,649] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n[2023-08-29 12:06:58,650] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n[2023-08-29 12:06:58,650] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n[2023-08-29 12:06:58,650] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n[2023-08-29 12:06:58,650] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n[2023-08-29 12:06:58,650] [INFO] [launch.py:163:main] dist_world_size=2\n[2023-08-29 12:06:58,650] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n[2023-08-29 12:07:02,044] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-08-29 12:07:02,095] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:36<00:00, 13.72s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:36<00:00, 13.73s/it]\nmemory footprint of model: 3.6520424485206604 GB\n\u001b[32m2023-08-29 12:08:50.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m415\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\nmemory footprint of model: 3.6520424485206604 GB\n\u001b[32m2023-08-29 12:08:50.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m415\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\n\u001b[32m2023-08-29 12:10:34.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m450\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-29 12:10:34.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mdata files: ./data/augment_staff298_qa70and14/augment_0715_qa_small.json, ./data/augment_staff298_qa70and14/refuse_phone.json, ./data/augment_staff298_qa70and14/augment_0715_qa70.json, ./data/augment_staff298_qa70and14/staff_short_answer.json\u001b[0m\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\n\u001b[32m2023-08-29 12:10:35.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m450\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-29 12:10:35.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mdata files: ./data/augment_staff298_qa70and14/augment_0715_qa_small.json, ./data/augment_staff298_qa70and14/refuse_phone.json, ./data/augment_staff298_qa70and14/augment_0715_qa70.json, ./data/augment_staff298_qa70and14/staff_short_answer.json\u001b[0m\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 10.31it/s]\n\u001b[32m2023-08-29 12:10:35.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1m在取样之前 data len =2854\u001b[0m\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  9.10it/s]\n\u001b[32m2023-08-29 12:10:35.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1m在取样之后 data len =2854\u001b[0m\n\u001b[32m2023-08-29 12:10:35.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n\u001b[32m2023-08-29 12:10:35.457\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1m在取样之前 data len =2854\u001b[0m\n\u001b[32m2023-08-29 12:10:35.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1m在取样之后 data len =2854\u001b[0m\n\u001b[32m2023-08-29 12:10:35.458\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n\u001b[32m2023-08-29 12:10:35.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mdata files: ./data/augment_staff298_qa70and14/augment_0715_qa_small.json, ./data/augment_staff298_qa70and14/refuse_phone.json, ./data/augment_staff298_qa70and14/augment_0715_qa70.json, ./data/augment_staff298_qa70and14/staff_short_answer.json\u001b[0m\n\u001b[32m2023-08-29 12:10:35.561\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mdata files: ./data/augment_staff298_qa70and14/augment_0715_qa_small.json, ./data/augment_staff298_qa70and14/refuse_phone.json, ./data/augment_staff298_qa70and14/augment_0715_qa70.json, ./data/augment_staff298_qa70and14/staff_short_answer.json\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 475.71it/s]\n\u001b[32m2023-08-29 12:10:35.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1m在取样之前 data len =2854\u001b[0m\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[32m2023-08-29 12:10:35.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1m在取样之后 data len =100\u001b[0m\n\u001b[32m2023-08-29 12:10:35.757\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 443.28it/s]\n\u001b[32m2023-08-29 12:10:35.758\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1m在取样之前 data len =2854\u001b[0m\n\u001b[32m2023-08-29 12:10:35.761\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1m在取样之后 data len =100\u001b[0m\n\u001b[32m2023-08-29 12:10:35.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\nnumber_train_samples=2854\nnumber_of_eval_numbers=100\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=100,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=1e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=1,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output_yungaun_0827_v1,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=20.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output_yungaun_0827_v1,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=2,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=100,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\nnumber_train_samples=2854\nnumber_of_eval_numbers=100\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=100,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=1e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output_yungaun_0827_v1,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=20.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output_yungaun_0827_v1,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=2,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=100,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\n{'loss': 4.9534, 'learning_rate': 9e-06, 'epoch': 0.01}                         \n{'loss': 4.3591, 'learning_rate': 9.993693062368606e-06, 'epoch': 0.03}         \n{'loss': 3.7717, 'learning_rate': 9.987386124737211e-06, 'epoch': 0.04}         \n{'loss': 3.0602, 'learning_rate': 9.980378416257884e-06, 'epoch': 0.06}         \n{'loss': 2.6684, 'learning_rate': 9.973370707778557e-06, 'epoch': 0.07}         \n{'loss': 1.8622, 'learning_rate': 9.96636299929923e-06, 'epoch': 0.08}          \n{'loss': 1.7939, 'learning_rate': 9.959355290819903e-06, 'epoch': 0.1}          \n{'loss': 1.7142, 'learning_rate': 9.952347582340575e-06, 'epoch': 0.11}         \n{'loss': 1.6426, 'learning_rate': 9.945339873861248e-06, 'epoch': 0.13}         \n{'loss': 1.3934, 'learning_rate': 9.938332165381921e-06, 'epoch': 0.14}         \n  1%|▎                                    | 100/14280 [01:52<4:36:20,  1.17s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.52it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.26it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:07,  2.68it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:08,  2.38it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:02<00:08,  2.13it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:08,  2.16it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:03<00:07,  2.24it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:03<00:06,  2.31it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:04<00:06,  2.34it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:04<00:06,  2.26it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:05<00:05,  2.19it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:05<00:05,  2.07it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:06<00:05,  2.09it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:06<00:04,  2.21it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:07<00:04,  2.11it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:07<00:03,  2.18it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:07<00:03,  2.28it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:08<00:02,  2.38it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:08<00:02,  2.40it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:09<00:01,  2.28it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:09<00:01,  2.19it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:09<00:00,  2.29it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:10<00:00,  2.30it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 3.1766605377197266, 'eval_runtime': 12.4635, 'eval_samples_per_second': 8.023, 'eval_steps_per_second': 2.006, 'epoch': 0.14}\n  1%|▎                                    | 100/14280 [02:04<4:36:20,  1.17s/it]\n100%|███████████████████████████████████████████| 25/25 [00:11<00:00,  2.32it/s]\u001b[A\n                                                                                \u001b[ABegin to save...\nBegin to save...\nthis process is not main process , do not save model.[for distributed training scenario]\n\u001b[32m2023-08-29 12:12:43.788\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m191\u001b[0m - \u001b[31m\u001b[1mSave done.\u001b[0m\n{'loss': 1.4296, 'learning_rate': 9.931324456902594e-06, 'epoch': 0.15}         \n{'loss': 1.7425, 'learning_rate': 9.924316748423267e-06, 'epoch': 0.17}         \n{'loss': 1.0783, 'learning_rate': 9.917309039943938e-06, 'epoch': 0.18}         \n{'loss': 1.6165, 'learning_rate': 9.910301331464612e-06, 'epoch': 0.2}          \n{'loss': 1.4152, 'learning_rate': 9.903293622985285e-06, 'epoch': 0.21}         \n{'loss': 0.8503, 'learning_rate': 9.896285914505958e-06, 'epoch': 0.22}         \n{'loss': 1.1237, 'learning_rate': 9.889278206026631e-06, 'epoch': 0.24}         \n{'loss': 0.9925, 'learning_rate': 9.882270497547302e-06, 'epoch': 0.25}         \n{'loss': 0.7133, 'learning_rate': 9.875262789067975e-06, 'epoch': 0.27}         \n{'loss': 0.6313, 'learning_rate': 9.868255080588648e-06, 'epoch': 0.28}         \n  1%|▌                                    | 200/14280 [03:58<3:51:23,  1.01it/s]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:06,  3.83it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.45it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:08,  2.58it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:08,  2.32it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:02<00:08,  2.16it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:08,  2.17it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:03<00:07,  2.23it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:03<00:06,  2.29it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:04<00:06,  2.31it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:04<00:06,  2.25it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:05<00:06,  2.15it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:05<00:05,  2.03it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:06<00:05,  2.03it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:06<00:04,  2.16it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:07<00:04,  2.04it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:07<00:03,  2.13it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:07<00:03,  2.26it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:08<00:02,  2.35it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:08<00:02,  2.37it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:09<00:01,  2.23it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:09<00:01,  2.14it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:10<00:00,  2.24it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:10<00:00,  2.25it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.2252094745635986, 'eval_runtime': 11.4495, 'eval_samples_per_second': 8.734, 'eval_steps_per_second': 2.183, 'epoch': 0.28}\n  1%|▌                                    | 200/14280 [04:10<3:51:23,  1.01it/s]\n100%|███████████████████████████████████████████| 25/25 [00:11<00:00,  2.29it/s]\u001b[A\n                                                                                \u001b[ABegin to save...\nthis process is not main process , do not save model.[for distributed training scenario]\nBegin to save...\n\u001b[32m2023-08-29 12:14:48.996\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m191\u001b[0m - \u001b[31m\u001b[1mSave done.\u001b[0m\n{'loss': 1.1241, 'learning_rate': 9.861247372109322e-06, 'epoch': 0.29}         \n{'loss': 0.871, 'learning_rate': 9.854239663629993e-06, 'epoch': 0.31}          \n{'loss': 0.7436, 'learning_rate': 9.847231955150666e-06, 'epoch': 0.32}         \n{'loss': 0.8167, 'learning_rate': 9.840224246671339e-06, 'epoch': 0.34}         \n{'loss': 0.5391, 'learning_rate': 9.833216538192012e-06, 'epoch': 0.35}         \n{'loss': 0.6225, 'learning_rate': 9.826208829712685e-06, 'epoch': 0.36}         \n{'loss': 1.1126, 'learning_rate': 9.819201121233358e-06, 'epoch': 0.38}         \n{'loss': 0.5675, 'learning_rate': 9.81219341275403e-06, 'epoch': 0.39}          \n{'loss': 0.6669, 'learning_rate': 9.805185704274703e-06, 'epoch': 0.41}         \n{'loss': 0.5068, 'learning_rate': 9.798177995795376e-06, 'epoch': 0.42}         \n  2%|▊                                    | 300/14280 [06:00<4:06:46,  1.06s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:04,  5.25it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.29it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:07,  2.67it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:08,  2.32it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:02<00:09,  2.10it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:08,  2.13it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:03<00:07,  2.21it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:03<00:07,  2.27it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:04<00:06,  2.28it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:04<00:06,  2.23it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:05<00:06,  2.14it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:05<00:05,  2.04it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:06<00:05,  1.97it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:06<00:04,  2.06it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:07<00:04,  2.00it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:07<00:03,  2.08it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:07<00:03,  2.33it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:08<00:02,  2.41it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:08<00:02,  2.41it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:09<00:01,  2.27it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:09<00:01,  2.17it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:10<00:00,  2.23it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:10<00:00,  2.26it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.48621848225593567, 'eval_runtime': 11.4547, 'eval_samples_per_second': 8.73, 'eval_steps_per_second': 2.183, 'epoch': 0.42}\n  2%|▊                                    | 300/14280 [06:11<4:06:46,  1.06s/it]\n100%|███████████████████████████████████████████| 25/25 [00:11<00:00,  2.25it/s]\u001b[A\n                                                                                \u001b[ABegin to save...\nthis process is not main process , do not save model.[for distributed training scenario]\nBegin to save...\n\u001b[32m2023-08-29 12:16:50.579\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m191\u001b[0m - \u001b[31m\u001b[1mSave done.\u001b[0m\n{'loss': 0.5029, 'learning_rate': 9.791170287316049e-06, 'epoch': 0.43}         \n{'loss': 0.5686, 'learning_rate': 9.784162578836722e-06, 'epoch': 0.45}         \n{'loss': 0.4554, 'learning_rate': 9.777154870357393e-06, 'epoch': 0.46}         \n{'loss': 0.5087, 'learning_rate': 9.770147161878066e-06, 'epoch': 0.48}         \n{'loss': 0.6345, 'learning_rate': 9.763139453398739e-06, 'epoch': 0.49}         \n{'loss': 0.5143, 'learning_rate': 9.756131744919413e-06, 'epoch': 0.5}          \n{'loss': 0.4177, 'learning_rate': 9.749124036440086e-06, 'epoch': 0.52}         \n{'loss': 0.4534, 'learning_rate': 9.742116327960757e-06, 'epoch': 0.53}         \n{'loss': 0.6328, 'learning_rate': 9.73510861948143e-06, 'epoch': 0.55}          \n{'loss': 0.439, 'learning_rate': 9.728100911002103e-06, 'epoch': 0.56}          \n  3%|█                                    | 400/14280 [08:01<4:18:03,  1.12s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  6.10it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.49it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:07,  2.77it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:08,  2.38it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:02<00:08,  2.13it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:08,  2.15it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:03<00:07,  2.22it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:03<00:07,  2.28it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:04<00:06,  2.30it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:04<00:06,  2.24it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:05<00:06,  2.14it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:05<00:05,  2.05it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:06<00:05,  2.06it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:06<00:04,  2.19it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:07<00:04,  2.07it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:07<00:03,  2.14it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:07<00:03,  2.27it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:08<00:02,  2.36it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:08<00:02,  2.38it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:09<00:01,  2.24it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:09<00:01,  2.17it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:10<00:00,  2.20it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:10<00:00,  2.27it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.21976551413536072, 'eval_runtime': 11.4012, 'eval_samples_per_second': 8.771, 'eval_steps_per_second': 2.193, 'epoch': 0.56}\n  3%|█                                    | 400/14280 [08:12<4:18:03,  1.12s/it]\n100%|███████████████████████████████████████████| 25/25 [00:11<00:00,  2.29it/s]\u001b[A\n                                                                                \u001b[ABegin to save...\nBegin to save...this process is not main process , do not save model.[for distributed training scenario]\n\n\u001b[32m2023-08-29 12:18:52.003\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m191\u001b[0m - \u001b[31m\u001b[1mSave done.\u001b[0m\n{'loss': 0.396, 'learning_rate': 9.721093202522776e-06, 'epoch': 0.57}          \n{'loss': 0.4236, 'learning_rate': 9.714085494043449e-06, 'epoch': 0.59}         \n{'loss': 0.4135, 'learning_rate': 9.707077785564121e-06, 'epoch': 0.6}          \n{'loss': 0.2358, 'learning_rate': 9.700070077084794e-06, 'epoch': 0.62}         \n{'loss': 0.4956, 'learning_rate': 9.693062368605467e-06, 'epoch': 0.63}         \n{'loss': 0.2974, 'learning_rate': 9.68605466012614e-06, 'epoch': 0.64}          \n{'loss': 0.1807, 'learning_rate': 9.679046951646813e-06, 'epoch': 0.66}         \n{'loss': 0.1988, 'learning_rate': 9.672039243167486e-06, 'epoch': 0.67}         \n{'loss': 0.3278, 'learning_rate': 9.665031534688157e-06, 'epoch': 0.69}         \n{'loss': 0.1737, 'learning_rate': 9.658023826208831e-06, 'epoch': 0.7}          \n  4%|█▎                                   | 500/14280 [10:04<4:28:46,  1.17s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:04,  5.26it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.34it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:07,  2.68it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:08,  2.35it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:02<00:09,  2.10it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:08,  2.13it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:03<00:07,  2.21it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:03<00:07,  2.27it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:04<00:06,  2.29it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:04<00:06,  2.23it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:05<00:06,  2.15it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:05<00:05,  2.05it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:06<00:05,  2.06it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:06<00:04,  2.19it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:07<00:04,  2.06it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:07<00:03,  2.14it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:07<00:03,  2.27it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:08<00:02,  2.35it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:08<00:02,  2.37it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:09<00:01,  2.24it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:09<00:01,  2.16it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:10<00:00,  2.23it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:10<00:00,  2.25it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.09729776531457901, 'eval_runtime': 11.4029, 'eval_samples_per_second': 8.77, 'eval_steps_per_second': 2.192, 'epoch': 0.7}\n  4%|█▎                                   | 500/14280 [10:15<4:28:46,  1.17s/it]\n100%|███████████████████████████████████████████| 25/25 [00:11<00:00,  2.28it/s]\u001b[A\n                                                                                \u001b[ABegin to save...\nthis process is not main process , do not save model.[for distributed training scenario]\nBegin to save...\n\u001b[32m2023-08-29 12:20:54.403\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m191\u001b[0m - \u001b[31m\u001b[1mSave done.\u001b[0m\n{'loss': 0.1865, 'learning_rate': 9.651016117729504e-06, 'epoch': 0.71}         \n  4%|█▎                                   | 517/14280 [10:36<4:02:19,  1.06s/it]^C\n[2023-08-29 12:21:15,155] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 399\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone -b medGPT_0828  https://github.com/valkryhx/MedicalGPT","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:21:27.569194Z","iopub.execute_input":"2023-08-29T12:21:27.569602Z","iopub.status.idle":"2023-08-29T12:21:28.700174Z","shell.execute_reply.started":"2023-08-29T12:21:27.569564Z","shell.execute_reply":"2023-08-29T12:21:28.699004Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/working\nfatal: destination path 'MedicalGPT' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd MedicalGPT\n!git status","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:21:32.311336Z","iopub.execute_input":"2023-08-29T12:21:32.312550Z","iopub.status.idle":"2023-08-29T12:21:33.467120Z","shell.execute_reply.started":"2023-08-29T12:21:32.312486Z","shell.execute_reply":"2023-08-29T12:21:33.465889Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/working/MedicalGPT\nOn branch medGPT_0828\nYour branch is up to date with 'origin/medGPT_0828'.\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n\t\u001b[31mmodified:   data/vocab/word_freq.txt\u001b[m\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n\t\u001b[31mcache/\u001b[m\n\t\u001b[31moutputs-dpo-v1/\u001b[m\n\t\u001b[31moutputs-dpo-yunguan-v1/\u001b[m\n\t\u001b[31mstate.db\u001b[m\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n","output_type":"stream"}]},{"cell_type":"code","source":"!cat ./requirements.txt\n# 发现这项目的依赖和上面重合 不用再装一次","metadata":{"execution":{"iopub.status.busy":"2023-08-29T12:21:49.750662Z","iopub.execute_input":"2023-08-29T12:21:49.751087Z","iopub.status.idle":"2023-08-29T12:21:50.871895Z","shell.execute_reply.started":"2023-08-29T12:21:49.751050Z","shell.execute_reply":"2023-08-29T12:21:50.870686Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"loguru\ntransformers==4.30.2\nsentencepiece\ndatasets==2.12.0\ntqdm\ntensorboard\ntqdm>=4.47.0\npeft>=0.5.0\naccelerate==0.21.0\ntrl>=0.6.0\nbitsandbytes==0.39.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 参考这里 colab的dpo运行demo  是使用base model（或者sft合并后的model 仍然是base model格式）传入的\nhttps://colab.research.google.com/drive/1kMIe3pTec2snQvLBA00Br8ND1_zwy3Gr?usp=sharing","metadata":{}},{"cell_type":"markdown","source":"# 使用运管数据 ./data/reward_yunguan\n# <font color=red>注意这里没有使用--qlora True 说明是使用普通lora来训练base model的</font>  代码中会加载两次chatglm2 一次作为可训练的model  一次作为ref_model 注意不要oom","metadata":{"execution":{"iopub.status.busy":"2023-08-29T01:55:58.634621Z","iopub.execute_input":"2023-08-29T01:55:58.635007Z","iopub.status.idle":"2023-08-29T01:55:59.623378Z","shell.execute_reply.started":"2023-08-29T01:55:58.634975Z","shell.execute_reply":"2023-08-29T01:55:59.622075Z"}}},{"cell_type":"markdown","source":"# 注意dpo_training.py 代码中 optim默认=adamw_hf  在lora训练时可以 但是qlora训练一定换成paged_adamw_8bit 不然loss=0 其他指标为nan\n# 注意dpo_training.py 代码中 find_all_linear_names(peft_model, int4=False, int8=False)定义时加入了判断是否int4 or int8 量化 lora训练一般不量化 此时cls = torch.nn.Linear  注意find_all_linear_names的层<font color=red>好像比我自己写qlora时的find_all_linear_names多了一层default  今天测试好像没多这个奇怪的default层</font>","metadata":{}},{"cell_type":"markdown","source":" # --lora_rank 64 \\ --lora_alpha 32 \\ 会oom\n # 我改成 --lora_rank 8 \\ --lora_alpha 16 \\ 没有oom\n # learning_rate 默认 5e-4 太大导致loss不稳定 我换成较小的1e-5\n # warmup_steps 默认100 我改成 10\n # <font color=red>train/eval数据从./data/reward_yunguan 换成./data/reward loss瞬间下降 毕竟这个只有100条数据 而reward_yunguan有2800多条</font>\n # [未实施]optim由adamw_hf 改成 paged_adamw_32bit？ 后者是trl官网dpo例子用的","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_training.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 100 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-v1 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"execution":{"iopub.status.busy":"2023-08-29T05:53:20.993913Z","iopub.execute_input":"2023-08-29T05:53:20.994330Z","iopub.status.idle":"2023-08-29T05:53:28.858053Z","shell.execute_reply.started":"2023-08-29T05:53:20.994293Z","shell.execute_reply":"2023-08-29T05:53:28.856820Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/working/MedicalGPT\nFetching origin\nAlready up to date.\n^C\nTraceback (most recent call last):\n  File \"/kaggle/working/MedicalGPT/dpo_training.py\", line 16, in <module>\n    from peft import LoraConfig, TaskType\n  File \"/opt/conda/lib/python3.10/site-packages/peft/__init__.py\", line 22, in <module>\n    from .auto import (\n  File \"/opt/conda/lib/python3.10/site-packages/peft/auto.py\", line 30, in <module>\n    from .config import PeftConfig\n  File \"/opt/conda/lib/python3.10/site-packages/peft/config.py\", line 24, in <module>\n    from .utils import CONFIG_NAME, PeftType, TaskType\n  File \"/opt/conda/lib/python3.10/site-packages/peft/utils/__init__.py\", line 22, in <module>\n    from .other import (\n  File \"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py\", line 21, in <module>\n    import accelerate\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/__init__.py\", line 3, in <module>\n    from .accelerator import Accelerator\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 35, in <module>\n    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/checkpointing.py\", line 24, in <module>\n    from .utils import (\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/__init__.py\", line 64, in <module>\n    from .modeling import (\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 30, in <module>\n    from ..state import AcceleratorState\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/state.py\", line 47, in <module>\n    if is_tpu_available(check_device=False):\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/imports.py\", line 84, in is_tpu_available\n    if torch.cuda.is_available():\n  File \"/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 107, in is_available\n    return torch._C._cuda_getDeviceCount() > 0\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# optim 换成 paged_lion_32bit    lora_rank从8换到16 似乎有可见的波动 loss从0.6到1 而且显存占用明显比paged_adamw_32bit小（14.4G） 才12.9G\n# 我试试继续增加lora_rank 到64 \n# <font color=red>可以跑 train loss好像波动大 但是eval的margin和acc倒是越来越大</font>","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_training.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-yunguan-v1 \\\n    --target_modules all \\\n    --lora_rank 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"execution":{"iopub.status.busy":"2023-08-29T06:32:25.544851Z","iopub.execute_input":"2023-08-29T06:32:25.545614Z","iopub.status.idle":"2023-08-29T06:56:58.509782Z","shell.execute_reply.started":"2023-08-29T06:32:25.545572Z","shell.execute_reply":"2023-08-29T06:56:58.508301Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/working/MedicalGPT\nFetching origin\nremote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (3/3), 833 bytes | 277.00 KiB/s, done.\nFrom https://github.com/valkryhx/MedicalGPT\n   8138fc4..317eeb7  medGPT_0828 -> origin/medGPT_0828\nUpdating 8138fc4..317eeb7\nFast-forward\n dpo_training.py | 4 \u001b[32m++++\u001b[m\n 1 file changed, 4 insertions(+)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2023-08-29 06:32:33,054] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n\u001b[32m2023-08-29 06:32:39.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m239\u001b[0m - \u001b[1mParse args: ScriptArguments(model_type='chatglm', model_name_or_path='THUDM/chatglm2-6b', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir='./cache', use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True, dataset_name=None, dataset_config_name=None, train_file_dir='./data/reward_yunguan', validation_file_dir='./data/reward_yunguan', template_name='vicuna', per_device_train_batch_size=1, per_device_eval_batch_size=1, max_source_length=256, max_target_length=128, min_target_length=4, max_train_samples=1000, max_eval_samples=20, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=4, use_peft=True, qlora=False, target_modules='all', lora_rank=64, lora_dropout=0.05, lora_alpha=32.0, peft_path=None, do_train=True, do_eval=True, beta=0.1, learning_rate=1e-05, lr_scheduler_type='cosine', warmup_steps=10, weight_decay=0.05, optim='paged_lion_32bit', fp16=True, bf16=False, gradient_checkpointing=True, gradient_accumulation_steps=4, save_total_limit=2, load_best_model_at_end=True, save_steps=40, eval_steps=10, logging_steps=1, output_dir='outputs-dpo-yunguan-v1', max_steps=200, eval_strategy='steps', remove_unused_columns=False, report_to='tensorboard')\u001b[0m\n\u001b[32m2023-08-29 06:32:40.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mtrain files: ./data/reward_yunguan/paired_yunguan.json\u001b[0m\n\u001b[32m2023-08-29 06:32:40.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1meval files: ./data/reward_yunguan/paired_yunguan.json\u001b[0m\n100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 515.21it/s]\n\u001b[32m2023-08-29 06:32:40.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m309\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n    train: Dataset({\n        features: ['question', 'response_chosen', 'response_rejected'],\n        num_rows: 2855\n    })\n    validation: Dataset({\n        features: ['question', 'response_chosen', 'response_rejected'],\n        num_rows: 2855\n    })\n})\u001b[0m\n\u001b[32m2023-08-29 06:32:40.352\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m327\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'question': '现在提供如下信息：\\n1. 审批出差申请的流程中，如何查询审批人的状态？ \\n\\n答：此问题是平台流程服务异常，IT会及时处理，您可以稍后再试，或上IT吧发帖反馈此问题\\n3. 负责 FMR 系统的是谁？ 请联系刘玉莎 (liuyusha@fiberhome.com)\\n4. 单位收款信息在哪里可以找到？ \\n\\n答：收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。\\n请根据提供的信息回答问题。问：4. 单位收款信息在哪里可以找到？ \\n\\n答：', 'response_chosen': '收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。', 'response_rejected': '请联系刘玉莎 (liuyusha@fiberhome.com)'}\u001b[0m\n\u001b[32m2023-08-29 06:32:40.762\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m340\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 807\u001b[0m\n\u001b[32m2023-08-29 06:32:40.762\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m341\u001b[0m - \u001b[34m\u001b[1mFirst train example:\u001b[0m\n\u001b[32m2023-08-29 06:32:40.763\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m342\u001b[0m - \u001b[34m\u001b[1mQuestion: 现在提供如下信息：\n关于系统责任人。ITPM的责任人是谁请联系黄凤 (huangfeng@fiberhome.com)\n6. 能提供具体的操作指南吗？ \n\n答：发票抬头需与费用承担公司保持一致；请再次确认费用承担公司；\n场景1：若费用由B公司承担，请重新开具发票，使发票抬头为B公司;\n场景2：若存在借用等情况，费用应由A公司承担，请选择相应委派人\n5. 国际项目经理的负责人是谁？ 请联系章宗波 (zhangzongbo@fiberhome.com)\n请根据提供的信息回答问题。问：关于系统责任人。ITPM的责任人是谁\n\nAnswer: 请联系黄凤 (huangfeng@fiberhome.com)\u001b[0m\n\u001b[32m2023-08-29 06:32:40.765\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m354\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'question': '现在提供如下信息：\\n1. 审批出差申请的流程中，如何查询审批人的状态？ \\n\\n答：此问题是平台流程服务异常，IT会及时处理，您可以稍后再试，或上IT吧发帖反馈此问题\\n3. 负责 FMR 系统的是谁？ 请联系刘玉莎 (liuyusha@fiberhome.com)\\n4. 单位收款信息在哪里可以找到？ \\n\\n答：收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。\\n请根据提供的信息回答问题。问：4. 单位收款信息在哪里可以找到？ \\n\\n答：', 'response_chosen': '收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。', 'response_rejected': '请联系刘玉莎 (liuyusha@fiberhome.com)'}\u001b[0m\n\u001b[32m2023-08-29 06:32:40.784\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m367\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 14\u001b[0m\n\u001b[32m2023-08-29 06:32:40.785\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m368\u001b[0m - \u001b[34m\u001b[1mFirst eval example:\u001b[0m\n\u001b[32m2023-08-29 06:32:40.785\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[34m\u001b[1mQuestion: 现在提供如下信息：\n1. 审批出差申请的流程中，如何查询审批人的状态？ \n\n答：此问题是平台流程服务异常，IT会及时处理，您可以稍后再试，或上IT吧发帖反馈此问题\n3. 负责 FMR 系统的是谁？ 请联系刘玉莎 (liuyusha@fiberhome.com)\n4. 单位收款信息在哪里可以找到？ \n\n答：收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。\n请根据提供的信息回答问题。问：4. 单位收款信息在哪里可以找到？ \n\n答：\n\nAnswer: 收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。\u001b[0m\n\u001b[32m2023-08-29 06:32:40.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m371\u001b[0m - \u001b[1mLoading model\u001b[0m\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:27<00:00, 12.45s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:25<00:00, 12.27s/it]\n\u001b[32m2023-08-29 06:35:43.157\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[31m\u001b[1mid(model)=137020010737728\u001b[0m\n\u001b[32m2023-08-29 06:35:43.158\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m448\u001b[0m - \u001b[31m\u001b[1mid(model_ref)=137020011266496\u001b[0m\n\u001b[32m2023-08-29 06:35:43.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m484\u001b[0m - \u001b[1mPeft target_modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\u001b[0m\n\u001b[32m2023-08-29 06:35:43.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m493\u001b[0m - \u001b[1mChatGLMForConditionalGeneration(\n  (transformer): ChatGLMModel(\n    (embedding): Embedding(\n      (word_embeddings): Embedding(65024, 4096)\n    )\n    (rotary_pos_emb): RotaryEmbedding()\n    (encoder): GLMTransformer(\n      (layers): ModuleList(\n        (0-27): 28 x GLMBlock(\n          (input_layernorm): RMSNorm()\n          (self_attention): SelfAttention(\n            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n            (core_attention): CoreAttention(\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n            )\n            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n          )\n          (post_attention_layernorm): RMSNorm()\n          (mlp): MLP(\n            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n          )\n        )\n      )\n      (final_layernorm): RMSNorm()\n    )\n    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n  )\n)\u001b[0m\ntrainable params: 118587392 || all params: 6362171392 || trainable%: 1.86394525851843\n\u001b[32m2023-08-29 06:37:15.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m510\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n  0%|                                                   | 0/200 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6935, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': -0.0019216537475585938, 'rewards/rejected': -0.0012372017372399569, 'rewards/accuracies': 0.0, 'rewards/margins': -0.0006844520103186369, 'logps/rejected': -97.67422485351562, 'logps/chosen': -102.3606185913086, 'logits/rejected': -2.1285784244537354, 'logits/chosen': -2.1420798301696777, 'epoch': 0.0}\n{'loss': 0.6914, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0011323929065838456, 'rewards/rejected': -0.0022886276710778475, 'rewards/accuracies': 0.25, 'rewards/margins': 0.003421020694077015, 'logps/rejected': -74.89596557617188, 'logps/chosen': -168.3209228515625, 'logits/rejected': -2.2373104095458984, 'logits/chosen': -2.243072509765625, 'epoch': 0.01}\n{'loss': 0.6938, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': -0.0003963470517192036, 'rewards/rejected': 0.000968551670666784, 'rewards/accuracies': 0.0, 'rewards/margins': -0.001364898751489818, 'logps/rejected': -106.35012817382812, 'logps/chosen': -90.72676849365234, 'logits/rejected': -2.327390193939209, 'logits/chosen': -2.302342176437378, 'epoch': 0.01}\n{'loss': 0.6911, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.00031375885009765625, 'rewards/rejected': -0.00386219029314816, 'rewards/accuracies': 0.75, 'rewards/margins': 0.004175948910415173, 'logps/rejected': -122.91868591308594, 'logps/chosen': -85.00923919677734, 'logits/rejected': -2.3137447834014893, 'logits/chosen': -2.262476921081543, 'epoch': 0.02}\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -166.3670196533203, 'logps/chosen': -71.9852523803711, 'logits/rejected': -2.2815661430358887, 'logits/chosen': -2.2693264484405518, 'epoch': 0.02}\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -134.58218383789062, 'logps/chosen': -128.9730224609375, 'logits/rejected': -2.2659924030303955, 'logits/chosen': -2.2437238693237305, 'epoch': 0.03}\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -98.4620361328125, 'logps/chosen': -148.78790283203125, 'logits/rejected': -2.403538942337036, 'logits/chosen': -2.4449009895324707, 'epoch': 0.03}\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -138.17979431152344, 'logps/chosen': -124.75907897949219, 'logits/rejected': -2.4626145362854004, 'logits/chosen': -2.474393606185913, 'epoch': 0.04}\n{'loss': 0.6931, 'learning_rate': 2.0000000000000003e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -116.71502685546875, 'logps/chosen': -93.36486053466797, 'logits/rejected': -2.149590253829956, 'logits/chosen': -2.0959086418151855, 'epoch': 0.04}\n{'loss': 0.6887, 'learning_rate': 2.0000000000000003e-06, 'rewards/chosen': 0.008715629577636719, 'rewards/rejected': -0.00017585745081305504, 'rewards/accuracies': 0.75, 'rewards/margins': 0.008891486562788486, 'logps/rejected': -124.97222137451172, 'logps/chosen': -103.98553466796875, 'logits/rejected': -2.187238931655884, 'logits/chosen': -2.123025894165039, 'epoch': 0.05}\n  5%|██                                        | 10/200 [00:50<14:51,  4.69s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.89it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.49it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.17it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  2.00it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.88it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.63it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.66it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.72it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.75it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:06<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6938377022743225, 'eval_runtime': 8.2361, 'eval_samples_per_second': 1.7, 'eval_steps_per_second': 1.7, 'eval_rewards/chosen': -0.00046659199870191514, 'eval_rewards/rejected': 0.0008904592250473797, 'eval_rewards/accuracies': 0.5714285969734192, 'eval_rewards/margins': -0.001357051427476108, 'eval_logps/rejected': -113.08318328857422, 'eval_logps/chosen': -103.80133819580078, 'eval_logits/rejected': -2.314068555831909, 'eval_logits/chosen': -2.3430426120758057, 'epoch': 0.05}\n  5%|██                                        | 10/200 [00:58<14:51,  4.69s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.79it/s]\u001b[A\n{'loss': 0.6914, 'learning_rate': 3e-06, 'rewards/chosen': 0.001918888185173273, 'rewards/rejected': -0.001607990125194192, 'rewards/accuracies': 0.75, 'rewards/margins': 0.0035268785431981087, 'logps/rejected': -104.27052307128906, 'logps/chosen': -88.1939926147461, 'logits/rejected': -2.1779351234436035, 'logits/chosen': -2.1798195838928223, 'epoch': 0.05}\n{'loss': 0.6921, 'learning_rate': 4.000000000000001e-06, 'rewards/chosen': 0.010631943121552467, 'rewards/rejected': 0.008528899401426315, 'rewards/accuracies': 0.25, 'rewards/margins': 0.00210304232314229, 'logps/rejected': -120.74568176269531, 'logps/chosen': -137.833251953125, 'logits/rejected': -2.0830087661743164, 'logits/chosen': -2.0835015773773193, 'epoch': 0.06}\n{'loss': 0.7082, 'learning_rate': 4.000000000000001e-06, 'rewards/chosen': 0.09460887312889099, 'rewards/rejected': 0.121698759496212, 'rewards/accuracies': 0.5, 'rewards/margins': -0.027089877054095268, 'logps/rejected': -86.66114807128906, 'logps/chosen': -96.99356842041016, 'logits/rejected': -2.369619369506836, 'logits/chosen': -2.309704542160034, 'epoch': 0.06}\n{'loss': 0.6904, 'learning_rate': 5e-06, 'rewards/chosen': 0.07320242375135422, 'rewards/rejected': 0.06764259189367294, 'rewards/accuracies': 0.75, 'rewards/margins': 0.005559826735407114, 'logps/rejected': -153.59463500976562, 'logps/chosen': -80.0596923828125, 'logits/rejected': -2.2906408309936523, 'logits/chosen': -2.287689208984375, 'epoch': 0.07}\n{'loss': 0.7106, 'learning_rate': 6e-06, 'rewards/chosen': 0.07932853698730469, 'rewards/rejected': 0.11367073655128479, 'rewards/accuracies': 0.25, 'rewards/margins': -0.034342195838689804, 'logps/rejected': -125.385498046875, 'logps/chosen': -72.45525360107422, 'logits/rejected': -2.342149257659912, 'logits/chosen': -2.31501841545105, 'epoch': 0.07}\n{'loss': 0.6866, 'learning_rate': 7e-06, 'rewards/chosen': -0.017121315002441406, 'rewards/rejected': -0.03066844865679741, 'rewards/accuracies': 0.75, 'rewards/margins': 0.013547133654356003, 'logps/rejected': -125.40754699707031, 'logps/chosen': -114.59750366210938, 'logits/rejected': -2.349266290664673, 'logits/chosen': -2.25903582572937, 'epoch': 0.08}\n{'loss': 0.6658, 'learning_rate': 8.000000000000001e-06, 'rewards/chosen': -0.04161129146814346, 'rewards/rejected': -0.10039082169532776, 'rewards/accuracies': 0.25, 'rewards/margins': 0.058779530227184296, 'logps/rejected': -111.59349822998047, 'logps/chosen': -115.39810180664062, 'logits/rejected': -2.324641466140747, 'logits/chosen': -2.316283941268921, 'epoch': 0.08}\n{'loss': 0.7501, 'learning_rate': 9e-06, 'rewards/chosen': -0.36194202303886414, 'rewards/rejected': -0.2773754298686981, 'rewards/accuracies': 0.5, 'rewards/margins': -0.08456658571958542, 'logps/rejected': -111.53948974609375, 'logps/chosen': -118.37718200683594, 'logits/rejected': -2.1883962154388428, 'logits/chosen': -2.231349468231201, 'epoch': 0.09}\n{'loss': 0.7402, 'learning_rate': 1e-05, 'rewards/chosen': -0.10702934861183167, 'rewards/rejected': -0.0249238982796669, 'rewards/accuracies': 0.25, 'rewards/margins': -0.08210545033216476, 'logps/rejected': -107.01744079589844, 'logps/chosen': -179.61251831054688, 'logits/rejected': -2.332029342651367, 'logits/chosen': -2.3627517223358154, 'epoch': 0.09}\n{'loss': 0.6878, 'learning_rate': 9.999316524962347e-06, 'rewards/chosen': 0.08916664123535156, 'rewards/rejected': 0.07849512249231339, 'rewards/accuracies': 0.75, 'rewards/margins': 0.01067152339965105, 'logps/rejected': -99.97476196289062, 'logps/chosen': -67.59458923339844, 'logits/rejected': -2.415463447570801, 'logits/chosen': -2.3758697509765625, 'epoch': 0.1}\n 10%|████▏                                     | 20/200 [01:48<15:39,  5.22s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.69it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.37it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.08it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.91it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.83it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.80it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.76it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.59it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.62it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:06<00:01,  1.67it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.69it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.71it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6944856643676758, 'eval_runtime': 8.4964, 'eval_samples_per_second': 1.648, 'eval_steps_per_second': 1.648, 'eval_rewards/chosen': 0.017901692539453506, 'eval_rewards/rejected': 0.00985687505453825, 'eval_rewards/accuracies': 0.4285714328289032, 'eval_rewards/margins': 0.008044813759624958, 'eval_logps/rejected': -112.98590850830078, 'eval_logps/chosen': -103.61775970458984, 'eval_logits/rejected': -2.2993900775909424, 'eval_logits/chosen': -2.3269152641296387, 'epoch': 0.1}\n 10%|████▏                                     | 20/200 [01:57<15:39,  5.22s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.7452, 'learning_rate': 9.99726628670463e-06, 'rewards/chosen': 0.11124276369810104, 'rewards/rejected': 0.20906467735767365, 'rewards/accuracies': 0.25, 'rewards/margins': -0.097821906208992, 'logps/rejected': -106.0499267578125, 'logps/chosen': -130.07229614257812, 'logits/rejected': -2.169130563735962, 'logits/chosen': -2.166376829147339, 'epoch': 0.1}\n{'loss': 0.6918, 'learning_rate': 9.993849845741525e-06, 'rewards/chosen': 0.018381692469120026, 'rewards/rejected': 0.01538162212818861, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0030000689439475536, 'logps/rejected': -81.2919692993164, 'logps/chosen': -207.7109832763672, 'logits/rejected': -2.201326608657837, 'logits/chosen': -2.3787529468536377, 'epoch': 0.11}\n{'loss': 0.6551, 'learning_rate': 9.989068136093873e-06, 'rewards/chosen': 0.24150648713111877, 'rewards/rejected': 0.1623561829328537, 'rewards/accuracies': 0.75, 'rewards/margins': 0.07915030419826508, 'logps/rejected': -133.18063354492188, 'logps/chosen': -85.20259857177734, 'logits/rejected': -2.359422206878662, 'logits/chosen': -2.3189613819122314, 'epoch': 0.11}\n{'loss': 0.6943, 'learning_rate': 9.98292246503335e-06, 'rewards/chosen': 0.1485918015241623, 'rewards/rejected': 0.14507122337818146, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0035205818712711334, 'logps/rejected': -127.60098266601562, 'logps/chosen': -99.94499969482422, 'logits/rejected': -2.338723659515381, 'logits/chosen': -2.338752508163452, 'epoch': 0.12}\n{'loss': 0.6509, 'learning_rate': 9.975414512725058e-06, 'rewards/chosen': 0.025324538350105286, 'rewards/rejected': -0.061762236058712006, 'rewards/accuracies': 1.0, 'rewards/margins': 0.08708677440881729, 'logps/rejected': -115.22166442871094, 'logps/chosen': -102.48136138916016, 'logits/rejected': -2.2485368251800537, 'logits/chosen': -2.214449882507324, 'epoch': 0.12}\n{'loss': 0.6801, 'learning_rate': 9.966546331768192e-06, 'rewards/chosen': 0.15830935537815094, 'rewards/rejected': 0.11269283294677734, 'rewards/accuracies': 0.5, 'rewards/margins': 0.04561652988195419, 'logps/rejected': -151.8159637451172, 'logps/chosen': -86.45504760742188, 'logits/rejected': -2.3617348670959473, 'logits/chosen': -2.381107807159424, 'epoch': 0.13}\n{'loss': 0.5772, 'learning_rate': 9.956320346634877e-06, 'rewards/chosen': 0.23022577166557312, 'rewards/rejected': -0.027568243443965912, 'rewards/accuracies': 0.75, 'rewards/margins': 0.25779399275779724, 'logps/rejected': -117.77189636230469, 'logps/chosen': -113.97484588623047, 'logits/rejected': -2.2530863285064697, 'logits/chosen': -2.2588963508605957, 'epoch': 0.13}\n{'loss': 0.6602, 'learning_rate': 9.944739353007344e-06, 'rewards/chosen': 0.2985437214374542, 'rewards/rejected': 0.22487527132034302, 'rewards/accuracies': 0.75, 'rewards/margins': 0.0736684799194336, 'logps/rejected': -119.70062255859375, 'logps/chosen': -108.93060302734375, 'logits/rejected': -2.1806986331939697, 'logits/chosen': -2.1790411472320557, 'epoch': 0.14}\n{'loss': 0.6981, 'learning_rate': 9.931806517013612e-06, 'rewards/chosen': 0.20134258270263672, 'rewards/rejected': 0.18738386034965515, 'rewards/accuracies': 0.5, 'rewards/margins': 0.013958729803562164, 'logps/rejected': -118.55982971191406, 'logps/chosen': -81.14852142333984, 'logits/rejected': -2.1378393173217773, 'logits/chosen': -2.129189968109131, 'epoch': 0.14}\n{'loss': 0.6445, 'learning_rate': 9.917525374361913e-06, 'rewards/chosen': -0.12465924769639969, 'rewards/rejected': -0.22897128760814667, 'rewards/accuracies': 0.75, 'rewards/margins': 0.10431204736232758, 'logps/rejected': -181.94822692871094, 'logps/chosen': -111.80596923828125, 'logits/rejected': -2.410860300064087, 'logits/chosen': -2.344099521636963, 'epoch': 0.15}\n 15%|██████▎                                   | 30/200 [02:48<15:21,  5.42s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.79it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.41it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.11it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.93it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.7177758812904358, 'eval_runtime': 8.4173, 'eval_samples_per_second': 1.663, 'eval_steps_per_second': 1.663, 'eval_rewards/chosen': -0.14843378961086273, 'eval_rewards/rejected': -0.20374265313148499, 'eval_rewards/accuracies': 0.3571428656578064, 'eval_rewards/margins': 0.055308904498815536, 'eval_logps/rejected': -115.12669372558594, 'eval_logps/chosen': -105.278076171875, 'eval_logits/rejected': -2.260317087173462, 'eval_logits/chosen': -2.2864701747894287, 'epoch': 0.15}\n 15%|██████▎                                   | 30/200 [02:56<15:21,  5.42s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'loss': 0.5956, 'learning_rate': 9.901899829374048e-06, 'rewards/chosen': -0.4279385507106781, 'rewards/rejected': -0.6812868714332581, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2533482611179352, 'logps/rejected': -144.64450073242188, 'logps/chosen': -133.74343872070312, 'logits/rejected': -2.3861231803894043, 'logits/chosen': -2.3697752952575684, 'epoch': 0.15}\n{'loss': 0.5511, 'learning_rate': 9.884934153917998e-06, 'rewards/chosen': -0.2551708221435547, 'rewards/rejected': -0.6660255193710327, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4108547270298004, 'logps/rejected': -146.3880615234375, 'logps/chosen': -115.25679016113281, 'logits/rejected': -1.9768139123916626, 'logits/chosen': -1.9981755018234253, 'epoch': 0.16}\n{'loss': 1.4025, 'learning_rate': 9.86663298624003e-06, 'rewards/chosen': -1.7730672359466553, 'rewards/rejected': -1.2982161045074463, 'rewards/accuracies': 0.75, 'rewards/margins': -0.4748513102531433, 'logps/rejected': -122.05397033691406, 'logps/chosen': -163.419189453125, 'logits/rejected': -2.031280994415283, 'logits/chosen': -2.0594184398651123, 'epoch': 0.16}\n{'loss': 1.2428, 'learning_rate': 9.86663298624003e-06, 'rewards/chosen': -1.7959824800491333, 'rewards/rejected': -1.1919639110565186, 'rewards/accuracies': 0.25, 'rewards/margins': -0.6040185689926147, 'logps/rejected': -75.53244018554688, 'logps/chosen': -114.48846435546875, 'logits/rejected': -2.4785499572753906, 'logits/chosen': -2.434246778488159, 'epoch': 0.17}\n{'loss': 0.9812, 'learning_rate': 9.847001329696653e-06, 'rewards/chosen': -0.9084550738334656, 'rewards/rejected': -0.5049871802330017, 'rewards/accuracies': 0.25, 'rewards/margins': -0.40346795320510864, 'logps/rejected': -106.50262451171875, 'logps/chosen': -124.36741638183594, 'logits/rejected': -2.078813314437866, 'logits/chosen': -2.0696654319763184, 'epoch': 0.17}\n{'loss': 0.7254, 'learning_rate': 9.826044551386743e-06, 'rewards/chosen': -0.4198274612426758, 'rewards/rejected': -0.39654219150543213, 'rewards/accuracies': 0.75, 'rewards/margins': -0.023285288363695145, 'logps/rejected': -127.98709869384766, 'logps/chosen': -146.69630432128906, 'logits/rejected': -2.041670799255371, 'logits/chosen': -2.098533868789673, 'epoch': 0.18}\n{'loss': 0.6666, 'learning_rate': 9.803768380684242e-06, 'rewards/chosen': -0.28659361600875854, 'rewards/rejected': -0.3524782061576843, 'rewards/accuracies': 0.25, 'rewards/margins': 0.06588459014892578, 'logps/rejected': -83.14369201660156, 'logps/chosen': -120.10918426513672, 'logits/rejected': -2.1529197692871094, 'logits/chosen': -2.1742589473724365, 'epoch': 0.18}\n{'loss': 1.1493, 'learning_rate': 9.780178907671788e-06, 'rewards/chosen': -1.063890814781189, 'rewards/rejected': -0.35053005814552307, 'rewards/accuracies': 0.0, 'rewards/margins': -0.7133607864379883, 'logps/rejected': -102.8150634765625, 'logps/chosen': -166.700439453125, 'logits/rejected': -2.166860818862915, 'logits/chosen': -2.2133264541625977, 'epoch': 0.19}\n{'loss': 0.6995, 'learning_rate': 9.755282581475769e-06, 'rewards/chosen': -0.30702248215675354, 'rewards/rejected': -0.3086793124675751, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0016568005084991455, 'logps/rejected': -75.49368286132812, 'logps/chosen': -121.30908203125, 'logits/rejected': -2.3861024379730225, 'logits/chosen': -2.374629497528076, 'epoch': 0.19}\n{'loss': 0.641, 'learning_rate': 9.729086208503174e-06, 'rewards/chosen': -0.019498243927955627, 'rewards/rejected': -0.17317695915699005, 'rewards/accuracies': 0.5, 'rewards/margins': 0.15367870032787323, 'logps/rejected': -166.86666870117188, 'logps/chosen': -95.9603500366211, 'logits/rejected': -2.349468469619751, 'logits/chosen': -2.414372205734253, 'epoch': 0.2}\n 20%|████████▍                                 | 40/200 [03:45<13:27,  5.04s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.72it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.38it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.09it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.92it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.84it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.81it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.60it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.68it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.634048581123352, 'eval_runtime': 8.4369, 'eval_samples_per_second': 1.659, 'eval_steps_per_second': 1.659, 'eval_rewards/chosen': 0.09911990910768509, 'eval_rewards/rejected': -0.057869914919137955, 'eval_rewards/accuracies': 0.5714285969734192, 'eval_rewards/margins': 0.15698981285095215, 'eval_logps/rejected': -113.6791000366211, 'eval_logps/chosen': -102.8042984008789, 'eval_logits/rejected': -2.27067232131958, 'eval_logits/chosen': -2.3015403747558594, 'epoch': 0.2}\n 20%|████████▍                                 | 40/200 [03:53<13:27,  5.04s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.6752, 'learning_rate': 9.701596950580807e-06, 'rewards/chosen': 0.14388924837112427, 'rewards/rejected': 0.06799955666065216, 'rewards/accuracies': 0.25, 'rewards/margins': 0.07588966935873032, 'logps/rejected': -200.54037475585938, 'logps/chosen': -91.75777435302734, 'logits/rejected': -2.302642583847046, 'logits/chosen': -2.2226204872131348, 'epoch': 0.2}\n{'loss': 0.7505, 'learning_rate': 9.672822322997305e-06, 'rewards/chosen': -0.03391799330711365, 'rewards/rejected': 0.0294162780046463, 'rewards/accuracies': 0.5, 'rewards/margins': -0.06333425641059875, 'logps/rejected': -97.46537780761719, 'logps/chosen': -117.61724853515625, 'logits/rejected': -2.297260284423828, 'logits/chosen': -2.270313262939453, 'epoch': 0.21}\n{'loss': 1.0448, 'learning_rate': 9.642770192448537e-06, 'rewards/chosen': 0.0960640013217926, 'rewards/rejected': 0.5583469271659851, 'rewards/accuracies': 0.25, 'rewards/margins': -0.4622829258441925, 'logps/rejected': -101.79601287841797, 'logps/chosen': -130.52001953125, 'logits/rejected': -2.2664272785186768, 'logits/chosen': -2.2235007286071777, 'epoch': 0.21}\n{'loss': 0.556, 'learning_rate': 9.611448774886925e-06, 'rewards/chosen': 0.009050272405147552, 'rewards/rejected': -0.31645917892456055, 'rewards/accuracies': 1.0, 'rewards/margins': 0.3255094587802887, 'logps/rejected': -147.33566284179688, 'logps/chosen': -97.83287048339844, 'logits/rejected': -2.288034677505493, 'logits/chosen': -2.1496095657348633, 'epoch': 0.22}\n{'loss': 0.8574, 'learning_rate': 9.578866633275289e-06, 'rewards/chosen': -0.3656940758228302, 'rewards/rejected': -0.06444845348596573, 'rewards/accuracies': 0.0, 'rewards/margins': -0.3012455999851227, 'logps/rejected': -96.15985107421875, 'logps/chosen': -125.19760131835938, 'logits/rejected': -1.964979648590088, 'logits/chosen': -2.035102128982544, 'epoch': 0.22}\n{'loss': 0.7379, 'learning_rate': 9.545032675245814e-06, 'rewards/chosen': -0.21516533195972443, 'rewards/rejected': -0.24452613294124603, 'rewards/accuracies': 0.5, 'rewards/margins': 0.029360756278038025, 'logps/rejected': -121.06071472167969, 'logps/chosen': -135.98728942871094, 'logits/rejected': -2.2739813327789307, 'logits/chosen': -2.2644522190093994, 'epoch': 0.23}\n{'loss': 0.5768, 'learning_rate': 9.509956150664796e-06, 'rewards/chosen': -0.09231939166784286, 'rewards/rejected': -0.3784334063529968, 'rewards/accuracies': 0.75, 'rewards/margins': 0.28611400723457336, 'logps/rejected': -109.9211654663086, 'logps/chosen': -104.58917236328125, 'logits/rejected': -2.1990580558776855, 'logits/chosen': -2.1594130992889404, 'epoch': 0.23}\n{'loss': 0.7182, 'learning_rate': 9.473646649103819e-06, 'rewards/chosen': -0.6633284091949463, 'rewards/rejected': -0.9150059223175049, 'rewards/accuracies': 0.5, 'rewards/margins': 0.251677542924881, 'logps/rejected': -159.4810791015625, 'logps/chosen': -136.24151611328125, 'logits/rejected': -2.3964200019836426, 'logits/chosen': -2.345308542251587, 'epoch': 0.24}\n{'loss': 0.7002, 'learning_rate': 9.43611409721806e-06, 'rewards/chosen': -0.3579499423503876, 'rewards/rejected': -0.4237062335014343, 'rewards/accuracies': 0.5, 'rewards/margins': 0.06575629115104675, 'logps/rejected': -176.96939086914062, 'logps/chosen': -104.97953796386719, 'logits/rejected': -2.18326473236084, 'logits/chosen': -2.160659074783325, 'epoch': 0.24}\n{'loss': 0.7592, 'learning_rate': 9.397368756032445e-06, 'rewards/chosen': -0.9704792499542236, 'rewards/rejected': -1.1013303995132446, 'rewards/accuracies': 0.75, 'rewards/margins': 0.130851149559021, 'logps/rejected': -147.1068115234375, 'logps/chosen': -138.86776733398438, 'logits/rejected': -2.348175525665283, 'logits/chosen': -2.294774055480957, 'epoch': 0.25}\n 25%|██████████▌                               | 50/200 [04:45<12:58,  5.19s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.76it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.40it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.10it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.93it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.84it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.81it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.64it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.71it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.73it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6666569709777832, 'eval_runtime': 8.402, 'eval_samples_per_second': 1.666, 'eval_steps_per_second': 1.666, 'eval_rewards/chosen': -0.20403626561164856, 'eval_rewards/rejected': -0.30988383293151855, 'eval_rewards/accuracies': 0.5714285969734192, 'eval_rewards/margins': 0.10584752261638641, 'eval_logps/rejected': -116.19890594482422, 'eval_logps/chosen': -105.83769989013672, 'eval_logits/rejected': -2.271559476852417, 'eval_logits/chosen': -2.296387195587158, 'epoch': 0.25}\n 25%|██████████▌                               | 50/200 [04:53<12:58,  5.19s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'loss': 0.7667, 'learning_rate': 9.357421218136387e-06, 'rewards/chosen': -0.138301283121109, 'rewards/rejected': -0.03367023915052414, 'rewards/accuracies': 0.5, 'rewards/margins': -0.10463105142116547, 'logps/rejected': -130.04006958007812, 'logps/chosen': -132.72731018066406, 'logits/rejected': -2.11200213432312, 'logits/chosen': -2.0883004665374756, 'epoch': 0.25}\n{'loss': 0.6869, 'learning_rate': 9.31628240478787e-06, 'rewards/chosen': -0.29440584778785706, 'rewards/rejected': -0.33069857954978943, 'rewards/accuracies': 0.25, 'rewards/margins': 0.03629274666309357, 'logps/rejected': -73.62582397460938, 'logps/chosen': -96.61023712158203, 'logits/rejected': -2.1718785762786865, 'logits/chosen': -2.17506742477417, 'epoch': 0.26}\n{'loss': 0.6848, 'learning_rate': 9.273963562927695e-06, 'rewards/chosen': -0.4696083068847656, 'rewards/rejected': -0.5058906674385071, 'rewards/accuracies': 0.75, 'rewards/margins': 0.03628233075141907, 'logps/rejected': -120.53440856933594, 'logps/chosen': -124.6690444946289, 'logits/rejected': -2.205211639404297, 'logits/chosen': -2.2001566886901855, 'epoch': 0.26}\n{'loss': 0.8042, 'learning_rate': 9.230476262104678e-06, 'rewards/chosen': -0.7850643396377563, 'rewards/rejected': -0.6087796092033386, 'rewards/accuracies': 0.25, 'rewards/margins': -0.17628471553325653, 'logps/rejected': -128.16188049316406, 'logps/chosen': -126.76538848876953, 'logits/rejected': -2.1186115741729736, 'logits/chosen': -2.1369876861572266, 'epoch': 0.27}\n{'loss': 0.6742, 'learning_rate': 9.185832391312644e-06, 'rewards/chosen': -0.16765108704566956, 'rewards/rejected': -0.219293013215065, 'rewards/accuracies': 0.25, 'rewards/margins': 0.05164194107055664, 'logps/rejected': -87.82640838623047, 'logps/chosen': -103.0865249633789, 'logits/rejected': -2.3954617977142334, 'logits/chosen': -2.3803822994232178, 'epoch': 0.27}\n{'loss': 0.8434, 'learning_rate': 9.140044155740102e-06, 'rewards/chosen': -0.7670438885688782, 'rewards/rejected': -0.5427091717720032, 'rewards/accuracies': 0.5, 'rewards/margins': -0.2243347465991974, 'logps/rejected': -115.99378967285156, 'logps/chosen': -150.05633544921875, 'logits/rejected': -2.510782241821289, 'logits/chosen': -2.57707142829895, 'epoch': 0.28}\n{'loss': 0.5688, 'learning_rate': 9.093124073433464e-06, 'rewards/chosen': -0.4735204577445984, 'rewards/rejected': -1.0254878997802734, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5519673824310303, 'logps/rejected': -181.6017608642578, 'logps/chosen': -145.75961303710938, 'logits/rejected': -2.069411039352417, 'logits/chosen': -2.1208040714263916, 'epoch': 0.28}\n{'loss': 0.7526, 'learning_rate': 9.045084971874738e-06, 'rewards/chosen': -0.15067176520824432, 'rewards/rejected': -0.03646650165319443, 'rewards/accuracies': 0.0, 'rewards/margins': -0.1142052710056305, 'logps/rejected': -97.3380126953125, 'logps/chosen': -116.57825469970703, 'logits/rejected': -2.1970162391662598, 'logits/chosen': -2.3049495220184326, 'epoch': 0.29}\n{'loss': 0.8901, 'learning_rate': 8.995939984474624e-06, 'rewards/chosen': -0.31505611538887024, 'rewards/rejected': 0.003915974870324135, 'rewards/accuracies': 0.5, 'rewards/margins': -0.318972110748291, 'logps/rejected': -96.93345642089844, 'logps/chosen': -129.27796936035156, 'logits/rejected': -2.349555015563965, 'logits/chosen': -2.3836660385131836, 'epoch': 0.29}\n{'loss': 0.5906, 'learning_rate': 8.94570254698197e-06, 'rewards/chosen': -0.2317361831665039, 'rewards/rejected': -0.4574998915195465, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2257637083530426, 'logps/rejected': -122.36360931396484, 'logps/chosen': -129.42791748046875, 'logits/rejected': -2.3164985179901123, 'logits/chosen': -2.2678780555725098, 'epoch': 0.3}\n 30%|████████████▌                             | 60/200 [05:42<11:52,  5.09s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.75it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.38it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.08it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.92it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.84it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.81it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.68it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.71it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5897849798202515, 'eval_runtime': 8.4537, 'eval_samples_per_second': 1.656, 'eval_steps_per_second': 1.656, 'eval_rewards/chosen': -0.30343109369277954, 'eval_rewards/rejected': -0.5907675623893738, 'eval_rewards/accuracies': 0.6428571343421936, 'eval_rewards/margins': 0.2873365581035614, 'eval_logps/rejected': -118.99662017822266, 'eval_logps/chosen': -106.82722473144531, 'eval_logits/rejected': -2.2790873050689697, 'eval_logits/chosen': -2.3046133518218994, 'epoch': 0.3}\n 30%|████████████▌                             | 60/200 [05:50<11:52,  5.09s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.8939, 'learning_rate': 8.894386393810563e-06, 'rewards/chosen': -0.7317586541175842, 'rewards/rejected': -0.4012323319911957, 'rewards/accuracies': 0.25, 'rewards/margins': -0.3305263817310333, 'logps/rejected': -115.0791015625, 'logps/chosen': -126.80056762695312, 'logits/rejected': -2.0741117000579834, 'logits/chosen': -2.1300716400146484, 'epoch': 0.3}\n{'loss': 0.5945, 'learning_rate': 8.842005554284296e-06, 'rewards/chosen': -0.32762661576271057, 'rewards/rejected': -0.5392856597900391, 'rewards/accuracies': 1.0, 'rewards/margins': 0.2116590440273285, 'logps/rejected': -126.79901123046875, 'logps/chosen': -127.64359283447266, 'logits/rejected': -2.1268115043640137, 'logits/chosen': -2.1400444507598877, 'epoch': 0.31}\n{'loss': 0.8599, 'learning_rate': 8.788574348801676e-06, 'rewards/chosen': -0.7133185267448425, 'rewards/rejected': -0.4390525817871094, 'rewards/accuracies': 0.25, 'rewards/margins': -0.27426594495773315, 'logps/rejected': -71.94491577148438, 'logps/chosen': -102.37250518798828, 'logits/rejected': -2.562676429748535, 'logits/chosen': -2.5636212825775146, 'epoch': 0.31}\n{'loss': 0.6686, 'learning_rate': 8.734107384920771e-06, 'rewards/chosen': -0.2684503197669983, 'rewards/rejected': -0.34821218252182007, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0797618106007576, 'logps/rejected': -107.5083999633789, 'logps/chosen': -137.84312438964844, 'logits/rejected': -2.261207103729248, 'logits/chosen': -2.320985794067383, 'epoch': 0.32}\n{'loss': 0.5858, 'learning_rate': 8.67861955336566e-06, 'rewards/chosen': -0.01173514872789383, 'rewards/rejected': -0.2559414803981781, 'rewards/accuracies': 0.75, 'rewards/margins': 0.24420633912086487, 'logps/rejected': -89.30864715576172, 'logps/chosen': -108.68010711669922, 'logits/rejected': -2.0021755695343018, 'logits/chosen': -2.101821184158325, 'epoch': 0.32}\n{'loss': 0.6133, 'learning_rate': 8.622126023955446e-06, 'rewards/chosen': -0.1420365273952484, 'rewards/rejected': -0.32477036118507385, 'rewards/accuracies': 0.75, 'rewards/margins': 0.18273383378982544, 'logps/rejected': -102.82149505615234, 'logps/chosen': -119.79644775390625, 'logits/rejected': -2.2083330154418945, 'logits/chosen': -2.1937882900238037, 'epoch': 0.33}\n{'loss': 0.8249, 'learning_rate': 8.564642241456986e-06, 'rewards/chosen': -0.029839135706424713, 'rewards/rejected': 0.12794151902198792, 'rewards/accuracies': 0.5, 'rewards/margins': -0.15778064727783203, 'logps/rejected': -223.67733764648438, 'logps/chosen': -132.22509765625, 'logits/rejected': -2.426008701324463, 'logits/chosen': -2.2937097549438477, 'epoch': 0.33}\n{'loss': 0.8372, 'learning_rate': 8.506183921362443e-06, 'rewards/chosen': -0.18866348266601562, 'rewards/rejected': 0.06706658005714417, 'rewards/accuracies': 0.25, 'rewards/margins': -0.2557300627231598, 'logps/rejected': -164.12332153320312, 'logps/chosen': -97.4971694946289, 'logits/rejected': -2.2525815963745117, 'logits/chosen': -2.2420077323913574, 'epoch': 0.34}\n{'loss': 0.6783, 'learning_rate': 8.446767045592829e-06, 'rewards/chosen': -0.39970722794532776, 'rewards/rejected': -0.44395333528518677, 'rewards/accuracies': 0.75, 'rewards/margins': 0.04424609988927841, 'logps/rejected': -91.77420043945312, 'logps/chosen': -134.86398315429688, 'logits/rejected': -2.168060064315796, 'logits/chosen': -2.1454813480377197, 'epoch': 0.34}\n{'loss': 0.7416, 'learning_rate': 8.386407858128707e-06, 'rewards/chosen': -0.23014917969703674, 'rewards/rejected': -0.14564847946166992, 'rewards/accuracies': 0.25, 'rewards/margins': -0.08450070023536682, 'logps/rejected': -95.42481994628906, 'logps/chosen': -94.3245849609375, 'logits/rejected': -2.117699146270752, 'logits/chosen': -2.139134645462036, 'epoch': 0.35}\n 35%|██████████████▋                           | 70/200 [06:38<10:27,  4.82s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.78it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.42it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.11it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.94it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.83it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.62it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.62it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.66it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.67it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.70it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6029277443885803, 'eval_runtime': 8.437, 'eval_samples_per_second': 1.659, 'eval_steps_per_second': 1.659, 'eval_rewards/chosen': 0.08199413865804672, 'eval_rewards/rejected': -0.13777364790439606, 'eval_rewards/accuracies': 0.7857142686843872, 'eval_rewards/margins': 0.21976779401302338, 'eval_logps/rejected': -114.47135925292969, 'eval_logps/chosen': -102.97706604003906, 'eval_logits/rejected': -2.3075125217437744, 'eval_logits/chosen': -2.338988780975342, 'epoch': 0.35}\n 35%|██████████████▋                           | 70/200 [06:47<10:27,  4.82s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.73it/s]\u001b[A\n{'loss': 0.7017, 'learning_rate': 8.325122860569241e-06, 'rewards/chosen': 0.09832152724266052, 'rewards/rejected': 0.0998561829328537, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0015346482396125793, 'logps/rejected': -134.9952392578125, 'logps/chosen': -101.25859069824219, 'logits/rejected': -2.431161642074585, 'logits/chosen': -2.341209888458252, 'epoch': 0.35}\n{'loss': 0.7052, 'learning_rate': 8.262928807620843e-06, 'rewards/chosen': -0.4544169306755066, 'rewards/rejected': -0.43840092420578003, 'rewards/accuracies': 0.5, 'rewards/margins': -0.016016006469726562, 'logps/rejected': -102.92047119140625, 'logps/chosen': -96.13658905029297, 'logits/rejected': -2.3208961486816406, 'logits/chosen': -2.3394229412078857, 'epoch': 0.36}\n{'loss': 0.5836, 'learning_rate': 8.199842702516584e-06, 'rewards/chosen': -0.36648619174957275, 'rewards/rejected': -0.6276918649673462, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2612057030200958, 'logps/rejected': -184.73931884765625, 'logps/chosen': -117.72496795654297, 'logits/rejected': -2.2298920154571533, 'logits/chosen': -2.146333694458008, 'epoch': 0.36}\n{'loss': 0.5592, 'learning_rate': 8.135881792367686e-06, 'rewards/chosen': -0.7098903059959412, 'rewards/rejected': -1.0604714155197144, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3505811095237732, 'logps/rejected': -150.81378173828125, 'logps/chosen': -88.68057250976562, 'logits/rejected': -2.4144973754882812, 'logits/chosen': -2.2397334575653076, 'epoch': 0.37}\n{'loss': 0.6961, 'learning_rate': 8.071063563448341e-06, 'rewards/chosen': -0.764286458492279, 'rewards/rejected': -0.7771908044815063, 'rewards/accuracies': 0.5, 'rewards/margins': 0.012904360890388489, 'logps/rejected': -117.7968521118164, 'logps/chosen': -122.11934661865234, 'logits/rejected': -2.404627799987793, 'logits/chosen': -2.4076175689697266, 'epoch': 0.37}\n{'loss': 0.7389, 'learning_rate': 8.005405736415127e-06, 'rewards/chosen': -0.37753239274024963, 'rewards/rejected': -0.29866278171539307, 'rewards/accuracies': 0.25, 'rewards/margins': -0.07886962592601776, 'logps/rejected': -158.72157287597656, 'logps/chosen': -89.10360717773438, 'logits/rejected': -2.254279136657715, 'logits/chosen': -2.127307415008545, 'epoch': 0.38}\n{'loss': 0.8106, 'learning_rate': 7.938926261462366e-06, 'rewards/chosen': -0.7879164814949036, 'rewards/rejected': -0.7150481939315796, 'rewards/accuracies': 0.5, 'rewards/margins': -0.07286831736564636, 'logps/rejected': -87.52706909179688, 'logps/chosen': -119.69851684570312, 'logits/rejected': -2.2044994831085205, 'logits/chosen': -2.250840187072754, 'epoch': 0.38}\n{'loss': 0.7518, 'learning_rate': 7.871643313414718e-06, 'rewards/chosen': -0.3695196211338043, 'rewards/rejected': -0.2629600465297699, 'rewards/accuracies': 0.25, 'rewards/margins': -0.10655956715345383, 'logps/rejected': -121.51233673095703, 'logps/chosen': -80.16676330566406, 'logits/rejected': -2.366947650909424, 'logits/chosen': -2.3179287910461426, 'epoch': 0.39}\n{'loss': 0.7159, 'learning_rate': 7.803575286758365e-06, 'rewards/chosen': -0.19766941666603088, 'rewards/rejected': -0.1546114981174469, 'rewards/accuracies': 0.25, 'rewards/margins': -0.04305792227387428, 'logps/rejected': -109.10250854492188, 'logps/chosen': -95.15968322753906, 'logits/rejected': -2.227935791015625, 'logits/chosen': -2.2505760192871094, 'epoch': 0.39}\n{'loss': 0.6484, 'learning_rate': 7.734740790612137e-06, 'rewards/chosen': -0.11824969947338104, 'rewards/rejected': -0.22126692533493042, 'rewards/accuracies': 0.75, 'rewards/margins': 0.10301724076271057, 'logps/rejected': -117.75838470458984, 'logps/chosen': -79.21528625488281, 'logits/rejected': -2.2921111583709717, 'logits/chosen': -2.3146653175354004, 'epoch': 0.4}\n 40%|████████████████▊                         | 80/200 [07:36<09:58,  4.99s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.75it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.38it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.10it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.94it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5877200365066528, 'eval_runtime': 8.4091, 'eval_samples_per_second': 1.665, 'eval_steps_per_second': 1.665, 'eval_rewards/chosen': 0.14839936792850494, 'eval_rewards/rejected': -0.11156190931797028, 'eval_rewards/accuracies': 0.7142857313156128, 'eval_rewards/margins': 0.2599613070487976, 'eval_logps/rejected': -114.2059555053711, 'eval_logps/chosen': -102.31415557861328, 'eval_logits/rejected': -2.2930424213409424, 'eval_logits/chosen': -2.3255209922790527, 'epoch': 0.4}\n 40%|████████████████▊                         | 80/200 [07:45<09:58,  4.99s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.5963, 'learning_rate': 7.66515864363997e-06, 'rewards/chosen': 0.2958391308784485, 'rewards/rejected': 0.008177563548088074, 'rewards/accuracies': 0.5, 'rewards/margins': 0.2876615524291992, 'logps/rejected': -128.892578125, 'logps/chosen': -98.20291137695312, 'logits/rejected': -2.266298532485962, 'logits/chosen': -2.3860549926757812, 'epoch': 0.4}\n{'loss': 0.7102, 'learning_rate': 7.594847868906076e-06, 'rewards/chosen': 0.04822817072272301, 'rewards/rejected': 0.04425162822008133, 'rewards/accuracies': 0.5, 'rewards/margins': 0.003976538777351379, 'logps/rejected': -94.65940856933594, 'logps/chosen': -141.16714477539062, 'logits/rejected': -2.1911392211914062, 'logits/chosen': -2.231396198272705, 'epoch': 0.41}\n{'loss': 0.7695, 'learning_rate': 7.52382768867422e-06, 'rewards/chosen': -0.0448760986328125, 'rewards/rejected': 0.08996257930994034, 'rewards/accuracies': 0.25, 'rewards/margins': -0.13483867049217224, 'logps/rejected': -118.1875, 'logps/chosen': -104.19668579101562, 'logits/rejected': -2.2808890342712402, 'logits/chosen': -2.163461685180664, 'epoch': 0.41}\n{'loss': 0.5979, 'learning_rate': 7.452117519152542e-06, 'rewards/chosen': 0.04151706397533417, 'rewards/rejected': -0.236273393034935, 'rewards/accuracies': 0.5, 'rewards/margins': 0.27779045701026917, 'logps/rejected': -118.37644958496094, 'logps/chosen': -129.42361450195312, 'logits/rejected': -2.190288543701172, 'logits/chosen': -2.2411131858825684, 'epoch': 0.42}\n{'loss': 0.5926, 'learning_rate': 7.379736965185369e-06, 'rewards/chosen': -0.17387476563453674, 'rewards/rejected': -0.391274631023407, 'rewards/accuracies': 1.0, 'rewards/margins': 0.21739989519119263, 'logps/rejected': -121.69223022460938, 'logps/chosen': -115.34539794921875, 'logits/rejected': -2.194821834564209, 'logits/chosen': -2.1843128204345703, 'epoch': 0.42}\n{'loss': 0.6347, 'learning_rate': 7.30670581489344e-06, 'rewards/chosen': 0.2521434724330902, 'rewards/rejected': 0.12567368149757385, 'rewards/accuracies': 0.5, 'rewards/margins': 0.12646980583667755, 'logps/rejected': -82.0815200805664, 'logps/chosen': -129.73056030273438, 'logits/rejected': -2.336522340774536, 'logits/chosen': -2.3799211978912354, 'epoch': 0.43}\n{'loss': 0.5907, 'learning_rate': 7.233044034264034e-06, 'rewards/chosen': -0.028875358402729034, 'rewards/rejected': -0.27330607175827026, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2444307506084442, 'logps/rejected': -89.65494537353516, 'logps/chosen': -105.95094299316406, 'logits/rejected': -2.165029287338257, 'logits/chosen': -2.187882900238037, 'epoch': 0.43}\n{'loss': 0.664, 'learning_rate': 7.158771761692464e-06, 'rewards/chosen': 0.21975861489772797, 'rewards/rejected': 0.10076884925365448, 'rewards/accuracies': 0.5, 'rewards/margins': 0.11898977309465408, 'logps/rejected': -113.5553970336914, 'logps/chosen': -82.4651870727539, 'logits/rejected': -2.2974905967712402, 'logits/chosen': -2.3285322189331055, 'epoch': 0.44}\n{'loss': 0.7368, 'learning_rate': 7.083909302476453e-06, 'rewards/chosen': -0.3157958984375, 'rewards/rejected': -0.2739132046699524, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0418827086687088, 'logps/rejected': -103.0902328491211, 'logps/chosen': -128.7098846435547, 'logits/rejected': -2.267637252807617, 'logits/chosen': -2.233525037765503, 'epoch': 0.44}\n{'loss': 0.5818, 'learning_rate': 7.008477123264849e-06, 'rewards/chosen': 0.06450271606445312, 'rewards/rejected': -0.19632931053638458, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2608320415019989, 'logps/rejected': -140.60879516601562, 'logps/chosen': -128.72650146484375, 'logits/rejected': -2.3647327423095703, 'logits/chosen': -2.327679395675659, 'epoch': 0.45}\n 45%|██████████████████▉                       | 90/200 [08:37<09:27,  5.16s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.73it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.39it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.09it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.91it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.83it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.81it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.60it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.62it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.68it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.70it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5744305849075317, 'eval_runtime': 8.4596, 'eval_samples_per_second': 1.655, 'eval_steps_per_second': 1.655, 'eval_rewards/chosen': -0.011951317079365253, 'eval_rewards/rejected': -0.3476598858833313, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.33570852875709534, 'eval_logps/rejected': -116.57219696044922, 'eval_logps/chosen': -103.91019439697266, 'eval_logits/rejected': -2.2654008865356445, 'eval_logits/chosen': -2.2951619625091553, 'epoch': 0.45}\n 45%|██████████████████▉                       | 90/200 [08:46<09:27,  5.16s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.9779, 'learning_rate': 6.932495846462262e-06, 'rewards/chosen': -0.4732060730457306, 'rewards/rejected': 0.02329426258802414, 'rewards/accuracies': 0.0, 'rewards/margins': -0.49650031328201294, 'logps/rejected': -113.52742767333984, 'logps/chosen': -170.2861328125, 'logits/rejected': -2.203646659851074, 'logits/chosen': -2.236123561859131, 'epoch': 0.45}\n{'loss': 0.557, 'learning_rate': 6.855986244591104e-06, 'rewards/chosen': 0.00449104979634285, 'rewards/rejected': -0.3229757249355316, 'rewards/accuracies': 0.75, 'rewards/margins': 0.32746678590774536, 'logps/rejected': -146.80734252929688, 'logps/chosen': -139.24737548828125, 'logits/rejected': -2.3866629600524902, 'logits/chosen': -2.428678512573242, 'epoch': 0.46}\n{'loss': 0.6822, 'learning_rate': 6.778969234612583e-06, 'rewards/chosen': -0.2048528641462326, 'rewards/rejected': -0.36102598905563354, 'rewards/accuracies': 0.5, 'rewards/margins': 0.15617315471172333, 'logps/rejected': -114.12565612792969, 'logps/chosen': -117.03590393066406, 'logits/rejected': -2.1086955070495605, 'logits/chosen': -2.1588358879089355, 'epoch': 0.46}\n{'loss': 0.6461, 'learning_rate': 6.701465872208216e-06, 'rewards/chosen': -0.2516515851020813, 'rewards/rejected': -0.3727443814277649, 'rewards/accuracies': 0.75, 'rewards/margins': 0.121092788875103, 'logps/rejected': -111.35701751708984, 'logps/chosen': -120.38690185546875, 'logits/rejected': -2.222804069519043, 'logits/chosen': -2.2605350017547607, 'epoch': 0.47}\n{'loss': 0.7291, 'learning_rate': 6.6234973460234184e-06, 'rewards/chosen': -0.35090065002441406, 'rewards/rejected': -0.2886175215244293, 'rewards/accuracies': 0.5, 'rewards/margins': -0.06228313967585564, 'logps/rejected': -117.16732788085938, 'logps/chosen': -78.43695068359375, 'logits/rejected': -2.2299575805664062, 'logits/chosen': -2.240114450454712, 'epoch': 0.47}\n{'loss': 0.6146, 'learning_rate': 6.545084971874738e-06, 'rewards/chosen': -0.13642291724681854, 'rewards/rejected': -0.48008766770362854, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3436647057533264, 'logps/rejected': -100.09236145019531, 'logps/chosen': -135.00408935546875, 'logits/rejected': -2.3079233169555664, 'logits/chosen': -2.2491350173950195, 'epoch': 0.48}\n{'loss': 0.6563, 'learning_rate': 6.466250186922325e-06, 'rewards/chosen': -0.3942551612854004, 'rewards/rejected': -0.47738611698150635, 'rewards/accuracies': 0.75, 'rewards/margins': 0.08313094079494476, 'logps/rejected': -169.19107055664062, 'logps/chosen': -108.52898406982422, 'logits/rejected': -2.192497730255127, 'logits/chosen': -2.1027374267578125, 'epoch': 0.48}\n{'loss': 0.5166, 'learning_rate': 6.387014543809224e-06, 'rewards/chosen': -0.3256729245185852, 'rewards/rejected': -1.0516823530197144, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7260093688964844, 'logps/rejected': -185.88201904296875, 'logps/chosen': -134.02789306640625, 'logits/rejected': -2.1305322647094727, 'logits/chosen': -2.1514604091644287, 'epoch': 0.49}\n{'loss': 0.862, 'learning_rate': 6.3073997047691e-06, 'rewards/chosen': -0.3736627697944641, 'rewards/rejected': -0.09482298046350479, 'rewards/accuracies': 0.5, 'rewards/margins': -0.27883976697921753, 'logps/rejected': -134.21856689453125, 'logps/chosen': -140.59234619140625, 'logits/rejected': -2.2726035118103027, 'logits/chosen': -2.41162371635437, 'epoch': 0.49}\n{'loss': 0.6272, 'learning_rate': 6.227427435703997e-06, 'rewards/chosen': -0.09308873116970062, 'rewards/rejected': -0.28701460361480713, 'rewards/accuracies': 0.75, 'rewards/margins': 0.1939258575439453, 'logps/rejected': -115.81063079833984, 'logps/chosen': -111.91725158691406, 'logits/rejected': -2.270048141479492, 'logits/chosen': -2.2103304862976074, 'epoch': 0.5}\n 50%|████████████████████▌                    | 100/200 [09:36<08:19,  4.99s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.68it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.39it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.09it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.92it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.83it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.81it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.77it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.60it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.62it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:06<00:01,  1.68it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5492036938667297, 'eval_runtime': 8.4462, 'eval_samples_per_second': 1.658, 'eval_steps_per_second': 1.658, 'eval_rewards/chosen': 0.09976626932621002, 'eval_rewards/rejected': -0.28743189573287964, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.38719817996025085, 'eval_logps/rejected': -115.96735382080078, 'eval_logps/chosen': -102.79481506347656, 'eval_logits/rejected': -2.2666285037994385, 'eval_logits/chosen': -2.2974491119384766, 'epoch': 0.5}\n 50%|████████████████████▌                    | 100/200 [09:44<08:19,  4.99s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'loss': 0.504, 'learning_rate': 6.147119600233758e-06, 'rewards/chosen': -0.20826560258865356, 'rewards/rejected': -0.648124098777771, 'rewards/accuracies': 1.0, 'rewards/margins': 0.4398585259914398, 'logps/rejected': -103.57199096679688, 'logps/chosen': -144.52232360839844, 'logits/rejected': -2.2999587059020996, 'logits/chosen': -2.3608624935150146, 'epoch': 0.5}\n{'loss': 0.6323, 'learning_rate': 6.066498153718735e-06, 'rewards/chosen': -0.048190876841545105, 'rewards/rejected': -0.2111542820930481, 'rewards/accuracies': 0.5, 'rewards/margins': 0.1629633903503418, 'logps/rejected': -101.35612487792969, 'logps/chosen': -130.24908447265625, 'logits/rejected': -2.232198715209961, 'logits/chosen': -2.265880584716797, 'epoch': 0.51}\n{'loss': 0.6435, 'learning_rate': 5.985585137257401e-06, 'rewards/chosen': 0.38874131441116333, 'rewards/rejected': 0.17828789353370667, 'rewards/accuracies': 0.75, 'rewards/margins': 0.21045342087745667, 'logps/rejected': -132.51011657714844, 'logps/chosen': -114.73834991455078, 'logits/rejected': -2.2522542476654053, 'logits/chosen': -2.2428882122039795, 'epoch': 0.51}\n{'loss': 0.6824, 'learning_rate': 5.904402671660551e-06, 'rewards/chosen': 0.12723541259765625, 'rewards/rejected': 0.07152271270751953, 'rewards/accuracies': 0.5, 'rewards/margins': 0.05571272224187851, 'logps/rejected': -112.29330444335938, 'logps/chosen': -93.77050018310547, 'logits/rejected': -2.412010908126831, 'logits/chosen': -2.3964734077453613, 'epoch': 0.52}\n{'loss': 0.7088, 'learning_rate': 5.82297295140367e-06, 'rewards/chosen': -0.587088406085968, 'rewards/rejected': -0.5959501266479492, 'rewards/accuracies': 0.5, 'rewards/margins': 0.008861705660820007, 'logps/rejected': -126.2348861694336, 'logps/chosen': -102.46080017089844, 'logits/rejected': -2.3839592933654785, 'logits/chosen': -2.3618271350860596, 'epoch': 0.52}\n{'loss': 0.756, 'learning_rate': 5.74131823855921e-06, 'rewards/chosen': -0.08792829513549805, 'rewards/rejected': -0.00036678602918982506, 'rewards/accuracies': 0.5, 'rewards/margins': -0.08756150305271149, 'logps/rejected': -120.61567687988281, 'logps/chosen': -90.5877456665039, 'logits/rejected': -2.4091944694519043, 'logits/chosen': -2.4058890342712402, 'epoch': 0.53}\n{'loss': 0.7023, 'learning_rate': 5.659460856710346e-06, 'rewards/chosen': -0.03629055246710777, 'rewards/rejected': -0.0928570032119751, 'rewards/accuracies': 0.25, 'rewards/margins': 0.05656643211841583, 'logps/rejected': -130.48828125, 'logps/chosen': -113.90535736083984, 'logits/rejected': -2.0984272956848145, 'logits/chosen': -2.068150520324707, 'epoch': 0.53}\n{'loss': 0.5674, 'learning_rate': 5.577423184847932e-06, 'rewards/chosen': -0.11329993605613708, 'rewards/rejected': -0.40903663635253906, 'rewards/accuracies': 0.75, 'rewards/margins': 0.295736700296402, 'logps/rejected': -102.74098205566406, 'logps/chosen': -139.0262908935547, 'logits/rejected': -2.2523136138916016, 'logits/chosen': -2.3205485343933105, 'epoch': 0.54}\n{'loss': 0.8166, 'learning_rate': 5.495227651252315e-06, 'rewards/chosen': -0.20902805030345917, 'rewards/rejected': 0.011392973363399506, 'rewards/accuracies': 0.25, 'rewards/margins': -0.22042103111743927, 'logps/rejected': -109.96540832519531, 'logps/chosen': -140.22555541992188, 'logits/rejected': -2.079477310180664, 'logits/chosen': -2.0848898887634277, 'epoch': 0.54}\n{'loss': 0.7547, 'learning_rate': 5.412896727361663e-06, 'rewards/chosen': -0.15422649681568146, 'rewards/rejected': -0.08554516732692719, 'rewards/accuracies': 0.25, 'rewards/margins': -0.06868132203817368, 'logps/rejected': -98.45774841308594, 'logps/chosen': -91.62554931640625, 'logits/rejected': -2.2823572158813477, 'logits/chosen': -2.287074089050293, 'epoch': 0.55}\n 55%|██████████████████████▌                  | 110/200 [10:32<07:20,  4.89s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.78it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.40it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.11it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.93it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.71it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5524441003799438, 'eval_runtime': 8.4212, 'eval_samples_per_second': 1.662, 'eval_steps_per_second': 1.662, 'eval_rewards/chosen': 0.05793747678399086, 'eval_rewards/rejected': -0.3312292695045471, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.38916677236557007, 'eval_logps/rejected': -116.40494537353516, 'eval_logps/chosen': -103.22257232666016, 'eval_logits/rejected': -2.2633392810821533, 'eval_logits/chosen': -2.292834520339966, 'epoch': 0.55}\n 55%|██████████████████████▌                  | 110/200 [10:40<07:20,  4.89s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.8762, 'learning_rate': 5.3304529216284974e-06, 'rewards/chosen': -0.6647822856903076, 'rewards/rejected': -0.354108601808548, 'rewards/accuracies': 0.25, 'rewards/margins': -0.31067371368408203, 'logps/rejected': -142.38333129882812, 'logps/chosen': -113.67633056640625, 'logits/rejected': -2.1080832481384277, 'logits/chosen': -2.081188201904297, 'epoch': 0.55}\n{'loss': 0.582, 'learning_rate': 5.247918773366112e-06, 'rewards/chosen': -0.16947384178638458, 'rewards/rejected': -0.5149418115615845, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3454679846763611, 'logps/rejected': -127.28495788574219, 'logps/chosen': -64.9236068725586, 'logits/rejected': -2.1814990043640137, 'logits/chosen': -2.177823066711426, 'epoch': 0.56}\n{'loss': 0.5209, 'learning_rate': 5.165316846586541e-06, 'rewards/chosen': -0.11473121494054794, 'rewards/rejected': -0.510617196559906, 'rewards/accuracies': 1.0, 'rewards/margins': 0.3958859443664551, 'logps/rejected': -99.26361083984375, 'logps/chosen': -95.96941375732422, 'logits/rejected': -2.246098279953003, 'logits/chosen': -2.2224433422088623, 'epoch': 0.56}\n{'loss': 0.5913, 'learning_rate': 5.082669723831793e-06, 'rewards/chosen': -0.05599784851074219, 'rewards/rejected': -0.2765091061592102, 'rewards/accuracies': 1.0, 'rewards/margins': 0.22051125764846802, 'logps/rejected': -85.1817398071289, 'logps/chosen': -118.6495132446289, 'logits/rejected': -2.297943115234375, 'logits/chosen': -2.2446060180664062, 'epoch': 0.57}\n{'loss': 0.7794, 'learning_rate': 5e-06, 'rewards/chosen': -0.37888815999031067, 'rewards/rejected': -0.27070748805999756, 'rewards/accuracies': 0.25, 'rewards/margins': -0.10818061977624893, 'logps/rejected': -93.4561767578125, 'logps/chosen': -124.32605743408203, 'logits/rejected': -2.6136832237243652, 'logits/chosen': -2.6477766036987305, 'epoch': 0.57}\n{'loss': 0.5042, 'learning_rate': 4.917330276168208e-06, 'rewards/chosen': -0.4572904706001282, 'rewards/rejected': -0.9212554693222046, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4639650285243988, 'logps/rejected': -135.52926635742188, 'logps/chosen': -108.62969970703125, 'logits/rejected': -2.430795192718506, 'logits/chosen': -2.371744394302368, 'epoch': 0.57}\n{'loss': 0.697, 'learning_rate': 4.8346831534134595e-06, 'rewards/chosen': -0.10867710411548615, 'rewards/rejected': -0.13651971518993378, 'rewards/accuracies': 0.75, 'rewards/margins': 0.027842611074447632, 'logps/rejected': -124.94499206542969, 'logps/chosen': -125.57977294921875, 'logits/rejected': -2.155470371246338, 'logits/chosen': -2.2166411876678467, 'epoch': 0.58}\n{'loss': 0.5975, 'learning_rate': 4.752081226633888e-06, 'rewards/chosen': -0.04967441409826279, 'rewards/rejected': -0.2718934118747711, 'rewards/accuracies': 0.75, 'rewards/margins': 0.22221899032592773, 'logps/rejected': -150.3457794189453, 'logps/chosen': -109.57388305664062, 'logits/rejected': -2.2181801795959473, 'logits/chosen': -2.1698806285858154, 'epoch': 0.58}\n{'loss': 0.7341, 'learning_rate': 4.669547078371503e-06, 'rewards/chosen': -0.32206955552101135, 'rewards/rejected': -0.2506023645401001, 'rewards/accuracies': 0.5, 'rewards/margins': -0.07146719098091125, 'logps/rejected': -84.94087219238281, 'logps/chosen': -98.3436050415039, 'logits/rejected': -2.069119930267334, 'logits/chosen': -2.0939676761627197, 'epoch': 0.59}\n{'loss': 0.9169, 'learning_rate': 4.587103272638339e-06, 'rewards/chosen': -0.5439796447753906, 'rewards/rejected': -0.18151283264160156, 'rewards/accuracies': 0.25, 'rewards/margins': -0.36246684193611145, 'logps/rejected': -96.15278625488281, 'logps/chosen': -115.66669464111328, 'logits/rejected': -2.3175055980682373, 'logits/chosen': -2.2380101680755615, 'epoch': 0.59}\n 60%|████████████████████████▌                | 120/200 [11:30<06:43,  5.05s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.77it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.33it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:05,  2.00it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.84it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:03<00:04,  1.79it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.78it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.76it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.60it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.64it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:06<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.71it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.73it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5491026043891907, 'eval_runtime': 8.5001, 'eval_samples_per_second': 1.647, 'eval_steps_per_second': 1.647, 'eval_rewards/chosen': -0.028803469613194466, 'eval_rewards/rejected': -0.4537075459957123, 'eval_rewards/accuracies': 0.7857142686843872, 'eval_rewards/margins': 0.42490407824516296, 'eval_logps/rejected': -117.63745880126953, 'eval_logps/chosen': -104.0828628540039, 'eval_logits/rejected': -2.250178575515747, 'eval_logits/chosen': -2.2772886753082275, 'epoch': 0.59}\n 60%|████████████████████████▌                | 120/200 [11:38<06:43,  5.05s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'loss': 0.5842, 'learning_rate': 4.504772348747687e-06, 'rewards/chosen': -0.1878805011510849, 'rewards/rejected': -0.4244133234024048, 'rewards/accuracies': 1.0, 'rewards/margins': 0.2365328073501587, 'logps/rejected': -126.90711212158203, 'logps/chosen': -97.16021728515625, 'logits/rejected': -2.3461551666259766, 'logits/chosen': -2.3054537773132324, 'epoch': 0.6}\n{'loss': 0.6996, 'learning_rate': 4.42257681515207e-06, 'rewards/chosen': -0.0423557311296463, 'rewards/rejected': -0.037217140197753906, 'rewards/accuracies': 0.5, 'rewards/margins': -0.005138590931892395, 'logps/rejected': -91.956787109375, 'logps/chosen': -67.70832061767578, 'logits/rejected': -2.621406078338623, 'logits/chosen': -2.5619900226593018, 'epoch': 0.6}\n{'loss': 0.8823, 'learning_rate': 4.340539143289655e-06, 'rewards/chosen': -0.6115579605102539, 'rewards/rejected': -0.28153878450393677, 'rewards/accuracies': 0.0, 'rewards/margins': -0.3300192058086395, 'logps/rejected': -110.26561737060547, 'logps/chosen': -96.55545043945312, 'logits/rejected': -2.3468902111053467, 'logits/chosen': -2.3401896953582764, 'epoch': 0.61}\n{'loss': 0.7794, 'learning_rate': 4.25868176144079e-06, 'rewards/chosen': -0.7295070290565491, 'rewards/rejected': -0.5769122838973999, 'rewards/accuracies': 0.25, 'rewards/margins': -0.152594655752182, 'logps/rejected': -95.17176055908203, 'logps/chosen': -94.40348815917969, 'logits/rejected': -2.186061382293701, 'logits/chosen': -2.1705827713012695, 'epoch': 0.61}\n{'loss': 0.4946, 'learning_rate': 4.17702704859633e-06, 'rewards/chosen': -0.013461112976074219, 'rewards/rejected': -0.5295145511627197, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5160533785820007, 'logps/rejected': -138.70758056640625, 'logps/chosen': -113.85184478759766, 'logits/rejected': -2.199688196182251, 'logits/chosen': -2.1689095497131348, 'epoch': 0.62}\n{'loss': 0.7506, 'learning_rate': 4.0955973283394525e-06, 'rewards/chosen': -0.5916504263877869, 'rewards/rejected': -0.5199298858642578, 'rewards/accuracies': 0.25, 'rewards/margins': -0.07172049582004547, 'logps/rejected': -170.7754364013672, 'logps/chosen': -110.38255310058594, 'logits/rejected': -2.185206413269043, 'logits/chosen': -2.153118371963501, 'epoch': 0.62}\n{'loss': 0.533, 'learning_rate': 4.0144148627426e-06, 'rewards/chosen': 0.16573886573314667, 'rewards/rejected': -0.3094167709350586, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4751555919647217, 'logps/rejected': -111.71102142333984, 'logps/chosen': -117.17031860351562, 'logits/rejected': -2.188375473022461, 'logits/chosen': -2.1487419605255127, 'epoch': 0.63}\n{'loss': 0.5326, 'learning_rate': 3.9335018462812664e-06, 'rewards/chosen': -0.6573309302330017, 'rewards/rejected': -1.0718207359313965, 'rewards/accuracies': 0.5, 'rewards/margins': 0.41448983550071716, 'logps/rejected': -143.34906005859375, 'logps/chosen': -163.57553100585938, 'logits/rejected': -2.3688299655914307, 'logits/chosen': -2.2888145446777344, 'epoch': 0.63}\n{'loss': 0.8021, 'learning_rate': 3.852880399766243e-06, 'rewards/chosen': -0.20752602815628052, 'rewards/rejected': -0.22732488811016083, 'rewards/accuracies': 0.75, 'rewards/margins': 0.01979885995388031, 'logps/rejected': -150.42495727539062, 'logps/chosen': -130.96609497070312, 'logits/rejected': -2.2094733715057373, 'logits/chosen': -2.232966423034668, 'epoch': 0.64}\n{'loss': 0.5787, 'learning_rate': 3.7725725642960047e-06, 'rewards/chosen': -0.41008442640304565, 'rewards/rejected': -0.6784877777099609, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2684033513069153, 'logps/rejected': -116.35021209716797, 'logps/chosen': -117.28099060058594, 'logits/rejected': -2.10856556892395, 'logits/chosen': -2.0828042030334473, 'epoch': 0.64}\n 65%|██████████████████████████▋              | 130/200 [12:29<05:44,  4.93s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.74it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.41it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.12it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.94it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.86it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.64it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.68it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.68it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.70it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5713546276092529, 'eval_runtime': 8.4336, 'eval_samples_per_second': 1.66, 'eval_steps_per_second': 1.66, 'eval_rewards/chosen': -0.14049802720546722, 'eval_rewards/rejected': -0.552146852016449, 'eval_rewards/accuracies': 0.7142857313156128, 'eval_rewards/margins': 0.41164880990982056, 'eval_logps/rejected': -118.61778259277344, 'eval_logps/chosen': -105.20162200927734, 'eval_logits/rejected': -2.230299949645996, 'eval_logits/chosen': -2.25427508354187, 'epoch': 0.64}\n 65%|██████████████████████████▋              | 130/200 [12:38<05:44,  4.93s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.73it/s]\u001b[A\n{'loss': 0.9981, 'learning_rate': 3.6926002952309015e-06, 'rewards/chosen': -1.0039722919464111, 'rewards/rejected': -0.6203083992004395, 'rewards/accuracies': 0.5, 'rewards/margins': -0.3836638927459717, 'logps/rejected': -85.98194122314453, 'logps/chosen': -205.2849578857422, 'logits/rejected': -2.226928234100342, 'logits/chosen': -2.3255465030670166, 'epoch': 0.65}\n{'loss': 0.4717, 'learning_rate': 3.6129854561907786e-06, 'rewards/chosen': -0.2587640881538391, 'rewards/rejected': -0.8392797708511353, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5805156826972961, 'logps/rejected': -97.79995727539062, 'logps/chosen': -133.33663940429688, 'logits/rejected': -2.015497922897339, 'logits/chosen': -2.0499489307403564, 'epoch': 0.65}\n{'loss': 0.826, 'learning_rate': 3.533749813077677e-06, 'rewards/chosen': -0.1086919754743576, 'rewards/rejected': -0.025826558470726013, 'rewards/accuracies': 0.25, 'rewards/margins': -0.08286543190479279, 'logps/rejected': -102.94427490234375, 'logps/chosen': -107.09754180908203, 'logits/rejected': -2.205599784851074, 'logits/chosen': -2.268705129623413, 'epoch': 0.66}\n{'loss': 1.0674, 'learning_rate': 3.4549150281252635e-06, 'rewards/chosen': -0.2639055550098419, 'rewards/rejected': 0.3070606589317322, 'rewards/accuracies': 0.25, 'rewards/margins': -0.5709662437438965, 'logps/rejected': -134.70620727539062, 'logps/chosen': -125.89028930664062, 'logits/rejected': -2.273911714553833, 'logits/chosen': -2.1693763732910156, 'epoch': 0.66}\n{'loss': 0.6641, 'learning_rate': 3.3765026539765832e-06, 'rewards/chosen': -0.8195433020591736, 'rewards/rejected': -0.8927778005599976, 'rewards/accuracies': 0.75, 'rewards/margins': 0.07323455810546875, 'logps/rejected': -140.13229370117188, 'logps/chosen': -101.67398071289062, 'logits/rejected': -2.052867889404297, 'logits/chosen': -2.0487542152404785, 'epoch': 0.67}\n{'loss': 0.764, 'learning_rate': 3.298534127791785e-06, 'rewards/chosen': -0.3842884302139282, 'rewards/rejected': -0.28789177536964417, 'rewards/accuracies': 0.5, 'rewards/margins': -0.09639663994312286, 'logps/rejected': -120.60920715332031, 'logps/chosen': -116.05459594726562, 'logits/rejected': -2.103639602661133, 'logits/chosen': -2.069507122039795, 'epoch': 0.67}\n{'loss': 0.8239, 'learning_rate': 3.2210307653874175e-06, 'rewards/chosen': -0.48668423295021057, 'rewards/rejected': -0.27971383929252625, 'rewards/accuracies': 0.25, 'rewards/margins': -0.2069704383611679, 'logps/rejected': -94.65376281738281, 'logps/chosen': -126.0706787109375, 'logits/rejected': -2.036482095718384, 'logits/chosen': -2.038994550704956, 'epoch': 0.68}\n{'loss': 0.6889, 'learning_rate': 3.1440137554088957e-06, 'rewards/chosen': -0.1500801146030426, 'rewards/rejected': -0.16400966048240662, 'rewards/accuracies': 0.5, 'rewards/margins': 0.01392955332994461, 'logps/rejected': -102.80762481689453, 'logps/chosen': -122.60296630859375, 'logits/rejected': -2.202357530593872, 'logits/chosen': -2.1973745822906494, 'epoch': 0.68}\n{'loss': 0.5907, 'learning_rate': 3.06750415353774e-06, 'rewards/chosen': -0.06067008897662163, 'rewards/rejected': -0.3496454358100891, 'rewards/accuracies': 0.75, 'rewards/margins': 0.288975328207016, 'logps/rejected': -129.32162475585938, 'logps/chosen': -128.9515380859375, 'logits/rejected': -2.1137938499450684, 'logits/chosen': -2.038193941116333, 'epoch': 0.69}\n{'loss': 0.637, 'learning_rate': 2.991522876735154e-06, 'rewards/chosen': -0.9569200873374939, 'rewards/rejected': -1.2682183980941772, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3112984299659729, 'logps/rejected': -122.01348876953125, 'logps/chosen': -113.63137817382812, 'logits/rejected': -2.2338900566101074, 'logits/chosen': -2.289188861846924, 'epoch': 0.69}\n 70%|████████████████████████████▋            | 140/200 [13:27<05:01,  5.03s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.75it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.40it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.12it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.94it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5733384490013123, 'eval_runtime': 8.4068, 'eval_samples_per_second': 1.665, 'eval_steps_per_second': 1.665, 'eval_rewards/chosen': -0.07727957516908646, 'eval_rewards/rejected': -0.47656527161598206, 'eval_rewards/accuracies': 0.7142857313156128, 'eval_rewards/margins': 0.39928561449050903, 'eval_logps/rejected': -117.8591079711914, 'eval_logps/chosen': -104.57173919677734, 'eval_logits/rejected': -2.2325949668884277, 'eval_logits/chosen': -2.2571048736572266, 'epoch': 0.69}\n 70%|████████████████████████████▋            | 140/200 [13:35<05:01,  5.03s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'loss': 0.8207, 'learning_rate': 2.9160906975235493e-06, 'rewards/chosen': -0.3592282235622406, 'rewards/rejected': -0.15551146864891052, 'rewards/accuracies': 0.25, 'rewards/margins': -0.20371676981449127, 'logps/rejected': -122.23600006103516, 'logps/chosen': -140.25148010253906, 'logits/rejected': -2.3251967430114746, 'logits/chosen': -2.3710741996765137, 'epoch': 0.7}\n{'loss': 0.5682, 'learning_rate': 2.8412282383075362e-06, 'rewards/chosen': -0.17918434739112854, 'rewards/rejected': -0.4928590953350067, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3136747479438782, 'logps/rejected': -104.51468658447266, 'logps/chosen': -124.36669921875, 'logits/rejected': -2.3165814876556396, 'logits/chosen': -2.2727599143981934, 'epoch': 0.7}\n{'loss': 0.6573, 'learning_rate': 2.766955965735968e-06, 'rewards/chosen': -0.49219608306884766, 'rewards/rejected': -0.6829427480697632, 'rewards/accuracies': 0.75, 'rewards/margins': 0.19074667990207672, 'logps/rejected': -140.22572326660156, 'logps/chosen': -126.23463439941406, 'logits/rejected': -2.256582021713257, 'logits/chosen': -2.237950325012207, 'epoch': 0.71}\n{'loss': 0.5338, 'learning_rate': 2.693294185106562e-06, 'rewards/chosen': -0.19031298160552979, 'rewards/rejected': -0.5706799626350403, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3803669810295105, 'logps/rejected': -80.46182250976562, 'logps/chosen': -114.93450927734375, 'logits/rejected': -2.190629482269287, 'logits/chosen': -2.1110520362854004, 'epoch': 0.71}\n{'loss': 0.657, 'learning_rate': 2.6202630348146323e-06, 'rewards/chosen': -0.0159318745136261, 'rewards/rejected': -0.13039188086986542, 'rewards/accuracies': 0.5, 'rewards/margins': 0.11445997655391693, 'logps/rejected': -124.72073364257812, 'logps/chosen': -100.8228988647461, 'logits/rejected': -2.353508949279785, 'logits/chosen': -2.3953943252563477, 'epoch': 0.72}\n{'loss': 0.5325, 'learning_rate': 2.5478824808474613e-06, 'rewards/chosen': 0.034163013100624084, 'rewards/rejected': -0.4877764582633972, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5219395160675049, 'logps/rejected': -113.94723510742188, 'logps/chosen': -82.88286590576172, 'logits/rejected': -2.5506865978240967, 'logits/chosen': -2.6192378997802734, 'epoch': 0.72}\n{'loss': 0.9233, 'learning_rate': 2.476172311325783e-06, 'rewards/chosen': -0.6560637950897217, 'rewards/rejected': -0.4015262722969055, 'rewards/accuracies': 0.5, 'rewards/margins': -0.25453758239746094, 'logps/rejected': -147.23255920410156, 'logps/chosen': -125.783203125, 'logits/rejected': -2.055279493331909, 'logits/chosen': -2.1058430671691895, 'epoch': 0.73}\n{'loss': 0.6729, 'learning_rate': 2.4051521310939258e-06, 'rewards/chosen': -0.09241695702075958, 'rewards/rejected': -0.1455797255039215, 'rewards/accuracies': 0.75, 'rewards/margins': 0.05316277593374252, 'logps/rejected': -99.51459503173828, 'logps/chosen': -80.99408721923828, 'logits/rejected': -2.188040018081665, 'logits/chosen': -2.18747615814209, 'epoch': 0.73}\n{'loss': 0.7016, 'learning_rate': 2.3348413563600324e-06, 'rewards/chosen': -0.41151562333106995, 'rewards/rejected': -0.43556728959083557, 'rewards/accuracies': 0.5, 'rewards/margins': 0.024051658809185028, 'logps/rejected': -124.37750244140625, 'logps/chosen': -101.81924438476562, 'logits/rejected': -2.2074079513549805, 'logits/chosen': -2.2187306880950928, 'epoch': 0.74}\n{'loss': 0.808, 'learning_rate': 2.265259209387867e-06, 'rewards/chosen': -0.031808093190193176, 'rewards/rejected': 0.17047519981861115, 'rewards/accuracies': 0.25, 'rewards/margins': -0.20228329300880432, 'logps/rejected': -109.40990447998047, 'logps/chosen': -102.36497497558594, 'logits/rejected': -2.316999673843384, 'logits/chosen': -2.298165798187256, 'epoch': 0.74}\n 75%|██████████████████████████████▊          | 150/200 [14:24<04:03,  4.88s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.75it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.41it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.11it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.95it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.86it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.83it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.64it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.71it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.73it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5477612018585205, 'eval_runtime': 8.3802, 'eval_samples_per_second': 1.671, 'eval_steps_per_second': 1.671, 'eval_rewards/chosen': 0.04745468869805336, 'eval_rewards/rejected': -0.38618797063827515, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.4336426258087158, 'eval_logps/rejected': -116.95218658447266, 'eval_logps/chosen': -103.31724548339844, 'eval_logits/rejected': -2.2413876056671143, 'eval_logits/chosen': -2.2686283588409424, 'epoch': 0.74}\n 75%|██████████████████████████████▊          | 150/200 [14:33<04:03,  4.88s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'loss': 0.6495, 'learning_rate': 2.1964247132416373e-06, 'rewards/chosen': -0.11803160607814789, 'rewards/rejected': -0.27623414993286133, 'rewards/accuracies': 0.75, 'rewards/margins': 0.15820252895355225, 'logps/rejected': -109.64385986328125, 'logps/chosen': -80.39884948730469, 'logits/rejected': -2.2501068115234375, 'logits/chosen': -2.1932919025421143, 'epoch': 0.75}\n{'loss': 0.6169, 'learning_rate': 2.1283566865852824e-06, 'rewards/chosen': -0.28932610154151917, 'rewards/rejected': -0.5146116018295288, 'rewards/accuracies': 0.5, 'rewards/margins': 0.22528551518917084, 'logps/rejected': -128.5802001953125, 'logps/chosen': -141.18870544433594, 'logits/rejected': -2.077361583709717, 'logits/chosen': -2.1364099979400635, 'epoch': 0.75}\n{'loss': 0.4876, 'learning_rate': 2.061073738537635e-06, 'rewards/chosen': -0.051020413637161255, 'rewards/rejected': -0.6158731579780579, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5648527145385742, 'logps/rejected': -102.65143585205078, 'logps/chosen': -124.80823516845703, 'logits/rejected': -2.185981035232544, 'logits/chosen': -2.1578640937805176, 'epoch': 0.76}\n{'loss': 0.5262, 'learning_rate': 1.9945942635848745e-06, 'rewards/chosen': -0.01691855490207672, 'rewards/rejected': -0.4166816771030426, 'rewards/accuracies': 1.0, 'rewards/margins': 0.3997631072998047, 'logps/rejected': -129.48928833007812, 'logps/chosen': -118.8655776977539, 'logits/rejected': -2.242530345916748, 'logits/chosen': -2.2412571907043457, 'epoch': 0.76}\n{'loss': 0.7503, 'learning_rate': 1.928936436551661e-06, 'rewards/chosen': 0.16471585631370544, 'rewards/rejected': 0.23002280294895172, 'rewards/accuracies': 0.5, 'rewards/margins': -0.06530695408582687, 'logps/rejected': -89.1732177734375, 'logps/chosen': -103.87870025634766, 'logits/rejected': -2.17620849609375, 'logits/chosen': -2.1364364624023438, 'epoch': 0.77}\n{'loss': 0.5323, 'learning_rate': 1.864118207632315e-06, 'rewards/chosen': -0.6867464184761047, 'rewards/rejected': -1.0772064924240112, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3904600143432617, 'logps/rejected': -112.89700317382812, 'logps/chosen': -164.12278747558594, 'logits/rejected': -2.281097412109375, 'logits/chosen': -2.3858160972595215, 'epoch': 0.77}\n{'loss': 0.7201, 'learning_rate': 1.8001572974834169e-06, 'rewards/chosen': 0.014404874294996262, 'rewards/rejected': 0.02910805493593216, 'rewards/accuracies': 0.5, 'rewards/margins': -0.014703188091516495, 'logps/rejected': -133.76602172851562, 'logps/chosen': -89.84551239013672, 'logits/rejected': -2.209813117980957, 'logits/chosen': -2.274606943130493, 'epoch': 0.78}\n{'loss': 0.7021, 'learning_rate': 1.7370711923791567e-06, 'rewards/chosen': -0.440355122089386, 'rewards/rejected': -0.4570634961128235, 'rewards/accuracies': 0.25, 'rewards/margins': 0.016708359122276306, 'logps/rejected': -104.65005493164062, 'logps/chosen': -146.04278564453125, 'logits/rejected': -2.1464972496032715, 'logits/chosen': -2.353030204772949, 'epoch': 0.78}\n{'loss': 0.6491, 'learning_rate': 1.6748771394307584e-06, 'rewards/chosen': 0.17864112555980682, 'rewards/rejected': 0.03858375549316406, 'rewards/accuracies': 0.25, 'rewards/margins': 0.14005735516548157, 'logps/rejected': -154.8218994140625, 'logps/chosen': -119.69464111328125, 'logits/rejected': -2.1559035778045654, 'logits/chosen': -1.9812848567962646, 'epoch': 0.79}\n{'loss': 0.762, 'learning_rate': 1.6135921418712959e-06, 'rewards/chosen': 0.11944331228733063, 'rewards/rejected': 0.0019882195629179478, 'rewards/accuracies': 0.5, 'rewards/margins': 0.11745510995388031, 'logps/rejected': -109.88758850097656, 'logps/chosen': -105.59062194824219, 'logits/rejected': -2.2209060192108154, 'logits/chosen': -2.245943307876587, 'epoch': 0.79}\n 80%|████████████████████████████████▊        | 160/200 [15:22<03:28,  5.20s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.77it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.41it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.11it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.93it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.59it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.62it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.71it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5555102825164795, 'eval_runtime': 8.415, 'eval_samples_per_second': 1.664, 'eval_steps_per_second': 1.664, 'eval_rewards/chosen': 0.162082239985466, 'eval_rewards/rejected': -0.22625823318958282, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.3883405029773712, 'eval_logps/rejected': -115.35581970214844, 'eval_logps/chosen': -102.17002868652344, 'eval_logits/rejected': -2.246828556060791, 'eval_logits/chosen': -2.2756714820861816, 'epoch': 0.79}\n 80%|████████████████████████████████▊        | 160/200 [15:31<03:28,  5.20s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'loss': 0.7541, 'learning_rate': 1.5532329544071712e-06, 'rewards/chosen': -0.2508794069290161, 'rewards/rejected': -0.3301442861557007, 'rewards/accuracies': 0.5, 'rewards/margins': 0.07926490157842636, 'logps/rejected': -97.83953857421875, 'logps/chosen': -171.20623779296875, 'logits/rejected': -2.352820873260498, 'logits/chosen': -2.294919490814209, 'epoch': 0.8}\n{'loss': 0.5094, 'learning_rate': 1.4938160786375571e-06, 'rewards/chosen': 0.08949051052331924, 'rewards/rejected': -0.35974961519241333, 'rewards/accuracies': 0.75, 'rewards/margins': 0.449240118265152, 'logps/rejected': -118.29203796386719, 'logps/chosen': -160.46990966796875, 'logits/rejected': -2.130030393600464, 'logits/chosen': -2.267148494720459, 'epoch': 0.8}\n{'loss': 0.5393, 'learning_rate': 1.4353577585430152e-06, 'rewards/chosen': 0.19430390000343323, 'rewards/rejected': -0.24507008492946625, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4393739700317383, 'logps/rejected': -122.81305694580078, 'logps/chosen': -128.9703369140625, 'logits/rejected': -2.220336675643921, 'logits/chosen': -2.1469786167144775, 'epoch': 0.81}\n{'loss': 0.7111, 'learning_rate': 1.3778739760445552e-06, 'rewards/chosen': -0.3869834840297699, 'rewards/rejected': -0.3675439953804016, 'rewards/accuracies': 0.75, 'rewards/margins': -0.01943949982523918, 'logps/rejected': -124.88424682617188, 'logps/chosen': -103.70780944824219, 'logits/rejected': -2.0986804962158203, 'logits/chosen': -2.0547213554382324, 'epoch': 0.81}\n{'loss': 0.575, 'learning_rate': 1.321380446634342e-06, 'rewards/chosen': -0.13734082877635956, 'rewards/rejected': -0.4000706076622009, 'rewards/accuracies': 1.0, 'rewards/margins': 0.2627297341823578, 'logps/rejected': -134.4195098876953, 'logps/chosen': -110.25556182861328, 'logits/rejected': -2.247833251953125, 'logits/chosen': -2.2435314655303955, 'epoch': 0.82}\n{'loss': 0.465, 'learning_rate': 1.2658926150792321e-06, 'rewards/chosen': 0.20814648270606995, 'rewards/rejected': -0.33069345355033875, 'rewards/accuracies': 1.0, 'rewards/margins': 0.5388399362564087, 'logps/rejected': -175.93783569335938, 'logps/chosen': -100.13555145263672, 'logits/rejected': -2.279209852218628, 'logits/chosen': -2.1914210319519043, 'epoch': 0.82}\n{'loss': 0.5166, 'learning_rate': 1.2114256511983274e-06, 'rewards/chosen': 0.06082974001765251, 'rewards/rejected': -0.4170486629009247, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4778783917427063, 'logps/rejected': -177.69882202148438, 'logps/chosen': -82.77381896972656, 'logits/rejected': -2.154141426086426, 'logits/chosen': -2.035543203353882, 'epoch': 0.83}\n{'loss': 0.6421, 'learning_rate': 1.157994445715706e-06, 'rewards/chosen': 0.07492045313119888, 'rewards/rejected': -0.2403295487165451, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3152500092983246, 'logps/rejected': -130.6972198486328, 'logps/chosen': -137.43487548828125, 'logits/rejected': -2.1437082290649414, 'logits/chosen': -2.156923294067383, 'epoch': 0.83}\n{'loss': 0.6854, 'learning_rate': 1.1056136061894386e-06, 'rewards/chosen': 0.06270560622215271, 'rewards/rejected': -0.028501495718955994, 'rewards/accuracies': 0.5, 'rewards/margins': 0.09120713174343109, 'logps/rejected': -118.74700927734375, 'logps/chosen': -100.85810852050781, 'logits/rejected': -2.190828800201416, 'logits/chosen': -2.173140048980713, 'epoch': 0.84}\n{'loss': 0.71, 'learning_rate': 1.0542974530180327e-06, 'rewards/chosen': -0.7483255863189697, 'rewards/rejected': -0.7261053323745728, 'rewards/accuracies': 0.5, 'rewards/margins': -0.022220231592655182, 'logps/rejected': -98.7616958618164, 'logps/chosen': -165.96267700195312, 'logits/rejected': -2.082031726837158, 'logits/chosen': -2.1144444942474365, 'epoch': 0.84}\n 85%|██████████████████████████████████▊      | 170/200 [16:23<02:36,  5.21s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.73it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.38it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.08it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.92it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.84it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.81it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.68it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5564833879470825, 'eval_runtime': 8.446, 'eval_samples_per_second': 1.658, 'eval_steps_per_second': 1.658, 'eval_rewards/chosen': 0.15826375782489777, 'eval_rewards/rejected': -0.23470041155815125, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.3929641842842102, 'eval_logps/rejected': -115.44031524658203, 'eval_logps/chosen': -102.21570587158203, 'eval_logits/rejected': -2.247478723526001, 'eval_logits/chosen': -2.2762558460235596, 'epoch': 0.84}\n 85%|██████████████████████████████████▊      | 170/200 [16:31<02:36,  5.21s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.6607, 'learning_rate': 1.0040600155253766e-06, 'rewards/chosen': -0.0504581481218338, 'rewards/rejected': -0.2607080638408661, 'rewards/accuracies': 0.5, 'rewards/margins': 0.2102499157190323, 'logps/rejected': -137.0328826904297, 'logps/chosen': -90.78585815429688, 'logits/rejected': -2.331476926803589, 'logits/chosen': -2.2835934162139893, 'epoch': 0.85}\n{'loss': 0.5333, 'learning_rate': 9.549150281252633e-07, 'rewards/chosen': 0.0032985731959342957, 'rewards/rejected': -0.3680528402328491, 'rewards/accuracies': 1.0, 'rewards/margins': 0.371351420879364, 'logps/rejected': -153.6200408935547, 'logps/chosen': -115.32137298583984, 'logits/rejected': -2.1544857025146484, 'logits/chosen': -2.1701390743255615, 'epoch': 0.85}\n{'loss': 0.4084, 'learning_rate': 9.068759265665384e-07, 'rewards/chosen': 0.3637876510620117, 'rewards/rejected': -0.3599754273891449, 'rewards/accuracies': 1.0, 'rewards/margins': 0.723763108253479, 'logps/rejected': -110.73251342773438, 'logps/chosen': -111.91661071777344, 'logits/rejected': -2.2682275772094727, 'logits/chosen': -2.2657692432403564, 'epoch': 0.86}\n{'loss': 0.6506, 'learning_rate': 8.599558442598998e-07, 'rewards/chosen': 0.0253390334546566, 'rewards/rejected': -0.06671313941478729, 'rewards/accuracies': 0.5, 'rewards/margins': 0.09205217659473419, 'logps/rejected': -85.79058837890625, 'logps/chosen': -109.76498413085938, 'logits/rejected': -2.162346363067627, 'logits/chosen': -2.1389787197113037, 'epoch': 0.86}\n{'loss': 0.7881, 'learning_rate': 8.141676086873574e-07, 'rewards/chosen': -0.44062289595603943, 'rewards/rejected': -0.3059154748916626, 'rewards/accuracies': 0.5, 'rewards/margins': -0.13470743596553802, 'logps/rejected': -84.67840576171875, 'logps/chosen': -118.59027099609375, 'logits/rejected': -2.4115469455718994, 'logits/chosen': -2.3914690017700195, 'epoch': 0.87}\n{'loss': 1.044, 'learning_rate': 7.695237378953224e-07, 'rewards/chosen': -0.2620542645454407, 'rewards/rejected': 0.29337942600250244, 'rewards/accuracies': 0.25, 'rewards/margins': -0.5554336309432983, 'logps/rejected': -101.819580078125, 'logps/chosen': -114.35030364990234, 'logits/rejected': -2.036181926727295, 'logits/chosen': -2.0605666637420654, 'epoch': 0.87}\n{'loss': 0.8024, 'learning_rate': 7.260364370723044e-07, 'rewards/chosen': -0.0886305719614029, 'rewards/rejected': 0.09035491943359375, 'rewards/accuracies': 0.25, 'rewards/margins': -0.17898550629615784, 'logps/rejected': -113.32881927490234, 'logps/chosen': -96.23834228515625, 'logits/rejected': -2.2053327560424805, 'logits/chosen': -2.2310426235198975, 'epoch': 0.88}\n{'loss': 0.502, 'learning_rate': 6.837175952121305e-07, 'rewards/chosen': -0.11014670878648758, 'rewards/rejected': -0.5714784860610962, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4613317549228668, 'logps/rejected': -127.86273956298828, 'logps/chosen': -151.5797576904297, 'logits/rejected': -2.416029691696167, 'logits/chosen': -2.335081100463867, 'epoch': 0.88}\n{'loss': 0.6988, 'learning_rate': 6.425787818636131e-07, 'rewards/chosen': -0.19667282700538635, 'rewards/rejected': -0.2549058794975281, 'rewards/accuracies': 0.75, 'rewards/margins': 0.058233074843883514, 'logps/rejected': -93.03540802001953, 'logps/chosen': -134.44720458984375, 'logits/rejected': -2.2769775390625, 'logits/chosen': -2.282745599746704, 'epoch': 0.89}\n{'loss': 0.5881, 'learning_rate': 6.026312439675553e-07, 'rewards/chosen': 0.07004242390394211, 'rewards/rejected': -0.17274609208106995, 'rewards/accuracies': 0.75, 'rewards/margins': 0.24278852343559265, 'logps/rejected': -129.76695251464844, 'logps/chosen': -168.041748046875, 'logits/rejected': -2.2671358585357666, 'logits/chosen': -2.335000514984131, 'epoch': 0.89}\n 90%|████████████████████████████████████▉    | 180/200 [17:21<01:41,  5.08s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.76it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.35it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.03it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.86it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:03<00:04,  1.79it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.79it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.76it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.60it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:06<00:01,  1.68it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.71it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5557319521903992, 'eval_runtime': 8.4947, 'eval_samples_per_second': 1.648, 'eval_steps_per_second': 1.648, 'eval_rewards/chosen': 0.14448155462741852, 'eval_rewards/rejected': -0.256437212228775, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.40091878175735474, 'eval_logps/rejected': -115.64654541015625, 'eval_logps/chosen': -102.35057830810547, 'eval_logits/rejected': -2.246309995651245, 'eval_logits/chosen': -2.274465560913086, 'epoch': 0.89}\n 90%|████████████████████████████████████▉    | 180/200 [17:29<01:41,  5.08s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.8294, 'learning_rate': 5.63885902781941e-07, 'rewards/chosen': -0.1768035888671875, 'rewards/rejected': 0.0492643341422081, 'rewards/accuracies': 0.5, 'rewards/margins': -0.2260679304599762, 'logps/rejected': -112.95271301269531, 'logps/chosen': -92.77352142333984, 'logits/rejected': -2.181387186050415, 'logits/chosen': -2.1638786792755127, 'epoch': 0.9}\n{'loss': 0.6697, 'learning_rate': 5.263533508961827e-07, 'rewards/chosen': -0.12769317626953125, 'rewards/rejected': -0.27000296115875244, 'rewards/accuracies': 0.5, 'rewards/margins': 0.1423097401857376, 'logps/rejected': -116.63251495361328, 'logps/chosen': -150.8187713623047, 'logits/rejected': -2.2391867637634277, 'logits/chosen': -2.2015187740325928, 'epoch': 0.9}\n{'loss': 0.5878, 'learning_rate': 4.900438493352056e-07, 'rewards/chosen': -0.17398393154144287, 'rewards/rejected': -0.44265443086624146, 'rewards/accuracies': 0.5, 'rewards/margins': 0.2686704993247986, 'logps/rejected': -121.06477355957031, 'logps/chosen': -108.74259948730469, 'logits/rejected': -2.1355035305023193, 'logits/chosen': -2.193474531173706, 'epoch': 0.91}\n{'loss': 0.6036, 'learning_rate': 4.549673247541875e-07, 'rewards/chosen': -0.3336694836616516, 'rewards/rejected': -0.5389828085899353, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2053132951259613, 'logps/rejected': -140.68101501464844, 'logps/chosen': -108.93353271484375, 'logits/rejected': -2.013340950012207, 'logits/chosen': -1.9903185367584229, 'epoch': 0.91}\n{'loss': 0.5116, 'learning_rate': 4.211333667247125e-07, 'rewards/chosen': 0.048481330275535583, 'rewards/rejected': -0.4853011965751648, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5337825417518616, 'logps/rejected': -118.19440460205078, 'logps/chosen': -128.12075805664062, 'logits/rejected': -2.2299413681030273, 'logits/chosen': -2.266401529312134, 'epoch': 0.92}\n{'loss': 0.6535, 'learning_rate': 3.885512251130763e-07, 'rewards/chosen': -0.15854358673095703, 'rewards/rejected': -0.25830650329589844, 'rewards/accuracies': 0.75, 'rewards/margins': 0.0997629165649414, 'logps/rejected': -86.08502197265625, 'logps/chosen': -104.73957824707031, 'logits/rejected': -2.4362618923187256, 'logits/chosen': -2.4417386054992676, 'epoch': 0.92}\n{'loss': 0.5236, 'learning_rate': 3.572298075514652e-07, 'rewards/chosen': -0.33173638582229614, 'rewards/rejected': -0.7249648571014404, 'rewards/accuracies': 1.0, 'rewards/margins': 0.3932284712791443, 'logps/rejected': -129.77740478515625, 'logps/chosen': -110.92383575439453, 'logits/rejected': -2.3804469108581543, 'logits/chosen': -2.3727974891662598, 'epoch': 0.93}\n{'loss': 0.5699, 'learning_rate': 3.271776770026963e-07, 'rewards/chosen': 0.17072030901908875, 'rewards/rejected': -0.27281761169433594, 'rewards/accuracies': 0.5, 'rewards/margins': 0.4435379207134247, 'logps/rejected': -115.14811706542969, 'logps/chosen': -163.80905151367188, 'logits/rejected': -2.3941869735717773, 'logits/chosen': -2.4519903659820557, 'epoch': 0.93}\n{'loss': 0.7112, 'learning_rate': 2.984030494191942e-07, 'rewards/chosen': -0.011527830734848976, 'rewards/rejected': -0.016722679138183594, 'rewards/accuracies': 0.25, 'rewards/margins': 0.005194850265979767, 'logps/rejected': -98.59882354736328, 'logps/chosen': -92.22637939453125, 'logits/rejected': -2.224766492843628, 'logits/chosen': -2.2030723094940186, 'epoch': 0.94}\n{'loss': 0.6954, 'learning_rate': 2.7091379149682683e-07, 'rewards/chosen': -0.38888072967529297, 'rewards/rejected': -0.47543489933013916, 'rewards/accuracies': 0.5, 'rewards/margins': 0.08655418455600739, 'logps/rejected': -160.16017150878906, 'logps/chosen': -129.22695922851562, 'logits/rejected': -2.3134403228759766, 'logits/chosen': -2.408531904220581, 'epoch': 0.94}\n 95%|██████████████████████████████████████▉  | 190/200 [18:18<00:51,  5.11s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.77it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.42it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.10it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.94it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.64it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.71it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5530140995979309, 'eval_runtime': 8.4246, 'eval_samples_per_second': 1.662, 'eval_steps_per_second': 1.662, 'eval_rewards/chosen': 0.14370636641979218, 'eval_rewards/rejected': -0.263485312461853, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.4071916937828064, 'eval_logps/rejected': -115.71808624267578, 'eval_logps/chosen': -102.3586196899414, 'eval_logits/rejected': -2.2457432746887207, 'eval_logits/chosen': -2.2738184928894043, 'epoch': 0.94}\n 95%|██████████████████████████████████████▉  | 190/200 [18:26<00:51,  5.11s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.72it/s]\u001b[A\n{'loss': 0.5765, 'learning_rate': 2.447174185242324e-07, 'rewards/chosen': -0.22828826308250427, 'rewards/rejected': -0.7288055419921875, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5005172491073608, 'logps/rejected': -134.2462158203125, 'logps/chosen': -161.24966430664062, 'logits/rejected': -2.229872226715088, 'logits/chosen': -2.347209930419922, 'epoch': 0.95}\n{'loss': 1.1381, 'learning_rate': 2.198210923282118e-07, 'rewards/chosen': -0.3667421340942383, 'rewards/rejected': 0.24262407422065735, 'rewards/accuracies': 0.25, 'rewards/margins': -0.6093661785125732, 'logps/rejected': -134.03823852539062, 'logps/chosen': -130.5983123779297, 'logits/rejected': -2.1603403091430664, 'logits/chosen': -2.189744472503662, 'epoch': 0.95}\n{'loss': 0.6307, 'learning_rate': 1.962316193157593e-07, 'rewards/chosen': -0.35103437304496765, 'rewards/rejected': -0.5349096059799194, 'rewards/accuracies': 0.5, 'rewards/margins': 0.18387527763843536, 'logps/rejected': -140.82223510742188, 'logps/chosen': -145.33404541015625, 'logits/rejected': -2.3972225189208984, 'logits/chosen': -2.483729839324951, 'epoch': 0.96}\n{'loss': 0.4001, 'learning_rate': 1.7395544861325718e-07, 'rewards/chosen': 0.6503192782402039, 'rewards/rejected': -0.20394106209278107, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8542603254318237, 'logps/rejected': -113.98917388916016, 'logps/chosen': -132.031494140625, 'logits/rejected': -2.1649951934814453, 'logits/chosen': -2.342914581298828, 'epoch': 0.96}\n{'loss': 0.5493, 'learning_rate': 1.5299867030334815e-07, 'rewards/chosen': 0.5040995478630066, 'rewards/rejected': 0.1081380844116211, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3959614932537079, 'logps/rejected': -157.78550720214844, 'logps/chosen': -109.3871078491211, 'logits/rejected': -2.2867283821105957, 'logits/chosen': -2.2348644733428955, 'epoch': 0.97}\n{'loss': 0.7431, 'learning_rate': 1.333670137599713e-07, 'rewards/chosen': -0.35173627734184265, 'rewards/rejected': -0.2887565791606903, 'rewards/accuracies': 0.5, 'rewards/margins': -0.06297969818115234, 'logps/rejected': -134.78720092773438, 'logps/chosen': -128.4801025390625, 'logits/rejected': -2.049255609512329, 'logits/chosen': -2.0640335083007812, 'epoch': 0.97}\n{'loss': 0.6109, 'learning_rate': 1.1506584608200366e-07, 'rewards/chosen': 0.08927477896213531, 'rewards/rejected': -0.09407443553209305, 'rewards/accuracies': 0.75, 'rewards/margins': 0.18334922194480896, 'logps/rejected': -141.15994262695312, 'logps/chosen': -146.44586181640625, 'logits/rejected': -2.461162805557251, 'logits/chosen': -2.5367648601531982, 'epoch': 0.98}\n{'loss': 0.6535, 'learning_rate': 9.810017062595322e-08, 'rewards/chosen': -0.35427266359329224, 'rewards/rejected': -0.48995810747146606, 'rewards/accuracies': 0.5, 'rewards/margins': 0.13568544387817383, 'logps/rejected': -103.5339126586914, 'logps/chosen': -78.73564147949219, 'logits/rejected': -2.0760910511016846, 'logits/chosen': -2.124873638153076, 'epoch': 0.98}\n{'loss': 0.5606, 'learning_rate': 8.247462563808816e-08, 'rewards/chosen': -0.08639984577894211, 'rewards/rejected': -0.47226953506469727, 'rewards/accuracies': 0.75, 'rewards/margins': 0.38586968183517456, 'logps/rejected': -141.27639770507812, 'logps/chosen': -114.65870666503906, 'logits/rejected': -2.5024166107177734, 'logits/chosen': -2.4054954051971436, 'epoch': 0.99}\n{'loss': 0.585, 'learning_rate': 6.819348298638839e-08, 'rewards/chosen': -0.07396393269300461, 'rewards/rejected': -0.3473716974258423, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2734077274799347, 'logps/rejected': -186.9609375, 'logps/chosen': -131.30178833007812, 'logits/rejected': -2.135554075241089, 'logits/chosen': -2.1027982234954834, 'epoch': 0.99}\n100%|█████████████████████████████████████████| 200/200 [19:18<00:00,  5.17s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.70it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.39it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.09it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.93it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.62it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.64it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.70it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.72it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.73it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5563143491744995, 'eval_runtime': 8.4005, 'eval_samples_per_second': 1.667, 'eval_steps_per_second': 1.667, 'eval_rewards/chosen': 0.1451559066772461, 'eval_rewards/rejected': -0.2517659366130829, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.39692187309265137, 'eval_logps/rejected': -115.61251068115234, 'eval_logps/chosen': -102.33592224121094, 'eval_logits/rejected': -2.2467074394226074, 'eval_logits/chosen': -2.274960994720459, 'epoch': 0.99}\n100%|█████████████████████████████████████████| 200/200 [19:26<00:00,  5.17s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'train_runtime': 1168.427, 'train_samples_per_second': 0.685, 'train_steps_per_second': 0.171, 'train_loss': 0.690433207154274, 'epoch': 0.99}\n100%|█████████████████████████████████████████| 200/200 [19:28<00:00,  5.84s/it]\n\u001b[32m2023-08-29 06:56:44.389\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m514\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 1168.427, 'train_samples_per_second': 0.685, 'train_steps_per_second': 0.171, 'train_loss': 0.690433207154274, 'epoch': 0.99, 'train_samples': 1000}\u001b[0m\n***** train metrics *****\n  epoch                    =       0.99\n  train_loss               =     0.6904\n  train_runtime            = 0:19:28.42\n  train_samples            =       1000\n  train_samples_per_second =      0.685\n  train_steps_per_second   =      0.171\n\u001b[32m2023-08-29 06:56:44.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m518\u001b[0m - \u001b[1mSaving model checkpoint to outputs-dpo-yunguan-v1\u001b[0m\n\u001b[32m2023-08-29 06:56:47.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m525\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.99it/s]\n\u001b[32m2023-08-29 06:56:55.202\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m528\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 0.548906147480011, 'eval_runtime': 7.9597, 'eval_samples_per_second': 1.759, 'eval_steps_per_second': 1.759, 'eval_rewards/chosen': -0.02844368852674961, 'eval_rewards/rejected': -0.45395463705062866, 'eval_rewards/accuracies': 0.7857142686843872, 'eval_rewards/margins': 0.4255109429359436, 'eval_logps/rejected': -117.62999725341797, 'eval_logps/chosen': -104.08196258544922, 'eval_logits/rejected': -2.2502076625823975, 'eval_logits/chosen': -2.2772185802459717, 'epoch': 0.99, 'eval_samples': 20}\u001b[0m\n***** eval metrics *****\n  epoch                   =       0.99\n  eval_logits/chosen      =    -2.2772\n  eval_logits/rejected    =    -2.2502\n  eval_logps/chosen       =   -104.082\n  eval_logps/rejected     =    -117.63\n  eval_loss               =     0.5489\n  eval_rewards/accuracies =     0.7857\n  eval_rewards/chosen     =    -0.0284\n  eval_rewards/margins    =     0.4255\n  eval_rewards/rejected   =     -0.454\n  eval_runtime            = 0:00:07.95\n  eval_samples            =         20\n  eval_samples_per_second =      1.759\n  eval_steps_per_second   =      1.759\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 查看一下policy_chosen_logps 到底是啥\n# 这个reward_accuracies跟gradient_accumulation_steps有关 代码中默认是4 目前gradient_accumulation_steps=4  我在下面调用时显式的写出来\n# 所以每次是4对chosen_rewards与rejected_rewards比较 那难怪会在 [0 , 0.25 , 0.5 ,0.75] 之间跳 很离散 而且这样的话train过程中的acc其实没有多大指示性 毕竟只有4对\n# 还是要看eval_acc才有用 理解了！\n# 参考 https://github.com/valkryhx/MedicalGPT/blob/medGPT_0828/dpo_3.py#L327 和 L334 \n # 求了 reward_acc的mean() 因为reward_acc是类似[1,0,0,1]这种比较结果后得到的True/False 再float()转成1./0.的tensor！参考下面的代码：","metadata":{}},{"cell_type":"code","source":"import torch\na=torch.tensor([1])\nb=torch.tensor([5])\nc=(a>b)\nprint(c)\nprint(c.float())","metadata":{"execution":{"iopub.status.busy":"2023-08-29T08:00:05.366634Z","iopub.execute_input":"2023-08-29T08:00:05.367295Z","iopub.status.idle":"2023-08-29T08:00:05.389561Z","shell.execute_reply.started":"2023-08-29T08:00:05.367262Z","shell.execute_reply":"2023-08-29T08:00:05.388345Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"tensor([False])\ntensor([0.])\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_only_for_show_acc.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-yunguan-v1 \\\n    --target_modules all \\\n    --lora_rank 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"execution":{"iopub.status.idle":"2023-08-29T08:29:59.078055Z","shell.execute_reply.started":"2023-08-29T08:23:42.014545Z","shell.execute_reply":"2023-08-29T08:29:59.076567Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Loading checkpoint shards: 100%|██████████████████| 7/7 [01:29<00:00, 12.77s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:28<00:00, 12.69s/it]\n\u001b[32m2023-08-29 08:27:03.930\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m574\u001b[0m - \u001b[31m\u001b[1mid(model)=134526130352192\u001b[0m\n\u001b[32m2023-08-29 08:27:03.930\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m575\u001b[0m - \u001b[31m\u001b[1mid(model_ref)=134526128800048\u001b[0m\n\u001b[32m2023-08-29 08:27:03.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m611\u001b[0m - \u001b[1mPeft target_modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\u001b[0m\n\u001b[32m2023-08-29 08:27:03.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m620\u001b[0m - \u001b[1mChatGLMForConditionalGeneration(\n  (transformer): ChatGLMModel(\n    (embedding): Embedding(\n      (word_embeddings): Embedding(65024, 4096)\n    )\n    (rotary_pos_emb): RotaryEmbedding()\n    (encoder): GLMTransformer(\n      (layers): ModuleList(\n        (0-27): 28 x GLMBlock(\n          (input_layernorm): RMSNorm()\n          (self_attention): SelfAttention(\n            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n            (core_attention): CoreAttention(\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n            )\n            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n          )\n          (post_attention_layernorm): RMSNorm()\n          (mlp): MLP(\n            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n          )\n        )\n      )\n      (final_layernorm): RMSNorm()\n    )\n    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n  )\n)\u001b[0m\ntrainable params: 118587392 || all params: 6362171392 || trainable%: 1.86394525851843\n\u001b[32m2023-08-29 08:28:35.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m637\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n  0%|                                                   | 0/200 [00:00<?, ?it/s]\u001b[32m2023-08-29 08:28:37.717\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-109.5506], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:37.717\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:37.719\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:37.719\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:37.720\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:37.721\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:37.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\nCould not estimate the number of tokens of the input, floating-point operations will not be computed\n\u001b[32m2023-08-29 08:28:40.688\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-152.0067], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:40.689\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:40.690\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:40.690\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:40.691\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:40.691\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:40.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:41.809\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-133.1151], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:41.810\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:41.811\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:41.811\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:41.812\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:41.812\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:41.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:42.920\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-82.2880], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:42.921\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:42.922\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:42.922\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:42.923\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:42.923\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:42.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -147.03704833984375, 'logps/chosen': -119.24008178710938, 'logits/rejected': -2.103128671646118, 'logits/chosen': -2.0758676528930664, 'epoch': 0.0}\n  0%|▏                                          | 1/200 [00:07<26:23,  7.96s/it]\u001b[32m2023-08-29 08:28:44.198\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-49.8658], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:44.199\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:44.200\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:44.200\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:44.201\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:44.201\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:44.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:45.606\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-70.3549], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:45.606\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:45.607\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:45.608\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:45.608\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:45.609\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:45.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:46.644\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-117.7442], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:46.645\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:46.646\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:46.646\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:46.647\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:46.647\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:46.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:47.747\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-168.8973], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:47.747\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:47.748\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:47.749\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:47.749\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:47.750\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:47.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -105.12542724609375, 'logps/chosen': -101.7155532836914, 'logits/rejected': -2.3501272201538086, 'logits/chosen': -2.3333637714385986, 'epoch': 0.01}\n  1%|▍                                          | 2/200 [00:12<20:14,  6.14s/it]\u001b[32m2023-08-29 08:28:48.885\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-91.6243], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:48.885\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:48.886\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:48.886\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:48.887\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:48.887\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:48.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:50.041\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-136.9045], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:50.042\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:50.043\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:50.043\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:50.043\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:50.044\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:50.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:51.154\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-151.3369], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:51.154\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:51.155\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:51.155\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:51.156\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:51.157\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:51.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:52.207\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-114.6818], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:52.208\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:52.209\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0040], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:52.209\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:52.210\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0040], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:52.211\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0075], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:52.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n{'loss': 0.6927, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': -0.0009887695778161287, 'rewards/rejected': -0.0018695831531658769, 'rewards/accuracies': 0.25, 'rewards/margins': 0.0008808135753497481, 'logps/rejected': -108.24522399902344, 'logps/chosen': -123.63688659667969, 'logits/rejected': -2.2839434146881104, 'logits/chosen': -2.319284677505493, 'epoch': 0.01}\n  2%|▋                                          | 3/200 [00:17<18:34,  5.66s/it]\u001b[32m2023-08-29 08:28:53.955\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-136.3919], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:53.955\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:53.956\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:53.956\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:53.957\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:53.958\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:53.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:55.050\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-140.8412], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:55.050\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:55.051\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:55.052\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:55.052\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:55.053\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:55.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:56.236\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-177.0666], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:56.237\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:56.238\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:56.238\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:56.239\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:56.239\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:56.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:57.638\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-151.7849], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:57.638\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:57.639\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:57.639\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:57.640\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:57.640\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:57.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -115.29510498046875, 'logps/chosen': -151.52113342285156, 'logits/rejected': -2.263528347015381, 'logits/chosen': -2.350731134414673, 'epoch': 0.02}\n  2%|▊                                          | 4/200 [00:22<17:44,  5.43s/it]\u001b[32m2023-08-29 08:28:59.062\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-137.0175], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:59.062\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:59.063\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:59.063\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:59.064\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:59.065\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:59.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:00.133\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-134.8269], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:00.134\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:00.135\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0019], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:00.135\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:00.136\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0019], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:00.137\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0103], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:00.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:01.374\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-156.4256], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:01.375\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:01.376\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:01.376\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:01.376\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:01.377\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:01.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:02.745\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-137.0572], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:02.745\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:02.746\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0034], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:02.746\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:02.747\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0034], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:02.748\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0019], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:02.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n{'loss': 0.691, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0013263702858239412, 'rewards/rejected': -0.0030450820922851562, 'rewards/accuracies': 0.5, 'rewards/margins': 0.004371452145278454, 'logps/rejected': -125.99724578857422, 'logps/chosen': -141.33181762695312, 'logits/rejected': -2.1282036304473877, 'logits/chosen': -2.1059060096740723, 'epoch': 0.02}\n  2%|█                                          | 5/200 [00:27<16:56,  5.21s/it]\u001b[32m2023-08-29 08:29:03.879\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-60.8033], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:03.879\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:03.880\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:03.880\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:03.881\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:03.882\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:03.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:04.979\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-72.1315], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:04.979\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:04.980\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:04.980\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:04.981\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:04.982\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:04.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:06.051\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-42.1474], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:06.051\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:06.052\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:06.052\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:06.053\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:06.054\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:06.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:07.130\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-61.9071], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:07.131\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:07.132\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:07.132\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:07.133\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:07.133\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:07.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -113.37882995605469, 'logps/chosen': -59.24733352661133, 'logits/rejected': -2.4135544300079346, 'logits/chosen': -2.40352463722229, 'epoch': 0.03}\n  3%|█▎                                         | 6/200 [00:32<15:57,  4.93s/it]\u001b[32m2023-08-29 08:29:08.464\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-115.8058], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:08.464\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:08.465\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:08.465\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:08.466\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:08.467\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:08.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:09.945\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-89.6816], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:09.945\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:09.947\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:09.947\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:09.948\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:09.948\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:09.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:11.090\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-167.0477], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:11.091\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:11.092\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:11.092\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:11.092\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:11.093\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:11.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:12.262\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-142.7734], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:12.263\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:12.263\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:12.264\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:12.264\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:12.265\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:12.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -107.94503784179688, 'logps/chosen': -128.82713317871094, 'logits/rejected': -2.219214916229248, 'logits/chosen': -2.306650400161743, 'epoch': 0.03}\n  4%|█▌                                         | 7/200 [00:37<16:02,  4.99s/it]\u001b[32m2023-08-29 08:29:13.408\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-55.5227], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:13.409\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:13.410\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:13.410\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:13.411\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:13.411\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:13.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:14.552\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-77.0541], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:14.552\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:14.553\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([9.7656e-05], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:14.553\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:14.554\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([9.7656e-05], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:14.555\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0054], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:14.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:15.633\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-69.6429], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:15.633\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:15.635\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0030], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:15.635\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:15.636\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0030], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:15.637\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0069], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:15.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:16.701\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-72.5967], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:16.701\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:16.702\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:16.702\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:16.703\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:16.703\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:16.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6912, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0007825851207599044, 'rewards/rejected': -0.0030591965187340975, 'rewards/accuracies': 0.5, 'rewards/margins': 0.00384178152307868, 'logps/rejected': -97.47293090820312, 'logps/chosen': -68.70408630371094, 'logits/rejected': -2.419541358947754, 'logits/chosen': -2.3982093334198, 'epoch': 0.04}\n  4%|█▋                                         | 8/200 [00:41<15:27,  4.83s/it]\u001b[32m2023-08-29 08:29:17.907\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-111.3756], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:17.907\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:17.908\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:17.908\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:17.909\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:17.909\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:17.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:19.112\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-76.3296], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:19.113\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:19.114\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:19.114\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:19.114\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:19.115\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:19.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:20.207\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-74.9378], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:20.207\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:20.208\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0019], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:20.209\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:20.209\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0019], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:20.210\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0012], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:20.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:21.466\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-158.5807], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:21.466\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:21.467\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:21.468\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:21.468\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:21.469\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:21.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6935, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': -0.0004823684867005795, 'rewards/rejected': 0.0003021240408997983, 'rewards/accuracies': 0.0, 'rewards/margins': -0.0007844925276003778, 'logps/rejected': -139.98971557617188, 'logps/chosen': -105.30595397949219, 'logits/rejected': -2.2986958026885986, 'logits/chosen': -2.2279067039489746, 'epoch': 0.04}\n  4%|█▉                                         | 9/200 [00:46<15:32,  4.88s/it]\u001b[32m2023-08-29 08:29:22.886\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-121.7330], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:22.887\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:22.888\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:22.888\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:22.888\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:22.889\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:22.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:24.043\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-104.1884], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:24.043\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:24.044\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0011], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:24.044\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:24.045\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0011], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:24.046\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0016], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:24.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:25.137\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-116.7800], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:25.138\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:25.139\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0060], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:25.139\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:25.140\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0060], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:25.141\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0081], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:25.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:26.234\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-108.3063], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:26.234\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:26.236\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0087], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:26.236\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:26.237\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0087], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:26.237\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0005], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:26.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n{'loss': 0.6926, 'learning_rate': 2.0000000000000003e-06, 'rewards/chosen': 0.0034038545563817024, 'rewards/rejected': 0.0023120881523936987, 'rewards/accuracies': 0.25, 'rewards/margins': 0.0010917665204033256, 'logps/rejected': -137.2055206298828, 'logps/chosen': -112.75192260742188, 'logits/rejected': -2.286799907684326, 'logits/chosen': -2.2781553268432617, 'epoch': 0.05}\n  5%|██                                        | 10/200 [00:51<15:19,  4.84s/it]\u001b[32m2023-08-29 08:29:27.767\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-85.3374], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:27.768\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:27.769\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0178], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:27.770\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:27.771\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0178], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:27.772\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0203], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:27.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\u001b[32m2023-08-29 08:29:28.356\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-79.0076], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.357\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:28.358\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0052], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.358\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:28.359\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0052], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.360\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0050], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.69it/s]\u001b[A\u001b[32m2023-08-29 08:29:28.963\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-133.6930], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.964\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:28.965\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0107], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.965\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:28.966\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0107], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.967\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0188], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.38it/s]\u001b[A\u001b[32m2023-08-29 08:29:29.555\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-103.1097], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:29.555\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:29.556\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0085], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:29.557\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:29.558\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0085], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:29.558\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0155], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:29.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.07it/s]\u001b[A\u001b[32m2023-08-29 08:29:30.156\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-53.5065], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.156\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:30.157\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0018], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.158\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:30.158\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0018], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.159\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0059], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.90it/s]\u001b[A\u001b[32m2023-08-29 08:29:30.763\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-167.0971], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.764\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:30.765\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0049], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.765\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:30.766\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0049], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.767\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0017], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.81it/s]\u001b[A\u001b[32m2023-08-29 08:29:31.348\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-165.3296], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.348\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:31.350\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0040], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.350\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:31.351\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0040], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.351\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0071], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.78it/s]\u001b[A\u001b[32m2023-08-29 08:29:31.953\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-105.9407], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.953\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:31.954\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0153], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.955\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:31.956\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0153], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.956\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0129], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.75it/s]\u001b[A\u001b[32m2023-08-29 08:29:32.682\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-59.0383], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:32.683\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:32.684\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0025], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:32.684\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:32.685\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0025], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:32.686\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0228], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:32.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.58it/s]\u001b[A\u001b[32m2023-08-29 08:29:33.330\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-106.1311], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.330\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:33.331\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0083], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.331\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:33.332\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0083], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.333\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0194], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.60it/s]\u001b[A\u001b[32m2023-08-29 08:29:33.894\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-74.9697], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.895\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:33.896\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0005], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.896\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:33.897\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0005], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.898\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0033], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\n 79%|█████████████████████████████████▊         | 11/14 [00:06<00:01,  1.65it/s]\u001b[A\u001b[32m2023-08-29 08:29:34.476\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-63.7998], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:34.477\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:34.479\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0107], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:34.479\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:34.480\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0107], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:34.481\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0047], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:34.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.67it/s]\u001b[A\u001b[32m2023-08-29 08:29:35.067\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-129.4322], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.067\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:35.068\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0066], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.069\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:35.070\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0066], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.070\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0098], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.68it/s]\u001b[A\u001b[32m2023-08-29 08:29:35.628\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-126.9661], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.628\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:35.629\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0029], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.630\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:35.630\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0029], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.631\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0019], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6945472955703735, 'eval_runtime': 8.5904, 'eval_samples_per_second': 1.63, 'eval_steps_per_second': 1.63, 'eval_rewards/chosen': -0.0008251736289821565, 'eval_rewards/rejected': 0.0019584381952881813, 'eval_rewards/accuracies': 0.3571428656578064, 'eval_rewards/margins': -0.0027836114168167114, 'eval_logps/rejected': -113.07808685302734, 'eval_logps/chosen': -103.81133270263672, 'eval_logits/rejected': -2.314446210861206, 'eval_logits/chosen': -2.3433568477630615, 'epoch': 0.05}\n  5%|██                                        | 10/200 [01:00<15:19,  4.84s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.71it/s]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-29 08:29:36.188\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-111.6594], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:36.189\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:36.190\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0068], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:36.190\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:36.191\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0068], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:36.192\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0100], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:36.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:37.497\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-157.3923], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:37.498\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:37.499\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0231], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:37.499\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:37.500\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0231], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:37.501\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0034], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:37.502\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:38.929\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-133.1376], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:38.929\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:38.931\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0015], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:38.931\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:38.932\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0015], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:38.933\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0009], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:38.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:40.261\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-259.0260], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:40.262\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:40.263\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0138], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:40.263\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:40.264\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0138], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:40.265\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0011], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:40.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n{'loss': 0.6898, 'learning_rate': 2.0000000000000003e-06, 'rewards/chosen': 0.010559845715761185, 'rewards/rejected': 0.0038682937156409025, 'rewards/accuracies': 0.5, 'rewards/margins': 0.006691551301628351, 'logps/rejected': -117.17643737792969, 'logps/chosen': -165.30381774902344, 'logits/rejected': -2.258463144302368, 'logits/chosen': -2.298445701599121, 'epoch': 0.05}\n  6%|██▎                                       | 11/200 [01:05<24:08,  7.67s/it]\u001b[32m2023-08-29 08:29:41.673\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-61.7607], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:41.674\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:41.675\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0136], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:41.676\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:41.677\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0136], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:41.678\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0001], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:41.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:42.747\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-140.6223], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:42.748\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:42.749\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0168], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:42.749\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:42.750\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0168], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:42.751\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0030], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:42.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:43.880\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-128.0094], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:43.880\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:43.882\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0225], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:43.882\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:43.883\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0225], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:43.883\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0150], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:43.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:45.034\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-136.8325], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:45.034\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:45.035\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0037], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:45.036\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:45.036\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0037], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:45.037\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0089], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:45.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6923, 'learning_rate': 3e-06, 'rewards/chosen': 0.0010628702584654093, 'rewards/rejected': -0.000742149306461215, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0018050195649266243, 'logps/rejected': -105.93048095703125, 'logps/chosen': -116.80622100830078, 'logits/rejected': -2.2781856060028076, 'logits/chosen': -2.2318453788757324, 'epoch': 0.06}\n  6%|██▌                                       | 12/200 [01:10<21:04,  6.73s/it]\u001b[32m2023-08-29 08:29:46.298\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-37.3800], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:46.299\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:46.300\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0047], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:46.300\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:46.301\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0047], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:46.302\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0044], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:46.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:47.484\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-88.0652], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:47.484\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:47.486\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0104], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:47.486\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:47.487\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0104], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:47.487\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0001], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:47.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:48.614\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-70.9821], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:48.615\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:48.616\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0037], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:48.616\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:48.617\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0037], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:48.618\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0082], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:48.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:49.788\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-144.4202], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:49.788\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:49.790\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0038], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:49.790\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:49.791\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0038], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:49.792\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0073], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:49.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6917, 'learning_rate': 4.000000000000001e-06, 'rewards/chosen': 0.00381298060528934, 'rewards/rejected': 0.0008800507057458162, 'rewards/accuracies': 0.75, 'rewards/margins': 0.002932929899543524, 'logps/rejected': -103.56449127197266, 'logps/chosen': -85.2118911743164, 'logits/rejected': -2.4584577083587646, 'logits/chosen': -2.4871561527252197, 'epoch': 0.06}\n  6%|██▋                                       | 13/200 [01:14<19:02,  6.11s/it]\u001b[32m2023-08-29 08:29:50.972\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-132.8958], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:50.972\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:50.973\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0075], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:50.973\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:50.974\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0075], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:50.975\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0064], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:50.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:52.164\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-105.9191], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:52.164\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:52.166\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0171], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:52.166\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:52.167\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0171], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:52.168\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0054], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:52.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:53.316\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-139.6573], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:53.317\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:53.318\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0281], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:53.318\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:53.319\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0281], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:53.320\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0065], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:53.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:54.588\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-127.0432], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:54.588\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:54.590\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0173], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:54.590\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:54.591\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0173], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:54.592\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0103], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:54.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6884, 'learning_rate': 5e-06, 'rewards/chosen': 0.005101585295051336, 'rewards/rejected': -0.004467964172363281, 'rewards/accuracies': 0.5, 'rewards/margins': 0.00956954900175333, 'logps/rejected': -98.54788970947266, 'logps/chosen': -126.37882232666016, 'logits/rejected': -2.2618069648742676, 'logits/chosen': -2.2950801849365234, 'epoch': 0.07}\n  7%|██▉                                       | 14/200 [01:19<17:52,  5.77s/it]\u001b[32m2023-08-29 08:29:55.928\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-48.8199], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:55.929\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:55.930\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0044], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:55.930\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:55.931\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0044], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:55.932\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0077], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:55.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:57.027\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-160.4555], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:57.027\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:57.028\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0024], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:57.028\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:57.029\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0024], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:57.030\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0206], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:57.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n^C\nTraceback (most recent call last):\n  File \"/kaggle/working/MedicalGPT/dpo_only_for_show_acc.py\", line 661, in <module>\n    main()\n  File \"/kaggle/working/MedicalGPT/dpo_only_for_show_acc.py\", line 638, in main\n    train_result = trainer.train()\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1645, in train\n    return inner_training_loop(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1938, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2759, in training_step\n    loss = self.compute_loss(model, inputs)\n  File \"/opt/conda/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py\", line 396, in compute_loss\n    loss, metrics = self.get_batch_metrics(model, inputs, train_eval=\"train\")\n  File \"/kaggle/working/MedicalGPT/dpo_only_for_show_acc.py\", line 331, in get_batch_metrics\n    ) = self.concatenated_forward(self.ref_model, batch)\n  File \"/opt/conda/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py\", line 317, in concatenated_forward\n    all_logits = model(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 581, in forward\n    return model_forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 569, in __call__\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\", line 932, in forward\n    transformer_outputs = self.transformer(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\", line 828, in forward\n    hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\", line 638, in forward\n    layer_ret = layer(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\", line 542, in forward\n    attention_output, kv_cache = self.self_attention(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\", line 407, in forward\n    key_layer = apply_rotary_pos_emb(key_layer, rotary_pos_emb)\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# dpo_for_peftmodel 直接加载一个 adapter\n# 这是  qlora的 还是感觉train loss 和eval loss 不怎么收敛 ","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_for_peftmodel.py \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/chatGLM-6B-QLoRA/output_yungaun_0827_v1/checkpoint-400 \\\n    --tokenizer_name_or_path THUDM/chatglm2-6b \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora True \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-yunguan-v1 \\\n    --target_modules all \\\n    --lora_rank 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"execution":{"iopub.status.idle":"2023-08-29T15:20:56.631132Z","shell.execute_reply.started":"2023-08-29T13:38:15.466612Z","shell.execute_reply":"2023-08-29T15:20:56.629700Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{'loss': 0.6929, 'learning_rate': 5e-06, 'rewards/chosen': 0.004874515347182751, 'rewards/rejected': 0.004236030858010054, 'rewards/accuracies': 0.625, 'rewards/margins': 0.0006384849548339844, 'logps/rejected': -78.71856689453125, 'logps/chosen': -74.25202941894531, 'logits/rejected': -2.110980272293091, 'logits/chosen': -2.101668357849121, 'epoch': 0.05}\n{'loss': 0.6698, 'learning_rate': 6e-06, 'rewards/chosen': 0.08982916176319122, 'rewards/rejected': 0.04108934849500656, 'rewards/accuracies': 0.625, 'rewards/margins': 0.04873981326818466, 'logps/rejected': -96.49650573730469, 'logps/chosen': -87.66525268554688, 'logits/rejected': -1.9265261888504028, 'logits/chosen': -2.0274808406829834, 'epoch': 0.06}\n{'loss': 0.663, 'learning_rate': 7e-06, 'rewards/chosen': 0.06729288399219513, 'rewards/rejected': 0.004191780462861061, 'rewards/accuracies': 0.75, 'rewards/margins': 0.06310110539197922, 'logps/rejected': -53.67744064331055, 'logps/chosen': -97.54354858398438, 'logits/rejected': -2.1116297245025635, 'logits/chosen': -2.1470353603363037, 'epoch': 0.07}\n{'loss': 0.6755, 'learning_rate': 8.000000000000001e-06, 'rewards/chosen': 0.10492478311061859, 'rewards/rejected': 0.06444373726844788, 'rewards/accuracies': 0.375, 'rewards/margins': 0.040481045842170715, 'logps/rejected': -93.58477783203125, 'logps/chosen': -67.92510986328125, 'logits/rejected': -2.074760913848877, 'logits/chosen': -2.1384968757629395, 'epoch': 0.08}\n{'loss': 0.6917, 'learning_rate': 9e-06, 'rewards/chosen': 0.021429061889648438, 'rewards/rejected': 0.01853790134191513, 'rewards/accuracies': 0.5, 'rewards/margins': 0.002891159150749445, 'logps/rejected': -57.795379638671875, 'logps/chosen': -62.1436653137207, 'logits/rejected': -2.3120856285095215, 'logits/chosen': -2.2852182388305664, 'epoch': 0.09}\n{'loss': 0.7028, 'learning_rate': 1e-05, 'rewards/chosen': 0.04120051860809326, 'rewards/rejected': 0.05968137085437775, 'rewards/accuracies': 0.625, 'rewards/margins': -0.018480850383639336, 'logps/rejected': -89.48704528808594, 'logps/chosen': -67.53783416748047, 'logits/rejected': -2.2077465057373047, 'logits/chosen': -2.208588123321533, 'epoch': 0.1}\n  5%|██                                      | 10/200 [08:25<2:42:56, 51.46s/it]\n  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n 29%|████████████▊                                | 2/7 [00:05<00:13,  2.63s/it]\u001b[A\n 43%|███████████████████▎                         | 3/7 [00:10<00:14,  3.73s/it]\u001b[A\n 57%|█████████████████████████▋                   | 4/7 [00:15<00:12,  4.31s/it]\u001b[A\n 71%|████████████████████████████████▏            | 5/7 [00:21<00:09,  4.93s/it]\u001b[A\n 86%|██████████████████████████████████████▌      | 6/7 [00:26<00:04,  4.98s/it]\u001b[A\n100%|█████████████████████████████████████████████| 7/7 [00:32<00:00,  5.00s/it]\u001b[A\n{'eval_loss': 0.695655882358551, 'eval_runtime': 38.596, 'eval_samples_per_second': 0.363, 'eval_steps_per_second': 0.181, 'eval_rewards/chosen': 0.03676262125372887, 'eval_rewards/rejected': 0.039579637348651886, 'eval_rewards/accuracies': 0.6428571343421936, 'eval_rewards/margins': -0.0028170181903988123, 'eval_logps/rejected': -72.11510467529297, 'eval_logps/chosen': -68.3202133178711, 'eval_logits/rejected': -2.2039005756378174, 'eval_logits/chosen': -2.2171578407287598, 'epoch': 0.1}\n\n  5%|██                                      | 10/200 [09:03<2:42:56, 51.46s/it]\u001b[A\n{'loss': 0.7101, 'learning_rate': 9.999316524962347e-06, 'rewards/chosen': 0.03595152124762535, 'rewards/rejected': 0.06829805672168732, 'rewards/accuracies': 0.375, 'rewards/margins': -0.03234653174877167, 'logps/rejected': -75.80503845214844, 'logps/chosen': -72.3102798461914, 'logits/rejected': -2.233509063720703, 'logits/chosen': -2.175945520401001, 'epoch': 0.11}\n{'loss': 0.6948, 'learning_rate': 9.99726628670463e-06, 'rewards/chosen': 0.005229569040238857, 'rewards/rejected': 0.00853819865733385, 'rewards/accuracies': 0.375, 'rewards/margins': -0.003308630082756281, 'logps/rejected': -83.46852111816406, 'logps/chosen': -74.52440643310547, 'logits/rejected': -2.0736069679260254, 'logits/chosen': -2.0429909229278564, 'epoch': 0.12}\n{'loss': 0.6948, 'learning_rate': 9.993849845741525e-06, 'rewards/chosen': -0.02511129528284073, 'rewards/rejected': -0.021868467330932617, 'rewards/accuracies': 0.5, 'rewards/margins': -0.003242826322093606, 'logps/rejected': -73.79026794433594, 'logps/chosen': -68.80955505371094, 'logits/rejected': -2.2081103324890137, 'logits/chosen': -2.18608021736145, 'epoch': 0.13}\n{'loss': 0.6899, 'learning_rate': 9.989068136093873e-06, 'rewards/chosen': -0.032386064529418945, 'rewards/rejected': -0.03928790241479874, 'rewards/accuracies': 0.625, 'rewards/margins': 0.006901836022734642, 'logps/rejected': -72.91661071777344, 'logps/chosen': -85.47712707519531, 'logits/rejected': -2.147580862045288, 'logits/chosen': -2.2173922061920166, 'epoch': 0.14}\n{'loss': 0.6851, 'learning_rate': 9.98292246503335e-06, 'rewards/chosen': -0.013852501288056374, 'rewards/rejected': -0.03009057231247425, 'rewards/accuracies': 0.75, 'rewards/margins': 0.016238071024417877, 'logps/rejected': -85.35005187988281, 'logps/chosen': -63.217159271240234, 'logits/rejected': -2.176375150680542, 'logits/chosen': -2.0566258430480957, 'epoch': 0.15}\n{'loss': 0.6894, 'learning_rate': 9.975414512725058e-06, 'rewards/chosen': -0.040177248418331146, 'rewards/rejected': -0.047995712608098984, 'rewards/accuracies': 0.5, 'rewards/margins': 0.007818461395800114, 'logps/rejected': -108.07218933105469, 'logps/chosen': -73.06786346435547, 'logits/rejected': -2.0954341888427734, 'logits/chosen': -1.9722920656204224, 'epoch': 0.16}\n{'loss': 0.6855, 'learning_rate': 9.966546331768192e-06, 'rewards/chosen': -0.028126144781708717, 'rewards/rejected': -0.043817758560180664, 'rewards/accuracies': 0.75, 'rewards/margins': 0.015691613778471947, 'logps/rejected': -81.15846252441406, 'logps/chosen': -74.47703552246094, 'logits/rejected': -2.177471160888672, 'logits/chosen': -2.081462860107422, 'epoch': 0.17}\n{'loss': 0.6856, 'learning_rate': 9.956320346634877e-06, 'rewards/chosen': -0.005670976359397173, 'rewards/rejected': -0.021052932366728783, 'rewards/accuracies': 0.625, 'rewards/margins': 0.015381955541670322, 'logps/rejected': -113.27359008789062, 'logps/chosen': -86.14396667480469, 'logits/rejected': -2.1153457164764404, 'logits/chosen': -2.1016712188720703, 'epoch': 0.18}\n{'loss': 0.6834, 'learning_rate': 9.944739353007344e-06, 'rewards/chosen': 0.020551204681396484, 'rewards/rejected': 0.0005376813933253288, 'rewards/accuracies': 0.625, 'rewards/margins': 0.02001352235674858, 'logps/rejected': -99.52400207519531, 'logps/chosen': -128.03326416015625, 'logits/rejected': -2.0231809616088867, 'logits/chosen': -1.9709193706512451, 'epoch': 0.19}\n{'loss': 0.6967, 'learning_rate': 9.931806517013612e-06, 'rewards/chosen': -0.02008504793047905, 'rewards/rejected': -0.01310033816844225, 'rewards/accuracies': 0.375, 'rewards/margins': -0.00698471162468195, 'logps/rejected': -84.01275634765625, 'logps/chosen': -101.30284118652344, 'logits/rejected': -2.0688798427581787, 'logits/chosen': -2.1177570819854736, 'epoch': 0.2}\n 10%|████                                    | 20/200 [17:26<2:33:15, 51.09s/it]\n  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n 29%|████████████▊                                | 2/7 [00:05<00:13,  2.64s/it]\u001b[A\n 43%|███████████████████▎                         | 3/7 [00:10<00:14,  3.75s/it]\u001b[A\n 57%|█████████████████████████▋                   | 4/7 [00:15<00:12,  4.31s/it]\u001b[A\n 71%|████████████████████████████████▏            | 5/7 [00:21<00:09,  4.93s/it]\u001b[A\n 86%|██████████████████████████████████████▌      | 6/7 [00:27<00:05,  5.00s/it]\u001b[A\n100%|█████████████████████████████████████████████| 7/7 [00:32<00:00,  5.02s/it]\u001b[A\n{'eval_loss': 0.6972202062606812, 'eval_runtime': 38.6909, 'eval_samples_per_second': 0.362, 'eval_steps_per_second': 0.181, 'eval_rewards/chosen': 0.022356877103447914, 'eval_rewards/rejected': 0.030012650415301323, 'eval_rewards/accuracies': 0.5, 'eval_rewards/margins': -0.00765576958656311, 'eval_logps/rejected': -90.47322082519531, 'eval_logps/chosen': -82.7828369140625, 'eval_logits/rejected': -2.102090835571289, 'eval_logits/chosen': -2.1092381477355957, 'epoch': 0.2}\n\n 10%|████                                    | 20/200 [18:05<2:33:15, 51.09s/it]\u001b[A\n{'loss': 0.6958, 'learning_rate': 9.917525374361913e-06, 'rewards/chosen': 0.03478245809674263, 'rewards/rejected': 0.03991213068366051, 'rewards/accuracies': 0.375, 'rewards/margins': -0.005129670258611441, 'logps/rejected': -87.70789337158203, 'logps/chosen': -80.63758850097656, 'logits/rejected': -2.099879264831543, 'logits/chosen': -2.0924832820892334, 'epoch': 0.21}\n{'loss': 0.6897, 'learning_rate': 9.901899829374048e-06, 'rewards/chosen': 0.0017556187231093645, 'rewards/rejected': -0.00518488883972168, 'rewards/accuracies': 0.5, 'rewards/margins': 0.006940507795661688, 'logps/rejected': -67.1878890991211, 'logps/chosen': -69.14759826660156, 'logits/rejected': -2.2199525833129883, 'logits/chosen': -2.2136149406433105, 'epoch': 0.22}\n{'loss': 0.6907, 'learning_rate': 9.884934153917998e-06, 'rewards/chosen': 0.02612132951617241, 'rewards/rejected': 0.021062517538666725, 'rewards/accuracies': 0.625, 'rewards/margins': 0.005058813374489546, 'logps/rejected': -96.22352600097656, 'logps/chosen': -66.54048156738281, 'logits/rejected': -2.0915486812591553, 'logits/chosen': -2.0969674587249756, 'epoch': 0.23}\n{'loss': 0.6952, 'learning_rate': 9.86663298624003e-06, 'rewards/chosen': 0.010288810357451439, 'rewards/rejected': 0.014298009686172009, 'rewards/accuracies': 0.375, 'rewards/margins': -0.00400919932872057, 'logps/rejected': -111.62826538085938, 'logps/chosen': -84.16902160644531, 'logits/rejected': -2.022111415863037, 'logits/chosen': -1.9400218725204468, 'epoch': 0.24}\n{'loss': 0.6984, 'learning_rate': 9.847001329696653e-06, 'rewards/chosen': -0.03458423912525177, 'rewards/rejected': -0.024145029485225677, 'rewards/accuracies': 0.125, 'rewards/margins': -0.010439205914735794, 'logps/rejected': -98.00540924072266, 'logps/chosen': -84.42485046386719, 'logits/rejected': -2.0114552974700928, 'logits/chosen': -1.9999526739120483, 'epoch': 0.25}\n{'loss': 0.6889, 'learning_rate': 9.826044551386743e-06, 'rewards/chosen': -0.010931206867098808, 'rewards/rejected': -0.019741680473089218, 'rewards/accuracies': 0.625, 'rewards/margins': 0.00881047360599041, 'logps/rejected': -111.59215545654297, 'logps/chosen': -80.34507751464844, 'logits/rejected': -2.1417670249938965, 'logits/chosen': -2.076463460922241, 'epoch': 0.26}\n{'loss': 0.6943, 'learning_rate': 9.803768380684242e-06, 'rewards/chosen': 0.032693054527044296, 'rewards/rejected': 0.034469034522771835, 'rewards/accuracies': 0.375, 'rewards/margins': -0.0017759789479896426, 'logps/rejected': -103.70650482177734, 'logps/chosen': -82.36697387695312, 'logits/rejected': -2.0685031414031982, 'logits/chosen': -2.018252372741699, 'epoch': 0.27}\n{'loss': 0.7015, 'learning_rate': 9.780178907671788e-06, 'rewards/chosen': 0.055527690798044205, 'rewards/rejected': 0.07094822078943253, 'rewards/accuracies': 0.625, 'rewards/margins': -0.01542053371667862, 'logps/rejected': -172.30978393554688, 'logps/chosen': -109.10771942138672, 'logits/rejected': -2.095346450805664, 'logits/chosen': -1.9420363903045654, 'epoch': 0.28}\n{'loss': 0.6691, 'learning_rate': 9.755282581475769e-06, 'rewards/chosen': -0.02212810516357422, 'rewards/rejected': -0.07191066443920135, 'rewards/accuracies': 0.625, 'rewards/margins': 0.049782563000917435, 'logps/rejected': -119.37439727783203, 'logps/chosen': -120.9002685546875, 'logits/rejected': -2.090221405029297, 'logits/chosen': -2.0459084510803223, 'epoch': 0.29}\n{'loss': 0.694, 'learning_rate': 9.729086208503174e-06, 'rewards/chosen': -0.005591869354248047, 'rewards/rejected': -0.0040489197708666325, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0015429495833814144, 'logps/rejected': -98.44440460205078, 'logps/chosen': -119.51358032226562, 'logits/rejected': -1.9749618768692017, 'logits/chosen': -1.9262616634368896, 'epoch': 0.3}\n 15%|██████                                  | 30/200 [26:09<2:19:45, 49.33s/it]\n  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n 29%|████████████▊                                | 2/7 [00:05<00:13,  2.64s/it]\u001b[A\n 43%|███████████████████▎                         | 3/7 [00:10<00:14,  3.73s/it]\u001b[A\n 57%|█████████████████████████▋                   | 4/7 [00:15<00:12,  4.29s/it]\u001b[A\n 71%|████████████████████████████████▏            | 5/7 [00:21<00:09,  4.95s/it]\u001b[A\n 86%|██████████████████████████████████████▌      | 6/7 [00:27<00:04,  5.00s/it]\u001b[A\n100%|█████████████████████████████████████████████| 7/7 [00:32<00:00,  5.02s/it]\u001b[A\n{'eval_loss': 0.6910156607627869, 'eval_runtime': 38.6331, 'eval_samples_per_second': 0.362, 'eval_steps_per_second': 0.181, 'eval_rewards/chosen': 0.003034537425264716, 'eval_rewards/rejected': -0.0016362877795472741, 'eval_rewards/accuracies': 0.6428571343421936, 'eval_rewards/margins': 0.004670825321227312, 'eval_logps/rejected': -136.7843475341797, 'eval_logps/chosen': -131.67628479003906, 'eval_logits/rejected': -1.8741570711135864, 'eval_logits/chosen': -1.8838813304901123, 'epoch': 0.3}\n\n 15%|██████                                  | 30/200 [26:48<2:19:45, 49.33s/it]\u001b[A\n{'loss': 0.6889, 'learning_rate': 9.701596950580807e-06, 'rewards/chosen': 0.00794076919555664, 'rewards/rejected': -0.0007234573131427169, 'rewards/accuracies': 0.625, 'rewards/margins': 0.008664226159453392, 'logps/rejected': -135.13815307617188, 'logps/chosen': -148.93832397460938, 'logits/rejected': -1.6652636528015137, 'logits/chosen': -1.6996713876724243, 'epoch': 0.31}\n{'loss': 0.7023, 'learning_rate': 9.672822322997305e-06, 'rewards/chosen': -0.0007928851991891861, 'rewards/rejected': 0.017143823206424713, 'rewards/accuracies': 0.375, 'rewards/margins': -0.01793670654296875, 'logps/rejected': -96.05262756347656, 'logps/chosen': -210.08885192871094, 'logits/rejected': -1.6839663982391357, 'logits/chosen': -1.794538974761963, 'epoch': 0.32}\n{'loss': 0.6967, 'learning_rate': 9.642770192448537e-06, 'rewards/chosen': -0.04886899143457413, 'rewards/rejected': -0.04212961345911026, 'rewards/accuracies': 0.75, 'rewards/margins': -0.006739379372447729, 'logps/rejected': -118.63817596435547, 'logps/chosen': -117.63548278808594, 'logits/rejected': -2.1552083492279053, 'logits/chosen': -2.121363639831543, 'epoch': 0.33}\n{'loss': 0.688, 'learning_rate': 9.611448774886925e-06, 'rewards/chosen': 0.0202057845890522, 'rewards/rejected': 0.009682083502411842, 'rewards/accuracies': 0.625, 'rewards/margins': 0.010523701086640358, 'logps/rejected': -143.71258544921875, 'logps/chosen': -133.97625732421875, 'logits/rejected': -2.025142192840576, 'logits/chosen': -1.9516410827636719, 'epoch': 0.34}\n{'loss': 0.687, 'learning_rate': 9.578866633275289e-06, 'rewards/chosen': 0.0012779233511537313, 'rewards/rejected': -0.01114349439740181, 'rewards/accuracies': 0.75, 'rewards/margins': 0.01242141705006361, 'logps/rejected': -136.38795471191406, 'logps/chosen': -126.40267944335938, 'logits/rejected': -1.9152828454971313, 'logits/chosen': -1.879928708076477, 'epoch': 0.35}\n{'loss': 0.6924, 'learning_rate': 9.545032675245814e-06, 'rewards/chosen': -0.05263948813080788, 'rewards/rejected': -0.05529623106122017, 'rewards/accuracies': 0.25, 'rewards/margins': 0.0026567475870251656, 'logps/rejected': -163.54132080078125, 'logps/chosen': -167.5109100341797, 'logits/rejected': -1.6932213306427002, 'logits/chosen': -1.6938371658325195, 'epoch': 0.36}\n{'loss': 0.6966, 'learning_rate': 9.509956150664796e-06, 'rewards/chosen': 0.011277580633759499, 'rewards/rejected': 0.018051626160740852, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0067740450613200665, 'logps/rejected': -182.50514221191406, 'logps/chosen': -125.69769287109375, 'logits/rejected': -1.7327325344085693, 'logits/chosen': -1.6549313068389893, 'epoch': 0.37}\n{'loss': 0.7291, 'learning_rate': 9.473646649103819e-06, 'rewards/chosen': 0.09698276966810226, 'rewards/rejected': 0.16544094681739807, 'rewards/accuracies': 0.25, 'rewards/margins': -0.06845816969871521, 'logps/rejected': -168.44281005859375, 'logps/chosen': -193.375, 'logits/rejected': -1.6271727085113525, 'logits/chosen': -1.592268705368042, 'epoch': 0.38}\n{'loss': 0.6792, 'learning_rate': 9.43611409721806e-06, 'rewards/chosen': -0.0837303102016449, 'rewards/rejected': -0.11223917454481125, 'rewards/accuracies': 0.75, 'rewards/margins': 0.028508855029940605, 'logps/rejected': -170.4009552001953, 'logps/chosen': -147.60389709472656, 'logits/rejected': -1.3522331714630127, 'logits/chosen': -1.335604190826416, 'epoch': 0.39}\n{'loss': 0.69, 'learning_rate': 9.397368756032445e-06, 'rewards/chosen': -0.015231895260512829, 'rewards/rejected': -0.0218218807131052, 'rewards/accuracies': 0.75, 'rewards/margins': 0.006589984521269798, 'logps/rejected': -185.48875427246094, 'logps/chosen': -198.64556884765625, 'logits/rejected': -1.0094244480133057, 'logits/chosen': -0.9663819074630737, 'epoch': 0.4}\n 20%|████████                                | 40/200 [35:16<2:18:32, 51.95s/it]\n  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n 29%|████████████▊                                | 2/7 [00:05<00:13,  2.64s/it]\u001b[A\n 43%|███████████████████▎                         | 3/7 [00:10<00:14,  3.74s/it]\u001b[A\n 57%|█████████████████████████▋                   | 4/7 [00:15<00:12,  4.31s/it]\u001b[A\n 71%|████████████████████████████████▏            | 5/7 [00:21<00:09,  4.93s/it]\u001b[A\n 86%|██████████████████████████████████████▌      | 6/7 [00:26<00:04,  4.97s/it]\u001b[A\n100%|█████████████████████████████████████████████| 7/7 [00:32<00:00,  5.00s/it]\u001b[A\n{'eval_loss': 0.6912757158279419, 'eval_runtime': 38.5114, 'eval_samples_per_second': 0.364, 'eval_steps_per_second': 0.182, 'eval_rewards/chosen': 0.033161599189043045, 'eval_rewards/rejected': 0.028766198083758354, 'eval_rewards/accuracies': 0.5714285969734192, 'eval_rewards/margins': 0.004395402502268553, 'eval_logps/rejected': -193.63539123535156, 'eval_logps/chosen': -206.1060333251953, 'eval_logits/rejected': -1.3437442779541016, 'eval_logits/chosen': -1.3620473146438599, 'epoch': 0.4}\n\n 20%|████████                                | 40/200 [35:54<2:18:32, 51.95s/it]\u001b[A\n                                                                                \u001b[A/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n{'loss': 0.7022, 'learning_rate': 9.357421218136387e-06, 'rewards/chosen': 0.02958660200238228, 'rewards/rejected': 0.0474979393184185, 'rewards/accuracies': 0.25, 'rewards/margins': -0.017911341041326523, 'logps/rejected': -172.03822326660156, 'logps/chosen': -226.36175537109375, 'logits/rejected': -1.1390875577926636, 'logits/chosen': -1.1711599826812744, 'epoch': 0.41}\n{'loss': 0.6899, 'learning_rate': 9.31628240478787e-06, 'rewards/chosen': -0.010643578134477139, 'rewards/rejected': -0.01744518242776394, 'rewards/accuracies': 0.375, 'rewards/margins': 0.006801605690270662, 'logps/rejected': -236.6951904296875, 'logps/chosen': -184.34478759765625, 'logits/rejected': -1.2106564044952393, 'logits/chosen': -1.1963032484054565, 'epoch': 0.42}\n{'loss': 0.6923, 'learning_rate': 9.273963562927695e-06, 'rewards/chosen': -0.00044803612399846315, 'rewards/rejected': -0.002132225316017866, 'rewards/accuracies': 0.375, 'rewards/margins': 0.0016841889591887593, 'logps/rejected': -252.76304626464844, 'logps/chosen': -178.970947265625, 'logits/rejected': -1.3514915704727173, 'logits/chosen': -1.33737313747406, 'epoch': 0.43}\n{'loss': 0.6934, 'learning_rate': 9.230476262104678e-06, 'rewards/chosen': -0.0005760195199400187, 'rewards/rejected': -0.00015487661585211754, 'rewards/accuracies': 0.375, 'rewards/margins': -0.00042114267125725746, 'logps/rejected': -180.93154907226562, 'logps/chosen': -190.814697265625, 'logits/rejected': -1.4639992713928223, 'logits/chosen': -1.4767324924468994, 'epoch': 0.44}\n{'loss': 0.6926, 'learning_rate': 9.185832391312644e-06, 'rewards/chosen': 0.002365684835240245, 'rewards/rejected': 0.0011959077091887593, 'rewards/accuracies': 0.5, 'rewards/margins': 0.00116977677680552, 'logps/rejected': -171.05967712402344, 'logps/chosen': -177.3852081298828, 'logits/rejected': -1.7476768493652344, 'logits/chosen': -1.7648658752441406, 'epoch': 0.45}\n{'loss': 0.6936, 'learning_rate': 9.140044155740102e-06, 'rewards/chosen': -0.006003761664032936, 'rewards/rejected': -0.005032348912209272, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0009714127518236637, 'logps/rejected': -182.70565795898438, 'logps/chosen': -180.53517150878906, 'logits/rejected': -1.8090283870697021, 'logits/chosen': -1.8192722797393799, 'epoch': 0.46}\n{'loss': 0.6975, 'learning_rate': 9.093124073433464e-06, 'rewards/chosen': -0.008317566476762295, 'rewards/rejected': 0.0002571108052507043, 'rewards/accuracies': 0.25, 'rewards/margins': -0.00857467669993639, 'logps/rejected': -175.71713256835938, 'logps/chosen': -178.1771240234375, 'logits/rejected': -2.041675329208374, 'logits/chosen': -2.034623861312866, 'epoch': 0.47}\n{'loss': 0.6907, 'learning_rate': 9.045084971874738e-06, 'rewards/chosen': -0.0013134003384038806, 'rewards/rejected': -0.006175804417580366, 'rewards/accuracies': 0.5, 'rewards/margins': 0.004862403497099876, 'logps/rejected': -181.41412353515625, 'logps/chosen': -175.53497314453125, 'logits/rejected': -2.284287929534912, 'logits/chosen': -2.3130042552948, 'epoch': 0.48}\n{'loss': 0.6941, 'learning_rate': 8.995939984474624e-06, 'rewards/chosen': 0.0005136490799486637, 'rewards/rejected': 0.0022439956665039062, 'rewards/accuracies': 0.625, 'rewards/margins': -0.0017303461208939552, 'logps/rejected': -171.9976806640625, 'logps/chosen': -180.6932373046875, 'logits/rejected': -2.5802054405212402, 'logits/chosen': -2.5751378536224365, 'epoch': 0.49}\n{'loss': 0.6927, 'learning_rate': 8.94570254698197e-06, 'rewards/chosen': -0.0026673320680856705, 'rewards/rejected': -0.0035123825073242188, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0008450507884845138, 'logps/rejected': -223.89633178710938, 'logps/chosen': -198.4292755126953, 'logits/rejected': -2.6267247200012207, 'logits/chosen': -2.6261816024780273, 'epoch': 0.5}\n 25%|██████████                              | 50/200 [43:51<2:02:05, 48.84s/it]\n  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n 29%|████████████▊                                | 2/7 [00:05<00:13,  2.64s/it]\u001b[A\n 43%|███████████████████▎                         | 3/7 [00:10<00:14,  3.74s/it]\u001b[A\n 57%|█████████████████████████▋                   | 4/7 [00:15<00:12,  4.30s/it]\u001b[A\n 71%|████████████████████████████████▏            | 5/7 [00:21<00:09,  4.93s/it]\u001b[A\n 86%|██████████████████████████████████████▌      | 6/7 [00:27<00:04,  4.98s/it]\u001b[A\n100%|█████████████████████████████████████████████| 7/7 [00:32<00:00,  5.01s/it]\u001b[A\n{'eval_loss': 0.6867632269859314, 'eval_runtime': 38.6282, 'eval_samples_per_second': 0.362, 'eval_steps_per_second': 0.181, 'eval_rewards/chosen': 0.00583114568144083, 'eval_rewards/rejected': -0.0072653633542358875, 'eval_rewards/accuracies': 0.5714285969734192, 'eval_rewards/margins': 0.013096509501338005, 'eval_logps/rejected': -203.99549865722656, 'eval_logps/chosen': -207.91969299316406, 'eval_logits/rejected': -2.4931085109710693, 'eval_logits/chosen': -2.487464427947998, 'epoch': 0.5}\n\n 25%|██████████                              | 50/200 [44:30<2:02:05, 48.84s/it]\u001b[A\n{'loss': 0.6916, 'learning_rate': 8.894386393810563e-06, 'rewards/chosen': 0.0012052538804709911, 'rewards/rejected': -0.00203285226598382, 'rewards/accuracies': 0.375, 'rewards/margins': 0.003238105680793524, 'logps/rejected': -193.97097778320312, 'logps/chosen': -181.3045654296875, 'logits/rejected': -2.488050699234009, 'logits/chosen': -2.4957187175750732, 'epoch': 0.5}\n{'loss': 0.691, 'learning_rate': 8.842005554284296e-06, 'rewards/chosen': 0.007134628482162952, 'rewards/rejected': 0.0026945113204419613, 'rewards/accuracies': 0.625, 'rewards/margins': 0.004440116696059704, 'logps/rejected': -193.7044219970703, 'logps/chosen': -188.328369140625, 'logits/rejected': -2.2116332054138184, 'logits/chosen': -2.173584222793579, 'epoch': 0.51}\n{'loss': 0.6883, 'learning_rate': 8.788574348801676e-06, 'rewards/chosen': -0.0012654303573071957, 'rewards/rejected': -0.011090278625488281, 'rewards/accuracies': 0.625, 'rewards/margins': 0.009824848733842373, 'logps/rejected': -181.1998291015625, 'logps/chosen': -180.8038330078125, 'logits/rejected': -2.3110969066619873, 'logits/chosen': -2.3085098266601562, 'epoch': 0.52}\n{'loss': 0.701, 'learning_rate': 8.734107384920771e-06, 'rewards/chosen': -0.01363067701458931, 'rewards/rejected': 0.0018944739131256938, 'rewards/accuracies': 0.25, 'rewards/margins': -0.01552515011280775, 'logps/rejected': -246.84555053710938, 'logps/chosen': -199.22406005859375, 'logits/rejected': -2.519907236099243, 'logits/chosen': -2.503220558166504, 'epoch': 0.53}\n{'loss': 0.6937, 'learning_rate': 8.67861955336566e-06, 'rewards/chosen': -0.003549384418874979, 'rewards/rejected': -0.0026926042046397924, 'rewards/accuracies': 0.25, 'rewards/margins': -0.0008567809127271175, 'logps/rejected': -234.05825805664062, 'logps/chosen': -293.2597961425781, 'logits/rejected': -2.378307580947876, 'logits/chosen': -2.397536516189575, 'epoch': 0.54}\n{'loss': 0.6874, 'learning_rate': 8.622126023955446e-06, 'rewards/chosen': 0.00589828472584486, 'rewards/rejected': -0.005787086673080921, 'rewards/accuracies': 0.75, 'rewards/margins': 0.011685371398925781, 'logps/rejected': -182.51849365234375, 'logps/chosen': -179.42034912109375, 'logits/rejected': -2.3673489093780518, 'logits/chosen': -2.4083001613616943, 'epoch': 0.55}\n{'loss': 0.6917, 'learning_rate': 8.564642241456986e-06, 'rewards/chosen': -0.0038478849455714226, 'rewards/rejected': -0.007029533851891756, 'rewards/accuracies': 0.5, 'rewards/margins': 0.003181649139150977, 'logps/rejected': -186.91897583007812, 'logps/chosen': -215.21405029296875, 'logits/rejected': -2.5707297325134277, 'logits/chosen': -2.5636062622070312, 'epoch': 0.56}\n{'loss': 0.6939, 'learning_rate': 8.506183921362443e-06, 'rewards/chosen': -0.004363632760941982, 'rewards/rejected': -0.0030584337655454874, 'rewards/accuracies': 0.375, 'rewards/margins': -0.001305198296904564, 'logps/rejected': -221.7945556640625, 'logps/chosen': -238.730712890625, 'logits/rejected': -2.4932174682617188, 'logits/chosen': -2.4902284145355225, 'epoch': 0.57}\n{'loss': 0.6956, 'learning_rate': 8.446767045592829e-06, 'rewards/chosen': -0.003405570052564144, 'rewards/rejected': 0.0013410565443336964, 'rewards/accuracies': 0.375, 'rewards/margins': -0.004746627062559128, 'logps/rejected': -183.77828979492188, 'logps/chosen': -226.88009643554688, 'logits/rejected': -2.5297234058380127, 'logits/chosen': -2.544769525527954, 'epoch': 0.58}\n{'loss': 0.6881, 'learning_rate': 8.386407858128707e-06, 'rewards/chosen': -0.008382225409150124, 'rewards/rejected': -0.01901264116168022, 'rewards/accuracies': 0.75, 'rewards/margins': 0.010630417615175247, 'logps/rejected': -259.966796875, 'logps/chosen': -233.27613830566406, 'logits/rejected': -2.646251916885376, 'logits/chosen': -2.6317551136016846, 'epoch': 0.59}\n 30%|████████████                            | 60/200 [52:49<1:58:12, 50.66s/it]\n  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n 29%|████████████▊                                | 2/7 [00:05<00:13,  2.64s/it]\u001b[A\n 43%|███████████████████▎                         | 3/7 [00:10<00:14,  3.74s/it]\u001b[A\n 57%|█████████████████████████▋                   | 4/7 [00:15<00:12,  4.30s/it]\u001b[A\n 71%|████████████████████████████████▏            | 5/7 [00:21<00:09,  4.94s/it]\u001b[A\n 86%|██████████████████████████████████████▌      | 6/7 [00:27<00:04,  4.99s/it]\u001b[A\n100%|█████████████████████████████████████████████| 7/7 [00:32<00:00,  5.01s/it]\u001b[A\n{'eval_loss': 0.6900214552879333, 'eval_runtime': 38.6137, 'eval_samples_per_second': 0.363, 'eval_steps_per_second': 0.181, 'eval_rewards/chosen': -0.0007391795516014099, 'eval_rewards/rejected': -0.0072502135299146175, 'eval_rewards/accuracies': 0.7142857313156128, 'eval_rewards/margins': 0.00651103351265192, 'eval_logps/rejected': -206.0416259765625, 'eval_logps/chosen': -208.25540161132812, 'eval_logits/rejected': -2.5421624183654785, 'eval_logits/chosen': -2.5308663845062256, 'epoch': 0.59}\n\n 30%|████████████                            | 60/200 [53:27<1:58:12, 50.66s/it]\u001b[A\n{'loss': 0.6936, 'learning_rate': 8.325122860569241e-06, 'rewards/chosen': 0.0015542989131063223, 'rewards/rejected': 0.0024995803833007812, 'rewards/accuracies': 0.625, 'rewards/margins': -0.0009452820522710681, 'logps/rejected': -193.80157470703125, 'logps/chosen': -223.0435028076172, 'logits/rejected': -2.5400757789611816, 'logits/chosen': -2.5679895877838135, 'epoch': 0.6}\n{'loss': 0.6891, 'learning_rate': 8.262928807620843e-06, 'rewards/chosen': -0.002995109651237726, 'rewards/rejected': -0.011352349072694778, 'rewards/accuracies': 0.75, 'rewards/margins': 0.008357238955795765, 'logps/rejected': -211.1697235107422, 'logps/chosen': -202.69967651367188, 'logits/rejected': -2.69256329536438, 'logits/chosen': -2.6848244667053223, 'epoch': 0.61}\n{'loss': 0.6997, 'learning_rate': 8.199842702516584e-06, 'rewards/chosen': -0.003963661380112171, 'rewards/rejected': 0.009016990661621094, 'rewards/accuracies': 0.25, 'rewards/margins': -0.01298065111041069, 'logps/rejected': -191.56475830078125, 'logps/chosen': -211.61703491210938, 'logits/rejected': -2.595398187637329, 'logits/chosen': -2.545665979385376, 'epoch': 0.62}\n{'loss': 0.7026, 'learning_rate': 8.135881792367686e-06, 'rewards/chosen': -0.011565590277314186, 'rewards/rejected': 0.007037163246423006, 'rewards/accuracies': 0.375, 'rewards/margins': -0.018602753058075905, 'logps/rejected': -182.97979736328125, 'logps/chosen': -198.2144317626953, 'logits/rejected': -2.5876264572143555, 'logits/chosen': -2.572299003601074, 'epoch': 0.63}\n{'loss': 0.6875, 'learning_rate': 8.071063563448341e-06, 'rewards/chosen': 0.006850051693618298, 'rewards/rejected': -0.0044914246536791325, 'rewards/accuracies': 0.75, 'rewards/margins': 0.011341476812958717, 'logps/rejected': -204.23350524902344, 'logps/chosen': -187.1356964111328, 'logits/rejected': -2.511204719543457, 'logits/chosen': -2.5023021697998047, 'epoch': 0.64}\n{'loss': 0.6982, 'learning_rate': 8.005405736415127e-06, 'rewards/chosen': -0.00795974675565958, 'rewards/rejected': 0.0017539975233376026, 'rewards/accuracies': 0.5, 'rewards/margins': -0.00971374474465847, 'logps/rejected': -190.49826049804688, 'logps/chosen': -203.09017944335938, 'logits/rejected': -2.527249336242676, 'logits/chosen': -2.5376358032226562, 'epoch': 0.65}\n{'loss': 0.6898, 'learning_rate': 7.938926261462366e-06, 'rewards/chosen': 0.005270767025649548, 'rewards/rejected': -0.0014297483721747994, 'rewards/accuracies': 0.75, 'rewards/margins': 0.0067005157470703125, 'logps/rejected': -220.2925262451172, 'logps/chosen': -172.85360717773438, 'logits/rejected': -2.639913558959961, 'logits/chosen': -2.6326870918273926, 'epoch': 0.66}\n{'loss': 0.6963, 'learning_rate': 7.871643313414718e-06, 'rewards/chosen': -0.0028132435400038958, 'rewards/rejected': 0.0029506683349609375, 'rewards/accuracies': 0.625, 'rewards/margins': -0.005763912573456764, 'logps/rejected': -230.34922790527344, 'logps/chosen': -216.7104034423828, 'logits/rejected': -2.5828490257263184, 'logits/chosen': -2.6214380264282227, 'epoch': 0.67}\n{'loss': 0.696, 'learning_rate': 7.803575286758365e-06, 'rewards/chosen': -0.009046554565429688, 'rewards/rejected': -0.003478622529655695, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0055679334327578545, 'logps/rejected': -217.99154663085938, 'logps/chosen': -218.716796875, 'logits/rejected': -2.5563998222351074, 'logits/chosen': -2.521083354949951, 'epoch': 0.68}\n{'loss': 0.6914, 'learning_rate': 7.734740790612137e-06, 'rewards/chosen': 0.008377266116440296, 'rewards/rejected': 0.0047002797946333885, 'rewards/accuracies': 0.625, 'rewards/margins': 0.0036769870202988386, 'logps/rejected': -245.39279174804688, 'logps/chosen': -211.2460479736328, 'logits/rejected': -2.4650936126708984, 'logits/chosen': -2.4081780910491943, 'epoch': 0.69}\n 35%|█████████████▎                        | 70/200 [1:01:48<1:51:47, 51.60s/it]\n  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n 29%|████████████▊                                | 2/7 [00:05<00:13,  2.65s/it]\u001b[A\n 43%|███████████████████▎                         | 3/7 [00:10<00:15,  3.76s/it]\u001b[A\n 57%|█████████████████████████▋                   | 4/7 [00:15<00:12,  4.32s/it]\u001b[A\n 71%|████████████████████████████████▏            | 5/7 [00:21<00:09,  4.94s/it]\u001b[A\n 86%|██████████████████████████████████████▌      | 6/7 [00:27<00:04,  5.00s/it]\u001b[A\n100%|█████████████████████████████████████████████| 7/7 [00:32<00:00,  5.03s/it]\u001b[A\n{'eval_loss': 0.6924780607223511, 'eval_runtime': 38.7503, 'eval_samples_per_second': 0.361, 'eval_steps_per_second': 0.181, 'eval_rewards/chosen': -0.0026741032488644123, 'eval_rewards/rejected': -0.00418134406208992, 'eval_rewards/accuracies': 0.5714285969734192, 'eval_rewards/margins': 0.001507240696810186, 'eval_logps/rejected': -206.15272521972656, 'eval_logps/chosen': -207.92953491210938, 'eval_logits/rejected': -2.3609588146209717, 'eval_logits/chosen': -2.352201461791992, 'epoch': 0.69}\n\n 35%|█████████████▎                        | 70/200 [1:02:27<1:51:47, 51.60s/it]\u001b[A\n{'loss': 0.691, 'learning_rate': 7.66515864363997e-06, 'rewards/chosen': 0.01690998114645481, 'rewards/rejected': 0.01225509587675333, 'rewards/accuracies': 0.625, 'rewards/margins': 0.004654883872717619, 'logps/rejected': -194.06443786621094, 'logps/chosen': -222.22056579589844, 'logits/rejected': -2.4956369400024414, 'logits/chosen': -2.5217182636260986, 'epoch': 0.7}\n{'loss': 0.6936, 'learning_rate': 7.594847868906076e-06, 'rewards/chosen': -0.010136794298887253, 'rewards/rejected': -0.00947723351418972, 'rewards/accuracies': 0.625, 'rewards/margins': -0.0006595607846975327, 'logps/rejected': -235.40475463867188, 'logps/chosen': -247.11793518066406, 'logits/rejected': -2.367526054382324, 'logits/chosen': -2.4215662479400635, 'epoch': 0.71}\n{'loss': 0.6926, 'learning_rate': 7.52382768867422e-06, 'rewards/chosen': 0.006032944656908512, 'rewards/rejected': 0.004973411560058594, 'rewards/accuracies': 0.625, 'rewards/margins': 0.001059532631188631, 'logps/rejected': -231.0869598388672, 'logps/chosen': -262.3051452636719, 'logits/rejected': -2.5291428565979004, 'logits/chosen': -2.554509401321411, 'epoch': 0.72}\n{'loss': 0.6922, 'learning_rate': 7.452117519152542e-06, 'rewards/chosen': 0.011623954400420189, 'rewards/rejected': 0.009672354906797409, 'rewards/accuracies': 0.75, 'rewards/margins': 0.001951599377207458, 'logps/rejected': -180.38319396972656, 'logps/chosen': -227.61978149414062, 'logits/rejected': -2.5391194820404053, 'logits/chosen': -2.597141742706299, 'epoch': 0.73}\n{'loss': 0.6955, 'learning_rate': 7.379736965185369e-06, 'rewards/chosen': -0.00035858154296875, 'rewards/rejected': 0.00410308875143528, 'rewards/accuracies': 0.375, 'rewards/margins': -0.00446167029440403, 'logps/rejected': -269.2716369628906, 'logps/chosen': -214.09201049804688, 'logits/rejected': -2.7522621154785156, 'logits/chosen': -2.7494213581085205, 'epoch': 0.74}\n{'loss': 0.7005, 'learning_rate': 7.30670581489344e-06, 'rewards/chosen': -0.010304450988769531, 'rewards/rejected': 0.004294204525649548, 'rewards/accuracies': 0.375, 'rewards/margins': -0.014598655514419079, 'logps/rejected': -170.39523315429688, 'logps/chosen': -234.4501953125, 'logits/rejected': -2.3491811752319336, 'logits/chosen': -2.385073661804199, 'epoch': 0.75}\n{'loss': 0.6986, 'learning_rate': 7.233044034264034e-06, 'rewards/chosen': -0.0038331986870616674, 'rewards/rejected': 0.007037544623017311, 'rewards/accuracies': 0.25, 'rewards/margins': -0.010870743542909622, 'logps/rejected': -177.84027099609375, 'logps/chosen': -157.82235717773438, 'logits/rejected': -2.4707000255584717, 'logits/chosen': -2.491549491882324, 'epoch': 0.76}\n{'loss': 0.684, 'learning_rate': 7.158771761692464e-06, 'rewards/chosen': 0.016954611986875534, 'rewards/rejected': -0.001519441488198936, 'rewards/accuracies': 0.75, 'rewards/margins': 0.018474053591489792, 'logps/rejected': -160.42767333984375, 'logps/chosen': -181.26040649414062, 'logits/rejected': -2.4630584716796875, 'logits/chosen': -2.4566760063171387, 'epoch': 0.77}\n{'loss': 0.6937, 'learning_rate': 7.083909302476453e-06, 'rewards/chosen': -0.0029361725319176912, 'rewards/rejected': -0.0020980837289243937, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0008380888029932976, 'logps/rejected': -180.7012939453125, 'logps/chosen': -231.5946044921875, 'logits/rejected': -2.589430332183838, 'logits/chosen': -2.6788666248321533, 'epoch': 0.78}\n{'loss': 0.6991, 'learning_rate': 7.008477123264849e-06, 'rewards/chosen': -0.0072311400435864925, 'rewards/rejected': 0.0042552947998046875, 'rewards/accuracies': 0.25, 'rewards/margins': -0.011486436240375042, 'logps/rejected': -277.02227783203125, 'logps/chosen': -185.03761291503906, 'logits/rejected': -2.656038999557495, 'logits/chosen': -2.5852301120758057, 'epoch': 0.79}\n 40%|███████████████▏                      | 80/200 [1:11:03<1:44:06, 52.06s/it]\n  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n 29%|████████████▊                                | 2/7 [00:05<00:13,  2.64s/it]\u001b[A\n 43%|███████████████████▎                         | 3/7 [00:10<00:14,  3.75s/it]\u001b[A\n 57%|█████████████████████████▋                   | 4/7 [00:15<00:12,  4.32s/it]\u001b[A\n 71%|████████████████████████████████▏            | 5/7 [00:22<00:09,  4.97s/it]\u001b[A\n 86%|██████████████████████████████████████▌      | 6/7 [00:27<00:05,  5.01s/it]\u001b[A\n100%|█████████████████████████████████████████████| 7/7 [00:32<00:00,  5.03s/it]\u001b[A\n{'eval_loss': 0.6941550970077515, 'eval_runtime': 38.7803, 'eval_samples_per_second': 0.361, 'eval_steps_per_second': 0.181, 'eval_rewards/chosen': -0.002689361572265625, 'eval_rewards/rejected': -0.000751985760871321, 'eval_rewards/accuracies': 0.4285714328289032, 'eval_rewards/margins': -0.0019373759860172868, 'eval_logps/rejected': -198.5491180419922, 'eval_logps/chosen': -205.1304168701172, 'eval_logits/rejected': -2.614600896835327, 'eval_logits/chosen': -2.5948097705841064, 'epoch': 0.79}\n\n 40%|███████████████▏                      | 80/200 [1:11:42<1:44:06, 52.06s/it]\u001b[A\n                                                                                \u001b[A/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n{'loss': 0.6936, 'learning_rate': 6.932495846462262e-06, 'rewards/chosen': 0.00135717389639467, 'rewards/rejected': 0.002181815914809704, 'rewards/accuracies': 0.5, 'rewards/margins': -0.000824641901999712, 'logps/rejected': -169.60447692871094, 'logps/chosen': -176.9607696533203, 'logits/rejected': -2.559507131576538, 'logits/chosen': -2.5303330421447754, 'epoch': 0.8}\n{'loss': 0.6951, 'learning_rate': 6.855986244591104e-06, 'rewards/chosen': -0.006751060485839844, 'rewards/rejected': -0.003074264619499445, 'rewards/accuracies': 0.375, 'rewards/margins': -0.003676796332001686, 'logps/rejected': -190.76893615722656, 'logps/chosen': -211.7750701904297, 'logits/rejected': -2.4681241512298584, 'logits/chosen': -2.479809284210205, 'epoch': 0.81}\n{'loss': 0.6859, 'learning_rate': 6.778969234612583e-06, 'rewards/chosen': 0.009009170345962048, 'rewards/rejected': -0.005566215142607689, 'rewards/accuracies': 0.75, 'rewards/margins': 0.014575385488569736, 'logps/rejected': -199.32034301757812, 'logps/chosen': -246.799072265625, 'logits/rejected': -2.6755149364471436, 'logits/chosen': -2.7007458209991455, 'epoch': 0.82}\n{'loss': 0.6964, 'learning_rate': 6.701465872208216e-06, 'rewards/chosen': -0.003352547064423561, 'rewards/rejected': 0.00293560023419559, 'rewards/accuracies': 0.5, 'rewards/margins': -0.006288147531449795, 'logps/rejected': -189.9105224609375, 'logps/chosen': -196.67385864257812, 'logits/rejected': -2.524860143661499, 'logits/chosen': -2.504823923110962, 'epoch': 0.83}\n{'loss': 0.6842, 'learning_rate': 6.6234973460234184e-06, 'rewards/chosen': 0.010011482983827591, 'rewards/rejected': -0.008223151788115501, 'rewards/accuracies': 0.75, 'rewards/margins': 0.018234634771943092, 'logps/rejected': -184.7745361328125, 'logps/chosen': -184.02426147460938, 'logits/rejected': -2.564768075942993, 'logits/chosen': -2.6009480953216553, 'epoch': 0.84}\n{'loss': 0.6964, 'learning_rate': 6.545084971874738e-06, 'rewards/chosen': -0.0024990085512399673, 'rewards/rejected': 0.0039020536933094263, 'rewards/accuracies': 0.5, 'rewards/margins': -0.006401062943041325, 'logps/rejected': -199.39369201660156, 'logps/chosen': -189.6354217529297, 'logits/rejected': -2.476546287536621, 'logits/chosen': -2.4525139331817627, 'epoch': 0.85}\n{'loss': 0.692, 'learning_rate': 6.466250186922325e-06, 'rewards/chosen': -0.0014842988457530737, 'rewards/rejected': -0.0037811279762536287, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0022968288976699114, 'logps/rejected': -202.97572326660156, 'logps/chosen': -173.67086791992188, 'logits/rejected': -2.531294822692871, 'logits/chosen': -2.513928174972534, 'epoch': 0.86}\n{'loss': 0.6947, 'learning_rate': 6.387014543809224e-06, 'rewards/chosen': -0.00010833702981472015, 'rewards/rejected': 0.0028339389245957136, 'rewards/accuracies': 0.375, 'rewards/margins': -0.002942276420071721, 'logps/rejected': -187.794677734375, 'logps/chosen': -209.29701232910156, 'logits/rejected': -2.6520750522613525, 'logits/chosen': -2.678149700164795, 'epoch': 0.87}\n{'loss': 0.6934, 'learning_rate': 6.3073997047691e-06, 'rewards/chosen': 0.008947372436523438, 'rewards/rejected': 0.009423637762665749, 'rewards/accuracies': 0.625, 'rewards/margins': -0.0004762652679346502, 'logps/rejected': -197.79261779785156, 'logps/chosen': -185.428466796875, 'logits/rejected': -2.356327772140503, 'logits/chosen': -2.3973565101623535, 'epoch': 0.88}\n{'loss': 0.6948, 'learning_rate': 6.227427435703997e-06, 'rewards/chosen': -0.0052818297408521175, 'rewards/rejected': -0.00204143556766212, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0032403944060206413, 'logps/rejected': -183.7181854248047, 'logps/chosen': -213.98373413085938, 'logits/rejected': -2.429995536804199, 'logits/chosen': -2.4714529514312744, 'epoch': 0.89}\n 45%|█████████████████                     | 90/200 [1:20:00<1:33:36, 51.06s/it]\n  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n 29%|████████████▊                                | 2/7 [00:05<00:13,  2.65s/it]\u001b[A\n 43%|███████████████████▎                         | 3/7 [00:10<00:14,  3.75s/it]\u001b[A\n 57%|█████████████████████████▋                   | 4/7 [00:15<00:12,  4.32s/it]\u001b[A\n 71%|████████████████████████████████▏            | 5/7 [00:21<00:09,  4.94s/it]\u001b[A\n 86%|██████████████████████████████████████▌      | 6/7 [00:27<00:04,  4.98s/it]\u001b[A\n100%|█████████████████████████████████████████████| 7/7 [00:32<00:00,  5.01s/it]\u001b[A\n{'eval_loss': 0.6882578134536743, 'eval_runtime': 38.7077, 'eval_samples_per_second': 0.362, 'eval_steps_per_second': 0.181, 'eval_rewards/chosen': 0.0036416188813745975, 'eval_rewards/rejected': -0.006278937216848135, 'eval_rewards/accuracies': 0.7857142686843872, 'eval_rewards/margins': 0.009920557029545307, 'eval_logps/rejected': -198.31968688964844, 'eval_logps/chosen': -201.84837341308594, 'eval_logits/rejected': -2.5567290782928467, 'eval_logits/chosen': -2.537604808807373, 'epoch': 0.89}\n\n 45%|█████████████████                     | 90/200 [1:20:39<1:33:36, 51.06s/it]\u001b[A\n{'loss': 0.6956, 'learning_rate': 6.147119600233758e-06, 'rewards/chosen': -0.005406570620834827, 'rewards/rejected': -0.0006465911865234375, 'rewards/accuracies': 0.5, 'rewards/margins': -0.00475997943431139, 'logps/rejected': -174.61233520507812, 'logps/chosen': -161.99087524414062, 'logits/rejected': -2.457535982131958, 'logits/chosen': -2.489872694015503, 'epoch': 0.9}\n{'loss': 0.6929, 'learning_rate': 6.066498153718735e-06, 'rewards/chosen': 0.005157661624252796, 'rewards/rejected': 0.0046358113177120686, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0005218508886173368, 'logps/rejected': -178.43731689453125, 'logps/chosen': -191.06675720214844, 'logits/rejected': -2.4369823932647705, 'logits/chosen': -2.4498093128204346, 'epoch': 0.91}\n{'loss': 0.6948, 'learning_rate': 5.985585137257401e-06, 'rewards/chosen': 0.0009928704239428043, 'rewards/rejected': 0.0041217803955078125, 'rewards/accuracies': 0.375, 'rewards/margins': -0.003128910204395652, 'logps/rejected': -179.0384063720703, 'logps/chosen': -195.250732421875, 'logits/rejected': -2.57360577583313, 'logits/chosen': -2.5823330879211426, 'epoch': 0.92}\n{'loss': 0.6943, 'learning_rate': 5.904402671660551e-06, 'rewards/chosen': -0.00018768315203487873, 'rewards/rejected': 0.0019836428109556437, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0021713264286518097, 'logps/rejected': -176.81312561035156, 'logps/chosen': -176.01141357421875, 'logits/rejected': -2.4303622245788574, 'logits/chosen': -2.428420066833496, 'epoch': 0.93}\n{'loss': 0.6963, 'learning_rate': 5.82297295140367e-06, 'rewards/chosen': 0.011829186230897903, 'rewards/rejected': 0.018009375780820847, 'rewards/accuracies': 0.25, 'rewards/margins': -0.006180190481245518, 'logps/rejected': -170.6905975341797, 'logps/chosen': -218.08111572265625, 'logits/rejected': -2.776576519012451, 'logits/chosen': -2.765336751937866, 'epoch': 0.94}\n{'loss': 0.6951, 'learning_rate': 5.74131823855921e-06, 'rewards/chosen': 0.004097938537597656, 'rewards/rejected': 0.00789642333984375, 'rewards/accuracies': 0.25, 'rewards/margins': -0.0037984843365848064, 'logps/rejected': -209.63836669921875, 'logps/chosen': -198.91107177734375, 'logits/rejected': -2.5729339122772217, 'logits/chosen': -2.573798656463623, 'epoch': 0.95}\n{'loss': 0.6906, 'learning_rate': 5.659460856710346e-06, 'rewards/chosen': 0.004267883487045765, 'rewards/rejected': -0.0009544373606331646, 'rewards/accuracies': 0.5, 'rewards/margins': 0.005222320556640625, 'logps/rejected': -203.04635620117188, 'logps/chosen': -271.25006103515625, 'logits/rejected': -2.638256311416626, 'logits/chosen': -2.640829563140869, 'epoch': 0.96}\n{'loss': 0.6984, 'learning_rate': 5.577423184847932e-06, 'rewards/chosen': -0.0032301906030625105, 'rewards/rejected': 0.007190036587417126, 'rewards/accuracies': 0.25, 'rewards/margins': -0.01042022742331028, 'logps/rejected': -164.9942626953125, 'logps/chosen': -158.68719482421875, 'logits/rejected': -2.64393949508667, 'logits/chosen': -2.6505885124206543, 'epoch': 0.97}\n{'loss': 0.6966, 'learning_rate': 5.495227651252315e-06, 'rewards/chosen': 0.010407066904008389, 'rewards/rejected': 0.01694669760763645, 'rewards/accuracies': 0.625, 'rewards/margins': -0.006539630703628063, 'logps/rejected': -198.42822265625, 'logps/chosen': -186.78758239746094, 'logits/rejected': -2.5674569606781006, 'logits/chosen': -2.5536928176879883, 'epoch': 0.98}\n{'loss': 0.6921, 'learning_rate': 5.412896727361663e-06, 'rewards/chosen': 0.00532798795029521, 'rewards/rejected': 0.0031705854926258326, 'rewards/accuracies': 0.375, 'rewards/margins': 0.0021574024576693773, 'logps/rejected': -233.23873901367188, 'logps/chosen': -193.629150390625, 'logits/rejected': -2.749372959136963, 'logits/chosen': -2.7123608589172363, 'epoch': 0.99}\n 50%|██████████████████▌                  | 100/200 [1:28:58<1:26:11, 51.71s/it]\n  0%|                                                     | 0/7 [00:00<?, ?it/s]\u001b[A\n 29%|████████████▊                                | 2/7 [00:05<00:13,  2.65s/it]\u001b[A\n 43%|███████████████████▎                         | 3/7 [00:10<00:14,  3.74s/it]\u001b[A\n 57%|█████████████████████████▋                   | 4/7 [00:15<00:12,  4.31s/it]\u001b[A\n 71%|████████████████████████████████▏            | 5/7 [00:21<00:09,  4.94s/it]\u001b[A\n 86%|██████████████████████████████████████▌      | 6/7 [00:27<00:04,  4.99s/it]\u001b[A\n100%|█████████████████████████████████████████████| 7/7 [00:32<00:00,  5.01s/it]\u001b[A\n{'eval_loss': 0.693472683429718, 'eval_runtime': 38.6512, 'eval_samples_per_second': 0.362, 'eval_steps_per_second': 0.181, 'eval_rewards/chosen': -0.0009064812329597771, 'eval_rewards/rejected': -0.0003723690169863403, 'eval_rewards/accuracies': 0.5714285969734192, 'eval_rewards/margins': -0.0005341120413504541, 'eval_logps/rejected': -197.13717651367188, 'eval_logps/chosen': -202.50572204589844, 'eval_logits/rejected': -2.684769868850708, 'eval_logits/chosen': -2.6633670330047607, 'epoch': 0.99}\n\n 50%|██████████████████▌                  | 100/200 [1:29:37<1:26:11, 51.71s/it]\u001b[A\n{'loss': 0.6927, 'learning_rate': 5.3304529216284974e-06, 'rewards/chosen': 0.0009246827103197575, 'rewards/rejected': 9.155366569757462e-06, 'rewards/accuracies': 0.25, 'rewards/margins': 0.0009155275765806437, 'logps/rejected': -195.94688415527344, 'logps/chosen': -214.19557189941406, 'logits/rejected': -2.645090341567993, 'logits/chosen': -2.657865524291992, 'epoch': 1.0}\n{'loss': 0.6977, 'learning_rate': 5.247918773366112e-06, 'rewards/chosen': 0.006843662355095148, 'rewards/rejected': 0.015888595953583717, 'rewards/accuracies': 0.375, 'rewards/margins': -0.009044934064149857, 'logps/rejected': -178.04241943359375, 'logps/chosen': -162.72964477539062, 'logits/rejected': -2.6522138118743896, 'logits/chosen': -2.647115707397461, 'epoch': 1.01}\n{'loss': 0.6959, 'learning_rate': 5.165316846586541e-06, 'rewards/chosen': 0.0022237778175622225, 'rewards/rejected': 0.00762100238353014, 'rewards/accuracies': 0.25, 'rewards/margins': -0.005397224798798561, 'logps/rejected': -201.3035888671875, 'logps/chosen': -187.9761962890625, 'logits/rejected': -2.526057481765747, 'logits/chosen': -2.503754138946533, 'epoch': 1.02}\n{'loss': 0.6938, 'learning_rate': 5.082669723831793e-06, 'rewards/chosen': -0.006748199462890625, 'rewards/rejected': -0.00580673199146986, 'rewards/accuracies': 0.25, 'rewards/margins': -0.0009414665400981903, 'logps/rejected': -190.09268188476562, 'logps/chosen': -174.5601043701172, 'logits/rejected': -2.489698886871338, 'logits/chosen': -2.451099395751953, 'epoch': 1.03}\n{'loss': 0.697, 'learning_rate': 5e-06, 'rewards/chosen': 0.0008666994981467724, 'rewards/rejected': 0.008311653509736061, 'rewards/accuracies': 0.5, 'rewards/margins': -0.007444954477250576, 'logps/rejected': -191.52999877929688, 'logps/chosen': -339.40460205078125, 'logits/rejected': -2.4674363136291504, 'logits/chosen': -2.528815269470215, 'epoch': 1.04}\n{'loss': 0.6896, 'learning_rate': 4.917330276168208e-06, 'rewards/chosen': 0.006729126907885075, 'rewards/rejected': -0.0005680079339072108, 'rewards/accuracies': 0.875, 'rewards/margins': 0.007297134958207607, 'logps/rejected': -191.36514282226562, 'logps/chosen': -205.29115295410156, 'logits/rejected': -2.4763941764831543, 'logits/chosen': -2.464643716812134, 'epoch': 1.05}\n{'loss': 0.6899, 'learning_rate': 4.8346831534134595e-06, 'rewards/chosen': 0.012161636725068092, 'rewards/rejected': 0.005553435999900103, 'rewards/accuracies': 0.625, 'rewards/margins': 0.006608201190829277, 'logps/rejected': -187.87673950195312, 'logps/chosen': -175.91629028320312, 'logits/rejected': -2.5531647205352783, 'logits/chosen': -2.5598437786102295, 'epoch': 1.06}\n 54%|███████████████████▊                 | 107/200 [1:35:14<1:17:43, 50.15s/it]^C\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 试试普通lora 设置--use_ref_model False 让程序自己去create ref model  避免直接copy model构造ref model导致的oom\n# 普通lora 当ref_model=None时 lora target modules=[default。。。]居然会混入奇怪的default module 我只好在函数中强制删除\n# 普通lora 仍然oom  不能传 load_in_4bit 会导致参与训练的modules为[]\nTarget modules [] not found in the base model. Please check the target modules and try again.","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_for_peftmodel.py \\\n    --model_type chatglm \\\n    --model_name_or_path /kaggle/working/chatGLM-6B-QLoRA/output_yungaun_0827_v1/checkpoint-400 \\\n    --tokenizer_name_or_path THUDM/chatglm2-6b \\\n    --use_ref_model False \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --load_in_4bit True \\\n    --qlora False \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-yunguan-lora-v1 \\\n    --target_modules all \\\n    --lora_rank 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"execution":{"iopub.status.busy":"2023-08-29T15:56:26.862063Z","iopub.execute_input":"2023-08-29T15:56:26.862610Z","iopub.status.idle":"2023-08-29T15:59:57.405020Z","shell.execute_reply.started":"2023-08-29T15:56:26.862565Z","shell.execute_reply":"2023-08-29T15:59:57.403508Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/kaggle/working/MedicalGPT\nFetching origin\nAlready up to date.\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2023-08-29 15:56:37,452] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n\u001b[32m2023-08-29 15:56:44.478\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m242\u001b[0m - \u001b[1mParse args: ScriptArguments(model_type='chatglm', model_name_or_path='/kaggle/working/chatGLM-6B-QLoRA/output_yungaun_0827_v1/checkpoint-400', tokenizer_name_or_path='THUDM/chatglm2-6b', load_in_8bit=False, load_in_4bit=True, cache_dir='./cache', use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True, dataset_name=None, dataset_config_name=None, train_file_dir='./data/reward_yunguan', validation_file_dir='./data/reward_yunguan', template_name='vicuna', per_device_train_batch_size=1, per_device_eval_batch_size=1, max_source_length=256, max_target_length=128, min_target_length=4, max_train_samples=1000, max_eval_samples=20, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=4, use_ref_model=False, use_peft=True, qlora=False, target_modules='all', lora_rank=64, lora_dropout=0.05, lora_alpha=32.0, peft_path=None, do_train=True, do_eval=True, beta=0.1, learning_rate=1e-05, lr_scheduler_type='cosine', warmup_steps=10, weight_decay=0.05, optim='paged_lion_32bit', fp16=True, bf16=False, gradient_checkpointing=True, gradient_accumulation_steps=4, save_total_limit=2, load_best_model_at_end=True, save_steps=40, eval_steps=10, logging_steps=1, output_dir='outputs-dpo-yunguan-lora-v1', max_steps=200, eval_strategy='steps', remove_unused_columns=False, report_to='tensorboard')\u001b[0m\n\u001b[32m2023-08-29 15:56:44.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m286\u001b[0m - \u001b[1mtrain files: ./data/reward_yunguan/paired_yunguan.json\u001b[0m\n\u001b[32m2023-08-29 15:56:44.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m291\u001b[0m - \u001b[1meval files: ./data/reward_yunguan/paired_yunguan.json\u001b[0m\n100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 87.85it/s]\n\u001b[32m2023-08-29 15:56:44.858\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m312\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n    train: Dataset({\n        features: ['question', 'response_chosen', 'response_rejected'],\n        num_rows: 2855\n    })\n    validation: Dataset({\n        features: ['question', 'response_chosen', 'response_rejected'],\n        num_rows: 2855\n    })\n})\u001b[0m\n\u001b[32m2023-08-29 15:56:44.861\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m330\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'question': '现在提供如下信息：\\n1. 审批出差申请的流程中，如何查询审批人的状态？ \\n\\n答：此问题是平台流程服务异常，IT会及时处理，您可以稍后再试，或上IT吧发帖反馈此问题\\n3. 负责 FMR 系统的是谁？ 请联系刘玉莎 (liuyusha@fiberhome.com)\\n4. 单位收款信息在哪里可以找到？ \\n\\n答：收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。\\n请根据提供的信息回答问题。问：4. 单位收款信息在哪里可以找到？ \\n\\n答：', 'response_chosen': '收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。', 'response_rejected': '请联系刘玉莎 (liuyusha@fiberhome.com)'}\u001b[0m\n\u001b[32m2023-08-29 15:56:45.320\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m343\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 807\u001b[0m\n\u001b[32m2023-08-29 15:56:45.320\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m344\u001b[0m - \u001b[34m\u001b[1mFirst train example:\u001b[0m\n\u001b[32m2023-08-29 15:56:45.321\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m345\u001b[0m - \u001b[34m\u001b[1mQuestion: 现在提供如下信息：\n2. 谁负责国际渠道管理系统？ 请联系陈良锋 (chenlf@fiberhome.com)\n5. 档案管理系统的所有者是谁？ 请联系张宏燕 (zhanghongyan@fiberhome.com)\n5. SSO 的主宰者是谁？ 请联系钟政昊 (zhzhong@fiberhome.com)\n请根据提供的信息回答问题。问：2. 谁负责国际渠道管理系统？ \n\nAnswer: 请联系陈良锋 (chenlf@fiberhome.com)\u001b[0m\n\u001b[32m2023-08-29 15:56:45.323\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m357\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'question': '现在提供如下信息：\\n1. 审批出差申请的流程中，如何查询审批人的状态？ \\n\\n答：此问题是平台流程服务异常，IT会及时处理，您可以稍后再试，或上IT吧发帖反馈此问题\\n3. 负责 FMR 系统的是谁？ 请联系刘玉莎 (liuyusha@fiberhome.com)\\n4. 单位收款信息在哪里可以找到？ \\n\\n答：收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。\\n请根据提供的信息回答问题。问：4. 单位收款信息在哪里可以找到？ \\n\\n答：', 'response_chosen': '收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。', 'response_rejected': '请联系刘玉莎 (liuyusha@fiberhome.com)'}\u001b[0m\n\u001b[32m2023-08-29 15:56:45.345\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m370\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 14\u001b[0m\n\u001b[32m2023-08-29 15:56:45.345\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m371\u001b[0m - \u001b[34m\u001b[1mFirst eval example:\u001b[0m\n\u001b[32m2023-08-29 15:56:45.346\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m372\u001b[0m - \u001b[34m\u001b[1mQuestion: 现在提供如下信息：\n1. 审批出差申请的流程中，如何查询审批人的状态？ \n\n答：此问题是平台流程服务异常，IT会及时处理，您可以稍后再试，或上IT吧发帖反馈此问题\n3. 负责 FMR 系统的是谁？ 请联系刘玉莎 (liuyusha@fiberhome.com)\n4. 单位收款信息在哪里可以找到？ \n\n答：收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。\n请根据提供的信息回答问题。问：4. 单位收款信息在哪里可以找到？ \n\n答：\n\nAnswer: 收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。\u001b[0m\n\u001b[32m2023-08-29 15:56:45.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m374\u001b[0m - \u001b[1mLoading model\u001b[0m\n\u001b[32m2023-08-29 15:56:45.346\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m407\u001b[0m - \u001b[31m\u001b[1margs.qlora=False\u001b[0m\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:23<00:00, 11.95s/it]\n\u001b[32m2023-08-29 15:59:53.686\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m454\u001b[0m - \u001b[31m\u001b[1mid(model)=138409871590448\u001b[0m\n\u001b[32m2023-08-29 15:59:53.687\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m455\u001b[0m - \u001b[31m\u001b[1mid(model_ref)=94343855171552\u001b[0m\n\u001b[32m2023-08-29 15:59:53.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m494\u001b[0m - \u001b[1mPeft target_modules: []\u001b[0m\n\u001b[32m2023-08-29 15:59:53.696\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1mPeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): ChatGLMForConditionalGeneration(\n      (transformer): ChatGLMModel(\n        (embedding): Embedding(\n          (word_embeddings): Embedding(65024, 4096)\n        )\n        (rotary_pos_emb): RotaryEmbedding()\n        (encoder): GLMTransformer(\n          (layers): ModuleList(\n            (0-27): 28 x GLMBlock(\n              (input_layernorm): RMSNorm()\n              (self_attention): SelfAttention(\n                (query_key_value): Linear(\n                  in_features=4096, out_features=4608, bias=True\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4608, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n                (core_attention): CoreAttention(\n                  (attention_dropout): Dropout(p=0.0, inplace=False)\n                )\n                (dense): Linear(\n                  in_features=4096, out_features=4096, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4096, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n              )\n              (post_attention_layernorm): RMSNorm()\n              (mlp): MLP(\n                (dense_h_to_4h): Linear(\n                  in_features=4096, out_features=27392, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=27392, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n                (dense_4h_to_h): Linear(\n                  in_features=13696, out_features=4096, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=13696, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4096, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n              )\n            )\n          )\n          (final_layernorm): RMSNorm()\n        )\n        (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n      )\n    )\n  )\n)\u001b[0m\nTraceback (most recent call last):\n  File \"/kaggle/working/MedicalGPT/dpo_for_peftmodel.py\", line 544, in <module>\n    main()\n  File \"/kaggle/working/MedicalGPT/dpo_for_peftmodel.py\", line 504, in main\n    trainer = DPOTrainer(\n  File \"/opt/conda/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py\", line 114, in __init__\n    model = get_peft_model(model, peft_config)\n  File \"/opt/conda/lib/python3.10/site-packages/peft/mapping.py\", line 106, in get_peft_model\n    return MODEL_TYPE_TO_PEFT_MODEL_MAPPING[peft_config.task_type](model, peft_config, adapter_name=adapter_name)\n  File \"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py\", line 889, in __init__\n    super().__init__(model, peft_config, adapter_name)\n  File \"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py\", line 111, in __init__\n    self.base_model = PEFT_TYPE_TO_MODEL_MAPPING[peft_config.peft_type](\n  File \"/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py\", line 274, in __init__\n    super().__init__(model, config, adapter_name)\n  File \"/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 88, in __init__\n    self.inject_adapter(self.model, adapter_name)\n  File \"/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 222, in inject_adapter\n    raise ValueError(\nValueError: Target modules [] not found in the base model. Please check the target modules and try again.\n","output_type":"stream"}]}]}