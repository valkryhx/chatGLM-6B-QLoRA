{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-29T05:42:42.425356Z","iopub.execute_input":"2023-08-29T05:42:42.425671Z","iopub.status.idle":"2023-08-29T05:42:42.438596Z","shell.execute_reply.started":"2023-08-29T05:42:42.425639Z","shell.execute_reply":"2023-08-29T05:42:42.437361Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone https://github.com/valkryhx/chatGLM-6B-QLoRA\n%cd chatGLM-6B-QLoRA\n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-08-29T05:43:38.005017Z","iopub.execute_input":"2023-08-29T05:43:38.005520Z","iopub.status.idle":"2023-08-29T05:44:17.584268Z","shell.execute_reply.started":"2023-08-29T05:43:38.005446Z","shell.execute_reply":"2023-08-29T05:44:17.583110Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working\nfatal: destination path 'chatGLM-6B-QLoRA' already exists and is not an empty directory.\n/kaggle/working/chatGLM-6B-QLoRA\nCollecting peft==0.4.0 (from -r requirements.txt (line 1))\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.30.2)\nCollecting datasets==2.12.0 (from -r requirements.txt (line 3))\n  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm==4.65.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.65.0)\nCollecting loguru==0.7.0 (from -r requirements.txt (line 5))\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fire==0.5.0 (from -r requirements.txt (line 6))\n  Downloading fire-0.5.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting bitsandbytes==0.39.0 (from -r requirements.txt (line 7))\n  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting wandb==0.15.3 (from -r requirements.txt (line 8))\n  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting cpm_kernels==1.0.11 (from -r requirements.txt (line 9))\n  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.20.3)\nRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.1.99)\nCollecting deepspeed==0.9.5 (from -r requirements.txt (line 12))\n  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting evaluate==0.4.0 (from -r requirements.txt (line 13))\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting trl==0.5.0 (from -r requirements.txt (line 14))\n  Downloading trl-0.5.0-py3-none-any.whl (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (2.0.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (0.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (11.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (2.3.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.1.31)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.20.3)\nCollecting hjson (from deepspeed==0.9.5->-r requirements.txt (line 12))\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.11.1)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (9.0.0)\nRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.10.10)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (4.0.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2->-r requirements.txt (line 2)) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0->-r requirements.txt (line 1)) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2023.5.7)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2023.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.3.0)\nBuilding wheels for collected packages: fire, deepspeed\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=6a653a3ae166e5e98d8c97dc9e806a326ac663c1b9e5ad98f35085b4377e804d\n  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844546 sha256=c4ec3b065c27632a5a1af3171c7a5b08cae8b0119a2ecae5d92722272d358e8e\n  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\nSuccessfully built fire deepspeed\nInstalling collected packages: hjson, cpm_kernels, bitsandbytes, loguru, fire, wandb, deepspeed, peft, datasets, trl, evaluate\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.5\n    Uninstalling wandb-0.15.5:\n      Successfully uninstalled wandb-0.15.5\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed bitsandbytes-0.39.0 cpm_kernels-1.0.11 datasets-2.12.0 deepspeed-0.9.5 evaluate-0.4.0 fire-0.5.0 hjson-3.1.0 loguru-0.7.0 peft-0.4.0 trl-0.5.0 wandb-0.15.3\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install bitsandbytes==0.41.1\n!pip install peft==0.5.0 \n!pip install accelerate==0.21.0 \n!pip install trl==0.6.0","metadata":{"execution":{"iopub.status.busy":"2023-08-29T05:44:26.006474Z","iopub.execute_input":"2023-08-29T05:44:26.007003Z","iopub.status.idle":"2023-08-29T05:45:19.498353Z","shell.execute_reply.started":"2023-08-29T05:44:26.006970Z","shell.execute_reply":"2023-08-29T05:45:19.497197Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting bitsandbytes==0.41.1\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\n  Attempting uninstall: bitsandbytes\n    Found existing installation: bitsandbytes 0.39.0\n    Uninstalling bitsandbytes-0.39.0:\n      Successfully uninstalled bitsandbytes-0.39.0\nSuccessfully installed bitsandbytes-0.41.1\nCollecting peft==0.5.0\n  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (4.30.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (4.65.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (0.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.5.0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0) (0.13.3)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.5.0) (2023.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.5.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.5.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.5.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.5.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.5.0) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.5.0) (1.3.0)\nInstalling collected packages: peft\n  Attempting uninstall: peft\n    Found existing installation: peft 0.4.0\n    Uninstalling peft-0.4.0:\n      Successfully uninstalled peft-0.4.0\nSuccessfully installed peft-0.5.0\nCollecting accelerate==0.21.0\n  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (6.0)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.21.0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.20.3\n    Uninstalling accelerate-0.20.3:\n      Successfully uninstalled accelerate-0.20.3\nSuccessfully installed accelerate-0.21.0\nCollecting trl==0.6.0\n  Downloading trl-0.6.0-py3-none-any.whl (110 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.0/110.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.6.0) (2.0.0)\nRequirement already satisfied: transformers>=4.18.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.6.0) (4.30.2)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.6.0) (1.23.5)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.6.0) (0.21.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.6.0) (2.12.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.6.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.6.0) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.6.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.6.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.6.0) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (0.16.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.18.0->trl==0.6.0) (4.65.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.6.0) (5.9.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (11.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.6.0) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.6.0) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.6.0) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.6.0) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.6.0) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.6.0) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.6.0) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.6.0) (1.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.18.0->trl==0.6.0) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.6.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.6.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.18.0->trl==0.6.0) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.6.0) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.6.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.6.0) (2023.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.6.0) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets->trl==0.6.0) (1.16.0)\nInstalling collected packages: trl\n  Attempting uninstall: trl\n    Found existing installation: trl 0.5.0\n    Uninstalling trl-0.5.0:\n      Successfully uninstalled trl-0.5.0\nSuccessfully installed trl-0.6.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":" * # <font color=red>首先sft  with qlora </font> ","metadata":{}},{"cell_type":"code","source":"!git pull --all --force \n!deepspeed --include localhost:0,1  train_qlora_deepspeed_zero.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output_yungaun_0827_v1 \\\n  --num_train_samples -1 \\\n  --num_eval_samples 100 \\\n  --train_data_path ./data/augment_staff298_qa70and14  \\\n  --eval_data_path  ./data/augment_staff298_qa70and14    \\\n  --max_input_length 256 \\\n  --max_output_length 400 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 2  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  1e-5 \\\n  --num_train_epochs  20  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"execution":{"iopub.status.busy":"2023-08-29T01:32:53.505927Z","iopub.execute_input":"2023-08-29T01:32:53.506393Z","iopub.status.idle":"2023-08-29T01:48:58.186605Z","shell.execute_reply.started":"2023-08-29T01:32:53.506329Z","shell.execute_reply":"2023-08-29T01:48:58.179213Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Fetching origin\nAlready up to date.\n[2023-08-29 01:32:57,832] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-29 01:33:11,849] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n[2023-08-29 01:33:11,850] [INFO] [runner.py:555:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None train_qlora_deepspeed_zero.py --train_args_json luzi.json --model_name_or_path THUDM/chatglm2-6b --output_dir output_yungaun_0827_v1 --num_train_samples -1 --num_eval_samples 100 --train_data_path ./data/augment_staff298_qa70and14 --eval_data_path ./data/augment_staff298_qa70and14 --max_input_length 256 --max_output_length 400 --lora_rank 64 --lora_dropout 0.05 --compute_dtype fp16 --per_device_train_batch_size 2 --per_device_eval_batch_size 2 --gradient_accumulation_steps 1 --learning_rate 1e-5 --num_train_epochs 20 --save_total_limit 2 --load_in_4bit True --deepspeed ds_zero2_config.json\n[2023-08-29 01:33:13,585] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-29 01:33:19,451] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n[2023-08-29 01:33:19,451] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n[2023-08-29 01:33:19,451] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n[2023-08-29 01:33:19,451] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n[2023-08-29 01:33:19,451] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n[2023-08-29 01:33:19,452] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n[2023-08-29 01:33:19,452] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n[2023-08-29 01:33:19,452] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n[2023-08-29 01:33:19,452] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n[2023-08-29 01:33:19,452] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n[2023-08-29 01:33:19,452] [INFO] [launch.py:163:main] dist_world_size=2\n[2023-08-29 01:33:19,452] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2023-08-29 01:33:23,087] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-08-29 01:33:23,093] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\nDownloading (…)okenizer_config.json: 100%|██████| 244/244 [00:00<00:00, 989kB/s]\nDownloading (…)enization_chatglm.py: 100%|█| 10.1k/10.1k [00:00<00:00, 50.8MB/s]\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nDownloading tokenizer.model: 100%|█████████| 1.02M/1.02M [00:00<00:00, 21.3MB/s]\nDownloading (…)lve/main/config.json: 100%|█| 1.22k/1.22k [00:00<00:00, 7.18MB/s]\nDownloading (…)iguration_chatglm.py: 100%|█| 2.25k/2.25k [00:00<00:00, 13.1MB/s]\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nDownloading (…)/modeling_chatglm.py: 100%|█| 50.7k/50.7k [00:00<00:00, 7.26MB/s]\nDownloading (…)main/quantization.py: 100%|█| 14.7k/14.7k [00:00<00:00, 61.1MB/s]\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- modeling_chatglm.py\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- modeling_chatglm.py\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nDownloading (…)model.bin.index.json: 100%|█| 20.4k/20.4k [00:00<00:00, 81.3MB/s]\nDownloading shards:   0%|                                 | 0/7 [00:00<?, ?it/s]\nDownloading (…)l-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:   2%|  | 31.5M/1.83G [00:00<00:07, 254MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:   3%|  | 62.9M/1.83G [00:00<00:06, 267MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:   5%|  | 94.4M/1.83G [00:00<00:06, 272MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:   7%|▏  | 126M/1.83G [00:00<00:06, 274MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:   9%|▎  | 157M/1.83G [00:00<00:06, 276MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  10%|▎  | 189M/1.83G [00:00<00:05, 277MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  12%|▎  | 220M/1.83G [00:00<00:05, 277MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  14%|▍  | 252M/1.83G [00:00<00:05, 278MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  15%|▍  | 283M/1.83G [00:01<00:05, 278MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  17%|▌  | 315M/1.83G [00:01<00:05, 278MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  19%|▌  | 346M/1.83G [00:01<00:05, 279MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  21%|▌  | 377M/1.83G [00:01<00:05, 273MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  22%|▋  | 409M/1.83G [00:01<00:05, 272MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  24%|▋  | 440M/1.83G [00:01<00:05, 274MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  26%|▊  | 472M/1.83G [00:01<00:04, 275MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  28%|▊  | 503M/1.83G [00:01<00:04, 274MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  29%|▉  | 535M/1.83G [00:01<00:04, 275MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  31%|▉  | 566M/1.83G [00:02<00:04, 275MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  33%|▉  | 598M/1.83G [00:02<00:04, 269MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  34%|█  | 629M/1.83G [00:02<00:04, 269MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  36%|█  | 661M/1.83G [00:02<00:04, 270MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  38%|█▏ | 692M/1.83G [00:02<00:04, 268MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  40%|█▏ | 724M/1.83G [00:02<00:04, 271MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  41%|█▏ | 755M/1.83G [00:02<00:03, 271MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  43%|█▎ | 786M/1.83G [00:02<00:03, 272MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  45%|█▎ | 818M/1.83G [00:02<00:03, 273MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  46%|█▍ | 849M/1.83G [00:03<00:03, 273MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  48%|█▍ | 881M/1.83G [00:03<00:03, 276MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  50%|█▍ | 912M/1.83G [00:03<00:03, 278MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  52%|█▌ | 944M/1.83G [00:03<00:03, 277MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  53%|█▌ | 975M/1.83G [00:03<00:03, 269MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  55%|█ | 1.01G/1.83G [00:03<00:03, 272MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  57%|█▏| 1.04G/1.83G [00:03<00:02, 274MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  59%|█▏| 1.07G/1.83G [00:03<00:02, 273MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  60%|█▏| 1.10G/1.83G [00:04<00:02, 272MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  62%|█▏| 1.13G/1.83G [00:04<00:02, 274MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  64%|█▎| 1.16G/1.83G [00:04<00:02, 275MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  65%|█▎| 1.20G/1.83G [00:04<00:02, 276MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  67%|█▎| 1.23G/1.83G [00:04<00:02, 276MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  69%|█▍| 1.26G/1.83G [00:04<00:02, 275MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  71%|█▍| 1.29G/1.83G [00:04<00:01, 274MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  72%|█▍| 1.32G/1.83G [00:04<00:01, 267MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  74%|█▍| 1.35G/1.83G [00:04<00:01, 259MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  76%|█▌| 1.38G/1.83G [00:05<00:01, 254MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  77%|█▌| 1.42G/1.83G [00:05<00:01, 248MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  79%|█▌| 1.45G/1.83G [00:05<00:01, 244MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  81%|█▌| 1.48G/1.83G [00:05<00:01, 239MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  83%|█▋| 1.51G/1.83G [00:05<00:01, 241MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  84%|█▋| 1.54G/1.83G [00:05<00:01, 240MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  86%|█▋| 1.57G/1.83G [00:05<00:01, 239MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  88%|█▊| 1.60G/1.83G [00:06<00:00, 240MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  89%|█▊| 1.64G/1.83G [00:06<00:00, 236MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  91%|█▊| 1.67G/1.83G [00:06<00:00, 235MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  93%|█▊| 1.70G/1.83G [00:06<00:00, 238MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  95%|█▉| 1.73G/1.83G [00:06<00:00, 235MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  96%|█▉| 1.76G/1.83G [00:06<00:00, 235MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  98%|█▉| 1.79G/1.83G [00:06<00:00, 238MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin: 100%|██| 1.83G/1.83G [00:07<00:00, 261MB/s]\u001b[A\nDownloading shards:  14%|███▌                     | 1/7 [00:07<00:42,  7.08s/it]\nDownloading (…)l-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:   2%|  | 31.5M/1.97G [00:00<00:08, 238MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:   3%|  | 62.9M/1.97G [00:00<00:08, 236MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:   5%|  | 94.4M/1.97G [00:00<00:08, 234MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:   6%|▏  | 126M/1.97G [00:00<00:07, 232MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:   8%|▏  | 157M/1.97G [00:00<00:07, 234MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  10%|▎  | 189M/1.97G [00:00<00:07, 232MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  11%|▎  | 220M/1.97G [00:00<00:07, 231MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  13%|▍  | 252M/1.97G [00:01<00:07, 226MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  14%|▍  | 283M/1.97G [00:01<00:07, 230MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  16%|▍  | 315M/1.97G [00:01<00:07, 232MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  18%|▌  | 346M/1.97G [00:01<00:06, 234MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  19%|▌  | 377M/1.97G [00:01<00:06, 238MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  21%|▌  | 409M/1.97G [00:01<00:06, 243MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  22%|▋  | 440M/1.97G [00:01<00:06, 249MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  24%|▋  | 472M/1.97G [00:01<00:05, 256MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  26%|▊  | 503M/1.97G [00:02<00:05, 261MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  27%|▊  | 535M/1.97G [00:02<00:05, 263MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  29%|▊  | 566M/1.97G [00:02<00:05, 266MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  30%|▉  | 598M/1.97G [00:02<00:05, 267MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  32%|▉  | 629M/1.97G [00:02<00:05, 267MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  34%|█  | 661M/1.97G [00:02<00:04, 266MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  35%|█  | 692M/1.97G [00:02<00:05, 254MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  37%|█  | 724M/1.97G [00:02<00:04, 260MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  38%|█▏ | 755M/1.97G [00:03<00:04, 260MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  40%|█▏ | 786M/1.97G [00:03<00:04, 255MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  42%|█▏ | 818M/1.97G [00:03<00:04, 262MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  43%|█▎ | 849M/1.97G [00:03<00:04, 265MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  45%|█▎ | 881M/1.97G [00:03<00:04, 268MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  46%|█▍ | 912M/1.97G [00:03<00:03, 269MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  48%|█▍ | 944M/1.97G [00:03<00:03, 262MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  50%|█▍ | 975M/1.97G [00:03<00:03, 265MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  51%|█ | 1.01G/1.97G [00:03<00:03, 267MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  53%|█ | 1.04G/1.97G [00:04<00:03, 268MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  54%|█ | 1.07G/1.97G [00:04<00:03, 269MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  56%|█ | 1.10G/1.97G [00:04<00:03, 269MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  58%|█▏| 1.13G/1.97G [00:04<00:03, 271MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  59%|█▏| 1.16G/1.97G [00:04<00:03, 268MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  61%|█▏| 1.20G/1.97G [00:04<00:02, 267MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  62%|█▏| 1.23G/1.97G [00:04<00:02, 269MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  64%|█▎| 1.26G/1.97G [00:04<00:02, 269MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  66%|█▎| 1.29G/1.97G [00:05<00:02, 269MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  67%|█▎| 1.32G/1.97G [00:05<00:02, 223MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  69%|█▎| 1.35G/1.97G [00:05<00:03, 159MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  70%|█▍| 1.37G/1.97G [00:05<00:04, 128MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  71%|█▍| 1.39G/1.97G [00:06<00:05, 106MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  72%|▋| 1.42G/1.97G [00:06<00:06, 79.3MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  74%|█▍| 1.45G/1.97G [00:06<00:04, 106MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  75%|█▌| 1.48G/1.97G [00:06<00:03, 132MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  77%|█▌| 1.51G/1.97G [00:06<00:02, 159MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  78%|█▌| 1.54G/1.97G [00:07<00:02, 183MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  80%|█▌| 1.57G/1.97G [00:07<00:01, 202MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  82%|█▋| 1.60G/1.97G [00:07<00:01, 210MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  83%|█▋| 1.64G/1.97G [00:07<00:01, 187MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  85%|▊| 1.67G/1.97G [00:09<00:05, 51.1MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  86%|▊| 1.69G/1.97G [00:09<00:04, 58.2MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  87%|▊| 1.71G/1.97G [00:09<00:03, 67.7MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  88%|▉| 1.73G/1.97G [00:09<00:02, 80.6MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  89%|▉| 1.75G/1.97G [00:09<00:02, 94.2MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  90%|█▊| 1.77G/1.97G [00:09<00:01, 110MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  91%|█▊| 1.79G/1.97G [00:09<00:01, 125MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  92%|█▊| 1.81G/1.97G [00:10<00:01, 139MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  94%|█▉| 1.85G/1.97G [00:10<00:00, 163MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  95%|█▉| 1.87G/1.97G [00:10<00:00, 148MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  96%|█▉| 1.89G/1.97G [00:10<00:00, 155MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  97%|█▉| 1.91G/1.97G [00:10<00:00, 162MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  99%|█▉| 1.94G/1.97G [00:10<00:00, 180MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin: 100%|██| 1.97G/1.97G [00:10<00:00, 180MB/s]\u001b[A\nDownloading shards:  29%|███████▏                 | 2/7 [00:18<00:47,  9.41s/it]\nDownloading (…)l-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   1%| | 10.5M/1.93G [00:00<02:42, 11.8MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   1%| | 21.0M/1.93G [00:02<03:22, 9.39MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   2%| | 31.5M/1.93G [00:02<01:58, 16.1MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   2%| | 41.9M/1.93G [00:03<02:47, 11.2MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   3%| | 62.9M/1.93G [00:03<01:23, 22.2MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   4%| | 83.9M/1.93G [00:05<01:38, 18.8MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   5%|  | 105M/1.93G [00:05<01:03, 28.6MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   6%|  | 115M/1.93G [00:05<00:54, 33.5MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   7%|▏ | 126M/1.93G [00:06<01:27, 20.6MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   8%|▏ | 147M/1.93G [00:07<01:39, 17.9MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   8%|▏ | 157M/1.93G [00:08<01:32, 19.2MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   9%|▏ | 168M/1.93G [00:08<01:21, 21.6MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   9%|▏ | 178M/1.93G [00:08<01:04, 27.0MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  10%|▏ | 199M/1.93G [00:08<00:42, 40.7MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  11%|▏ | 210M/1.93G [00:08<00:37, 46.4MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  12%|▏ | 231M/1.93G [00:09<00:25, 65.6MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  13%|▎ | 252M/1.93G [00:10<00:46, 36.1MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  14%|▎ | 273M/1.93G [00:10<00:33, 49.8MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  15%|▎ | 294M/1.93G [00:10<00:29, 55.2MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  16%|▎ | 315M/1.93G [00:10<00:23, 69.4MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  17%|▎ | 336M/1.93G [00:10<00:18, 87.7MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  19%|▌  | 367M/1.93G [00:10<00:13, 119MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  20%|▌  | 388M/1.93G [00:10<00:11, 134MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  21%|▋  | 409M/1.93G [00:11<00:12, 125MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  22%|▋  | 430M/1.93G [00:11<00:11, 126MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  23%|▋  | 451M/1.93G [00:11<00:10, 139MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  25%|▊  | 482M/1.93G [00:11<00:09, 155MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  26%|▊  | 503M/1.93G [00:11<00:09, 149MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  28%|▊  | 535M/1.93G [00:11<00:08, 168MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  29%|▊  | 556M/1.93G [00:12<00:09, 147MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  30%|▉  | 577M/1.93G [00:12<00:08, 152MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  31%|▉  | 598M/1.93G [00:12<00:08, 160MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  32%|▉  | 619M/1.93G [00:12<00:07, 169MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  34%|█  | 650M/1.93G [00:12<00:06, 193MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  35%|█  | 671M/1.93G [00:12<00:06, 193MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  36%|█  | 703M/1.93G [00:12<00:05, 221MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  38%|█▏ | 734M/1.93G [00:12<00:05, 225MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  40%|█▏ | 765M/1.93G [00:13<00:06, 190MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  41%|█▏ | 797M/1.93G [00:13<00:06, 174MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  42%|█▎ | 818M/1.93G [00:13<00:06, 166MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  44%|█▎ | 839M/1.93G [00:13<00:07, 151MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  45%|█▎ | 860M/1.93G [00:13<00:07, 149MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  46%|█▎ | 881M/1.93G [00:13<00:06, 161MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  47%|█▍ | 912M/1.93G [00:14<00:05, 193MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  48%|█▍ | 933M/1.93G [00:14<00:05, 183MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  50%|█▌ | 965M/1.93G [00:14<00:04, 194MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  51%|█▌ | 986M/1.93G [00:14<00:04, 194MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  52%|█ | 1.01G/1.93G [00:14<00:05, 183MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  53%|█ | 1.03G/1.93G [00:14<00:04, 185MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  54%|█ | 1.05G/1.93G [00:14<00:05, 166MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  55%|█ | 1.07G/1.93G [00:14<00:05, 160MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  57%|█▏| 1.09G/1.93G [00:15<00:06, 138MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  58%|█▏| 1.11G/1.93G [00:15<00:05, 146MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  59%|█▏| 1.13G/1.93G [00:15<00:05, 153MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  60%|█▏| 1.15G/1.93G [00:15<00:05, 147MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  61%|█▏| 1.17G/1.93G [00:15<00:04, 162MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  63%|█▎| 1.21G/1.93G [00:15<00:03, 192MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  64%|█▎| 1.23G/1.93G [00:15<00:03, 186MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  65%|█▎| 1.25G/1.93G [00:16<00:04, 166MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  66%|█▎| 1.27G/1.93G [00:16<00:04, 141MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  67%|█▎| 1.29G/1.93G [00:16<00:04, 146MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  68%|█▎| 1.31G/1.93G [00:16<00:03, 156MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  69%|█▍| 1.33G/1.93G [00:16<00:03, 160MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  70%|█▍| 1.35G/1.93G [00:16<00:03, 158MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  71%|█▍| 1.37G/1.93G [00:16<00:03, 167MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  72%|█▍| 1.39G/1.93G [00:17<00:03, 169MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  73%|█▍| 1.42G/1.93G [00:17<00:02, 172MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  75%|█▍| 1.44G/1.93G [00:17<00:02, 164MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  76%|█▌| 1.47G/1.93G [00:17<00:02, 187MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  78%|█▌| 1.50G/1.93G [00:17<00:02, 199MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  79%|█▌| 1.53G/1.93G [00:17<00:01, 213MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  81%|█▌| 1.56G/1.93G [00:17<00:01, 220MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  83%|█▋| 1.59G/1.93G [00:18<00:02, 150MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  84%|█▋| 1.61G/1.93G [00:18<00:02, 105MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  85%|▊| 1.64G/1.93G [00:18<00:03, 85.4MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  86%|▊| 1.66G/1.93G [00:19<00:03, 75.4MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  87%|▊| 1.68G/1.93G [00:19<00:02, 86.7MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  88%|█▊| 1.70G/1.93G [00:19<00:02, 101MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  89%|█▊| 1.72G/1.93G [00:19<00:01, 104MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  90%|█▊| 1.74G/1.93G [00:19<00:01, 119MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  91%|█▊| 1.76G/1.93G [00:20<00:01, 132MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  92%|█▊| 1.78G/1.93G [00:20<00:01, 126MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  94%|█▊| 1.80G/1.93G [00:20<00:00, 141MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  95%|█▉| 1.82G/1.93G [00:20<00:00, 150MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  96%|█▉| 1.85G/1.93G [00:20<00:00, 161MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  97%|█▉| 1.87G/1.93G [00:20<00:00, 135MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  98%|█▉| 1.89G/1.93G [00:20<00:00, 148MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin: 100%|█| 1.93G/1.93G [00:21<00:00, 91.6MB/s]\u001b[A\nDownloading shards:  43%|██████████▋              | 3/7 [00:39<00:59, 14.84s/it]\nDownloading (…)l-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:   1%|  | 21.0M/1.82G [00:00<00:10, 175MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:   2%|  | 41.9M/1.82G [00:00<00:09, 186MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:   3%|  | 62.9M/1.82G [00:00<00:10, 167MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:   5%|  | 83.9M/1.82G [00:00<00:11, 157MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:   6%|▏  | 105M/1.82G [00:00<00:10, 161MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:   7%|▏  | 126M/1.82G [00:00<00:09, 170MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:   9%|▎  | 157M/1.82G [00:00<00:08, 193MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  10%|▎  | 189M/1.82G [00:01<00:07, 204MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  12%|▎  | 210M/1.82G [00:01<00:08, 188MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  13%|▍  | 241M/1.82G [00:01<00:11, 138MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  15%|▍  | 273M/1.82G [00:01<00:09, 161MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  16%|▍  | 294M/1.82G [00:01<00:09, 160MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  17%|▌  | 315M/1.82G [00:01<00:09, 161MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  18%|▌  | 336M/1.82G [00:02<00:12, 120MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  20%|▌  | 357M/1.82G [00:02<00:12, 121MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  21%|▌  | 377M/1.82G [00:02<00:12, 119MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  22%|▋  | 398M/1.82G [00:02<00:10, 131MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  23%|▋  | 419M/1.82G [00:02<00:10, 134MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  24%|▋  | 440M/1.82G [00:02<00:09, 148MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  25%|▊  | 461M/1.82G [00:03<00:09, 139MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  27%|▊  | 482M/1.82G [00:03<00:13, 100MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  28%|▌ | 503M/1.82G [00:03<00:13, 98.2MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  29%|▌ | 524M/1.82G [00:03<00:13, 92.9MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  29%|▌ | 535M/1.82G [00:04<00:20, 62.3MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  30%|▌ | 545M/1.82G [00:04<00:25, 50.6MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  31%|▌ | 556M/1.82G [00:04<00:22, 55.8MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  31%|▌ | 566M/1.82G [00:04<00:20, 60.4MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  32%|▋ | 587M/1.82G [00:05<00:18, 67.4MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  33%|▋ | 598M/1.82G [00:05<00:23, 51.6MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  34%|▋ | 619M/1.82G [00:05<00:16, 72.6MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  35%|▋ | 640M/1.82G [00:05<00:14, 80.3MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  36%|▋ | 661M/1.82G [00:06<00:14, 78.4MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  38%|▊ | 682M/1.82G [00:06<00:12, 89.6MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  39%|█▏ | 703M/1.82G [00:06<00:10, 104MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  40%|█▏ | 724M/1.82G [00:06<00:09, 119MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  41%|█▏ | 744M/1.82G [00:06<00:07, 134MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  42%|▊ | 765M/1.82G [00:07<00:12, 85.4MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  43%|▊ | 786M/1.82G [00:07<00:14, 71.0MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  44%|▉ | 807M/1.82G [00:07<00:12, 83.5MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  46%|█▎ | 828M/1.82G [00:07<00:09, 101MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  47%|▉ | 849M/1.82G [00:08<00:14, 65.9MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  48%|▉ | 870M/1.82G [00:08<00:15, 62.9MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  49%|▉ | 881M/1.82G [00:09<00:19, 49.0MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  50%|▉ | 902M/1.82G [00:09<00:15, 60.9MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  50%|█ | 912M/1.82G [00:09<00:14, 61.7MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  51%|█ | 933M/1.82G [00:09<00:12, 71.7MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  53%|█ | 954M/1.82G [00:09<00:09, 89.8MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  54%|█ | 975M/1.82G [00:10<00:15, 55.1MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  55%|█ | 996M/1.82G [00:11<00:19, 41.4MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  55%|▌| 1.01G/1.82G [00:11<00:22, 36.3MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  56%|▌| 1.02G/1.82G [00:12<00:21, 36.4MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  57%|▌| 1.03G/1.82G [00:12<00:18, 41.6MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  58%|▌| 1.05G/1.82G [00:12<00:13, 57.6MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  58%|▌| 1.06G/1.82G [00:12<00:12, 59.0MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  59%|▌| 1.07G/1.82G [00:12<00:11, 63.9MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  59%|▌| 1.08G/1.82G [00:12<00:11, 61.3MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  61%|▌| 1.10G/1.82G [00:12<00:08, 82.5MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  61%|▌| 1.11G/1.82G [00:13<00:11, 61.7MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  62%|▌| 1.12G/1.82G [00:13<00:11, 61.5MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  64%|▋| 1.15G/1.82G [00:13<00:07, 86.6MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  65%|█▎| 1.18G/1.82G [00:13<00:05, 122MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  67%|█▎| 1.22G/1.82G [00:13<00:04, 149MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  68%|█▎| 1.24G/1.82G [00:14<00:03, 154MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  69%|█▍| 1.26G/1.82G [00:14<00:03, 148MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  71%|█▍| 1.29G/1.82G [00:14<00:03, 167MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  72%|█▍| 1.31G/1.82G [00:14<00:03, 153MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  73%|█▍| 1.33G/1.82G [00:14<00:03, 139MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  75%|█▍| 1.35G/1.82G [00:14<00:03, 128MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  76%|█▌| 1.37G/1.82G [00:14<00:03, 140MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  77%|█▌| 1.41G/1.82G [00:15<00:02, 165MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  79%|█▌| 1.44G/1.82G [00:15<00:02, 175MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  80%|█▌| 1.46G/1.82G [00:15<00:02, 138MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  81%|█▋| 1.48G/1.82G [00:15<00:02, 132MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  83%|█▋| 1.51G/1.82G [00:15<00:02, 147MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  85%|█▋| 1.54G/1.82G [00:16<00:01, 153MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  86%|█▋| 1.56G/1.82G [00:16<00:01, 147MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  87%|█▋| 1.58G/1.82G [00:16<00:01, 143MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  88%|█▊| 1.60G/1.82G [00:16<00:01, 117MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  90%|█▊| 1.63G/1.82G [00:16<00:01, 116MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  91%|█▊| 1.65G/1.82G [00:17<00:01, 109MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  92%|█▊| 1.67G/1.82G [00:17<00:01, 122MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  94%|█▊| 1.70G/1.82G [00:17<00:00, 148MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  95%|█▉| 1.72G/1.82G [00:17<00:00, 146MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  96%|█▉| 1.74G/1.82G [00:17<00:00, 144MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  97%|█▉| 1.76G/1.82G [00:17<00:00, 151MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  98%|█▉| 1.78G/1.82G [00:17<00:00, 161MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin: 100%|██| 1.82G/1.82G [00:18<00:00, 101MB/s]\u001b[A\nDownloading shards:  57%|██████████████▎          | 4/7 [00:57<00:48, 16.14s/it]\nDownloading (…)l-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:   1%|  | 21.0M/1.97G [00:00<00:10, 185MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:   3%|  | 52.4M/1.97G [00:00<00:09, 206MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:   4%|  | 73.4M/1.97G [00:00<00:10, 185MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:   5%|  | 94.4M/1.97G [00:00<00:10, 184MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:   6%|▏  | 115M/1.97G [00:00<00:11, 158MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:   7%|▏  | 136M/1.97G [00:00<00:12, 150MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:   8%|▏  | 157M/1.97G [00:00<00:11, 162MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  10%|▎  | 189M/1.97G [00:01<00:09, 185MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  11%|▎  | 220M/1.97G [00:01<00:08, 199MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  12%|▎  | 241M/1.97G [00:01<00:10, 165MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  13%|▍  | 262M/1.97G [00:01<00:12, 133MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  14%|▍  | 283M/1.97G [00:01<00:15, 108MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  15%|▎ | 304M/1.97G [00:02<00:19, 83.7MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  17%|▎ | 325M/1.97G [00:02<00:18, 86.6MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  17%|▎ | 336M/1.97G [00:02<00:18, 88.4MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  18%|▌  | 357M/1.97G [00:02<00:15, 105MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  19%|▌  | 377M/1.97G [00:02<00:14, 113MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  20%|▌  | 398M/1.97G [00:03<00:13, 113MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  21%|▋  | 419M/1.97G [00:03<00:12, 123MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  22%|▋  | 440M/1.97G [00:03<00:12, 127MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  23%|▋  | 461M/1.97G [00:03<00:11, 135MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  25%|▊  | 493M/1.97G [00:03<00:08, 164MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  26%|▊  | 514M/1.97G [00:03<00:08, 169MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  27%|▊  | 535M/1.97G [00:04<00:10, 132MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  29%|▊  | 566M/1.97G [00:04<00:09, 152MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  30%|▉  | 587M/1.97G [00:04<00:08, 161MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  31%|▉  | 619M/1.97G [00:04<00:07, 180MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  33%|▉  | 650M/1.97G [00:04<00:07, 184MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  34%|█  | 671M/1.97G [00:04<00:07, 178MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  35%|█  | 692M/1.97G [00:04<00:07, 168MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  36%|█  | 713M/1.97G [00:05<00:07, 160MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  38%|█▏ | 744M/1.97G [00:05<00:06, 180MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  39%|█▏ | 765M/1.97G [00:05<00:08, 149MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  40%|█▏ | 786M/1.97G [00:05<00:08, 145MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  41%|█▏ | 807M/1.97G [00:05<00:10, 111MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  43%|█▎ | 849M/1.97G [00:05<00:07, 152MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  44%|█▎ | 870M/1.97G [00:06<00:07, 155MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  46%|█▎ | 902M/1.97G [00:06<00:06, 170MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  47%|█▍ | 933M/1.97G [00:06<00:05, 189MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  49%|█▍ | 965M/1.97G [00:06<00:04, 206MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  51%|█▌ | 996M/1.97G [00:06<00:04, 203MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  52%|█ | 1.03G/1.97G [00:06<00:04, 202MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  53%|█ | 1.05G/1.97G [00:06<00:05, 174MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  54%|█ | 1.07G/1.97G [00:07<00:05, 166MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  55%|█ | 1.09G/1.97G [00:07<00:05, 171MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  57%|█▏| 1.12G/1.97G [00:07<00:05, 144MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  58%|█▏| 1.14G/1.97G [00:07<00:05, 150MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  60%|█▏| 1.17G/1.97G [00:07<00:04, 173MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  61%|█▏| 1.20G/1.97G [00:07<00:04, 179MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  62%|█▏| 1.22G/1.97G [00:08<00:05, 138MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  63%|█▎| 1.24G/1.97G [00:08<00:05, 122MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  64%|█▎| 1.26G/1.97G [00:08<00:05, 132MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  65%|█▎| 1.28G/1.97G [00:08<00:04, 144MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  66%|█▎| 1.30G/1.97G [00:08<00:04, 149MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  67%|█▎| 1.32G/1.97G [00:08<00:03, 163MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  69%|█▎| 1.35G/1.97G [00:08<00:03, 185MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  70%|█▍| 1.38G/1.97G [00:09<00:02, 205MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  72%|█▍| 1.42G/1.97G [00:09<00:02, 219MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  74%|█▍| 1.45G/1.97G [00:09<00:02, 209MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  75%|█▌| 1.48G/1.97G [00:09<00:02, 218MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  77%|█▌| 1.51G/1.97G [00:09<00:02, 226MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  78%|█▌| 1.54G/1.97G [00:09<00:01, 234MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  80%|█▌| 1.57G/1.97G [00:09<00:01, 230MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  82%|█▋| 1.60G/1.97G [00:10<00:01, 210MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  83%|█▋| 1.64G/1.97G [00:10<00:01, 175MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  84%|█▋| 1.66G/1.97G [00:10<00:01, 182MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  85%|█▋| 1.68G/1.97G [00:10<00:01, 168MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  86%|█▋| 1.70G/1.97G [00:10<00:01, 155MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  87%|█▋| 1.72G/1.97G [00:10<00:01, 143MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  89%|█▊| 1.75G/1.97G [00:11<00:01, 166MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  91%|█▊| 1.78G/1.97G [00:11<00:01, 166MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  92%|█▊| 1.80G/1.97G [00:11<00:00, 171MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  93%|█▊| 1.82G/1.97G [00:11<00:00, 175MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  94%|█▉| 1.85G/1.97G [00:11<00:00, 181MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  95%|█▉| 1.87G/1.97G [00:11<00:00, 166MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  96%|█▉| 1.89G/1.97G [00:11<00:00, 159MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  97%|█▉| 1.91G/1.97G [00:12<00:00, 153MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  98%|█▉| 1.93G/1.97G [00:12<00:00, 161MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin: 100%|██| 1.97G/1.97G [00:12<00:00, 160MB/s]\u001b[A\nDownloading shards:  71%|█████████████████▊       | 5/7 [01:09<00:29, 14.79s/it]\nDownloading (…)l-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:   2%|  | 31.5M/1.93G [00:00<00:09, 197MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:   3%|  | 52.4M/1.93G [00:00<00:10, 175MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:   4%|  | 83.9M/1.93G [00:00<00:09, 195MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:   5%|▏  | 105M/1.93G [00:00<00:09, 199MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:   7%|▏  | 126M/1.93G [00:00<00:09, 182MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:   8%|▏  | 147M/1.93G [00:00<00:09, 187MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:   9%|▎  | 178M/1.93G [00:00<00:08, 213MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  11%|▎  | 210M/1.93G [00:01<00:11, 150MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  12%|▎  | 231M/1.93G [00:01<00:11, 152MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  13%|▍  | 252M/1.93G [00:01<00:10, 155MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  14%|▍  | 273M/1.93G [00:01<00:10, 165MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  15%|▍  | 294M/1.93G [00:01<00:09, 172MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  16%|▍  | 315M/1.93G [00:01<00:10, 149MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  18%|▌  | 346M/1.93G [00:02<00:09, 166MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  20%|▌  | 377M/1.93G [00:02<00:08, 190MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  21%|▋  | 409M/1.93G [00:02<00:06, 219MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  23%|▋  | 440M/1.93G [00:02<00:06, 227MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  24%|▋  | 472M/1.93G [00:02<00:08, 177MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  26%|▊  | 493M/1.93G [00:02<00:09, 147MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  27%|▊  | 514M/1.93G [00:03<00:09, 149MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  28%|▊  | 545M/1.93G [00:03<00:08, 168MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  30%|▉  | 577M/1.93G [00:03<00:06, 197MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  32%|▉  | 608M/1.93G [00:03<00:06, 219MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  33%|▉  | 640M/1.93G [00:03<00:05, 218MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  35%|█  | 671M/1.93G [00:03<00:07, 170MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  36%|█  | 692M/1.93G [00:03<00:07, 173MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  37%|█  | 713M/1.93G [00:03<00:06, 180MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  39%|█▏ | 744M/1.93G [00:04<00:05, 201MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  40%|█▏ | 776M/1.93G [00:04<00:05, 210MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  42%|█▎ | 807M/1.93G [00:04<00:06, 172MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  44%|█▎ | 839M/1.93G [00:04<00:05, 183MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  45%|█▎ | 860M/1.93G [00:04<00:06, 166MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  46%|█▍ | 891M/1.93G [00:04<00:06, 171MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  47%|█▍ | 912M/1.93G [00:05<00:05, 177MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  48%|█▍ | 933M/1.93G [00:05<00:05, 183MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  50%|█▍ | 954M/1.93G [00:05<00:05, 179MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  51%|█▌ | 975M/1.93G [00:05<00:05, 185MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  52%|█▌ | 996M/1.93G [00:05<00:05, 161MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  53%|█ | 1.02G/1.93G [00:05<00:05, 170MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  54%|█ | 1.04G/1.93G [00:05<00:05, 178MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  55%|█ | 1.06G/1.93G [00:06<00:05, 145MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  56%|█ | 1.08G/1.93G [00:06<00:06, 137MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  57%|█▏| 1.10G/1.93G [00:06<00:05, 143MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  58%|█▏| 1.12G/1.93G [00:06<00:05, 144MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  59%|█▏| 1.14G/1.93G [00:06<00:05, 155MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  60%|█▏| 1.16G/1.93G [00:06<00:05, 152MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  61%|█▏| 1.18G/1.93G [00:06<00:05, 145MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  63%|█▎| 1.21G/1.93G [00:07<00:04, 153MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  64%|█▎| 1.23G/1.93G [00:07<00:04, 166MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  65%|█▎| 1.25G/1.93G [00:07<00:03, 175MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  66%|█▎| 1.27G/1.93G [00:07<00:03, 184MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  67%|█▎| 1.29G/1.93G [00:07<00:03, 189MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  68%|█▎| 1.31G/1.93G [00:07<00:03, 176MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  69%|█▍| 1.33G/1.93G [00:07<00:03, 171MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  70%|█▍| 1.35G/1.93G [00:07<00:03, 181MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  72%|█▍| 1.38G/1.93G [00:07<00:02, 198MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  73%|█▍| 1.41G/1.93G [00:08<00:03, 172MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  74%|█▍| 1.43G/1.93G [00:08<00:03, 153MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  75%|█▌| 1.45G/1.93G [00:08<00:02, 165MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  77%|█▌| 1.48G/1.93G [00:08<00:02, 195MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  78%|█▌| 1.50G/1.93G [00:08<00:02, 196MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  79%|█▌| 1.53G/1.93G [00:08<00:01, 202MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  81%|█▌| 1.55G/1.93G [00:08<00:02, 175MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  82%|█▋| 1.57G/1.93G [00:09<00:02, 159MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  83%|█▋| 1.60G/1.93G [00:09<00:01, 192MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  85%|█▋| 1.64G/1.93G [00:09<00:01, 211MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  87%|█▋| 1.67G/1.93G [00:09<00:01, 214MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  88%|█▊| 1.70G/1.93G [00:09<00:01, 228MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  90%|█▊| 1.73G/1.93G [00:09<00:00, 222MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  91%|█▊| 1.76G/1.93G [00:09<00:00, 193MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  92%|█▊| 1.78G/1.93G [00:10<00:00, 194MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  94%|█▊| 1.80G/1.93G [00:10<00:00, 195MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  95%|█▉| 1.84G/1.93G [00:10<00:00, 213MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  97%|█▉| 1.87G/1.93G [00:10<00:00, 158MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  98%|█▉| 1.89G/1.93G [00:10<00:00, 146MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  99%|█▉| 1.91G/1.93G [00:10<00:00, 152MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin: 100%|██| 1.93G/1.93G [00:10<00:00, 176MB/s]\u001b[A\nDownloading shards:  86%|█████████████████████▍   | 6/7 [01:21<00:13, 13.53s/it]\nDownloading (…)l-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:   3%|  | 31.5M/1.05G [00:00<00:04, 249MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:   6%|  | 62.9M/1.05G [00:00<00:03, 257MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:   9%|▏ | 94.4M/1.05G [00:00<00:05, 166MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  11%|▎  | 115M/1.05G [00:00<00:07, 130MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  13%|▍  | 136M/1.05G [00:00<00:07, 129MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  15%|▍  | 157M/1.05G [00:01<00:07, 114MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  17%|▌  | 178M/1.05G [00:01<00:08, 104MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  20%|▌  | 210M/1.05G [00:01<00:06, 137MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  23%|▋  | 241M/1.05G [00:01<00:04, 166MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  26%|▊  | 273M/1.05G [00:01<00:04, 180MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  28%|▊  | 294M/1.05G [00:01<00:04, 174MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  30%|▉  | 315M/1.05G [00:02<00:04, 181MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  33%|▉  | 346M/1.05G [00:02<00:03, 192MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  36%|█  | 377M/1.05G [00:02<00:03, 208MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  39%|█▏ | 409M/1.05G [00:02<00:02, 222MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  42%|█▎ | 440M/1.05G [00:02<00:02, 228MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  45%|█▎ | 472M/1.05G [00:02<00:02, 228MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  48%|█▍ | 503M/1.05G [00:02<00:02, 207MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  51%|█▌ | 535M/1.05G [00:03<00:02, 209MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  54%|█▌ | 566M/1.05G [00:03<00:02, 168MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  56%|█▋ | 587M/1.05G [00:03<00:03, 133MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  58%|█▋ | 608M/1.05G [00:03<00:04, 111MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  61%|█▊ | 640M/1.05G [00:04<00:03, 130MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  63%|█▉ | 661M/1.05G [00:04<00:02, 142MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  66%|█▉ | 692M/1.05G [00:04<00:02, 165MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  69%|██ | 724M/1.05G [00:04<00:01, 173MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  71%|██ | 744M/1.05G [00:04<00:01, 166MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  73%|██▏| 765M/1.05G [00:04<00:01, 152MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  75%|█▍| 786M/1.05G [00:05<00:02, 96.7MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  77%|██▎| 807M/1.05G [00:05<00:02, 113MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  79%|██▎| 828M/1.05G [00:05<00:01, 124MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  82%|██▍| 860M/1.05G [00:05<00:01, 155MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  84%|██▌| 881M/1.05G [00:05<00:01, 159MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  86%|█▋| 902M/1.05G [00:06<00:01, 79.2MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  88%|█▊| 923M/1.05G [00:06<00:01, 85.4MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  90%|█▊| 944M/1.05G [00:06<00:01, 96.4MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  92%|██▋| 965M/1.05G [00:06<00:00, 101MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  94%|██▊| 986M/1.05G [00:06<00:00, 104MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  96%|█▉| 1.01G/1.05G [00:07<00:00, 121MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  98%|█▉| 1.03G/1.05G [00:07<00:00, 130MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin: 100%|██| 1.05G/1.05G [00:07<00:00, 143MB/s]\u001b[A\nDownloading shards: 100%|█████████████████████████| 7/7 [01:28<00:00, 12.64s/it]\nDownloading shards: 100%|█████████████████████████| 7/7 [01:28<00:00, 12.65s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:36<00:00, 13.74s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:36<00:00, 13.74s/it]\nmemory footprint of model: 3.6520424485206604 GB\nmemory footprint of model: 3.6520424485206604 GB\n\u001b[32m2023-08-29 01:36:42.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\n\u001b[32m2023-08-29 01:36:42.513\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m413\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\n\u001b[32m2023-08-29 01:38:24.322\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m448\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-29 01:38:24.323\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mdata files: ./data/augment_staff298_qa70and14/augment_0715_qa_small.json, ./data/augment_staff298_qa70and14/refuse_phone.json, ./data/augment_staff298_qa70and14/staff_short_answer.json, ./data/augment_staff298_qa70and14/augment_0715_qa70.json\u001b[0m\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\n\u001b[32m2023-08-29 01:38:24.608\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m448\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-29 01:38:24.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mdata files: ./data/augment_staff298_qa70and14/augment_0715_qa_small.json, ./data/augment_staff298_qa70and14/refuse_phone.json, ./data/augment_staff298_qa70and14/staff_short_answer.json, ./data/augment_staff298_qa70and14/augment_0715_qa70.json\u001b[0m\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-e037413ec2ab6c33/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\nDownloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 4044.65it/s]\nExtracting data files: 100%|██████████████████████| 1/1 [00:00<00:00, 66.61it/s]\nDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-e037413ec2ab6c33/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 69.21it/s]\n\u001b[32m2023-08-29 01:38:25.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1m在取样之前 data len =2854\u001b[0m\n\u001b[32m2023-08-29 01:38:25.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1m在取样之后 data len =2854\u001b[0m\n\u001b[32m2023-08-29 01:38:25.072\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 470.79it/s]\n\u001b[32m2023-08-29 01:38:25.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1m在取样之前 data len =2854\u001b[0m\n\u001b[32m2023-08-29 01:38:25.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1m在取样之后 data len =2854\u001b[0m\n\u001b[32m2023-08-29 01:38:25.085\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n\u001b[32m2023-08-29 01:38:27.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mdata files: ./data/augment_staff298_qa70and14/augment_0715_qa_small.json, ./data/augment_staff298_qa70and14/refuse_phone.json, ./data/augment_staff298_qa70and14/staff_short_answer.json, ./data/augment_staff298_qa70and14/augment_0715_qa70.json\u001b[0m\n\u001b[32m2023-08-29 01:38:27.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m127\u001b[0m - \u001b[1mdata files: ./data/augment_staff298_qa70and14/augment_0715_qa_small.json, ./data/augment_staff298_qa70and14/refuse_phone.json, ./data/augment_staff298_qa70and14/staff_short_answer.json, ./data/augment_staff298_qa70and14/augment_0715_qa70.json\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 689.51it/s]\n\u001b[32m2023-08-29 01:38:28.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1m在取样之前 data len =2854\u001b[0m\n\u001b[32m2023-08-29 01:38:28.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1m在取样之后 data len =100\u001b[0m\n\u001b[32m2023-08-29 01:38:28.233\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 544.50it/s]\n\u001b[32m2023-08-29 01:38:28.277\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m133\u001b[0m - \u001b[1m在取样之前 data len =2854\u001b[0m\n\u001b[32m2023-08-29 01:38:28.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m137\u001b[0m - \u001b[1m在取样之后 data len =100\u001b[0m\n\u001b[32m2023-08-29 01:38:28.280\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m141\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\nnumber_train_samples=2854                                                       \nnumber_of_eval_numbers=100\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=100,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=1e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output_yungaun_0827_v1,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=20.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output_yungaun_0827_v1,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=2,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=100,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\nnumber_train_samples=2854                                                       \nnumber_of_eval_numbers=100\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=100,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=1e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=1,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output_yungaun_0827_v1,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=20.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output_yungaun_0827_v1,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=2,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=100,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\n{'loss': 5.5514, 'learning_rate': 1e-05, 'epoch': 0.01}                         \n{'loss': 4.1247, 'learning_rate': 9.993693062368606e-06, 'epoch': 0.03}         \n{'loss': 3.5796, 'learning_rate': 9.986685353889279e-06, 'epoch': 0.04}         \n{'loss': 3.2023, 'learning_rate': 9.979677645409952e-06, 'epoch': 0.06}         \n{'loss': 2.521, 'learning_rate': 9.972669936930625e-06, 'epoch': 0.07}          \n{'loss': 1.7666, 'learning_rate': 9.965662228451297e-06, 'epoch': 0.08}         \n{'loss': 1.7586, 'learning_rate': 9.95865451997197e-06, 'epoch': 0.1}           \n{'loss': 1.5191, 'learning_rate': 9.951646811492643e-06, 'epoch': 0.11}         \n{'loss': 1.5243, 'learning_rate': 9.944639103013314e-06, 'epoch': 0.13}         \n{'loss': 1.286, 'learning_rate': 9.937631394533989e-06, 'epoch': 0.14}          \n  1%|▎                                    | 100/14280 [01:44<3:45:23,  1.05it/s]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  5.88it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.47it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:06,  3.07it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:07,  2.85it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:02<00:07,  2.53it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:07,  2.44it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:06,  2.49it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:03<00:06,  2.62it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  2.67it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:04<00:05,  2.55it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:04<00:05,  2.45it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:05,  2.38it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:05<00:04,  2.45it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:05<00:03,  2.59it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:06<00:03,  2.48it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:06<00:03,  2.58it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:06<00:02,  2.59it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:07<00:02,  2.68it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:07<00:01,  2.60it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:08<00:01,  2.44it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:08<00:01,  2.51it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:08<00:00,  2.44it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:09<00:00,  2.52it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 3.1524431705474854, 'eval_runtime': 9.8777, 'eval_samples_per_second': 10.124, 'eval_steps_per_second': 2.531, 'epoch': 0.14}\n  1%|▎                                    | 100/14280 [01:54<3:45:23,  1.05it/s]\n100%|███████████████████████████████████████████| 25/25 [00:09<00:00,  2.60it/s]\u001b[A\n                                                                                \u001b[ABegin to save...\nthis process is not main process , do not save model.[for distributed training scenario]\nBegin to save...\n\u001b[32m2023-08-29 01:40:25.382\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m189\u001b[0m - \u001b[31m\u001b[1mSave done.\u001b[0m\n{'loss': 1.7069, 'learning_rate': 9.930623686054662e-06, 'epoch': 0.15}         \n{'loss': 0.6679, 'learning_rate': 9.923615977575334e-06, 'epoch': 0.17}         \n{'loss': 1.2865, 'learning_rate': 9.916608269096006e-06, 'epoch': 0.18}         \n{'loss': 1.0086, 'learning_rate': 9.909600560616678e-06, 'epoch': 0.2}          \n{'loss': 0.9681, 'learning_rate': 9.902592852137351e-06, 'epoch': 0.21}         \n{'loss': 1.1254, 'learning_rate': 9.895585143658024e-06, 'epoch': 0.22}         \n{'loss': 1.1543, 'learning_rate': 9.888577435178699e-06, 'epoch': 0.24}         \n{'loss': 1.002, 'learning_rate': 9.88156972669937e-06, 'epoch': 0.25}           \n{'loss': 1.0945, 'learning_rate': 9.874562018220043e-06, 'epoch': 0.27}         \n{'loss': 1.0868, 'learning_rate': 9.867554309740715e-06, 'epoch': 0.28}         \n  1%|▌                                    | 200/14280 [03:41<4:11:13,  1.07s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  5.97it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.63it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:06,  3.22it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:06,  2.93it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:02<00:07,  2.57it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:07,  2.50it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:06,  2.55it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:03<00:05,  2.69it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  2.75it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:05,  2.62it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:04<00:05,  2.52it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.43it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:05<00:04,  2.50it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:05<00:03,  2.66it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.53it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:06<00:03,  2.62it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:06<00:02,  2.64it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:06<00:02,  2.72it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:07<00:01,  2.65it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:07<00:01,  2.51it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:08<00:01,  2.57it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:08<00:00,  2.64it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:08<00:00,  2.69it/s]\u001b[A\n100%|███████████████████████████████████████████| 25/25 [00:09<00:00,  2.74it/s]\u001b[A\n{'eval_loss': 1.4490739107131958, 'eval_runtime': 9.5995, 'eval_samples_per_second': 10.417, 'eval_steps_per_second': 2.604, 'epoch': 0.28}\n\n  1%|▌                                    | 200/14280 [03:51<4:11:13,  1.07s/it]\u001b[A\n                                                                                \u001b[ABegin to save...\nBegin to save...\nthis process is not main process , do not save model.[for distributed training scenario]\n\u001b[32m2023-08-29 01:42:22.272\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m189\u001b[0m - \u001b[31m\u001b[1mSave done.\u001b[0m\n{'loss': 0.7467, 'learning_rate': 9.860546601261388e-06, 'epoch': 0.29}         \n{'loss': 0.6247, 'learning_rate': 9.853538892782061e-06, 'epoch': 0.31}         \n{'loss': 0.4765, 'learning_rate': 9.846531184302734e-06, 'epoch': 0.32}         \n{'loss': 0.9461, 'learning_rate': 9.839523475823407e-06, 'epoch': 0.34}         \n{'loss': 0.7514, 'learning_rate': 9.83251576734408e-06, 'epoch': 0.35}          \n{'loss': 0.9371, 'learning_rate': 9.825508058864752e-06, 'epoch': 0.36}         \n{'loss': 0.6982, 'learning_rate': 9.818500350385425e-06, 'epoch': 0.38}         \n{'loss': 0.753, 'learning_rate': 9.811492641906098e-06, 'epoch': 0.39}          \n{'loss': 0.7664, 'learning_rate': 9.80448493342677e-06, 'epoch': 0.41}          \n{'loss': 0.4854, 'learning_rate': 9.797477224947442e-06, 'epoch': 0.42}         \n  2%|▊                                    | 300/14280 [05:38<3:58:26,  1.02s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  5.88it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.55it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:06,  3.18it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:06,  2.91it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:02<00:07,  2.56it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:07,  2.48it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:06,  2.52it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:03<00:05,  2.67it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  2.73it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:05,  2.60it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:04<00:05,  2.49it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.42it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:05<00:04,  2.49it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:05<00:03,  2.63it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.51it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:06<00:03,  2.60it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:06<00:02,  2.61it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:07<00:02,  2.67it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:07<00:01,  2.62it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:07<00:01,  2.47it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:08<00:01,  2.55it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:08<00:00,  2.63it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:08<00:00,  2.67it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.4569791555404663, 'eval_runtime': 9.687, 'eval_samples_per_second': 10.323, 'eval_steps_per_second': 2.581, 'epoch': 0.42}\n  2%|▊                                    | 300/14280 [05:48<3:58:26,  1.02s/it]\n100%|███████████████████████████████████████████| 25/25 [00:09<00:00,  2.71it/s]\u001b[A\n                                                                                \u001b[ABegin to save...\nthis process is not main process , do not save model.[for distributed training scenario]\nBegin to save...\n\u001b[32m2023-08-29 01:44:19.413\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m189\u001b[0m - \u001b[31m\u001b[1mSave done.\u001b[0m\n{'loss': 0.3371, 'learning_rate': 9.790469516468117e-06, 'epoch': 0.43}         \n{'loss': 0.7718, 'learning_rate': 9.78346180798879e-06, 'epoch': 0.45}          \n{'loss': 0.4415, 'learning_rate': 9.77645409950946e-06, 'epoch': 0.46}          \n{'loss': 0.5628, 'learning_rate': 9.769446391030133e-06, 'epoch': 0.48}         \n{'loss': 0.4214, 'learning_rate': 9.762438682550806e-06, 'epoch': 0.49}         \n{'loss': 0.6865, 'learning_rate': 9.755430974071479e-06, 'epoch': 0.5}          \n{'loss': 0.5248, 'learning_rate': 9.748423265592152e-06, 'epoch': 0.52}         \n{'loss': 0.4015, 'learning_rate': 9.741415557112825e-06, 'epoch': 0.53}         \n{'loss': 0.3184, 'learning_rate': 9.734407848633498e-06, 'epoch': 0.55}         \n{'loss': 0.2098, 'learning_rate': 9.72740014015417e-06, 'epoch': 0.56}          \n  3%|█                                    | 400/14280 [07:35<3:38:44,  1.06it/s]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  5.95it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.63it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:06,  3.21it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:06,  2.92it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:02<00:07,  2.56it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:07,  2.49it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:06,  2.52it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:03<00:05,  2.67it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  2.73it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:05,  2.60it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:04<00:05,  2.50it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.42it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:05<00:04,  2.49it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:05<00:03,  2.64it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.51it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:06<00:03,  2.60it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:06<00:02,  2.63it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:07<00:02,  2.71it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:07<00:01,  2.65it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:07<00:01,  2.43it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:08<00:01,  2.58it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:08<00:00,  2.58it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:08<00:00,  2.62it/s]\u001b[A\n100%|███████████████████████████████████████████| 25/25 [00:09<00:00,  2.76it/s]\u001b[A\n{'eval_loss': 0.3220676779747009, 'eval_runtime': 9.6594, 'eval_samples_per_second': 10.353, 'eval_steps_per_second': 2.588, 'epoch': 0.56}\n\n  3%|█                                    | 400/14280 [07:45<3:38:44,  1.06it/s]\u001b[A\n                                                                                \u001b[ABegin to save...\nthis process is not main process , do not save model.[for distributed training scenario]\nBegin to save...\n\u001b[32m2023-08-29 01:46:16.686\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m189\u001b[0m - \u001b[31m\u001b[1mSave done.\u001b[0m\n{'loss': 0.4954, 'learning_rate': 9.720392431674843e-06, 'epoch': 0.57}         \n{'loss': 0.3487, 'learning_rate': 9.713384723195516e-06, 'epoch': 0.59}         \n{'loss': 0.3075, 'learning_rate': 9.706377014716189e-06, 'epoch': 0.6}          \n{'loss': 0.3381, 'learning_rate': 9.699369306236862e-06, 'epoch': 0.62}         \n{'loss': 0.2661, 'learning_rate': 9.692361597757533e-06, 'epoch': 0.63}         \n{'loss': 0.2382, 'learning_rate': 9.685353889278208e-06, 'epoch': 0.64}         \n{'loss': 0.2481, 'learning_rate': 9.67834618079888e-06, 'epoch': 0.66}          \n{'loss': 0.3201, 'learning_rate': 9.671338472319553e-06, 'epoch': 0.67}         \n{'loss': 0.1782, 'learning_rate': 9.664330763840224e-06, 'epoch': 0.69}         \n{'loss': 0.15, 'learning_rate': 9.657323055360897e-06, 'epoch': 0.7}            \n  4%|█▎                                   | 500/14280 [09:37<3:51:10,  1.01s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:03,  5.96it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.62it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:01<00:06,  3.22it/s]\u001b[A\n 20%|████████▊                                   | 5/25 [00:01<00:06,  2.92it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:02<00:07,  2.57it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:02<00:07,  2.49it/s]\u001b[A\n 32%|██████████████                              | 8/25 [00:02<00:06,  2.54it/s]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:03<00:05,  2.69it/s]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:03<00:05,  2.74it/s]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:03<00:05,  2.62it/s]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:04<00:05,  2.50it/s]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:04<00:04,  2.43it/s]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:05<00:04,  2.49it/s]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:05<00:03,  2.64it/s]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:05<00:03,  2.53it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:06<00:03,  2.62it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:06<00:02,  2.62it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:07<00:02,  2.70it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:07<00:01,  2.64it/s]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:07<00:01,  2.50it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:08<00:01,  2.55it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:08<00:00,  2.63it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:08<00:00,  2.67it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.07714197784662247, 'eval_runtime': 9.6447, 'eval_samples_per_second': 10.368, 'eval_steps_per_second': 2.592, 'epoch': 0.7}\n  4%|█▎                                   | 500/14280 [09:47<3:51:10,  1.01s/it]\n100%|███████████████████████████████████████████| 25/25 [00:09<00:00,  2.71it/s]\u001b[A\n                                                                                \u001b[ABegin to save...\nBegin to save...\nthis process is not main process , do not save model.[for distributed training scenario]\n\u001b[32m2023-08-29 01:48:18.205\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m189\u001b[0m - \u001b[31m\u001b[1mSave done.\u001b[0m\n{'loss': 0.1811, 'learning_rate': 9.65031534688157e-06, 'epoch': 0.71}          \n{'loss': 0.1331, 'learning_rate': 9.643307638402243e-06, 'epoch': 0.73}         \n{'loss': 0.1416, 'learning_rate': 9.636299929922916e-06, 'epoch': 0.74}         \n  4%|█▍                                   | 535/14280 [10:27<4:24:03,  1.15s/it]^C\n[2023-08-29 01:48:57,642] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 359\nTraceback (most recent call last):\n  File \"/kaggle/working/chatGLM-6B-QLoRA/train_qlora_deepspeed_zero.py\", line 488, in <module>\n    train(args)\n  File \"/kaggle/working/chatGLM-6B-QLoRA/train_qlora_deepspeed_zero.py\", line 482, in train\n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1645, in train\n    return inner_training_loop(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1938, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2770, in training_step\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone -b medGPT_0828  https://github.com/valkryhx/MedicalGPT","metadata":{"execution":{"iopub.status.busy":"2023-08-29T05:52:33.865281Z","iopub.execute_input":"2023-08-29T05:52:33.865696Z","iopub.status.idle":"2023-08-29T05:52:34.962753Z","shell.execute_reply.started":"2023-08-29T05:52:33.865663Z","shell.execute_reply":"2023-08-29T05:52:34.961229Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/working\nfatal: destination path 'MedicalGPT' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd MedicalGPT\n!git status","metadata":{"execution":{"iopub.status.busy":"2023-08-29T05:53:01.085007Z","iopub.execute_input":"2023-08-29T05:53:01.085413Z","iopub.status.idle":"2023-08-29T05:53:02.135810Z","shell.execute_reply.started":"2023-08-29T05:53:01.085379Z","shell.execute_reply":"2023-08-29T05:53:02.134557Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/working/MedicalGPT\nOn branch medGPT_0828\nYour branch is up to date with 'origin/medGPT_0828'.\n\nChanges not staged for commit:\n  (use \"git add <file>...\" to update what will be committed)\n  (use \"git restore <file>...\" to discard changes in working directory)\n\t\u001b[31mmodified:   data/vocab/word_freq.txt\u001b[m\n\nUntracked files:\n  (use \"git add <file>...\" to include in what will be committed)\n\t\u001b[31mcache/\u001b[m\n\t\u001b[31moutputs-dpo-v1/\u001b[m\n\t\u001b[31moutputs-dpo-yunguan-v1/\u001b[m\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n","output_type":"stream"}]},{"cell_type":"code","source":"cat ./requirements.txt\n# 发现这项目的依赖和上面重合 不用再装一次","metadata":{"execution":{"iopub.status.busy":"2023-08-29T01:49:25.276506Z","iopub.execute_input":"2023-08-29T01:49:25.276941Z","iopub.status.idle":"2023-08-29T01:49:26.550446Z","shell.execute_reply.started":"2023-08-29T01:49:25.276899Z","shell.execute_reply":"2023-08-29T01:49:26.549116Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"loguru\ntransformers==4.30.2\nsentencepiece\ndatasets==2.12.0\ntqdm\ntensorboard\ntqdm>=4.47.0\npeft>=0.5.0\naccelerate==0.21.0\ntrl>=0.6.0\nbitsandbytes==0.39.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 参考这里 colab的dpo运行demo  是使用base model（或者sft合并后的model 仍然是base model格式）传入的\nhttps://colab.research.google.com/drive/1kMIe3pTec2snQvLBA00Br8ND1_zwy3Gr?usp=sharing","metadata":{}},{"cell_type":"markdown","source":"# 使用运管数据 ./data/reward_yunguan\n# <font color=red>注意这里没有使用--qlora True 说明是使用普通lora来训练base model的</font>  代码中会加载两次chatglm2 一次作为可训练的model  一次作为ref_model 注意不要oom","metadata":{"execution":{"iopub.status.busy":"2023-08-29T01:55:58.634621Z","iopub.execute_input":"2023-08-29T01:55:58.635007Z","iopub.status.idle":"2023-08-29T01:55:59.623378Z","shell.execute_reply.started":"2023-08-29T01:55:58.634975Z","shell.execute_reply":"2023-08-29T01:55:59.622075Z"}}},{"cell_type":"markdown","source":"# 注意dpo_training.py 代码中 optim默认=adamw_hf  在lora训练时可以 但是qlora训练一定换成paged_adamw_8bit 不然loss=0 其他指标为nan\n# 注意dpo_training.py 代码中 find_all_linear_names(peft_model, int4=False, int8=False)定义时加入了判断是否int4 or int8 量化 lora训练一般不量化 此时cls = torch.nn.Linear  注意find_all_linear_names的层<font color=red>好像比我自己写qlora时的find_all_linear_names多了一层default  今天测试好像没多这个奇怪的default层</font>","metadata":{}},{"cell_type":"markdown","source":" # --lora_rank 64 \\ --lora_alpha 32 \\ 会oom\n # 我改成 --lora_rank 8 \\ --lora_alpha 16 \\ 没有oom\n # learning_rate 默认 5e-4 太大导致loss不稳定 我换成较小的1e-5\n # warmup_steps 默认100 我改成 10\n # <font color=red>train/eval数据从./data/reward_yunguan 换成./data/reward loss瞬间下降 毕竟这个只有100条数据 而reward_yunguan有2800多条</font>\n # [未实施]optim由adamw_hf 改成 paged_adamw_32bit？ 后者是trl官网dpo例子用的","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_training.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b \\\n    --train_file_dir ./data/reward \\\n    --validation_file_dir ./data/reward \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 100 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-v1 \\\n    --target_modules all \\\n    --lora_rank 8 \\\n    --lora_alpha 16 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"execution":{"iopub.status.busy":"2023-08-29T05:53:20.993913Z","iopub.execute_input":"2023-08-29T05:53:20.994330Z","iopub.status.idle":"2023-08-29T05:53:28.858053Z","shell.execute_reply.started":"2023-08-29T05:53:20.994293Z","shell.execute_reply":"2023-08-29T05:53:28.856820Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/working/MedicalGPT\nFetching origin\nAlready up to date.\n^C\nTraceback (most recent call last):\n  File \"/kaggle/working/MedicalGPT/dpo_training.py\", line 16, in <module>\n    from peft import LoraConfig, TaskType\n  File \"/opt/conda/lib/python3.10/site-packages/peft/__init__.py\", line 22, in <module>\n    from .auto import (\n  File \"/opt/conda/lib/python3.10/site-packages/peft/auto.py\", line 30, in <module>\n    from .config import PeftConfig\n  File \"/opt/conda/lib/python3.10/site-packages/peft/config.py\", line 24, in <module>\n    from .utils import CONFIG_NAME, PeftType, TaskType\n  File \"/opt/conda/lib/python3.10/site-packages/peft/utils/__init__.py\", line 22, in <module>\n    from .other import (\n  File \"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py\", line 21, in <module>\n    import accelerate\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/__init__.py\", line 3, in <module>\n    from .accelerator import Accelerator\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 35, in <module>\n    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/checkpointing.py\", line 24, in <module>\n    from .utils import (\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/__init__.py\", line 64, in <module>\n    from .modeling import (\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 30, in <module>\n    from ..state import AcceleratorState\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/state.py\", line 47, in <module>\n    if is_tpu_available(check_device=False):\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/imports.py\", line 84, in is_tpu_available\n    if torch.cuda.is_available():\n  File \"/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 107, in is_available\n    return torch._C._cuda_getDeviceCount() > 0\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# optim 换成 paged_lion_32bit    lora_rank从8换到16 似乎有可见的波动 loss从0.6到1 而且显存占用明显比paged_adamw_32bit小（14.4G） 才12.9G\n# 我试试继续增加lora_rank 到64 \n# <font color=red>可以跑 train loss好像波动大 但是eval的margin和acc倒是越来越大</font>","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_training.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-yunguan-v1 \\\n    --target_modules all \\\n    --lora_rank 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"execution":{"iopub.status.busy":"2023-08-29T06:32:25.544851Z","iopub.execute_input":"2023-08-29T06:32:25.545614Z","iopub.status.idle":"2023-08-29T06:56:58.509782Z","shell.execute_reply.started":"2023-08-29T06:32:25.545572Z","shell.execute_reply":"2023-08-29T06:56:58.508301Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"/kaggle/working/MedicalGPT\nFetching origin\nremote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (3/3), 833 bytes | 277.00 KiB/s, done.\nFrom https://github.com/valkryhx/MedicalGPT\n   8138fc4..317eeb7  medGPT_0828 -> origin/medGPT_0828\nUpdating 8138fc4..317eeb7\nFast-forward\n dpo_training.py | 4 \u001b[32m++++\u001b[m\n 1 file changed, 4 insertions(+)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[2023-08-29 06:32:33,054] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n\u001b[32m2023-08-29 06:32:39.927\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m239\u001b[0m - \u001b[1mParse args: ScriptArguments(model_type='chatglm', model_name_or_path='THUDM/chatglm2-6b', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir='./cache', use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True, dataset_name=None, dataset_config_name=None, train_file_dir='./data/reward_yunguan', validation_file_dir='./data/reward_yunguan', template_name='vicuna', per_device_train_batch_size=1, per_device_eval_batch_size=1, max_source_length=256, max_target_length=128, min_target_length=4, max_train_samples=1000, max_eval_samples=20, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=4, use_peft=True, qlora=False, target_modules='all', lora_rank=64, lora_dropout=0.05, lora_alpha=32.0, peft_path=None, do_train=True, do_eval=True, beta=0.1, learning_rate=1e-05, lr_scheduler_type='cosine', warmup_steps=10, weight_decay=0.05, optim='paged_lion_32bit', fp16=True, bf16=False, gradient_checkpointing=True, gradient_accumulation_steps=4, save_total_limit=2, load_best_model_at_end=True, save_steps=40, eval_steps=10, logging_steps=1, output_dir='outputs-dpo-yunguan-v1', max_steps=200, eval_strategy='steps', remove_unused_columns=False, report_to='tensorboard')\u001b[0m\n\u001b[32m2023-08-29 06:32:40.158\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mtrain files: ./data/reward_yunguan/paired_yunguan.json\u001b[0m\n\u001b[32m2023-08-29 06:32:40.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m288\u001b[0m - \u001b[1meval files: ./data/reward_yunguan/paired_yunguan.json\u001b[0m\n100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 515.21it/s]\n\u001b[32m2023-08-29 06:32:40.350\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m309\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n    train: Dataset({\n        features: ['question', 'response_chosen', 'response_rejected'],\n        num_rows: 2855\n    })\n    validation: Dataset({\n        features: ['question', 'response_chosen', 'response_rejected'],\n        num_rows: 2855\n    })\n})\u001b[0m\n\u001b[32m2023-08-29 06:32:40.352\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m327\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'question': '现在提供如下信息：\\n1. 审批出差申请的流程中，如何查询审批人的状态？ \\n\\n答：此问题是平台流程服务异常，IT会及时处理，您可以稍后再试，或上IT吧发帖反馈此问题\\n3. 负责 FMR 系统的是谁？ 请联系刘玉莎 (liuyusha@fiberhome.com)\\n4. 单位收款信息在哪里可以找到？ \\n\\n答：收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。\\n请根据提供的信息回答问题。问：4. 单位收款信息在哪里可以找到？ \\n\\n答：', 'response_chosen': '收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。', 'response_rejected': '请联系刘玉莎 (liuyusha@fiberhome.com)'}\u001b[0m\n\u001b[32m2023-08-29 06:32:40.762\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m340\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 807\u001b[0m\n\u001b[32m2023-08-29 06:32:40.762\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m341\u001b[0m - \u001b[34m\u001b[1mFirst train example:\u001b[0m\n\u001b[32m2023-08-29 06:32:40.763\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m342\u001b[0m - \u001b[34m\u001b[1mQuestion: 现在提供如下信息：\n关于系统责任人。ITPM的责任人是谁请联系黄凤 (huangfeng@fiberhome.com)\n6. 能提供具体的操作指南吗？ \n\n答：发票抬头需与费用承担公司保持一致；请再次确认费用承担公司；\n场景1：若费用由B公司承担，请重新开具发票，使发票抬头为B公司;\n场景2：若存在借用等情况，费用应由A公司承担，请选择相应委派人\n5. 国际项目经理的负责人是谁？ 请联系章宗波 (zhangzongbo@fiberhome.com)\n请根据提供的信息回答问题。问：关于系统责任人。ITPM的责任人是谁\n\nAnswer: 请联系黄凤 (huangfeng@fiberhome.com)\u001b[0m\n\u001b[32m2023-08-29 06:32:40.765\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m354\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'question': '现在提供如下信息：\\n1. 审批出差申请的流程中，如何查询审批人的状态？ \\n\\n答：此问题是平台流程服务异常，IT会及时处理，您可以稍后再试，或上IT吧发帖反馈此问题\\n3. 负责 FMR 系统的是谁？ 请联系刘玉莎 (liuyusha@fiberhome.com)\\n4. 单位收款信息在哪里可以找到？ \\n\\n答：收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。\\n请根据提供的信息回答问题。问：4. 单位收款信息在哪里可以找到？ \\n\\n答：', 'response_chosen': '收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。', 'response_rejected': '请联系刘玉莎 (liuyusha@fiberhome.com)'}\u001b[0m\n\u001b[32m2023-08-29 06:32:40.784\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m367\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 14\u001b[0m\n\u001b[32m2023-08-29 06:32:40.785\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m368\u001b[0m - \u001b[34m\u001b[1mFirst eval example:\u001b[0m\n\u001b[32m2023-08-29 06:32:40.785\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m369\u001b[0m - \u001b[34m\u001b[1mQuestion: 现在提供如下信息：\n1. 审批出差申请的流程中，如何查询审批人的状态？ \n\n答：此问题是平台流程服务异常，IT会及时处理，您可以稍后再试，或上IT吧发帖反馈此问题\n3. 负责 FMR 系统的是谁？ 请联系刘玉莎 (liuyusha@fiberhome.com)\n4. 单位收款信息在哪里可以找到？ \n\n答：收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。\n请根据提供的信息回答问题。问：4. 单位收款信息在哪里可以找到？ \n\n答：\n\nAnswer: 收款信息需报销人自行添加维护，首次添加后，下次可继续使用。报销人对本人维护的收款信息负责，若收款信息错误，审核会计将发回修改。\u001b[0m\n\u001b[32m2023-08-29 06:32:40.785\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m371\u001b[0m - \u001b[1mLoading model\u001b[0m\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:27<00:00, 12.45s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:25<00:00, 12.27s/it]\n\u001b[32m2023-08-29 06:35:43.157\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m447\u001b[0m - \u001b[31m\u001b[1mid(model)=137020010737728\u001b[0m\n\u001b[32m2023-08-29 06:35:43.158\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m448\u001b[0m - \u001b[31m\u001b[1mid(model_ref)=137020011266496\u001b[0m\n\u001b[32m2023-08-29 06:35:43.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m484\u001b[0m - \u001b[1mPeft target_modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\u001b[0m\n\u001b[32m2023-08-29 06:35:43.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m493\u001b[0m - \u001b[1mChatGLMForConditionalGeneration(\n  (transformer): ChatGLMModel(\n    (embedding): Embedding(\n      (word_embeddings): Embedding(65024, 4096)\n    )\n    (rotary_pos_emb): RotaryEmbedding()\n    (encoder): GLMTransformer(\n      (layers): ModuleList(\n        (0-27): 28 x GLMBlock(\n          (input_layernorm): RMSNorm()\n          (self_attention): SelfAttention(\n            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n            (core_attention): CoreAttention(\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n            )\n            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n          )\n          (post_attention_layernorm): RMSNorm()\n          (mlp): MLP(\n            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n          )\n        )\n      )\n      (final_layernorm): RMSNorm()\n    )\n    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n  )\n)\u001b[0m\ntrainable params: 118587392 || all params: 6362171392 || trainable%: 1.86394525851843\n\u001b[32m2023-08-29 06:37:15.671\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m510\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n  0%|                                                   | 0/200 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n{'loss': 0.6935, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': -0.0019216537475585938, 'rewards/rejected': -0.0012372017372399569, 'rewards/accuracies': 0.0, 'rewards/margins': -0.0006844520103186369, 'logps/rejected': -97.67422485351562, 'logps/chosen': -102.3606185913086, 'logits/rejected': -2.1285784244537354, 'logits/chosen': -2.1420798301696777, 'epoch': 0.0}\n{'loss': 0.6914, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0011323929065838456, 'rewards/rejected': -0.0022886276710778475, 'rewards/accuracies': 0.25, 'rewards/margins': 0.003421020694077015, 'logps/rejected': -74.89596557617188, 'logps/chosen': -168.3209228515625, 'logits/rejected': -2.2373104095458984, 'logits/chosen': -2.243072509765625, 'epoch': 0.01}\n{'loss': 0.6938, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': -0.0003963470517192036, 'rewards/rejected': 0.000968551670666784, 'rewards/accuracies': 0.0, 'rewards/margins': -0.001364898751489818, 'logps/rejected': -106.35012817382812, 'logps/chosen': -90.72676849365234, 'logits/rejected': -2.327390193939209, 'logits/chosen': -2.302342176437378, 'epoch': 0.01}\n{'loss': 0.6911, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.00031375885009765625, 'rewards/rejected': -0.00386219029314816, 'rewards/accuracies': 0.75, 'rewards/margins': 0.004175948910415173, 'logps/rejected': -122.91868591308594, 'logps/chosen': -85.00923919677734, 'logits/rejected': -2.3137447834014893, 'logits/chosen': -2.262476921081543, 'epoch': 0.02}\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -166.3670196533203, 'logps/chosen': -71.9852523803711, 'logits/rejected': -2.2815661430358887, 'logits/chosen': -2.2693264484405518, 'epoch': 0.02}\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -134.58218383789062, 'logps/chosen': -128.9730224609375, 'logits/rejected': -2.2659924030303955, 'logits/chosen': -2.2437238693237305, 'epoch': 0.03}\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -98.4620361328125, 'logps/chosen': -148.78790283203125, 'logits/rejected': -2.403538942337036, 'logits/chosen': -2.4449009895324707, 'epoch': 0.03}\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -138.17979431152344, 'logps/chosen': -124.75907897949219, 'logits/rejected': -2.4626145362854004, 'logits/chosen': -2.474393606185913, 'epoch': 0.04}\n{'loss': 0.6931, 'learning_rate': 2.0000000000000003e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -116.71502685546875, 'logps/chosen': -93.36486053466797, 'logits/rejected': -2.149590253829956, 'logits/chosen': -2.0959086418151855, 'epoch': 0.04}\n{'loss': 0.6887, 'learning_rate': 2.0000000000000003e-06, 'rewards/chosen': 0.008715629577636719, 'rewards/rejected': -0.00017585745081305504, 'rewards/accuracies': 0.75, 'rewards/margins': 0.008891486562788486, 'logps/rejected': -124.97222137451172, 'logps/chosen': -103.98553466796875, 'logits/rejected': -2.187238931655884, 'logits/chosen': -2.123025894165039, 'epoch': 0.05}\n  5%|██                                        | 10/200 [00:50<14:51,  4.69s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.89it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.49it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.17it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  2.00it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.88it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.63it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.66it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.72it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.75it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:06<00:00,  1.76it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6938377022743225, 'eval_runtime': 8.2361, 'eval_samples_per_second': 1.7, 'eval_steps_per_second': 1.7, 'eval_rewards/chosen': -0.00046659199870191514, 'eval_rewards/rejected': 0.0008904592250473797, 'eval_rewards/accuracies': 0.5714285969734192, 'eval_rewards/margins': -0.001357051427476108, 'eval_logps/rejected': -113.08318328857422, 'eval_logps/chosen': -103.80133819580078, 'eval_logits/rejected': -2.314068555831909, 'eval_logits/chosen': -2.3430426120758057, 'epoch': 0.05}\n  5%|██                                        | 10/200 [00:58<14:51,  4.69s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.79it/s]\u001b[A\n{'loss': 0.6914, 'learning_rate': 3e-06, 'rewards/chosen': 0.001918888185173273, 'rewards/rejected': -0.001607990125194192, 'rewards/accuracies': 0.75, 'rewards/margins': 0.0035268785431981087, 'logps/rejected': -104.27052307128906, 'logps/chosen': -88.1939926147461, 'logits/rejected': -2.1779351234436035, 'logits/chosen': -2.1798195838928223, 'epoch': 0.05}\n{'loss': 0.6921, 'learning_rate': 4.000000000000001e-06, 'rewards/chosen': 0.010631943121552467, 'rewards/rejected': 0.008528899401426315, 'rewards/accuracies': 0.25, 'rewards/margins': 0.00210304232314229, 'logps/rejected': -120.74568176269531, 'logps/chosen': -137.833251953125, 'logits/rejected': -2.0830087661743164, 'logits/chosen': -2.0835015773773193, 'epoch': 0.06}\n{'loss': 0.7082, 'learning_rate': 4.000000000000001e-06, 'rewards/chosen': 0.09460887312889099, 'rewards/rejected': 0.121698759496212, 'rewards/accuracies': 0.5, 'rewards/margins': -0.027089877054095268, 'logps/rejected': -86.66114807128906, 'logps/chosen': -96.99356842041016, 'logits/rejected': -2.369619369506836, 'logits/chosen': -2.309704542160034, 'epoch': 0.06}\n{'loss': 0.6904, 'learning_rate': 5e-06, 'rewards/chosen': 0.07320242375135422, 'rewards/rejected': 0.06764259189367294, 'rewards/accuracies': 0.75, 'rewards/margins': 0.005559826735407114, 'logps/rejected': -153.59463500976562, 'logps/chosen': -80.0596923828125, 'logits/rejected': -2.2906408309936523, 'logits/chosen': -2.287689208984375, 'epoch': 0.07}\n{'loss': 0.7106, 'learning_rate': 6e-06, 'rewards/chosen': 0.07932853698730469, 'rewards/rejected': 0.11367073655128479, 'rewards/accuracies': 0.25, 'rewards/margins': -0.034342195838689804, 'logps/rejected': -125.385498046875, 'logps/chosen': -72.45525360107422, 'logits/rejected': -2.342149257659912, 'logits/chosen': -2.31501841545105, 'epoch': 0.07}\n{'loss': 0.6866, 'learning_rate': 7e-06, 'rewards/chosen': -0.017121315002441406, 'rewards/rejected': -0.03066844865679741, 'rewards/accuracies': 0.75, 'rewards/margins': 0.013547133654356003, 'logps/rejected': -125.40754699707031, 'logps/chosen': -114.59750366210938, 'logits/rejected': -2.349266290664673, 'logits/chosen': -2.25903582572937, 'epoch': 0.08}\n{'loss': 0.6658, 'learning_rate': 8.000000000000001e-06, 'rewards/chosen': -0.04161129146814346, 'rewards/rejected': -0.10039082169532776, 'rewards/accuracies': 0.25, 'rewards/margins': 0.058779530227184296, 'logps/rejected': -111.59349822998047, 'logps/chosen': -115.39810180664062, 'logits/rejected': -2.324641466140747, 'logits/chosen': -2.316283941268921, 'epoch': 0.08}\n{'loss': 0.7501, 'learning_rate': 9e-06, 'rewards/chosen': -0.36194202303886414, 'rewards/rejected': -0.2773754298686981, 'rewards/accuracies': 0.5, 'rewards/margins': -0.08456658571958542, 'logps/rejected': -111.53948974609375, 'logps/chosen': -118.37718200683594, 'logits/rejected': -2.1883962154388428, 'logits/chosen': -2.231349468231201, 'epoch': 0.09}\n{'loss': 0.7402, 'learning_rate': 1e-05, 'rewards/chosen': -0.10702934861183167, 'rewards/rejected': -0.0249238982796669, 'rewards/accuracies': 0.25, 'rewards/margins': -0.08210545033216476, 'logps/rejected': -107.01744079589844, 'logps/chosen': -179.61251831054688, 'logits/rejected': -2.332029342651367, 'logits/chosen': -2.3627517223358154, 'epoch': 0.09}\n{'loss': 0.6878, 'learning_rate': 9.999316524962347e-06, 'rewards/chosen': 0.08916664123535156, 'rewards/rejected': 0.07849512249231339, 'rewards/accuracies': 0.75, 'rewards/margins': 0.01067152339965105, 'logps/rejected': -99.97476196289062, 'logps/chosen': -67.59458923339844, 'logits/rejected': -2.415463447570801, 'logits/chosen': -2.3758697509765625, 'epoch': 0.1}\n 10%|████▏                                     | 20/200 [01:48<15:39,  5.22s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.69it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.37it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.08it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.91it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.83it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.80it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.76it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.59it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.62it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:06<00:01,  1.67it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.69it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.71it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6944856643676758, 'eval_runtime': 8.4964, 'eval_samples_per_second': 1.648, 'eval_steps_per_second': 1.648, 'eval_rewards/chosen': 0.017901692539453506, 'eval_rewards/rejected': 0.00985687505453825, 'eval_rewards/accuracies': 0.4285714328289032, 'eval_rewards/margins': 0.008044813759624958, 'eval_logps/rejected': -112.98590850830078, 'eval_logps/chosen': -103.61775970458984, 'eval_logits/rejected': -2.2993900775909424, 'eval_logits/chosen': -2.3269152641296387, 'epoch': 0.1}\n 10%|████▏                                     | 20/200 [01:57<15:39,  5.22s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.7452, 'learning_rate': 9.99726628670463e-06, 'rewards/chosen': 0.11124276369810104, 'rewards/rejected': 0.20906467735767365, 'rewards/accuracies': 0.25, 'rewards/margins': -0.097821906208992, 'logps/rejected': -106.0499267578125, 'logps/chosen': -130.07229614257812, 'logits/rejected': -2.169130563735962, 'logits/chosen': -2.166376829147339, 'epoch': 0.1}\n{'loss': 0.6918, 'learning_rate': 9.993849845741525e-06, 'rewards/chosen': 0.018381692469120026, 'rewards/rejected': 0.01538162212818861, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0030000689439475536, 'logps/rejected': -81.2919692993164, 'logps/chosen': -207.7109832763672, 'logits/rejected': -2.201326608657837, 'logits/chosen': -2.3787529468536377, 'epoch': 0.11}\n{'loss': 0.6551, 'learning_rate': 9.989068136093873e-06, 'rewards/chosen': 0.24150648713111877, 'rewards/rejected': 0.1623561829328537, 'rewards/accuracies': 0.75, 'rewards/margins': 0.07915030419826508, 'logps/rejected': -133.18063354492188, 'logps/chosen': -85.20259857177734, 'logits/rejected': -2.359422206878662, 'logits/chosen': -2.3189613819122314, 'epoch': 0.11}\n{'loss': 0.6943, 'learning_rate': 9.98292246503335e-06, 'rewards/chosen': 0.1485918015241623, 'rewards/rejected': 0.14507122337818146, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0035205818712711334, 'logps/rejected': -127.60098266601562, 'logps/chosen': -99.94499969482422, 'logits/rejected': -2.338723659515381, 'logits/chosen': -2.338752508163452, 'epoch': 0.12}\n{'loss': 0.6509, 'learning_rate': 9.975414512725058e-06, 'rewards/chosen': 0.025324538350105286, 'rewards/rejected': -0.061762236058712006, 'rewards/accuracies': 1.0, 'rewards/margins': 0.08708677440881729, 'logps/rejected': -115.22166442871094, 'logps/chosen': -102.48136138916016, 'logits/rejected': -2.2485368251800537, 'logits/chosen': -2.214449882507324, 'epoch': 0.12}\n{'loss': 0.6801, 'learning_rate': 9.966546331768192e-06, 'rewards/chosen': 0.15830935537815094, 'rewards/rejected': 0.11269283294677734, 'rewards/accuracies': 0.5, 'rewards/margins': 0.04561652988195419, 'logps/rejected': -151.8159637451172, 'logps/chosen': -86.45504760742188, 'logits/rejected': -2.3617348670959473, 'logits/chosen': -2.381107807159424, 'epoch': 0.13}\n{'loss': 0.5772, 'learning_rate': 9.956320346634877e-06, 'rewards/chosen': 0.23022577166557312, 'rewards/rejected': -0.027568243443965912, 'rewards/accuracies': 0.75, 'rewards/margins': 0.25779399275779724, 'logps/rejected': -117.77189636230469, 'logps/chosen': -113.97484588623047, 'logits/rejected': -2.2530863285064697, 'logits/chosen': -2.2588963508605957, 'epoch': 0.13}\n{'loss': 0.6602, 'learning_rate': 9.944739353007344e-06, 'rewards/chosen': 0.2985437214374542, 'rewards/rejected': 0.22487527132034302, 'rewards/accuracies': 0.75, 'rewards/margins': 0.0736684799194336, 'logps/rejected': -119.70062255859375, 'logps/chosen': -108.93060302734375, 'logits/rejected': -2.1806986331939697, 'logits/chosen': -2.1790411472320557, 'epoch': 0.14}\n{'loss': 0.6981, 'learning_rate': 9.931806517013612e-06, 'rewards/chosen': 0.20134258270263672, 'rewards/rejected': 0.18738386034965515, 'rewards/accuracies': 0.5, 'rewards/margins': 0.013958729803562164, 'logps/rejected': -118.55982971191406, 'logps/chosen': -81.14852142333984, 'logits/rejected': -2.1378393173217773, 'logits/chosen': -2.129189968109131, 'epoch': 0.14}\n{'loss': 0.6445, 'learning_rate': 9.917525374361913e-06, 'rewards/chosen': -0.12465924769639969, 'rewards/rejected': -0.22897128760814667, 'rewards/accuracies': 0.75, 'rewards/margins': 0.10431204736232758, 'logps/rejected': -181.94822692871094, 'logps/chosen': -111.80596923828125, 'logits/rejected': -2.410860300064087, 'logits/chosen': -2.344099521636963, 'epoch': 0.15}\n 15%|██████▎                                   | 30/200 [02:48<15:21,  5.42s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.79it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.41it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.11it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.93it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.7177758812904358, 'eval_runtime': 8.4173, 'eval_samples_per_second': 1.663, 'eval_steps_per_second': 1.663, 'eval_rewards/chosen': -0.14843378961086273, 'eval_rewards/rejected': -0.20374265313148499, 'eval_rewards/accuracies': 0.3571428656578064, 'eval_rewards/margins': 0.055308904498815536, 'eval_logps/rejected': -115.12669372558594, 'eval_logps/chosen': -105.278076171875, 'eval_logits/rejected': -2.260317087173462, 'eval_logits/chosen': -2.2864701747894287, 'epoch': 0.15}\n 15%|██████▎                                   | 30/200 [02:56<15:21,  5.42s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'loss': 0.5956, 'learning_rate': 9.901899829374048e-06, 'rewards/chosen': -0.4279385507106781, 'rewards/rejected': -0.6812868714332581, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2533482611179352, 'logps/rejected': -144.64450073242188, 'logps/chosen': -133.74343872070312, 'logits/rejected': -2.3861231803894043, 'logits/chosen': -2.3697752952575684, 'epoch': 0.15}\n{'loss': 0.5511, 'learning_rate': 9.884934153917998e-06, 'rewards/chosen': -0.2551708221435547, 'rewards/rejected': -0.6660255193710327, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4108547270298004, 'logps/rejected': -146.3880615234375, 'logps/chosen': -115.25679016113281, 'logits/rejected': -1.9768139123916626, 'logits/chosen': -1.9981755018234253, 'epoch': 0.16}\n{'loss': 1.4025, 'learning_rate': 9.86663298624003e-06, 'rewards/chosen': -1.7730672359466553, 'rewards/rejected': -1.2982161045074463, 'rewards/accuracies': 0.75, 'rewards/margins': -0.4748513102531433, 'logps/rejected': -122.05397033691406, 'logps/chosen': -163.419189453125, 'logits/rejected': -2.031280994415283, 'logits/chosen': -2.0594184398651123, 'epoch': 0.16}\n{'loss': 1.2428, 'learning_rate': 9.86663298624003e-06, 'rewards/chosen': -1.7959824800491333, 'rewards/rejected': -1.1919639110565186, 'rewards/accuracies': 0.25, 'rewards/margins': -0.6040185689926147, 'logps/rejected': -75.53244018554688, 'logps/chosen': -114.48846435546875, 'logits/rejected': -2.4785499572753906, 'logits/chosen': -2.434246778488159, 'epoch': 0.17}\n{'loss': 0.9812, 'learning_rate': 9.847001329696653e-06, 'rewards/chosen': -0.9084550738334656, 'rewards/rejected': -0.5049871802330017, 'rewards/accuracies': 0.25, 'rewards/margins': -0.40346795320510864, 'logps/rejected': -106.50262451171875, 'logps/chosen': -124.36741638183594, 'logits/rejected': -2.078813314437866, 'logits/chosen': -2.0696654319763184, 'epoch': 0.17}\n{'loss': 0.7254, 'learning_rate': 9.826044551386743e-06, 'rewards/chosen': -0.4198274612426758, 'rewards/rejected': -0.39654219150543213, 'rewards/accuracies': 0.75, 'rewards/margins': -0.023285288363695145, 'logps/rejected': -127.98709869384766, 'logps/chosen': -146.69630432128906, 'logits/rejected': -2.041670799255371, 'logits/chosen': -2.098533868789673, 'epoch': 0.18}\n{'loss': 0.6666, 'learning_rate': 9.803768380684242e-06, 'rewards/chosen': -0.28659361600875854, 'rewards/rejected': -0.3524782061576843, 'rewards/accuracies': 0.25, 'rewards/margins': 0.06588459014892578, 'logps/rejected': -83.14369201660156, 'logps/chosen': -120.10918426513672, 'logits/rejected': -2.1529197692871094, 'logits/chosen': -2.1742589473724365, 'epoch': 0.18}\n{'loss': 1.1493, 'learning_rate': 9.780178907671788e-06, 'rewards/chosen': -1.063890814781189, 'rewards/rejected': -0.35053005814552307, 'rewards/accuracies': 0.0, 'rewards/margins': -0.7133607864379883, 'logps/rejected': -102.8150634765625, 'logps/chosen': -166.700439453125, 'logits/rejected': -2.166860818862915, 'logits/chosen': -2.2133264541625977, 'epoch': 0.19}\n{'loss': 0.6995, 'learning_rate': 9.755282581475769e-06, 'rewards/chosen': -0.30702248215675354, 'rewards/rejected': -0.3086793124675751, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0016568005084991455, 'logps/rejected': -75.49368286132812, 'logps/chosen': -121.30908203125, 'logits/rejected': -2.3861024379730225, 'logits/chosen': -2.374629497528076, 'epoch': 0.19}\n{'loss': 0.641, 'learning_rate': 9.729086208503174e-06, 'rewards/chosen': -0.019498243927955627, 'rewards/rejected': -0.17317695915699005, 'rewards/accuracies': 0.5, 'rewards/margins': 0.15367870032787323, 'logps/rejected': -166.86666870117188, 'logps/chosen': -95.9603500366211, 'logits/rejected': -2.349468469619751, 'logits/chosen': -2.414372205734253, 'epoch': 0.2}\n 20%|████████▍                                 | 40/200 [03:45<13:27,  5.04s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.72it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.38it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.09it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.92it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.84it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.81it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.60it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.68it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.634048581123352, 'eval_runtime': 8.4369, 'eval_samples_per_second': 1.659, 'eval_steps_per_second': 1.659, 'eval_rewards/chosen': 0.09911990910768509, 'eval_rewards/rejected': -0.057869914919137955, 'eval_rewards/accuracies': 0.5714285969734192, 'eval_rewards/margins': 0.15698981285095215, 'eval_logps/rejected': -113.6791000366211, 'eval_logps/chosen': -102.8042984008789, 'eval_logits/rejected': -2.27067232131958, 'eval_logits/chosen': -2.3015403747558594, 'epoch': 0.2}\n 20%|████████▍                                 | 40/200 [03:53<13:27,  5.04s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.6752, 'learning_rate': 9.701596950580807e-06, 'rewards/chosen': 0.14388924837112427, 'rewards/rejected': 0.06799955666065216, 'rewards/accuracies': 0.25, 'rewards/margins': 0.07588966935873032, 'logps/rejected': -200.54037475585938, 'logps/chosen': -91.75777435302734, 'logits/rejected': -2.302642583847046, 'logits/chosen': -2.2226204872131348, 'epoch': 0.2}\n{'loss': 0.7505, 'learning_rate': 9.672822322997305e-06, 'rewards/chosen': -0.03391799330711365, 'rewards/rejected': 0.0294162780046463, 'rewards/accuracies': 0.5, 'rewards/margins': -0.06333425641059875, 'logps/rejected': -97.46537780761719, 'logps/chosen': -117.61724853515625, 'logits/rejected': -2.297260284423828, 'logits/chosen': -2.270313262939453, 'epoch': 0.21}\n{'loss': 1.0448, 'learning_rate': 9.642770192448537e-06, 'rewards/chosen': 0.0960640013217926, 'rewards/rejected': 0.5583469271659851, 'rewards/accuracies': 0.25, 'rewards/margins': -0.4622829258441925, 'logps/rejected': -101.79601287841797, 'logps/chosen': -130.52001953125, 'logits/rejected': -2.2664272785186768, 'logits/chosen': -2.2235007286071777, 'epoch': 0.21}\n{'loss': 0.556, 'learning_rate': 9.611448774886925e-06, 'rewards/chosen': 0.009050272405147552, 'rewards/rejected': -0.31645917892456055, 'rewards/accuracies': 1.0, 'rewards/margins': 0.3255094587802887, 'logps/rejected': -147.33566284179688, 'logps/chosen': -97.83287048339844, 'logits/rejected': -2.288034677505493, 'logits/chosen': -2.1496095657348633, 'epoch': 0.22}\n{'loss': 0.8574, 'learning_rate': 9.578866633275289e-06, 'rewards/chosen': -0.3656940758228302, 'rewards/rejected': -0.06444845348596573, 'rewards/accuracies': 0.0, 'rewards/margins': -0.3012455999851227, 'logps/rejected': -96.15985107421875, 'logps/chosen': -125.19760131835938, 'logits/rejected': -1.964979648590088, 'logits/chosen': -2.035102128982544, 'epoch': 0.22}\n{'loss': 0.7379, 'learning_rate': 9.545032675245814e-06, 'rewards/chosen': -0.21516533195972443, 'rewards/rejected': -0.24452613294124603, 'rewards/accuracies': 0.5, 'rewards/margins': 0.029360756278038025, 'logps/rejected': -121.06071472167969, 'logps/chosen': -135.98728942871094, 'logits/rejected': -2.2739813327789307, 'logits/chosen': -2.2644522190093994, 'epoch': 0.23}\n{'loss': 0.5768, 'learning_rate': 9.509956150664796e-06, 'rewards/chosen': -0.09231939166784286, 'rewards/rejected': -0.3784334063529968, 'rewards/accuracies': 0.75, 'rewards/margins': 0.28611400723457336, 'logps/rejected': -109.9211654663086, 'logps/chosen': -104.58917236328125, 'logits/rejected': -2.1990580558776855, 'logits/chosen': -2.1594130992889404, 'epoch': 0.23}\n{'loss': 0.7182, 'learning_rate': 9.473646649103819e-06, 'rewards/chosen': -0.6633284091949463, 'rewards/rejected': -0.9150059223175049, 'rewards/accuracies': 0.5, 'rewards/margins': 0.251677542924881, 'logps/rejected': -159.4810791015625, 'logps/chosen': -136.24151611328125, 'logits/rejected': -2.3964200019836426, 'logits/chosen': -2.345308542251587, 'epoch': 0.24}\n{'loss': 0.7002, 'learning_rate': 9.43611409721806e-06, 'rewards/chosen': -0.3579499423503876, 'rewards/rejected': -0.4237062335014343, 'rewards/accuracies': 0.5, 'rewards/margins': 0.06575629115104675, 'logps/rejected': -176.96939086914062, 'logps/chosen': -104.97953796386719, 'logits/rejected': -2.18326473236084, 'logits/chosen': -2.160659074783325, 'epoch': 0.24}\n{'loss': 0.7592, 'learning_rate': 9.397368756032445e-06, 'rewards/chosen': -0.9704792499542236, 'rewards/rejected': -1.1013303995132446, 'rewards/accuracies': 0.75, 'rewards/margins': 0.130851149559021, 'logps/rejected': -147.1068115234375, 'logps/chosen': -138.86776733398438, 'logits/rejected': -2.348175525665283, 'logits/chosen': -2.294774055480957, 'epoch': 0.25}\n 25%|██████████▌                               | 50/200 [04:45<12:58,  5.19s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.76it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.40it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.10it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.93it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.84it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.81it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.64it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.71it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.73it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6666569709777832, 'eval_runtime': 8.402, 'eval_samples_per_second': 1.666, 'eval_steps_per_second': 1.666, 'eval_rewards/chosen': -0.20403626561164856, 'eval_rewards/rejected': -0.30988383293151855, 'eval_rewards/accuracies': 0.5714285969734192, 'eval_rewards/margins': 0.10584752261638641, 'eval_logps/rejected': -116.19890594482422, 'eval_logps/chosen': -105.83769989013672, 'eval_logits/rejected': -2.271559476852417, 'eval_logits/chosen': -2.296387195587158, 'epoch': 0.25}\n 25%|██████████▌                               | 50/200 [04:53<12:58,  5.19s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'loss': 0.7667, 'learning_rate': 9.357421218136387e-06, 'rewards/chosen': -0.138301283121109, 'rewards/rejected': -0.03367023915052414, 'rewards/accuracies': 0.5, 'rewards/margins': -0.10463105142116547, 'logps/rejected': -130.04006958007812, 'logps/chosen': -132.72731018066406, 'logits/rejected': -2.11200213432312, 'logits/chosen': -2.0883004665374756, 'epoch': 0.25}\n{'loss': 0.6869, 'learning_rate': 9.31628240478787e-06, 'rewards/chosen': -0.29440584778785706, 'rewards/rejected': -0.33069857954978943, 'rewards/accuracies': 0.25, 'rewards/margins': 0.03629274666309357, 'logps/rejected': -73.62582397460938, 'logps/chosen': -96.61023712158203, 'logits/rejected': -2.1718785762786865, 'logits/chosen': -2.17506742477417, 'epoch': 0.26}\n{'loss': 0.6848, 'learning_rate': 9.273963562927695e-06, 'rewards/chosen': -0.4696083068847656, 'rewards/rejected': -0.5058906674385071, 'rewards/accuracies': 0.75, 'rewards/margins': 0.03628233075141907, 'logps/rejected': -120.53440856933594, 'logps/chosen': -124.6690444946289, 'logits/rejected': -2.205211639404297, 'logits/chosen': -2.2001566886901855, 'epoch': 0.26}\n{'loss': 0.8042, 'learning_rate': 9.230476262104678e-06, 'rewards/chosen': -0.7850643396377563, 'rewards/rejected': -0.6087796092033386, 'rewards/accuracies': 0.25, 'rewards/margins': -0.17628471553325653, 'logps/rejected': -128.16188049316406, 'logps/chosen': -126.76538848876953, 'logits/rejected': -2.1186115741729736, 'logits/chosen': -2.1369876861572266, 'epoch': 0.27}\n{'loss': 0.6742, 'learning_rate': 9.185832391312644e-06, 'rewards/chosen': -0.16765108704566956, 'rewards/rejected': -0.219293013215065, 'rewards/accuracies': 0.25, 'rewards/margins': 0.05164194107055664, 'logps/rejected': -87.82640838623047, 'logps/chosen': -103.0865249633789, 'logits/rejected': -2.3954617977142334, 'logits/chosen': -2.3803822994232178, 'epoch': 0.27}\n{'loss': 0.8434, 'learning_rate': 9.140044155740102e-06, 'rewards/chosen': -0.7670438885688782, 'rewards/rejected': -0.5427091717720032, 'rewards/accuracies': 0.5, 'rewards/margins': -0.2243347465991974, 'logps/rejected': -115.99378967285156, 'logps/chosen': -150.05633544921875, 'logits/rejected': -2.510782241821289, 'logits/chosen': -2.57707142829895, 'epoch': 0.28}\n{'loss': 0.5688, 'learning_rate': 9.093124073433464e-06, 'rewards/chosen': -0.4735204577445984, 'rewards/rejected': -1.0254878997802734, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5519673824310303, 'logps/rejected': -181.6017608642578, 'logps/chosen': -145.75961303710938, 'logits/rejected': -2.069411039352417, 'logits/chosen': -2.1208040714263916, 'epoch': 0.28}\n{'loss': 0.7526, 'learning_rate': 9.045084971874738e-06, 'rewards/chosen': -0.15067176520824432, 'rewards/rejected': -0.03646650165319443, 'rewards/accuracies': 0.0, 'rewards/margins': -0.1142052710056305, 'logps/rejected': -97.3380126953125, 'logps/chosen': -116.57825469970703, 'logits/rejected': -2.1970162391662598, 'logits/chosen': -2.3049495220184326, 'epoch': 0.29}\n{'loss': 0.8901, 'learning_rate': 8.995939984474624e-06, 'rewards/chosen': -0.31505611538887024, 'rewards/rejected': 0.003915974870324135, 'rewards/accuracies': 0.5, 'rewards/margins': -0.318972110748291, 'logps/rejected': -96.93345642089844, 'logps/chosen': -129.27796936035156, 'logits/rejected': -2.349555015563965, 'logits/chosen': -2.3836660385131836, 'epoch': 0.29}\n{'loss': 0.5906, 'learning_rate': 8.94570254698197e-06, 'rewards/chosen': -0.2317361831665039, 'rewards/rejected': -0.4574998915195465, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2257637083530426, 'logps/rejected': -122.36360931396484, 'logps/chosen': -129.42791748046875, 'logits/rejected': -2.3164985179901123, 'logits/chosen': -2.2678780555725098, 'epoch': 0.3}\n 30%|████████████▌                             | 60/200 [05:42<11:52,  5.09s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.75it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.38it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.08it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.92it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.84it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.81it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.68it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.71it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5897849798202515, 'eval_runtime': 8.4537, 'eval_samples_per_second': 1.656, 'eval_steps_per_second': 1.656, 'eval_rewards/chosen': -0.30343109369277954, 'eval_rewards/rejected': -0.5907675623893738, 'eval_rewards/accuracies': 0.6428571343421936, 'eval_rewards/margins': 0.2873365581035614, 'eval_logps/rejected': -118.99662017822266, 'eval_logps/chosen': -106.82722473144531, 'eval_logits/rejected': -2.2790873050689697, 'eval_logits/chosen': -2.3046133518218994, 'epoch': 0.3}\n 30%|████████████▌                             | 60/200 [05:50<11:52,  5.09s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.8939, 'learning_rate': 8.894386393810563e-06, 'rewards/chosen': -0.7317586541175842, 'rewards/rejected': -0.4012323319911957, 'rewards/accuracies': 0.25, 'rewards/margins': -0.3305263817310333, 'logps/rejected': -115.0791015625, 'logps/chosen': -126.80056762695312, 'logits/rejected': -2.0741117000579834, 'logits/chosen': -2.1300716400146484, 'epoch': 0.3}\n{'loss': 0.5945, 'learning_rate': 8.842005554284296e-06, 'rewards/chosen': -0.32762661576271057, 'rewards/rejected': -0.5392856597900391, 'rewards/accuracies': 1.0, 'rewards/margins': 0.2116590440273285, 'logps/rejected': -126.79901123046875, 'logps/chosen': -127.64359283447266, 'logits/rejected': -2.1268115043640137, 'logits/chosen': -2.1400444507598877, 'epoch': 0.31}\n{'loss': 0.8599, 'learning_rate': 8.788574348801676e-06, 'rewards/chosen': -0.7133185267448425, 'rewards/rejected': -0.4390525817871094, 'rewards/accuracies': 0.25, 'rewards/margins': -0.27426594495773315, 'logps/rejected': -71.94491577148438, 'logps/chosen': -102.37250518798828, 'logits/rejected': -2.562676429748535, 'logits/chosen': -2.5636212825775146, 'epoch': 0.31}\n{'loss': 0.6686, 'learning_rate': 8.734107384920771e-06, 'rewards/chosen': -0.2684503197669983, 'rewards/rejected': -0.34821218252182007, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0797618106007576, 'logps/rejected': -107.5083999633789, 'logps/chosen': -137.84312438964844, 'logits/rejected': -2.261207103729248, 'logits/chosen': -2.320985794067383, 'epoch': 0.32}\n{'loss': 0.5858, 'learning_rate': 8.67861955336566e-06, 'rewards/chosen': -0.01173514872789383, 'rewards/rejected': -0.2559414803981781, 'rewards/accuracies': 0.75, 'rewards/margins': 0.24420633912086487, 'logps/rejected': -89.30864715576172, 'logps/chosen': -108.68010711669922, 'logits/rejected': -2.0021755695343018, 'logits/chosen': -2.101821184158325, 'epoch': 0.32}\n{'loss': 0.6133, 'learning_rate': 8.622126023955446e-06, 'rewards/chosen': -0.1420365273952484, 'rewards/rejected': -0.32477036118507385, 'rewards/accuracies': 0.75, 'rewards/margins': 0.18273383378982544, 'logps/rejected': -102.82149505615234, 'logps/chosen': -119.79644775390625, 'logits/rejected': -2.2083330154418945, 'logits/chosen': -2.1937882900238037, 'epoch': 0.33}\n{'loss': 0.8249, 'learning_rate': 8.564642241456986e-06, 'rewards/chosen': -0.029839135706424713, 'rewards/rejected': 0.12794151902198792, 'rewards/accuracies': 0.5, 'rewards/margins': -0.15778064727783203, 'logps/rejected': -223.67733764648438, 'logps/chosen': -132.22509765625, 'logits/rejected': -2.426008701324463, 'logits/chosen': -2.2937097549438477, 'epoch': 0.33}\n{'loss': 0.8372, 'learning_rate': 8.506183921362443e-06, 'rewards/chosen': -0.18866348266601562, 'rewards/rejected': 0.06706658005714417, 'rewards/accuracies': 0.25, 'rewards/margins': -0.2557300627231598, 'logps/rejected': -164.12332153320312, 'logps/chosen': -97.4971694946289, 'logits/rejected': -2.2525815963745117, 'logits/chosen': -2.2420077323913574, 'epoch': 0.34}\n{'loss': 0.6783, 'learning_rate': 8.446767045592829e-06, 'rewards/chosen': -0.39970722794532776, 'rewards/rejected': -0.44395333528518677, 'rewards/accuracies': 0.75, 'rewards/margins': 0.04424609988927841, 'logps/rejected': -91.77420043945312, 'logps/chosen': -134.86398315429688, 'logits/rejected': -2.168060064315796, 'logits/chosen': -2.1454813480377197, 'epoch': 0.34}\n{'loss': 0.7416, 'learning_rate': 8.386407858128707e-06, 'rewards/chosen': -0.23014917969703674, 'rewards/rejected': -0.14564847946166992, 'rewards/accuracies': 0.25, 'rewards/margins': -0.08450070023536682, 'logps/rejected': -95.42481994628906, 'logps/chosen': -94.3245849609375, 'logits/rejected': -2.117699146270752, 'logits/chosen': -2.139134645462036, 'epoch': 0.35}\n 35%|██████████████▋                           | 70/200 [06:38<10:27,  4.82s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.78it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.42it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.11it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.94it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.83it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.62it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.62it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.66it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.67it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.70it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6029277443885803, 'eval_runtime': 8.437, 'eval_samples_per_second': 1.659, 'eval_steps_per_second': 1.659, 'eval_rewards/chosen': 0.08199413865804672, 'eval_rewards/rejected': -0.13777364790439606, 'eval_rewards/accuracies': 0.7857142686843872, 'eval_rewards/margins': 0.21976779401302338, 'eval_logps/rejected': -114.47135925292969, 'eval_logps/chosen': -102.97706604003906, 'eval_logits/rejected': -2.3075125217437744, 'eval_logits/chosen': -2.338988780975342, 'epoch': 0.35}\n 35%|██████████████▋                           | 70/200 [06:47<10:27,  4.82s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.73it/s]\u001b[A\n{'loss': 0.7017, 'learning_rate': 8.325122860569241e-06, 'rewards/chosen': 0.09832152724266052, 'rewards/rejected': 0.0998561829328537, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0015346482396125793, 'logps/rejected': -134.9952392578125, 'logps/chosen': -101.25859069824219, 'logits/rejected': -2.431161642074585, 'logits/chosen': -2.341209888458252, 'epoch': 0.35}\n{'loss': 0.7052, 'learning_rate': 8.262928807620843e-06, 'rewards/chosen': -0.4544169306755066, 'rewards/rejected': -0.43840092420578003, 'rewards/accuracies': 0.5, 'rewards/margins': -0.016016006469726562, 'logps/rejected': -102.92047119140625, 'logps/chosen': -96.13658905029297, 'logits/rejected': -2.3208961486816406, 'logits/chosen': -2.3394229412078857, 'epoch': 0.36}\n{'loss': 0.5836, 'learning_rate': 8.199842702516584e-06, 'rewards/chosen': -0.36648619174957275, 'rewards/rejected': -0.6276918649673462, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2612057030200958, 'logps/rejected': -184.73931884765625, 'logps/chosen': -117.72496795654297, 'logits/rejected': -2.2298920154571533, 'logits/chosen': -2.146333694458008, 'epoch': 0.36}\n{'loss': 0.5592, 'learning_rate': 8.135881792367686e-06, 'rewards/chosen': -0.7098903059959412, 'rewards/rejected': -1.0604714155197144, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3505811095237732, 'logps/rejected': -150.81378173828125, 'logps/chosen': -88.68057250976562, 'logits/rejected': -2.4144973754882812, 'logits/chosen': -2.2397334575653076, 'epoch': 0.37}\n{'loss': 0.6961, 'learning_rate': 8.071063563448341e-06, 'rewards/chosen': -0.764286458492279, 'rewards/rejected': -0.7771908044815063, 'rewards/accuracies': 0.5, 'rewards/margins': 0.012904360890388489, 'logps/rejected': -117.7968521118164, 'logps/chosen': -122.11934661865234, 'logits/rejected': -2.404627799987793, 'logits/chosen': -2.4076175689697266, 'epoch': 0.37}\n{'loss': 0.7389, 'learning_rate': 8.005405736415127e-06, 'rewards/chosen': -0.37753239274024963, 'rewards/rejected': -0.29866278171539307, 'rewards/accuracies': 0.25, 'rewards/margins': -0.07886962592601776, 'logps/rejected': -158.72157287597656, 'logps/chosen': -89.10360717773438, 'logits/rejected': -2.254279136657715, 'logits/chosen': -2.127307415008545, 'epoch': 0.38}\n{'loss': 0.8106, 'learning_rate': 7.938926261462366e-06, 'rewards/chosen': -0.7879164814949036, 'rewards/rejected': -0.7150481939315796, 'rewards/accuracies': 0.5, 'rewards/margins': -0.07286831736564636, 'logps/rejected': -87.52706909179688, 'logps/chosen': -119.69851684570312, 'logits/rejected': -2.2044994831085205, 'logits/chosen': -2.250840187072754, 'epoch': 0.38}\n{'loss': 0.7518, 'learning_rate': 7.871643313414718e-06, 'rewards/chosen': -0.3695196211338043, 'rewards/rejected': -0.2629600465297699, 'rewards/accuracies': 0.25, 'rewards/margins': -0.10655956715345383, 'logps/rejected': -121.51233673095703, 'logps/chosen': -80.16676330566406, 'logits/rejected': -2.366947650909424, 'logits/chosen': -2.3179287910461426, 'epoch': 0.39}\n{'loss': 0.7159, 'learning_rate': 7.803575286758365e-06, 'rewards/chosen': -0.19766941666603088, 'rewards/rejected': -0.1546114981174469, 'rewards/accuracies': 0.25, 'rewards/margins': -0.04305792227387428, 'logps/rejected': -109.10250854492188, 'logps/chosen': -95.15968322753906, 'logits/rejected': -2.227935791015625, 'logits/chosen': -2.2505760192871094, 'epoch': 0.39}\n{'loss': 0.6484, 'learning_rate': 7.734740790612137e-06, 'rewards/chosen': -0.11824969947338104, 'rewards/rejected': -0.22126692533493042, 'rewards/accuracies': 0.75, 'rewards/margins': 0.10301724076271057, 'logps/rejected': -117.75838470458984, 'logps/chosen': -79.21528625488281, 'logits/rejected': -2.2921111583709717, 'logits/chosen': -2.3146653175354004, 'epoch': 0.4}\n 40%|████████████████▊                         | 80/200 [07:36<09:58,  4.99s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.75it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.38it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.10it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.94it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5877200365066528, 'eval_runtime': 8.4091, 'eval_samples_per_second': 1.665, 'eval_steps_per_second': 1.665, 'eval_rewards/chosen': 0.14839936792850494, 'eval_rewards/rejected': -0.11156190931797028, 'eval_rewards/accuracies': 0.7142857313156128, 'eval_rewards/margins': 0.2599613070487976, 'eval_logps/rejected': -114.2059555053711, 'eval_logps/chosen': -102.31415557861328, 'eval_logits/rejected': -2.2930424213409424, 'eval_logits/chosen': -2.3255209922790527, 'epoch': 0.4}\n 40%|████████████████▊                         | 80/200 [07:45<09:58,  4.99s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.5963, 'learning_rate': 7.66515864363997e-06, 'rewards/chosen': 0.2958391308784485, 'rewards/rejected': 0.008177563548088074, 'rewards/accuracies': 0.5, 'rewards/margins': 0.2876615524291992, 'logps/rejected': -128.892578125, 'logps/chosen': -98.20291137695312, 'logits/rejected': -2.266298532485962, 'logits/chosen': -2.3860549926757812, 'epoch': 0.4}\n{'loss': 0.7102, 'learning_rate': 7.594847868906076e-06, 'rewards/chosen': 0.04822817072272301, 'rewards/rejected': 0.04425162822008133, 'rewards/accuracies': 0.5, 'rewards/margins': 0.003976538777351379, 'logps/rejected': -94.65940856933594, 'logps/chosen': -141.16714477539062, 'logits/rejected': -2.1911392211914062, 'logits/chosen': -2.231396198272705, 'epoch': 0.41}\n{'loss': 0.7695, 'learning_rate': 7.52382768867422e-06, 'rewards/chosen': -0.0448760986328125, 'rewards/rejected': 0.08996257930994034, 'rewards/accuracies': 0.25, 'rewards/margins': -0.13483867049217224, 'logps/rejected': -118.1875, 'logps/chosen': -104.19668579101562, 'logits/rejected': -2.2808890342712402, 'logits/chosen': -2.163461685180664, 'epoch': 0.41}\n{'loss': 0.5979, 'learning_rate': 7.452117519152542e-06, 'rewards/chosen': 0.04151706397533417, 'rewards/rejected': -0.236273393034935, 'rewards/accuracies': 0.5, 'rewards/margins': 0.27779045701026917, 'logps/rejected': -118.37644958496094, 'logps/chosen': -129.42361450195312, 'logits/rejected': -2.190288543701172, 'logits/chosen': -2.2411131858825684, 'epoch': 0.42}\n{'loss': 0.5926, 'learning_rate': 7.379736965185369e-06, 'rewards/chosen': -0.17387476563453674, 'rewards/rejected': -0.391274631023407, 'rewards/accuracies': 1.0, 'rewards/margins': 0.21739989519119263, 'logps/rejected': -121.69223022460938, 'logps/chosen': -115.34539794921875, 'logits/rejected': -2.194821834564209, 'logits/chosen': -2.1843128204345703, 'epoch': 0.42}\n{'loss': 0.6347, 'learning_rate': 7.30670581489344e-06, 'rewards/chosen': 0.2521434724330902, 'rewards/rejected': 0.12567368149757385, 'rewards/accuracies': 0.5, 'rewards/margins': 0.12646980583667755, 'logps/rejected': -82.0815200805664, 'logps/chosen': -129.73056030273438, 'logits/rejected': -2.336522340774536, 'logits/chosen': -2.3799211978912354, 'epoch': 0.43}\n{'loss': 0.5907, 'learning_rate': 7.233044034264034e-06, 'rewards/chosen': -0.028875358402729034, 'rewards/rejected': -0.27330607175827026, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2444307506084442, 'logps/rejected': -89.65494537353516, 'logps/chosen': -105.95094299316406, 'logits/rejected': -2.165029287338257, 'logits/chosen': -2.187882900238037, 'epoch': 0.43}\n{'loss': 0.664, 'learning_rate': 7.158771761692464e-06, 'rewards/chosen': 0.21975861489772797, 'rewards/rejected': 0.10076884925365448, 'rewards/accuracies': 0.5, 'rewards/margins': 0.11898977309465408, 'logps/rejected': -113.5553970336914, 'logps/chosen': -82.4651870727539, 'logits/rejected': -2.2974905967712402, 'logits/chosen': -2.3285322189331055, 'epoch': 0.44}\n{'loss': 0.7368, 'learning_rate': 7.083909302476453e-06, 'rewards/chosen': -0.3157958984375, 'rewards/rejected': -0.2739132046699524, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0418827086687088, 'logps/rejected': -103.0902328491211, 'logps/chosen': -128.7098846435547, 'logits/rejected': -2.267637252807617, 'logits/chosen': -2.233525037765503, 'epoch': 0.44}\n{'loss': 0.5818, 'learning_rate': 7.008477123264849e-06, 'rewards/chosen': 0.06450271606445312, 'rewards/rejected': -0.19632931053638458, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2608320415019989, 'logps/rejected': -140.60879516601562, 'logps/chosen': -128.72650146484375, 'logits/rejected': -2.3647327423095703, 'logits/chosen': -2.327679395675659, 'epoch': 0.45}\n 45%|██████████████████▉                       | 90/200 [08:37<09:27,  5.16s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.73it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.39it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.09it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.91it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.83it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.81it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.60it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.62it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.68it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.70it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5744305849075317, 'eval_runtime': 8.4596, 'eval_samples_per_second': 1.655, 'eval_steps_per_second': 1.655, 'eval_rewards/chosen': -0.011951317079365253, 'eval_rewards/rejected': -0.3476598858833313, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.33570852875709534, 'eval_logps/rejected': -116.57219696044922, 'eval_logps/chosen': -103.91019439697266, 'eval_logits/rejected': -2.2654008865356445, 'eval_logits/chosen': -2.2951619625091553, 'epoch': 0.45}\n 45%|██████████████████▉                       | 90/200 [08:46<09:27,  5.16s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.9779, 'learning_rate': 6.932495846462262e-06, 'rewards/chosen': -0.4732060730457306, 'rewards/rejected': 0.02329426258802414, 'rewards/accuracies': 0.0, 'rewards/margins': -0.49650031328201294, 'logps/rejected': -113.52742767333984, 'logps/chosen': -170.2861328125, 'logits/rejected': -2.203646659851074, 'logits/chosen': -2.236123561859131, 'epoch': 0.45}\n{'loss': 0.557, 'learning_rate': 6.855986244591104e-06, 'rewards/chosen': 0.00449104979634285, 'rewards/rejected': -0.3229757249355316, 'rewards/accuracies': 0.75, 'rewards/margins': 0.32746678590774536, 'logps/rejected': -146.80734252929688, 'logps/chosen': -139.24737548828125, 'logits/rejected': -2.3866629600524902, 'logits/chosen': -2.428678512573242, 'epoch': 0.46}\n{'loss': 0.6822, 'learning_rate': 6.778969234612583e-06, 'rewards/chosen': -0.2048528641462326, 'rewards/rejected': -0.36102598905563354, 'rewards/accuracies': 0.5, 'rewards/margins': 0.15617315471172333, 'logps/rejected': -114.12565612792969, 'logps/chosen': -117.03590393066406, 'logits/rejected': -2.1086955070495605, 'logits/chosen': -2.1588358879089355, 'epoch': 0.46}\n{'loss': 0.6461, 'learning_rate': 6.701465872208216e-06, 'rewards/chosen': -0.2516515851020813, 'rewards/rejected': -0.3727443814277649, 'rewards/accuracies': 0.75, 'rewards/margins': 0.121092788875103, 'logps/rejected': -111.35701751708984, 'logps/chosen': -120.38690185546875, 'logits/rejected': -2.222804069519043, 'logits/chosen': -2.2605350017547607, 'epoch': 0.47}\n{'loss': 0.7291, 'learning_rate': 6.6234973460234184e-06, 'rewards/chosen': -0.35090065002441406, 'rewards/rejected': -0.2886175215244293, 'rewards/accuracies': 0.5, 'rewards/margins': -0.06228313967585564, 'logps/rejected': -117.16732788085938, 'logps/chosen': -78.43695068359375, 'logits/rejected': -2.2299575805664062, 'logits/chosen': -2.240114450454712, 'epoch': 0.47}\n{'loss': 0.6146, 'learning_rate': 6.545084971874738e-06, 'rewards/chosen': -0.13642291724681854, 'rewards/rejected': -0.48008766770362854, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3436647057533264, 'logps/rejected': -100.09236145019531, 'logps/chosen': -135.00408935546875, 'logits/rejected': -2.3079233169555664, 'logits/chosen': -2.2491350173950195, 'epoch': 0.48}\n{'loss': 0.6563, 'learning_rate': 6.466250186922325e-06, 'rewards/chosen': -0.3942551612854004, 'rewards/rejected': -0.47738611698150635, 'rewards/accuracies': 0.75, 'rewards/margins': 0.08313094079494476, 'logps/rejected': -169.19107055664062, 'logps/chosen': -108.52898406982422, 'logits/rejected': -2.192497730255127, 'logits/chosen': -2.1027374267578125, 'epoch': 0.48}\n{'loss': 0.5166, 'learning_rate': 6.387014543809224e-06, 'rewards/chosen': -0.3256729245185852, 'rewards/rejected': -1.0516823530197144, 'rewards/accuracies': 0.75, 'rewards/margins': 0.7260093688964844, 'logps/rejected': -185.88201904296875, 'logps/chosen': -134.02789306640625, 'logits/rejected': -2.1305322647094727, 'logits/chosen': -2.1514604091644287, 'epoch': 0.49}\n{'loss': 0.862, 'learning_rate': 6.3073997047691e-06, 'rewards/chosen': -0.3736627697944641, 'rewards/rejected': -0.09482298046350479, 'rewards/accuracies': 0.5, 'rewards/margins': -0.27883976697921753, 'logps/rejected': -134.21856689453125, 'logps/chosen': -140.59234619140625, 'logits/rejected': -2.2726035118103027, 'logits/chosen': -2.41162371635437, 'epoch': 0.49}\n{'loss': 0.6272, 'learning_rate': 6.227427435703997e-06, 'rewards/chosen': -0.09308873116970062, 'rewards/rejected': -0.28701460361480713, 'rewards/accuracies': 0.75, 'rewards/margins': 0.1939258575439453, 'logps/rejected': -115.81063079833984, 'logps/chosen': -111.91725158691406, 'logits/rejected': -2.270048141479492, 'logits/chosen': -2.2103304862976074, 'epoch': 0.5}\n 50%|████████████████████▌                    | 100/200 [09:36<08:19,  4.99s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.68it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.39it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.09it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.92it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.83it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.81it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.77it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.60it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.62it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:06<00:01,  1.68it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5492036938667297, 'eval_runtime': 8.4462, 'eval_samples_per_second': 1.658, 'eval_steps_per_second': 1.658, 'eval_rewards/chosen': 0.09976626932621002, 'eval_rewards/rejected': -0.28743189573287964, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.38719817996025085, 'eval_logps/rejected': -115.96735382080078, 'eval_logps/chosen': -102.79481506347656, 'eval_logits/rejected': -2.2666285037994385, 'eval_logits/chosen': -2.2974491119384766, 'epoch': 0.5}\n 50%|████████████████████▌                    | 100/200 [09:44<08:19,  4.99s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'loss': 0.504, 'learning_rate': 6.147119600233758e-06, 'rewards/chosen': -0.20826560258865356, 'rewards/rejected': -0.648124098777771, 'rewards/accuracies': 1.0, 'rewards/margins': 0.4398585259914398, 'logps/rejected': -103.57199096679688, 'logps/chosen': -144.52232360839844, 'logits/rejected': -2.2999587059020996, 'logits/chosen': -2.3608624935150146, 'epoch': 0.5}\n{'loss': 0.6323, 'learning_rate': 6.066498153718735e-06, 'rewards/chosen': -0.048190876841545105, 'rewards/rejected': -0.2111542820930481, 'rewards/accuracies': 0.5, 'rewards/margins': 0.1629633903503418, 'logps/rejected': -101.35612487792969, 'logps/chosen': -130.24908447265625, 'logits/rejected': -2.232198715209961, 'logits/chosen': -2.265880584716797, 'epoch': 0.51}\n{'loss': 0.6435, 'learning_rate': 5.985585137257401e-06, 'rewards/chosen': 0.38874131441116333, 'rewards/rejected': 0.17828789353370667, 'rewards/accuracies': 0.75, 'rewards/margins': 0.21045342087745667, 'logps/rejected': -132.51011657714844, 'logps/chosen': -114.73834991455078, 'logits/rejected': -2.2522542476654053, 'logits/chosen': -2.2428882122039795, 'epoch': 0.51}\n{'loss': 0.6824, 'learning_rate': 5.904402671660551e-06, 'rewards/chosen': 0.12723541259765625, 'rewards/rejected': 0.07152271270751953, 'rewards/accuracies': 0.5, 'rewards/margins': 0.05571272224187851, 'logps/rejected': -112.29330444335938, 'logps/chosen': -93.77050018310547, 'logits/rejected': -2.412010908126831, 'logits/chosen': -2.3964734077453613, 'epoch': 0.52}\n{'loss': 0.7088, 'learning_rate': 5.82297295140367e-06, 'rewards/chosen': -0.587088406085968, 'rewards/rejected': -0.5959501266479492, 'rewards/accuracies': 0.5, 'rewards/margins': 0.008861705660820007, 'logps/rejected': -126.2348861694336, 'logps/chosen': -102.46080017089844, 'logits/rejected': -2.3839592933654785, 'logits/chosen': -2.3618271350860596, 'epoch': 0.52}\n{'loss': 0.756, 'learning_rate': 5.74131823855921e-06, 'rewards/chosen': -0.08792829513549805, 'rewards/rejected': -0.00036678602918982506, 'rewards/accuracies': 0.5, 'rewards/margins': -0.08756150305271149, 'logps/rejected': -120.61567687988281, 'logps/chosen': -90.5877456665039, 'logits/rejected': -2.4091944694519043, 'logits/chosen': -2.4058890342712402, 'epoch': 0.53}\n{'loss': 0.7023, 'learning_rate': 5.659460856710346e-06, 'rewards/chosen': -0.03629055246710777, 'rewards/rejected': -0.0928570032119751, 'rewards/accuracies': 0.25, 'rewards/margins': 0.05656643211841583, 'logps/rejected': -130.48828125, 'logps/chosen': -113.90535736083984, 'logits/rejected': -2.0984272956848145, 'logits/chosen': -2.068150520324707, 'epoch': 0.53}\n{'loss': 0.5674, 'learning_rate': 5.577423184847932e-06, 'rewards/chosen': -0.11329993605613708, 'rewards/rejected': -0.40903663635253906, 'rewards/accuracies': 0.75, 'rewards/margins': 0.295736700296402, 'logps/rejected': -102.74098205566406, 'logps/chosen': -139.0262908935547, 'logits/rejected': -2.2523136138916016, 'logits/chosen': -2.3205485343933105, 'epoch': 0.54}\n{'loss': 0.8166, 'learning_rate': 5.495227651252315e-06, 'rewards/chosen': -0.20902805030345917, 'rewards/rejected': 0.011392973363399506, 'rewards/accuracies': 0.25, 'rewards/margins': -0.22042103111743927, 'logps/rejected': -109.96540832519531, 'logps/chosen': -140.22555541992188, 'logits/rejected': -2.079477310180664, 'logits/chosen': -2.0848898887634277, 'epoch': 0.54}\n{'loss': 0.7547, 'learning_rate': 5.412896727361663e-06, 'rewards/chosen': -0.15422649681568146, 'rewards/rejected': -0.08554516732692719, 'rewards/accuracies': 0.25, 'rewards/margins': -0.06868132203817368, 'logps/rejected': -98.45774841308594, 'logps/chosen': -91.62554931640625, 'logits/rejected': -2.2823572158813477, 'logits/chosen': -2.287074089050293, 'epoch': 0.55}\n 55%|██████████████████████▌                  | 110/200 [10:32<07:20,  4.89s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.78it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.40it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.11it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.93it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.71it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5524441003799438, 'eval_runtime': 8.4212, 'eval_samples_per_second': 1.662, 'eval_steps_per_second': 1.662, 'eval_rewards/chosen': 0.05793747678399086, 'eval_rewards/rejected': -0.3312292695045471, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.38916677236557007, 'eval_logps/rejected': -116.40494537353516, 'eval_logps/chosen': -103.22257232666016, 'eval_logits/rejected': -2.2633392810821533, 'eval_logits/chosen': -2.292834520339966, 'epoch': 0.55}\n 55%|██████████████████████▌                  | 110/200 [10:40<07:20,  4.89s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.8762, 'learning_rate': 5.3304529216284974e-06, 'rewards/chosen': -0.6647822856903076, 'rewards/rejected': -0.354108601808548, 'rewards/accuracies': 0.25, 'rewards/margins': -0.31067371368408203, 'logps/rejected': -142.38333129882812, 'logps/chosen': -113.67633056640625, 'logits/rejected': -2.1080832481384277, 'logits/chosen': -2.081188201904297, 'epoch': 0.55}\n{'loss': 0.582, 'learning_rate': 5.247918773366112e-06, 'rewards/chosen': -0.16947384178638458, 'rewards/rejected': -0.5149418115615845, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3454679846763611, 'logps/rejected': -127.28495788574219, 'logps/chosen': -64.9236068725586, 'logits/rejected': -2.1814990043640137, 'logits/chosen': -2.177823066711426, 'epoch': 0.56}\n{'loss': 0.5209, 'learning_rate': 5.165316846586541e-06, 'rewards/chosen': -0.11473121494054794, 'rewards/rejected': -0.510617196559906, 'rewards/accuracies': 1.0, 'rewards/margins': 0.3958859443664551, 'logps/rejected': -99.26361083984375, 'logps/chosen': -95.96941375732422, 'logits/rejected': -2.246098279953003, 'logits/chosen': -2.2224433422088623, 'epoch': 0.56}\n{'loss': 0.5913, 'learning_rate': 5.082669723831793e-06, 'rewards/chosen': -0.05599784851074219, 'rewards/rejected': -0.2765091061592102, 'rewards/accuracies': 1.0, 'rewards/margins': 0.22051125764846802, 'logps/rejected': -85.1817398071289, 'logps/chosen': -118.6495132446289, 'logits/rejected': -2.297943115234375, 'logits/chosen': -2.2446060180664062, 'epoch': 0.57}\n{'loss': 0.7794, 'learning_rate': 5e-06, 'rewards/chosen': -0.37888815999031067, 'rewards/rejected': -0.27070748805999756, 'rewards/accuracies': 0.25, 'rewards/margins': -0.10818061977624893, 'logps/rejected': -93.4561767578125, 'logps/chosen': -124.32605743408203, 'logits/rejected': -2.6136832237243652, 'logits/chosen': -2.6477766036987305, 'epoch': 0.57}\n{'loss': 0.5042, 'learning_rate': 4.917330276168208e-06, 'rewards/chosen': -0.4572904706001282, 'rewards/rejected': -0.9212554693222046, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4639650285243988, 'logps/rejected': -135.52926635742188, 'logps/chosen': -108.62969970703125, 'logits/rejected': -2.430795192718506, 'logits/chosen': -2.371744394302368, 'epoch': 0.57}\n{'loss': 0.697, 'learning_rate': 4.8346831534134595e-06, 'rewards/chosen': -0.10867710411548615, 'rewards/rejected': -0.13651971518993378, 'rewards/accuracies': 0.75, 'rewards/margins': 0.027842611074447632, 'logps/rejected': -124.94499206542969, 'logps/chosen': -125.57977294921875, 'logits/rejected': -2.155470371246338, 'logits/chosen': -2.2166411876678467, 'epoch': 0.58}\n{'loss': 0.5975, 'learning_rate': 4.752081226633888e-06, 'rewards/chosen': -0.04967441409826279, 'rewards/rejected': -0.2718934118747711, 'rewards/accuracies': 0.75, 'rewards/margins': 0.22221899032592773, 'logps/rejected': -150.3457794189453, 'logps/chosen': -109.57388305664062, 'logits/rejected': -2.2181801795959473, 'logits/chosen': -2.1698806285858154, 'epoch': 0.58}\n{'loss': 0.7341, 'learning_rate': 4.669547078371503e-06, 'rewards/chosen': -0.32206955552101135, 'rewards/rejected': -0.2506023645401001, 'rewards/accuracies': 0.5, 'rewards/margins': -0.07146719098091125, 'logps/rejected': -84.94087219238281, 'logps/chosen': -98.3436050415039, 'logits/rejected': -2.069119930267334, 'logits/chosen': -2.0939676761627197, 'epoch': 0.59}\n{'loss': 0.9169, 'learning_rate': 4.587103272638339e-06, 'rewards/chosen': -0.5439796447753906, 'rewards/rejected': -0.18151283264160156, 'rewards/accuracies': 0.25, 'rewards/margins': -0.36246684193611145, 'logps/rejected': -96.15278625488281, 'logps/chosen': -115.66669464111328, 'logits/rejected': -2.3175055980682373, 'logits/chosen': -2.2380101680755615, 'epoch': 0.59}\n 60%|████████████████████████▌                | 120/200 [11:30<06:43,  5.05s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.77it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.33it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:05,  2.00it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.84it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:03<00:04,  1.79it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.78it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.76it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.60it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.64it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:06<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.71it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.73it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5491026043891907, 'eval_runtime': 8.5001, 'eval_samples_per_second': 1.647, 'eval_steps_per_second': 1.647, 'eval_rewards/chosen': -0.028803469613194466, 'eval_rewards/rejected': -0.4537075459957123, 'eval_rewards/accuracies': 0.7857142686843872, 'eval_rewards/margins': 0.42490407824516296, 'eval_logps/rejected': -117.63745880126953, 'eval_logps/chosen': -104.0828628540039, 'eval_logits/rejected': -2.250178575515747, 'eval_logits/chosen': -2.2772886753082275, 'epoch': 0.59}\n 60%|████████████████████████▌                | 120/200 [11:38<06:43,  5.05s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'loss': 0.5842, 'learning_rate': 4.504772348747687e-06, 'rewards/chosen': -0.1878805011510849, 'rewards/rejected': -0.4244133234024048, 'rewards/accuracies': 1.0, 'rewards/margins': 0.2365328073501587, 'logps/rejected': -126.90711212158203, 'logps/chosen': -97.16021728515625, 'logits/rejected': -2.3461551666259766, 'logits/chosen': -2.3054537773132324, 'epoch': 0.6}\n{'loss': 0.6996, 'learning_rate': 4.42257681515207e-06, 'rewards/chosen': -0.0423557311296463, 'rewards/rejected': -0.037217140197753906, 'rewards/accuracies': 0.5, 'rewards/margins': -0.005138590931892395, 'logps/rejected': -91.956787109375, 'logps/chosen': -67.70832061767578, 'logits/rejected': -2.621406078338623, 'logits/chosen': -2.5619900226593018, 'epoch': 0.6}\n{'loss': 0.8823, 'learning_rate': 4.340539143289655e-06, 'rewards/chosen': -0.6115579605102539, 'rewards/rejected': -0.28153878450393677, 'rewards/accuracies': 0.0, 'rewards/margins': -0.3300192058086395, 'logps/rejected': -110.26561737060547, 'logps/chosen': -96.55545043945312, 'logits/rejected': -2.3468902111053467, 'logits/chosen': -2.3401896953582764, 'epoch': 0.61}\n{'loss': 0.7794, 'learning_rate': 4.25868176144079e-06, 'rewards/chosen': -0.7295070290565491, 'rewards/rejected': -0.5769122838973999, 'rewards/accuracies': 0.25, 'rewards/margins': -0.152594655752182, 'logps/rejected': -95.17176055908203, 'logps/chosen': -94.40348815917969, 'logits/rejected': -2.186061382293701, 'logits/chosen': -2.1705827713012695, 'epoch': 0.61}\n{'loss': 0.4946, 'learning_rate': 4.17702704859633e-06, 'rewards/chosen': -0.013461112976074219, 'rewards/rejected': -0.5295145511627197, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5160533785820007, 'logps/rejected': -138.70758056640625, 'logps/chosen': -113.85184478759766, 'logits/rejected': -2.199688196182251, 'logits/chosen': -2.1689095497131348, 'epoch': 0.62}\n{'loss': 0.7506, 'learning_rate': 4.0955973283394525e-06, 'rewards/chosen': -0.5916504263877869, 'rewards/rejected': -0.5199298858642578, 'rewards/accuracies': 0.25, 'rewards/margins': -0.07172049582004547, 'logps/rejected': -170.7754364013672, 'logps/chosen': -110.38255310058594, 'logits/rejected': -2.185206413269043, 'logits/chosen': -2.153118371963501, 'epoch': 0.62}\n{'loss': 0.533, 'learning_rate': 4.0144148627426e-06, 'rewards/chosen': 0.16573886573314667, 'rewards/rejected': -0.3094167709350586, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4751555919647217, 'logps/rejected': -111.71102142333984, 'logps/chosen': -117.17031860351562, 'logits/rejected': -2.188375473022461, 'logits/chosen': -2.1487419605255127, 'epoch': 0.63}\n{'loss': 0.5326, 'learning_rate': 3.9335018462812664e-06, 'rewards/chosen': -0.6573309302330017, 'rewards/rejected': -1.0718207359313965, 'rewards/accuracies': 0.5, 'rewards/margins': 0.41448983550071716, 'logps/rejected': -143.34906005859375, 'logps/chosen': -163.57553100585938, 'logits/rejected': -2.3688299655914307, 'logits/chosen': -2.2888145446777344, 'epoch': 0.63}\n{'loss': 0.8021, 'learning_rate': 3.852880399766243e-06, 'rewards/chosen': -0.20752602815628052, 'rewards/rejected': -0.22732488811016083, 'rewards/accuracies': 0.75, 'rewards/margins': 0.01979885995388031, 'logps/rejected': -150.42495727539062, 'logps/chosen': -130.96609497070312, 'logits/rejected': -2.2094733715057373, 'logits/chosen': -2.232966423034668, 'epoch': 0.64}\n{'loss': 0.5787, 'learning_rate': 3.7725725642960047e-06, 'rewards/chosen': -0.41008442640304565, 'rewards/rejected': -0.6784877777099609, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2684033513069153, 'logps/rejected': -116.35021209716797, 'logps/chosen': -117.28099060058594, 'logits/rejected': -2.10856556892395, 'logits/chosen': -2.0828042030334473, 'epoch': 0.64}\n 65%|██████████████████████████▋              | 130/200 [12:29<05:44,  4.93s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.74it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.41it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.12it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.94it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.86it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.64it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.68it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.68it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.70it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5713546276092529, 'eval_runtime': 8.4336, 'eval_samples_per_second': 1.66, 'eval_steps_per_second': 1.66, 'eval_rewards/chosen': -0.14049802720546722, 'eval_rewards/rejected': -0.552146852016449, 'eval_rewards/accuracies': 0.7142857313156128, 'eval_rewards/margins': 0.41164880990982056, 'eval_logps/rejected': -118.61778259277344, 'eval_logps/chosen': -105.20162200927734, 'eval_logits/rejected': -2.230299949645996, 'eval_logits/chosen': -2.25427508354187, 'epoch': 0.64}\n 65%|██████████████████████████▋              | 130/200 [12:38<05:44,  4.93s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.73it/s]\u001b[A\n{'loss': 0.9981, 'learning_rate': 3.6926002952309015e-06, 'rewards/chosen': -1.0039722919464111, 'rewards/rejected': -0.6203083992004395, 'rewards/accuracies': 0.5, 'rewards/margins': -0.3836638927459717, 'logps/rejected': -85.98194122314453, 'logps/chosen': -205.2849578857422, 'logits/rejected': -2.226928234100342, 'logits/chosen': -2.3255465030670166, 'epoch': 0.65}\n{'loss': 0.4717, 'learning_rate': 3.6129854561907786e-06, 'rewards/chosen': -0.2587640881538391, 'rewards/rejected': -0.8392797708511353, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5805156826972961, 'logps/rejected': -97.79995727539062, 'logps/chosen': -133.33663940429688, 'logits/rejected': -2.015497922897339, 'logits/chosen': -2.0499489307403564, 'epoch': 0.65}\n{'loss': 0.826, 'learning_rate': 3.533749813077677e-06, 'rewards/chosen': -0.1086919754743576, 'rewards/rejected': -0.025826558470726013, 'rewards/accuracies': 0.25, 'rewards/margins': -0.08286543190479279, 'logps/rejected': -102.94427490234375, 'logps/chosen': -107.09754180908203, 'logits/rejected': -2.205599784851074, 'logits/chosen': -2.268705129623413, 'epoch': 0.66}\n{'loss': 1.0674, 'learning_rate': 3.4549150281252635e-06, 'rewards/chosen': -0.2639055550098419, 'rewards/rejected': 0.3070606589317322, 'rewards/accuracies': 0.25, 'rewards/margins': -0.5709662437438965, 'logps/rejected': -134.70620727539062, 'logps/chosen': -125.89028930664062, 'logits/rejected': -2.273911714553833, 'logits/chosen': -2.1693763732910156, 'epoch': 0.66}\n{'loss': 0.6641, 'learning_rate': 3.3765026539765832e-06, 'rewards/chosen': -0.8195433020591736, 'rewards/rejected': -0.8927778005599976, 'rewards/accuracies': 0.75, 'rewards/margins': 0.07323455810546875, 'logps/rejected': -140.13229370117188, 'logps/chosen': -101.67398071289062, 'logits/rejected': -2.052867889404297, 'logits/chosen': -2.0487542152404785, 'epoch': 0.67}\n{'loss': 0.764, 'learning_rate': 3.298534127791785e-06, 'rewards/chosen': -0.3842884302139282, 'rewards/rejected': -0.28789177536964417, 'rewards/accuracies': 0.5, 'rewards/margins': -0.09639663994312286, 'logps/rejected': -120.60920715332031, 'logps/chosen': -116.05459594726562, 'logits/rejected': -2.103639602661133, 'logits/chosen': -2.069507122039795, 'epoch': 0.67}\n{'loss': 0.8239, 'learning_rate': 3.2210307653874175e-06, 'rewards/chosen': -0.48668423295021057, 'rewards/rejected': -0.27971383929252625, 'rewards/accuracies': 0.25, 'rewards/margins': -0.2069704383611679, 'logps/rejected': -94.65376281738281, 'logps/chosen': -126.0706787109375, 'logits/rejected': -2.036482095718384, 'logits/chosen': -2.038994550704956, 'epoch': 0.68}\n{'loss': 0.6889, 'learning_rate': 3.1440137554088957e-06, 'rewards/chosen': -0.1500801146030426, 'rewards/rejected': -0.16400966048240662, 'rewards/accuracies': 0.5, 'rewards/margins': 0.01392955332994461, 'logps/rejected': -102.80762481689453, 'logps/chosen': -122.60296630859375, 'logits/rejected': -2.202357530593872, 'logits/chosen': -2.1973745822906494, 'epoch': 0.68}\n{'loss': 0.5907, 'learning_rate': 3.06750415353774e-06, 'rewards/chosen': -0.06067008897662163, 'rewards/rejected': -0.3496454358100891, 'rewards/accuracies': 0.75, 'rewards/margins': 0.288975328207016, 'logps/rejected': -129.32162475585938, 'logps/chosen': -128.9515380859375, 'logits/rejected': -2.1137938499450684, 'logits/chosen': -2.038193941116333, 'epoch': 0.69}\n{'loss': 0.637, 'learning_rate': 2.991522876735154e-06, 'rewards/chosen': -0.9569200873374939, 'rewards/rejected': -1.2682183980941772, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3112984299659729, 'logps/rejected': -122.01348876953125, 'logps/chosen': -113.63137817382812, 'logits/rejected': -2.2338900566101074, 'logits/chosen': -2.289188861846924, 'epoch': 0.69}\n 70%|████████████████████████████▋            | 140/200 [13:27<05:01,  5.03s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.75it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.40it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.12it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.94it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5733384490013123, 'eval_runtime': 8.4068, 'eval_samples_per_second': 1.665, 'eval_steps_per_second': 1.665, 'eval_rewards/chosen': -0.07727957516908646, 'eval_rewards/rejected': -0.47656527161598206, 'eval_rewards/accuracies': 0.7142857313156128, 'eval_rewards/margins': 0.39928561449050903, 'eval_logps/rejected': -117.8591079711914, 'eval_logps/chosen': -104.57173919677734, 'eval_logits/rejected': -2.2325949668884277, 'eval_logits/chosen': -2.2571048736572266, 'epoch': 0.69}\n 70%|████████████████████████████▋            | 140/200 [13:35<05:01,  5.03s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'loss': 0.8207, 'learning_rate': 2.9160906975235493e-06, 'rewards/chosen': -0.3592282235622406, 'rewards/rejected': -0.15551146864891052, 'rewards/accuracies': 0.25, 'rewards/margins': -0.20371676981449127, 'logps/rejected': -122.23600006103516, 'logps/chosen': -140.25148010253906, 'logits/rejected': -2.3251967430114746, 'logits/chosen': -2.3710741996765137, 'epoch': 0.7}\n{'loss': 0.5682, 'learning_rate': 2.8412282383075362e-06, 'rewards/chosen': -0.17918434739112854, 'rewards/rejected': -0.4928590953350067, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3136747479438782, 'logps/rejected': -104.51468658447266, 'logps/chosen': -124.36669921875, 'logits/rejected': -2.3165814876556396, 'logits/chosen': -2.2727599143981934, 'epoch': 0.7}\n{'loss': 0.6573, 'learning_rate': 2.766955965735968e-06, 'rewards/chosen': -0.49219608306884766, 'rewards/rejected': -0.6829427480697632, 'rewards/accuracies': 0.75, 'rewards/margins': 0.19074667990207672, 'logps/rejected': -140.22572326660156, 'logps/chosen': -126.23463439941406, 'logits/rejected': -2.256582021713257, 'logits/chosen': -2.237950325012207, 'epoch': 0.71}\n{'loss': 0.5338, 'learning_rate': 2.693294185106562e-06, 'rewards/chosen': -0.19031298160552979, 'rewards/rejected': -0.5706799626350403, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3803669810295105, 'logps/rejected': -80.46182250976562, 'logps/chosen': -114.93450927734375, 'logits/rejected': -2.190629482269287, 'logits/chosen': -2.1110520362854004, 'epoch': 0.71}\n{'loss': 0.657, 'learning_rate': 2.6202630348146323e-06, 'rewards/chosen': -0.0159318745136261, 'rewards/rejected': -0.13039188086986542, 'rewards/accuracies': 0.5, 'rewards/margins': 0.11445997655391693, 'logps/rejected': -124.72073364257812, 'logps/chosen': -100.8228988647461, 'logits/rejected': -2.353508949279785, 'logits/chosen': -2.3953943252563477, 'epoch': 0.72}\n{'loss': 0.5325, 'learning_rate': 2.5478824808474613e-06, 'rewards/chosen': 0.034163013100624084, 'rewards/rejected': -0.4877764582633972, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5219395160675049, 'logps/rejected': -113.94723510742188, 'logps/chosen': -82.88286590576172, 'logits/rejected': -2.5506865978240967, 'logits/chosen': -2.6192378997802734, 'epoch': 0.72}\n{'loss': 0.9233, 'learning_rate': 2.476172311325783e-06, 'rewards/chosen': -0.6560637950897217, 'rewards/rejected': -0.4015262722969055, 'rewards/accuracies': 0.5, 'rewards/margins': -0.25453758239746094, 'logps/rejected': -147.23255920410156, 'logps/chosen': -125.783203125, 'logits/rejected': -2.055279493331909, 'logits/chosen': -2.1058430671691895, 'epoch': 0.73}\n{'loss': 0.6729, 'learning_rate': 2.4051521310939258e-06, 'rewards/chosen': -0.09241695702075958, 'rewards/rejected': -0.1455797255039215, 'rewards/accuracies': 0.75, 'rewards/margins': 0.05316277593374252, 'logps/rejected': -99.51459503173828, 'logps/chosen': -80.99408721923828, 'logits/rejected': -2.188040018081665, 'logits/chosen': -2.18747615814209, 'epoch': 0.73}\n{'loss': 0.7016, 'learning_rate': 2.3348413563600324e-06, 'rewards/chosen': -0.41151562333106995, 'rewards/rejected': -0.43556728959083557, 'rewards/accuracies': 0.5, 'rewards/margins': 0.024051658809185028, 'logps/rejected': -124.37750244140625, 'logps/chosen': -101.81924438476562, 'logits/rejected': -2.2074079513549805, 'logits/chosen': -2.2187306880950928, 'epoch': 0.74}\n{'loss': 0.808, 'learning_rate': 2.265259209387867e-06, 'rewards/chosen': -0.031808093190193176, 'rewards/rejected': 0.17047519981861115, 'rewards/accuracies': 0.25, 'rewards/margins': -0.20228329300880432, 'logps/rejected': -109.40990447998047, 'logps/chosen': -102.36497497558594, 'logits/rejected': -2.316999673843384, 'logits/chosen': -2.298165798187256, 'epoch': 0.74}\n 75%|██████████████████████████████▊          | 150/200 [14:24<04:03,  4.88s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.75it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.41it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.11it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.95it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.86it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.83it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.64it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.71it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.73it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5477612018585205, 'eval_runtime': 8.3802, 'eval_samples_per_second': 1.671, 'eval_steps_per_second': 1.671, 'eval_rewards/chosen': 0.04745468869805336, 'eval_rewards/rejected': -0.38618797063827515, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.4336426258087158, 'eval_logps/rejected': -116.95218658447266, 'eval_logps/chosen': -103.31724548339844, 'eval_logits/rejected': -2.2413876056671143, 'eval_logits/chosen': -2.2686283588409424, 'epoch': 0.74}\n 75%|██████████████████████████████▊          | 150/200 [14:33<04:03,  4.88s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'loss': 0.6495, 'learning_rate': 2.1964247132416373e-06, 'rewards/chosen': -0.11803160607814789, 'rewards/rejected': -0.27623414993286133, 'rewards/accuracies': 0.75, 'rewards/margins': 0.15820252895355225, 'logps/rejected': -109.64385986328125, 'logps/chosen': -80.39884948730469, 'logits/rejected': -2.2501068115234375, 'logits/chosen': -2.1932919025421143, 'epoch': 0.75}\n{'loss': 0.6169, 'learning_rate': 2.1283566865852824e-06, 'rewards/chosen': -0.28932610154151917, 'rewards/rejected': -0.5146116018295288, 'rewards/accuracies': 0.5, 'rewards/margins': 0.22528551518917084, 'logps/rejected': -128.5802001953125, 'logps/chosen': -141.18870544433594, 'logits/rejected': -2.077361583709717, 'logits/chosen': -2.1364099979400635, 'epoch': 0.75}\n{'loss': 0.4876, 'learning_rate': 2.061073738537635e-06, 'rewards/chosen': -0.051020413637161255, 'rewards/rejected': -0.6158731579780579, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5648527145385742, 'logps/rejected': -102.65143585205078, 'logps/chosen': -124.80823516845703, 'logits/rejected': -2.185981035232544, 'logits/chosen': -2.1578640937805176, 'epoch': 0.76}\n{'loss': 0.5262, 'learning_rate': 1.9945942635848745e-06, 'rewards/chosen': -0.01691855490207672, 'rewards/rejected': -0.4166816771030426, 'rewards/accuracies': 1.0, 'rewards/margins': 0.3997631072998047, 'logps/rejected': -129.48928833007812, 'logps/chosen': -118.8655776977539, 'logits/rejected': -2.242530345916748, 'logits/chosen': -2.2412571907043457, 'epoch': 0.76}\n{'loss': 0.7503, 'learning_rate': 1.928936436551661e-06, 'rewards/chosen': 0.16471585631370544, 'rewards/rejected': 0.23002280294895172, 'rewards/accuracies': 0.5, 'rewards/margins': -0.06530695408582687, 'logps/rejected': -89.1732177734375, 'logps/chosen': -103.87870025634766, 'logits/rejected': -2.17620849609375, 'logits/chosen': -2.1364364624023438, 'epoch': 0.77}\n{'loss': 0.5323, 'learning_rate': 1.864118207632315e-06, 'rewards/chosen': -0.6867464184761047, 'rewards/rejected': -1.0772064924240112, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3904600143432617, 'logps/rejected': -112.89700317382812, 'logps/chosen': -164.12278747558594, 'logits/rejected': -2.281097412109375, 'logits/chosen': -2.3858160972595215, 'epoch': 0.77}\n{'loss': 0.7201, 'learning_rate': 1.8001572974834169e-06, 'rewards/chosen': 0.014404874294996262, 'rewards/rejected': 0.02910805493593216, 'rewards/accuracies': 0.5, 'rewards/margins': -0.014703188091516495, 'logps/rejected': -133.76602172851562, 'logps/chosen': -89.84551239013672, 'logits/rejected': -2.209813117980957, 'logits/chosen': -2.274606943130493, 'epoch': 0.78}\n{'loss': 0.7021, 'learning_rate': 1.7370711923791567e-06, 'rewards/chosen': -0.440355122089386, 'rewards/rejected': -0.4570634961128235, 'rewards/accuracies': 0.25, 'rewards/margins': 0.016708359122276306, 'logps/rejected': -104.65005493164062, 'logps/chosen': -146.04278564453125, 'logits/rejected': -2.1464972496032715, 'logits/chosen': -2.353030204772949, 'epoch': 0.78}\n{'loss': 0.6491, 'learning_rate': 1.6748771394307584e-06, 'rewards/chosen': 0.17864112555980682, 'rewards/rejected': 0.03858375549316406, 'rewards/accuracies': 0.25, 'rewards/margins': 0.14005735516548157, 'logps/rejected': -154.8218994140625, 'logps/chosen': -119.69464111328125, 'logits/rejected': -2.1559035778045654, 'logits/chosen': -1.9812848567962646, 'epoch': 0.79}\n{'loss': 0.762, 'learning_rate': 1.6135921418712959e-06, 'rewards/chosen': 0.11944331228733063, 'rewards/rejected': 0.0019882195629179478, 'rewards/accuracies': 0.5, 'rewards/margins': 0.11745510995388031, 'logps/rejected': -109.88758850097656, 'logps/chosen': -105.59062194824219, 'logits/rejected': -2.2209060192108154, 'logits/chosen': -2.245943307876587, 'epoch': 0.79}\n 80%|████████████████████████████████▊        | 160/200 [15:22<03:28,  5.20s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.77it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.41it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.11it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.93it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.59it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.62it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.71it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5555102825164795, 'eval_runtime': 8.415, 'eval_samples_per_second': 1.664, 'eval_steps_per_second': 1.664, 'eval_rewards/chosen': 0.162082239985466, 'eval_rewards/rejected': -0.22625823318958282, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.3883405029773712, 'eval_logps/rejected': -115.35581970214844, 'eval_logps/chosen': -102.17002868652344, 'eval_logits/rejected': -2.246828556060791, 'eval_logits/chosen': -2.2756714820861816, 'epoch': 0.79}\n 80%|████████████████████████████████▊        | 160/200 [15:31<03:28,  5.20s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'loss': 0.7541, 'learning_rate': 1.5532329544071712e-06, 'rewards/chosen': -0.2508794069290161, 'rewards/rejected': -0.3301442861557007, 'rewards/accuracies': 0.5, 'rewards/margins': 0.07926490157842636, 'logps/rejected': -97.83953857421875, 'logps/chosen': -171.20623779296875, 'logits/rejected': -2.352820873260498, 'logits/chosen': -2.294919490814209, 'epoch': 0.8}\n{'loss': 0.5094, 'learning_rate': 1.4938160786375571e-06, 'rewards/chosen': 0.08949051052331924, 'rewards/rejected': -0.35974961519241333, 'rewards/accuracies': 0.75, 'rewards/margins': 0.449240118265152, 'logps/rejected': -118.29203796386719, 'logps/chosen': -160.46990966796875, 'logits/rejected': -2.130030393600464, 'logits/chosen': -2.267148494720459, 'epoch': 0.8}\n{'loss': 0.5393, 'learning_rate': 1.4353577585430152e-06, 'rewards/chosen': 0.19430390000343323, 'rewards/rejected': -0.24507008492946625, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4393739700317383, 'logps/rejected': -122.81305694580078, 'logps/chosen': -128.9703369140625, 'logits/rejected': -2.220336675643921, 'logits/chosen': -2.1469786167144775, 'epoch': 0.81}\n{'loss': 0.7111, 'learning_rate': 1.3778739760445552e-06, 'rewards/chosen': -0.3869834840297699, 'rewards/rejected': -0.3675439953804016, 'rewards/accuracies': 0.75, 'rewards/margins': -0.01943949982523918, 'logps/rejected': -124.88424682617188, 'logps/chosen': -103.70780944824219, 'logits/rejected': -2.0986804962158203, 'logits/chosen': -2.0547213554382324, 'epoch': 0.81}\n{'loss': 0.575, 'learning_rate': 1.321380446634342e-06, 'rewards/chosen': -0.13734082877635956, 'rewards/rejected': -0.4000706076622009, 'rewards/accuracies': 1.0, 'rewards/margins': 0.2627297341823578, 'logps/rejected': -134.4195098876953, 'logps/chosen': -110.25556182861328, 'logits/rejected': -2.247833251953125, 'logits/chosen': -2.2435314655303955, 'epoch': 0.82}\n{'loss': 0.465, 'learning_rate': 1.2658926150792321e-06, 'rewards/chosen': 0.20814648270606995, 'rewards/rejected': -0.33069345355033875, 'rewards/accuracies': 1.0, 'rewards/margins': 0.5388399362564087, 'logps/rejected': -175.93783569335938, 'logps/chosen': -100.13555145263672, 'logits/rejected': -2.279209852218628, 'logits/chosen': -2.1914210319519043, 'epoch': 0.82}\n{'loss': 0.5166, 'learning_rate': 1.2114256511983274e-06, 'rewards/chosen': 0.06082974001765251, 'rewards/rejected': -0.4170486629009247, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4778783917427063, 'logps/rejected': -177.69882202148438, 'logps/chosen': -82.77381896972656, 'logits/rejected': -2.154141426086426, 'logits/chosen': -2.035543203353882, 'epoch': 0.83}\n{'loss': 0.6421, 'learning_rate': 1.157994445715706e-06, 'rewards/chosen': 0.07492045313119888, 'rewards/rejected': -0.2403295487165451, 'rewards/accuracies': 0.5, 'rewards/margins': 0.3152500092983246, 'logps/rejected': -130.6972198486328, 'logps/chosen': -137.43487548828125, 'logits/rejected': -2.1437082290649414, 'logits/chosen': -2.156923294067383, 'epoch': 0.83}\n{'loss': 0.6854, 'learning_rate': 1.1056136061894386e-06, 'rewards/chosen': 0.06270560622215271, 'rewards/rejected': -0.028501495718955994, 'rewards/accuracies': 0.5, 'rewards/margins': 0.09120713174343109, 'logps/rejected': -118.74700927734375, 'logps/chosen': -100.85810852050781, 'logits/rejected': -2.190828800201416, 'logits/chosen': -2.173140048980713, 'epoch': 0.84}\n{'loss': 0.71, 'learning_rate': 1.0542974530180327e-06, 'rewards/chosen': -0.7483255863189697, 'rewards/rejected': -0.7261053323745728, 'rewards/accuracies': 0.5, 'rewards/margins': -0.022220231592655182, 'logps/rejected': -98.7616958618164, 'logps/chosen': -165.96267700195312, 'logits/rejected': -2.082031726837158, 'logits/chosen': -2.1144444942474365, 'epoch': 0.84}\n 85%|██████████████████████████████████▊      | 170/200 [16:23<02:36,  5.21s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.73it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.38it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.08it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.92it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.84it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.81it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.78it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.68it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5564833879470825, 'eval_runtime': 8.446, 'eval_samples_per_second': 1.658, 'eval_steps_per_second': 1.658, 'eval_rewards/chosen': 0.15826375782489777, 'eval_rewards/rejected': -0.23470041155815125, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.3929641842842102, 'eval_logps/rejected': -115.44031524658203, 'eval_logps/chosen': -102.21570587158203, 'eval_logits/rejected': -2.247478723526001, 'eval_logits/chosen': -2.2762558460235596, 'epoch': 0.84}\n 85%|██████████████████████████████████▊      | 170/200 [16:31<02:36,  5.21s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.6607, 'learning_rate': 1.0040600155253766e-06, 'rewards/chosen': -0.0504581481218338, 'rewards/rejected': -0.2607080638408661, 'rewards/accuracies': 0.5, 'rewards/margins': 0.2102499157190323, 'logps/rejected': -137.0328826904297, 'logps/chosen': -90.78585815429688, 'logits/rejected': -2.331476926803589, 'logits/chosen': -2.2835934162139893, 'epoch': 0.85}\n{'loss': 0.5333, 'learning_rate': 9.549150281252633e-07, 'rewards/chosen': 0.0032985731959342957, 'rewards/rejected': -0.3680528402328491, 'rewards/accuracies': 1.0, 'rewards/margins': 0.371351420879364, 'logps/rejected': -153.6200408935547, 'logps/chosen': -115.32137298583984, 'logits/rejected': -2.1544857025146484, 'logits/chosen': -2.1701390743255615, 'epoch': 0.85}\n{'loss': 0.4084, 'learning_rate': 9.068759265665384e-07, 'rewards/chosen': 0.3637876510620117, 'rewards/rejected': -0.3599754273891449, 'rewards/accuracies': 1.0, 'rewards/margins': 0.723763108253479, 'logps/rejected': -110.73251342773438, 'logps/chosen': -111.91661071777344, 'logits/rejected': -2.2682275772094727, 'logits/chosen': -2.2657692432403564, 'epoch': 0.86}\n{'loss': 0.6506, 'learning_rate': 8.599558442598998e-07, 'rewards/chosen': 0.0253390334546566, 'rewards/rejected': -0.06671313941478729, 'rewards/accuracies': 0.5, 'rewards/margins': 0.09205217659473419, 'logps/rejected': -85.79058837890625, 'logps/chosen': -109.76498413085938, 'logits/rejected': -2.162346363067627, 'logits/chosen': -2.1389787197113037, 'epoch': 0.86}\n{'loss': 0.7881, 'learning_rate': 8.141676086873574e-07, 'rewards/chosen': -0.44062289595603943, 'rewards/rejected': -0.3059154748916626, 'rewards/accuracies': 0.5, 'rewards/margins': -0.13470743596553802, 'logps/rejected': -84.67840576171875, 'logps/chosen': -118.59027099609375, 'logits/rejected': -2.4115469455718994, 'logits/chosen': -2.3914690017700195, 'epoch': 0.87}\n{'loss': 1.044, 'learning_rate': 7.695237378953224e-07, 'rewards/chosen': -0.2620542645454407, 'rewards/rejected': 0.29337942600250244, 'rewards/accuracies': 0.25, 'rewards/margins': -0.5554336309432983, 'logps/rejected': -101.819580078125, 'logps/chosen': -114.35030364990234, 'logits/rejected': -2.036181926727295, 'logits/chosen': -2.0605666637420654, 'epoch': 0.87}\n{'loss': 0.8024, 'learning_rate': 7.260364370723044e-07, 'rewards/chosen': -0.0886305719614029, 'rewards/rejected': 0.09035491943359375, 'rewards/accuracies': 0.25, 'rewards/margins': -0.17898550629615784, 'logps/rejected': -113.32881927490234, 'logps/chosen': -96.23834228515625, 'logits/rejected': -2.2053327560424805, 'logits/chosen': -2.2310426235198975, 'epoch': 0.88}\n{'loss': 0.502, 'learning_rate': 6.837175952121305e-07, 'rewards/chosen': -0.11014670878648758, 'rewards/rejected': -0.5714784860610962, 'rewards/accuracies': 0.75, 'rewards/margins': 0.4613317549228668, 'logps/rejected': -127.86273956298828, 'logps/chosen': -151.5797576904297, 'logits/rejected': -2.416029691696167, 'logits/chosen': -2.335081100463867, 'epoch': 0.88}\n{'loss': 0.6988, 'learning_rate': 6.425787818636131e-07, 'rewards/chosen': -0.19667282700538635, 'rewards/rejected': -0.2549058794975281, 'rewards/accuracies': 0.75, 'rewards/margins': 0.058233074843883514, 'logps/rejected': -93.03540802001953, 'logps/chosen': -134.44720458984375, 'logits/rejected': -2.2769775390625, 'logits/chosen': -2.282745599746704, 'epoch': 0.89}\n{'loss': 0.5881, 'learning_rate': 6.026312439675553e-07, 'rewards/chosen': 0.07004242390394211, 'rewards/rejected': -0.17274609208106995, 'rewards/accuracies': 0.75, 'rewards/margins': 0.24278852343559265, 'logps/rejected': -129.76695251464844, 'logps/chosen': -168.041748046875, 'logits/rejected': -2.2671358585357666, 'logits/chosen': -2.335000514984131, 'epoch': 0.89}\n 90%|████████████████████████████████████▉    | 180/200 [17:21<01:41,  5.08s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.76it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.35it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.03it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.86it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:03<00:04,  1.79it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.79it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.76it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.60it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.63it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:06<00:01,  1.68it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.70it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.71it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5557319521903992, 'eval_runtime': 8.4947, 'eval_samples_per_second': 1.648, 'eval_steps_per_second': 1.648, 'eval_rewards/chosen': 0.14448155462741852, 'eval_rewards/rejected': -0.256437212228775, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.40091878175735474, 'eval_logps/rejected': -115.64654541015625, 'eval_logps/chosen': -102.35057830810547, 'eval_logits/rejected': -2.246309995651245, 'eval_logits/chosen': -2.274465560913086, 'epoch': 0.89}\n 90%|████████████████████████████████████▉    | 180/200 [17:29<01:41,  5.08s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.74it/s]\u001b[A\n{'loss': 0.8294, 'learning_rate': 5.63885902781941e-07, 'rewards/chosen': -0.1768035888671875, 'rewards/rejected': 0.0492643341422081, 'rewards/accuracies': 0.5, 'rewards/margins': -0.2260679304599762, 'logps/rejected': -112.95271301269531, 'logps/chosen': -92.77352142333984, 'logits/rejected': -2.181387186050415, 'logits/chosen': -2.1638786792755127, 'epoch': 0.9}\n{'loss': 0.6697, 'learning_rate': 5.263533508961827e-07, 'rewards/chosen': -0.12769317626953125, 'rewards/rejected': -0.27000296115875244, 'rewards/accuracies': 0.5, 'rewards/margins': 0.1423097401857376, 'logps/rejected': -116.63251495361328, 'logps/chosen': -150.8187713623047, 'logits/rejected': -2.2391867637634277, 'logits/chosen': -2.2015187740325928, 'epoch': 0.9}\n{'loss': 0.5878, 'learning_rate': 4.900438493352056e-07, 'rewards/chosen': -0.17398393154144287, 'rewards/rejected': -0.44265443086624146, 'rewards/accuracies': 0.5, 'rewards/margins': 0.2686704993247986, 'logps/rejected': -121.06477355957031, 'logps/chosen': -108.74259948730469, 'logits/rejected': -2.1355035305023193, 'logits/chosen': -2.193474531173706, 'epoch': 0.91}\n{'loss': 0.6036, 'learning_rate': 4.549673247541875e-07, 'rewards/chosen': -0.3336694836616516, 'rewards/rejected': -0.5389828085899353, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2053132951259613, 'logps/rejected': -140.68101501464844, 'logps/chosen': -108.93353271484375, 'logits/rejected': -2.013340950012207, 'logits/chosen': -1.9903185367584229, 'epoch': 0.91}\n{'loss': 0.5116, 'learning_rate': 4.211333667247125e-07, 'rewards/chosen': 0.048481330275535583, 'rewards/rejected': -0.4853011965751648, 'rewards/accuracies': 0.5, 'rewards/margins': 0.5337825417518616, 'logps/rejected': -118.19440460205078, 'logps/chosen': -128.12075805664062, 'logits/rejected': -2.2299413681030273, 'logits/chosen': -2.266401529312134, 'epoch': 0.92}\n{'loss': 0.6535, 'learning_rate': 3.885512251130763e-07, 'rewards/chosen': -0.15854358673095703, 'rewards/rejected': -0.25830650329589844, 'rewards/accuracies': 0.75, 'rewards/margins': 0.0997629165649414, 'logps/rejected': -86.08502197265625, 'logps/chosen': -104.73957824707031, 'logits/rejected': -2.4362618923187256, 'logits/chosen': -2.4417386054992676, 'epoch': 0.92}\n{'loss': 0.5236, 'learning_rate': 3.572298075514652e-07, 'rewards/chosen': -0.33173638582229614, 'rewards/rejected': -0.7249648571014404, 'rewards/accuracies': 1.0, 'rewards/margins': 0.3932284712791443, 'logps/rejected': -129.77740478515625, 'logps/chosen': -110.92383575439453, 'logits/rejected': -2.3804469108581543, 'logits/chosen': -2.3727974891662598, 'epoch': 0.93}\n{'loss': 0.5699, 'learning_rate': 3.271776770026963e-07, 'rewards/chosen': 0.17072030901908875, 'rewards/rejected': -0.27281761169433594, 'rewards/accuracies': 0.5, 'rewards/margins': 0.4435379207134247, 'logps/rejected': -115.14811706542969, 'logps/chosen': -163.80905151367188, 'logits/rejected': -2.3941869735717773, 'logits/chosen': -2.4519903659820557, 'epoch': 0.93}\n{'loss': 0.7112, 'learning_rate': 2.984030494191942e-07, 'rewards/chosen': -0.011527830734848976, 'rewards/rejected': -0.016722679138183594, 'rewards/accuracies': 0.25, 'rewards/margins': 0.005194850265979767, 'logps/rejected': -98.59882354736328, 'logps/chosen': -92.22637939453125, 'logits/rejected': -2.224766492843628, 'logits/chosen': -2.2030723094940186, 'epoch': 0.94}\n{'loss': 0.6954, 'learning_rate': 2.7091379149682683e-07, 'rewards/chosen': -0.38888072967529297, 'rewards/rejected': -0.47543489933013916, 'rewards/accuracies': 0.5, 'rewards/margins': 0.08655418455600739, 'logps/rejected': -160.16017150878906, 'logps/chosen': -129.22695922851562, 'logits/rejected': -2.3134403228759766, 'logits/chosen': -2.408531904220581, 'epoch': 0.94}\n 95%|██████████████████████████████████████▉  | 190/200 [18:18<00:51,  5.11s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.77it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.42it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.10it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.94it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.61it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.64it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.69it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.71it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5530140995979309, 'eval_runtime': 8.4246, 'eval_samples_per_second': 1.662, 'eval_steps_per_second': 1.662, 'eval_rewards/chosen': 0.14370636641979218, 'eval_rewards/rejected': -0.263485312461853, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.4071916937828064, 'eval_logps/rejected': -115.71808624267578, 'eval_logps/chosen': -102.3586196899414, 'eval_logits/rejected': -2.2457432746887207, 'eval_logits/chosen': -2.2738184928894043, 'epoch': 0.94}\n 95%|██████████████████████████████████████▉  | 190/200 [18:26<00:51,  5.11s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.72it/s]\u001b[A\n{'loss': 0.5765, 'learning_rate': 2.447174185242324e-07, 'rewards/chosen': -0.22828826308250427, 'rewards/rejected': -0.7288055419921875, 'rewards/accuracies': 0.75, 'rewards/margins': 0.5005172491073608, 'logps/rejected': -134.2462158203125, 'logps/chosen': -161.24966430664062, 'logits/rejected': -2.229872226715088, 'logits/chosen': -2.347209930419922, 'epoch': 0.95}\n{'loss': 1.1381, 'learning_rate': 2.198210923282118e-07, 'rewards/chosen': -0.3667421340942383, 'rewards/rejected': 0.24262407422065735, 'rewards/accuracies': 0.25, 'rewards/margins': -0.6093661785125732, 'logps/rejected': -134.03823852539062, 'logps/chosen': -130.5983123779297, 'logits/rejected': -2.1603403091430664, 'logits/chosen': -2.189744472503662, 'epoch': 0.95}\n{'loss': 0.6307, 'learning_rate': 1.962316193157593e-07, 'rewards/chosen': -0.35103437304496765, 'rewards/rejected': -0.5349096059799194, 'rewards/accuracies': 0.5, 'rewards/margins': 0.18387527763843536, 'logps/rejected': -140.82223510742188, 'logps/chosen': -145.33404541015625, 'logits/rejected': -2.3972225189208984, 'logits/chosen': -2.483729839324951, 'epoch': 0.96}\n{'loss': 0.4001, 'learning_rate': 1.7395544861325718e-07, 'rewards/chosen': 0.6503192782402039, 'rewards/rejected': -0.20394106209278107, 'rewards/accuracies': 0.75, 'rewards/margins': 0.8542603254318237, 'logps/rejected': -113.98917388916016, 'logps/chosen': -132.031494140625, 'logits/rejected': -2.1649951934814453, 'logits/chosen': -2.342914581298828, 'epoch': 0.96}\n{'loss': 0.5493, 'learning_rate': 1.5299867030334815e-07, 'rewards/chosen': 0.5040995478630066, 'rewards/rejected': 0.1081380844116211, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3959614932537079, 'logps/rejected': -157.78550720214844, 'logps/chosen': -109.3871078491211, 'logits/rejected': -2.2867283821105957, 'logits/chosen': -2.2348644733428955, 'epoch': 0.97}\n{'loss': 0.7431, 'learning_rate': 1.333670137599713e-07, 'rewards/chosen': -0.35173627734184265, 'rewards/rejected': -0.2887565791606903, 'rewards/accuracies': 0.5, 'rewards/margins': -0.06297969818115234, 'logps/rejected': -134.78720092773438, 'logps/chosen': -128.4801025390625, 'logits/rejected': -2.049255609512329, 'logits/chosen': -2.0640335083007812, 'epoch': 0.97}\n{'loss': 0.6109, 'learning_rate': 1.1506584608200366e-07, 'rewards/chosen': 0.08927477896213531, 'rewards/rejected': -0.09407443553209305, 'rewards/accuracies': 0.75, 'rewards/margins': 0.18334922194480896, 'logps/rejected': -141.15994262695312, 'logps/chosen': -146.44586181640625, 'logits/rejected': -2.461162805557251, 'logits/chosen': -2.5367648601531982, 'epoch': 0.98}\n{'loss': 0.6535, 'learning_rate': 9.810017062595322e-08, 'rewards/chosen': -0.35427266359329224, 'rewards/rejected': -0.48995810747146606, 'rewards/accuracies': 0.5, 'rewards/margins': 0.13568544387817383, 'logps/rejected': -103.5339126586914, 'logps/chosen': -78.73564147949219, 'logits/rejected': -2.0760910511016846, 'logits/chosen': -2.124873638153076, 'epoch': 0.98}\n{'loss': 0.5606, 'learning_rate': 8.247462563808816e-08, 'rewards/chosen': -0.08639984577894211, 'rewards/rejected': -0.47226953506469727, 'rewards/accuracies': 0.75, 'rewards/margins': 0.38586968183517456, 'logps/rejected': -141.27639770507812, 'logps/chosen': -114.65870666503906, 'logits/rejected': -2.5024166107177734, 'logits/chosen': -2.4054954051971436, 'epoch': 0.99}\n{'loss': 0.585, 'learning_rate': 6.819348298638839e-08, 'rewards/chosen': -0.07396393269300461, 'rewards/rejected': -0.3473716974258423, 'rewards/accuracies': 0.75, 'rewards/margins': 0.2734077274799347, 'logps/rejected': -186.9609375, 'logps/chosen': -131.30178833007812, 'logits/rejected': -2.135554075241089, 'logits/chosen': -2.1027982234954834, 'epoch': 0.99}\n100%|█████████████████████████████████████████| 200/200 [19:18<00:00,  5.17s/it]\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.70it/s]\u001b[A\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.39it/s]\u001b[A\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.09it/s]\u001b[A\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.93it/s]\u001b[A\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.85it/s]\u001b[A\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.82it/s]\u001b[A\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.79it/s]\u001b[A\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.62it/s]\u001b[A\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.64it/s]\u001b[A\n 79%|█████████████████████████████████▊         | 11/14 [00:05<00:01,  1.70it/s]\u001b[A\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.72it/s]\u001b[A\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.73it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5563143491744995, 'eval_runtime': 8.4005, 'eval_samples_per_second': 1.667, 'eval_steps_per_second': 1.667, 'eval_rewards/chosen': 0.1451559066772461, 'eval_rewards/rejected': -0.2517659366130829, 'eval_rewards/accuracies': 0.8571428656578064, 'eval_rewards/margins': 0.39692187309265137, 'eval_logps/rejected': -115.61251068115234, 'eval_logps/chosen': -102.33592224121094, 'eval_logits/rejected': -2.2467074394226074, 'eval_logits/chosen': -2.274960994720459, 'epoch': 0.99}\n100%|█████████████████████████████████████████| 200/200 [19:26<00:00,  5.17s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.75it/s]\u001b[A\n{'train_runtime': 1168.427, 'train_samples_per_second': 0.685, 'train_steps_per_second': 0.171, 'train_loss': 0.690433207154274, 'epoch': 0.99}\n100%|█████████████████████████████████████████| 200/200 [19:28<00:00,  5.84s/it]\n\u001b[32m2023-08-29 06:56:44.389\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m514\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 1168.427, 'train_samples_per_second': 0.685, 'train_steps_per_second': 0.171, 'train_loss': 0.690433207154274, 'epoch': 0.99, 'train_samples': 1000}\u001b[0m\n***** train metrics *****\n  epoch                    =       0.99\n  train_loss               =     0.6904\n  train_runtime            = 0:19:28.42\n  train_samples            =       1000\n  train_samples_per_second =      0.685\n  train_steps_per_second   =      0.171\n\u001b[32m2023-08-29 06:56:44.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m518\u001b[0m - \u001b[1mSaving model checkpoint to outputs-dpo-yunguan-v1\u001b[0m\n\u001b[32m2023-08-29 06:56:47.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m525\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.99it/s]\n\u001b[32m2023-08-29 06:56:55.202\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m528\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 0.548906147480011, 'eval_runtime': 7.9597, 'eval_samples_per_second': 1.759, 'eval_steps_per_second': 1.759, 'eval_rewards/chosen': -0.02844368852674961, 'eval_rewards/rejected': -0.45395463705062866, 'eval_rewards/accuracies': 0.7857142686843872, 'eval_rewards/margins': 0.4255109429359436, 'eval_logps/rejected': -117.62999725341797, 'eval_logps/chosen': -104.08196258544922, 'eval_logits/rejected': -2.2502076625823975, 'eval_logits/chosen': -2.2772185802459717, 'epoch': 0.99, 'eval_samples': 20}\u001b[0m\n***** eval metrics *****\n  epoch                   =       0.99\n  eval_logits/chosen      =    -2.2772\n  eval_logits/rejected    =    -2.2502\n  eval_logps/chosen       =   -104.082\n  eval_logps/rejected     =    -117.63\n  eval_loss               =     0.5489\n  eval_rewards/accuracies =     0.7857\n  eval_rewards/chosen     =    -0.0284\n  eval_rewards/margins    =     0.4255\n  eval_rewards/rejected   =     -0.454\n  eval_runtime            = 0:00:07.95\n  eval_samples            =         20\n  eval_samples_per_second =      1.759\n  eval_steps_per_second   =      1.759\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 查看一下policy_chosen_logps 到底是啥\n# 这个reward_accuracies跟gradient_accumulation_steps有关 代码中默认是4 目前gradient_accumulation_steps=4  我在下面调用时显式的写出来\n# 所以每次是4对chosen_rewards与rejected_rewards比较 那难怪会在 [0 , 0.25 , 0.5 ,0.75] 之间跳 很离散 而且这样的话train过程中的acc其实没有多大指示性 毕竟只有4对\n# 还是要看eval_acc才有用 理解了！\n# 参考 https://github.com/valkryhx/MedicalGPT/blob/medGPT_0828/dpo_3.py#L327 和 L334 \n # 求了 reward_acc的mean() 因为reward_acc是类似[1,0,0,1]这种比较结果后得到的True/False 再float()转成1./0.的tensor！参考下面的代码：","metadata":{}},{"cell_type":"code","source":"import torch\na=torch.tensor([1])\nb=torch.tensor([5])\nc=(a>b)\nprint(c)\nprint(c.float())","metadata":{"execution":{"iopub.status.busy":"2023-08-29T08:00:05.366634Z","iopub.execute_input":"2023-08-29T08:00:05.367295Z","iopub.status.idle":"2023-08-29T08:00:05.389561Z","shell.execute_reply.started":"2023-08-29T08:00:05.367262Z","shell.execute_reply":"2023-08-29T08:00:05.388345Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"tensor([False])\ntensor([0.])\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd /kaggle/working/MedicalGPT\n!git pull --force --all\n!python dpo_only_for_show_acc.py \\\n    --model_type chatglm \\\n    --model_name_or_path THUDM/chatglm2-6b \\\n    --train_file_dir ./data/reward_yunguan \\\n    --validation_file_dir ./data/reward_yunguan \\\n    --learning_rate 1e-5 \\\n    --warmup_steps 10 \\\n    --optim  paged_lion_32bit \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 4 \\\n    --do_train \\\n    --do_eval \\\n    --use_peft True \\\n    --max_train_samples 1000 \\\n    --max_eval_samples 20 \\\n    --max_steps 200 \\\n    --eval_steps 10 \\\n    --save_steps 40 \\\n    --save_total_limit 2 \\\n    --load_best_model_at_end True \\\n    --max_source_length 256 \\\n    --max_target_length 128 \\\n    --output_dir outputs-dpo-yunguan-v1 \\\n    --target_modules all \\\n    --lora_rank 64 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.05 \\\n    --torch_dtype float16 \\\n    --fp16 True \\\n    --device_map auto \\\n    --report_to tensorboard \\\n    --remove_unused_columns False \\\n    --gradient_checkpointing True \\\n    --cache_dir ./cache","metadata":{"execution":{"iopub.status.idle":"2023-08-29T08:29:59.078055Z","shell.execute_reply.started":"2023-08-29T08:23:42.014545Z","shell.execute_reply":"2023-08-29T08:29:59.076567Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Loading checkpoint shards: 100%|██████████████████| 7/7 [01:29<00:00, 12.77s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:28<00:00, 12.69s/it]\n\u001b[32m2023-08-29 08:27:03.930\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m574\u001b[0m - \u001b[31m\u001b[1mid(model)=134526130352192\u001b[0m\n\u001b[32m2023-08-29 08:27:03.930\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m575\u001b[0m - \u001b[31m\u001b[1mid(model_ref)=134526128800048\u001b[0m\n\u001b[32m2023-08-29 08:27:03.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m611\u001b[0m - \u001b[1mPeft target_modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\u001b[0m\n\u001b[32m2023-08-29 08:27:03.934\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m620\u001b[0m - \u001b[1mChatGLMForConditionalGeneration(\n  (transformer): ChatGLMModel(\n    (embedding): Embedding(\n      (word_embeddings): Embedding(65024, 4096)\n    )\n    (rotary_pos_emb): RotaryEmbedding()\n    (encoder): GLMTransformer(\n      (layers): ModuleList(\n        (0-27): 28 x GLMBlock(\n          (input_layernorm): RMSNorm()\n          (self_attention): SelfAttention(\n            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)\n            (core_attention): CoreAttention(\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n            )\n            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n          )\n          (post_attention_layernorm): RMSNorm()\n          (mlp): MLP(\n            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n          )\n        )\n      )\n      (final_layernorm): RMSNorm()\n    )\n    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n  )\n)\u001b[0m\ntrainable params: 118587392 || all params: 6362171392 || trainable%: 1.86394525851843\n\u001b[32m2023-08-29 08:28:35.249\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m637\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n  0%|                                                   | 0/200 [00:00<?, ?it/s]\u001b[32m2023-08-29 08:28:37.717\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-109.5506], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:37.717\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:37.719\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:37.719\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:37.720\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:37.721\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:37.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\nCould not estimate the number of tokens of the input, floating-point operations will not be computed\n\u001b[32m2023-08-29 08:28:40.688\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-152.0067], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:40.689\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:40.690\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:40.690\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:40.691\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:40.691\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:40.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:41.809\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-133.1151], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:41.810\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:41.811\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:41.811\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:41.812\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:41.812\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:41.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:42.920\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-82.2880], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:42.921\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:42.922\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:42.922\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:42.923\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:42.923\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:42.924\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -147.03704833984375, 'logps/chosen': -119.24008178710938, 'logits/rejected': -2.103128671646118, 'logits/chosen': -2.0758676528930664, 'epoch': 0.0}\n  0%|▏                                          | 1/200 [00:07<26:23,  7.96s/it]\u001b[32m2023-08-29 08:28:44.198\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-49.8658], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:44.199\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:44.200\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:44.200\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:44.201\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:44.201\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:44.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:45.606\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-70.3549], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:45.606\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:45.607\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:45.608\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:45.608\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:45.609\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:45.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:46.644\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-117.7442], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:46.645\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:46.646\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:46.646\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:46.647\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:46.647\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:46.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:47.747\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-168.8973], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:47.747\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:47.748\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:47.749\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:47.749\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:47.750\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:47.750\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -105.12542724609375, 'logps/chosen': -101.7155532836914, 'logits/rejected': -2.3501272201538086, 'logits/chosen': -2.3333637714385986, 'epoch': 0.01}\n  1%|▍                                          | 2/200 [00:12<20:14,  6.14s/it]\u001b[32m2023-08-29 08:28:48.885\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-91.6243], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:48.885\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:48.886\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:48.886\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:48.887\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:48.887\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:48.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:50.041\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-136.9045], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:50.042\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:50.043\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:50.043\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:50.043\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:50.044\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:50.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:51.154\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-151.3369], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:51.154\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:51.155\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:51.155\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:51.156\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:51.157\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:51.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:52.207\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-114.6818], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:52.208\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:52.209\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0040], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:52.209\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:52.210\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0040], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:52.211\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0075], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:52.213\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n{'loss': 0.6927, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': -0.0009887695778161287, 'rewards/rejected': -0.0018695831531658769, 'rewards/accuracies': 0.25, 'rewards/margins': 0.0008808135753497481, 'logps/rejected': -108.24522399902344, 'logps/chosen': -123.63688659667969, 'logits/rejected': -2.2839434146881104, 'logits/chosen': -2.319284677505493, 'epoch': 0.01}\n  2%|▋                                          | 3/200 [00:17<18:34,  5.66s/it]\u001b[32m2023-08-29 08:28:53.955\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-136.3919], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:53.955\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:53.956\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:53.956\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:53.957\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:53.958\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:53.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:55.050\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-140.8412], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:55.050\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:55.051\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:55.052\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:55.052\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:55.053\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:55.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:56.236\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-177.0666], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:56.237\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:56.238\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:56.238\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:56.239\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:56.239\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:56.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:57.638\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-151.7849], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:57.638\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:57.639\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:57.639\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:57.640\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:57.640\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:57.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -115.29510498046875, 'logps/chosen': -151.52113342285156, 'logits/rejected': -2.263528347015381, 'logits/chosen': -2.350731134414673, 'epoch': 0.02}\n  2%|▊                                          | 4/200 [00:22<17:44,  5.43s/it]\u001b[32m2023-08-29 08:28:59.062\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-137.0175], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:28:59.062\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:59.063\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:59.063\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:28:59.064\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:59.065\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:28:59.065\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:00.133\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-134.8269], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:00.134\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:00.135\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0019], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:00.135\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:00.136\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0019], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:00.137\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0103], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:00.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:01.374\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-156.4256], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:01.375\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:01.376\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:01.376\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:01.376\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:01.377\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:01.378\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:02.745\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-137.0572], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:02.745\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:02.746\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0034], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:02.746\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:02.747\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0034], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:02.748\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0019], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:02.749\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n{'loss': 0.691, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0013263702858239412, 'rewards/rejected': -0.0030450820922851562, 'rewards/accuracies': 0.5, 'rewards/margins': 0.004371452145278454, 'logps/rejected': -125.99724578857422, 'logps/chosen': -141.33181762695312, 'logits/rejected': -2.1282036304473877, 'logits/chosen': -2.1059060096740723, 'epoch': 0.02}\n  2%|█                                          | 5/200 [00:27<16:56,  5.21s/it]\u001b[32m2023-08-29 08:29:03.879\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-60.8033], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:03.879\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:03.880\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:03.880\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:03.881\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:03.882\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:03.882\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:04.979\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-72.1315], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:04.979\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:04.980\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:04.980\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:04.981\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:04.982\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:04.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:06.051\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-42.1474], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:06.051\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:06.052\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:06.052\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:06.053\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:06.054\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:06.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:07.130\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-61.9071], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:07.131\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:07.132\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:07.132\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:07.133\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:07.133\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:07.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -113.37882995605469, 'logps/chosen': -59.24733352661133, 'logits/rejected': -2.4135544300079346, 'logits/chosen': -2.40352463722229, 'epoch': 0.03}\n  3%|█▎                                         | 6/200 [00:32<15:57,  4.93s/it]\u001b[32m2023-08-29 08:29:08.464\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-115.8058], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:08.464\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:08.465\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:08.465\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:08.466\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:08.467\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:08.467\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:09.945\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-89.6816], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:09.945\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:09.947\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:09.947\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:09.948\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:09.948\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:09.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:11.090\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-167.0477], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:11.091\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:11.092\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:11.092\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:11.092\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:11.093\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:11.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:12.262\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-142.7734], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:12.263\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:12.263\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:12.264\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:12.264\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:12.265\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:12.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6931, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -107.94503784179688, 'logps/chosen': -128.82713317871094, 'logits/rejected': -2.219214916229248, 'logits/chosen': -2.306650400161743, 'epoch': 0.03}\n  4%|█▌                                         | 7/200 [00:37<16:02,  4.99s/it]\u001b[32m2023-08-29 08:29:13.408\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-55.5227], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:13.409\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:13.410\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:13.410\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:13.411\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:13.411\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:13.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:14.552\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-77.0541], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:14.552\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:14.553\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([9.7656e-05], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:14.553\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:14.554\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([9.7656e-05], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:14.555\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0054], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:14.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:15.633\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-69.6429], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:15.633\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:15.635\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0030], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:15.635\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:15.636\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0030], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:15.637\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0069], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:15.638\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:16.701\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-72.5967], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:16.701\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:16.702\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:16.702\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:16.703\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:16.703\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:16.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6912, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': 0.0007825851207599044, 'rewards/rejected': -0.0030591965187340975, 'rewards/accuracies': 0.5, 'rewards/margins': 0.00384178152307868, 'logps/rejected': -97.47293090820312, 'logps/chosen': -68.70408630371094, 'logits/rejected': -2.419541358947754, 'logits/chosen': -2.3982093334198, 'epoch': 0.04}\n  4%|█▋                                         | 8/200 [00:41<15:27,  4.83s/it]\u001b[32m2023-08-29 08:29:17.907\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-111.3756], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:17.907\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:17.908\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:17.908\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:17.909\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:17.909\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:17.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:19.112\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-76.3296], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:19.113\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:19.114\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:19.114\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:19.114\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:19.115\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:19.116\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:20.207\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-74.9378], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:20.207\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:20.208\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0019], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:20.209\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:20.209\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0019], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:20.210\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0012], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:20.211\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:21.466\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-158.5807], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:21.466\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:21.467\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:21.468\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:21.468\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:21.469\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:21.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6935, 'learning_rate': 1.0000000000000002e-06, 'rewards/chosen': -0.0004823684867005795, 'rewards/rejected': 0.0003021240408997983, 'rewards/accuracies': 0.0, 'rewards/margins': -0.0007844925276003778, 'logps/rejected': -139.98971557617188, 'logps/chosen': -105.30595397949219, 'logits/rejected': -2.2986958026885986, 'logits/chosen': -2.2279067039489746, 'epoch': 0.04}\n  4%|█▉                                         | 9/200 [00:46<15:32,  4.88s/it]\u001b[32m2023-08-29 08:29:22.886\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-121.7330], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:22.887\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:22.888\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:22.888\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:22.888\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:22.889\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:22.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:24.043\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-104.1884], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:24.043\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:24.044\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0011], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:24.044\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:24.045\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0011], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:24.046\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0016], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:24.047\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:25.137\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-116.7800], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:25.138\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:25.139\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0060], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:25.139\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:25.140\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0060], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:25.141\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0081], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:25.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:26.234\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-108.3063], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:26.234\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:26.236\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0087], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:26.236\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:26.237\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0087], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:26.237\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0005], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:26.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n{'loss': 0.6926, 'learning_rate': 2.0000000000000003e-06, 'rewards/chosen': 0.0034038545563817024, 'rewards/rejected': 0.0023120881523936987, 'rewards/accuracies': 0.25, 'rewards/margins': 0.0010917665204033256, 'logps/rejected': -137.2055206298828, 'logps/chosen': -112.75192260742188, 'logits/rejected': -2.286799907684326, 'logits/chosen': -2.2781553268432617, 'epoch': 0.05}\n  5%|██                                        | 10/200 [00:51<15:19,  4.84s/it]\u001b[32m2023-08-29 08:29:27.767\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-85.3374], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:27.768\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:27.769\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0178], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:27.770\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:27.771\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0178], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:27.772\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0203], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:27.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\n  0%|                                                    | 0/14 [00:00<?, ?it/s]\u001b[A\u001b[32m2023-08-29 08:29:28.356\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-79.0076], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.357\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:28.358\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0052], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.358\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:28.359\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0052], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.360\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0050], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\n 14%|██████▎                                     | 2/14 [00:00<00:03,  3.69it/s]\u001b[A\u001b[32m2023-08-29 08:29:28.963\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-133.6930], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.964\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:28.965\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0107], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.965\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:28.966\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0107], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.967\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0188], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:28.967\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 21%|█████████▍                                  | 3/14 [00:01<00:04,  2.38it/s]\u001b[A\u001b[32m2023-08-29 08:29:29.555\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-103.1097], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:29.555\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:29.556\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0085], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:29.557\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:29.558\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0085], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:29.558\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0155], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:29.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 29%|████████████▌                               | 4/14 [00:01<00:04,  2.07it/s]\u001b[A\u001b[32m2023-08-29 08:29:30.156\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-53.5065], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.156\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:30.157\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0018], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.158\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:30.158\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0018], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.159\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0059], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\n 36%|███████████████▋                            | 5/14 [00:02<00:04,  1.90it/s]\u001b[A\u001b[32m2023-08-29 08:29:30.763\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-167.0971], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.764\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:30.765\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0049], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.765\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:30.766\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0049], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.767\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0017], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:30.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 43%|██████████████████▊                         | 6/14 [00:02<00:04,  1.81it/s]\u001b[A\u001b[32m2023-08-29 08:29:31.348\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-165.3296], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.348\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:31.350\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0040], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.350\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:31.351\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0040], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.351\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0071], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\n 50%|██████████████████████                      | 7/14 [00:03<00:03,  1.78it/s]\u001b[A\u001b[32m2023-08-29 08:29:31.953\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-105.9407], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.953\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:31.954\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0153], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.955\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:31.956\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0153], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.956\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0129], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:31.957\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 57%|█████████████████████████▏                  | 8/14 [00:04<00:03,  1.75it/s]\u001b[A\u001b[32m2023-08-29 08:29:32.682\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-59.0383], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:32.683\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:32.684\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0025], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:32.684\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:32.685\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0025], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:32.686\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0228], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:32.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 64%|████████████████████████████▎               | 9/14 [00:04<00:03,  1.58it/s]\u001b[A\u001b[32m2023-08-29 08:29:33.330\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-106.1311], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.330\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:33.331\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0083], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.331\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:33.332\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0083], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.333\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0194], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 71%|██████████████████████████████▋            | 10/14 [00:05<00:02,  1.60it/s]\u001b[A\u001b[32m2023-08-29 08:29:33.894\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-74.9697], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.895\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:33.896\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0005], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.896\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:33.897\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0005], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.898\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0033], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:33.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\n 79%|█████████████████████████████████▊         | 11/14 [00:06<00:01,  1.65it/s]\u001b[A\u001b[32m2023-08-29 08:29:34.476\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-63.7998], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:34.477\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:34.479\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0107], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:34.479\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:34.480\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0107], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:34.481\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0047], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:34.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 86%|████████████████████████████████████▊      | 12/14 [00:06<00:01,  1.67it/s]\u001b[A\u001b[32m2023-08-29 08:29:35.067\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-129.4322], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.067\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:35.068\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0066], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.069\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:35.070\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0066], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.070\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0098], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n 93%|███████████████████████████████████████▉   | 13/14 [00:07<00:00,  1.68it/s]\u001b[A\u001b[32m2023-08-29 08:29:35.628\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-126.9661], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.628\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:35.629\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0029], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.630\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:35.630\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0029], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.631\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0019], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:35.632\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6945472955703735, 'eval_runtime': 8.5904, 'eval_samples_per_second': 1.63, 'eval_steps_per_second': 1.63, 'eval_rewards/chosen': -0.0008251736289821565, 'eval_rewards/rejected': 0.0019584381952881813, 'eval_rewards/accuracies': 0.3571428656578064, 'eval_rewards/margins': -0.0027836114168167114, 'eval_logps/rejected': -113.07808685302734, 'eval_logps/chosen': -103.81133270263672, 'eval_logits/rejected': -2.314446210861206, 'eval_logits/chosen': -2.3433568477630615, 'epoch': 0.05}\n  5%|██                                        | 10/200 [01:00<15:19,  4.84s/it]\n100%|███████████████████████████████████████████| 14/14 [00:07<00:00,  1.71it/s]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-29 08:29:36.188\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-111.6594], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:36.189\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:36.190\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0068], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:36.190\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:36.191\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0068], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:36.192\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0100], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:36.192\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:37.497\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-157.3923], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:37.498\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:37.499\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0231], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:37.499\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:37.500\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0231], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:37.501\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0034], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:37.502\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:38.929\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-133.1376], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:38.929\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:38.931\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0015], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:38.931\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:38.932\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0015], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:38.933\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0009], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:38.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:40.261\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-259.0260], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:40.262\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:40.263\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0138], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:40.263\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:40.264\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0138], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:40.265\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0011], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:40.266\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n{'loss': 0.6898, 'learning_rate': 2.0000000000000003e-06, 'rewards/chosen': 0.010559845715761185, 'rewards/rejected': 0.0038682937156409025, 'rewards/accuracies': 0.5, 'rewards/margins': 0.006691551301628351, 'logps/rejected': -117.17643737792969, 'logps/chosen': -165.30381774902344, 'logits/rejected': -2.258463144302368, 'logits/chosen': -2.298445701599121, 'epoch': 0.05}\n  6%|██▎                                       | 11/200 [01:05<24:08,  7.67s/it]\u001b[32m2023-08-29 08:29:41.673\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-61.7607], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:41.674\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:41.675\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0136], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:41.676\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:41.677\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0136], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:41.678\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0001], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:41.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:42.747\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-140.6223], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:42.748\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:42.749\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0168], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:42.749\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:42.750\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0168], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:42.751\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0030], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:42.752\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:43.880\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-128.0094], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:43.880\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:43.882\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0225], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:43.882\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:43.883\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0225], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:43.883\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0150], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:43.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:45.034\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-136.8325], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:45.034\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:45.035\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0037], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:45.036\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:45.036\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0037], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:45.037\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0089], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:45.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6923, 'learning_rate': 3e-06, 'rewards/chosen': 0.0010628702584654093, 'rewards/rejected': -0.000742149306461215, 'rewards/accuracies': 0.5, 'rewards/margins': 0.0018050195649266243, 'logps/rejected': -105.93048095703125, 'logps/chosen': -116.80622100830078, 'logits/rejected': -2.2781856060028076, 'logits/chosen': -2.2318453788757324, 'epoch': 0.06}\n  6%|██▌                                       | 12/200 [01:10<21:04,  6.73s/it]\u001b[32m2023-08-29 08:29:46.298\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-37.3800], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:46.299\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:46.300\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0047], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:46.300\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:46.301\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0047], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:46.302\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0044], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:46.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:47.484\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-88.0652], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:47.484\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:47.486\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0104], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:47.486\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:47.487\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0104], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:47.487\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0001], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:47.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:48.614\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-70.9821], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:48.615\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:48.616\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0037], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:48.616\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:48.617\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0037], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:48.618\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0082], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:48.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:49.788\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-144.4202], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:49.788\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:49.790\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0038], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:49.790\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:49.791\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0038], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:49.792\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0073], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:49.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6917, 'learning_rate': 4.000000000000001e-06, 'rewards/chosen': 0.00381298060528934, 'rewards/rejected': 0.0008800507057458162, 'rewards/accuracies': 0.75, 'rewards/margins': 0.002932929899543524, 'logps/rejected': -103.56449127197266, 'logps/chosen': -85.2118911743164, 'logits/rejected': -2.4584577083587646, 'logits/chosen': -2.4871561527252197, 'epoch': 0.06}\n  6%|██▋                                       | 13/200 [01:14<19:02,  6.11s/it]\u001b[32m2023-08-29 08:29:50.972\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-132.8958], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:50.972\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:50.973\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0075], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:50.973\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:50.974\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0075], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:50.975\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0064], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:50.976\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:52.164\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-105.9191], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:52.164\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:52.166\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0171], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:52.166\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:52.167\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0171], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:52.168\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0054], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:52.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:53.316\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-139.6573], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:53.317\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:53.318\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0281], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:53.318\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:53.319\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0281], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:53.320\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0065], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:53.320\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([1.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:54.588\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-127.0432], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:54.588\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:54.590\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([-0.0173], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:54.590\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:54.591\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([-0.0173], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:54.592\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([-0.0103], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:54.592\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n{'loss': 0.6884, 'learning_rate': 5e-06, 'rewards/chosen': 0.005101585295051336, 'rewards/rejected': -0.004467964172363281, 'rewards/accuracies': 0.5, 'rewards/margins': 0.00956954900175333, 'logps/rejected': -98.54788970947266, 'logps/chosen': -126.37882232666016, 'logits/rejected': -2.2618069648742676, 'logits/chosen': -2.2950801849365234, 'epoch': 0.07}\n  7%|██▉                                       | 14/200 [01:19<17:52,  5.77s/it]\u001b[32m2023-08-29 08:29:55.928\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-48.8199], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:55.929\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:55.930\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0044], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:55.930\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:55.931\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0044], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:55.932\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0077], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:55.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:57.027\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m278\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps=tensor([-160.4555], device='cuda:0', grad_fn=<SliceBackward0>)\u001b[0m\n\u001b[32m2023-08-29 08:29:57.027\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m279\u001b[0m - \u001b[31m\u001b[1mpolicy_chosen_logps.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:57.028\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m296\u001b[0m - \u001b[31m\u001b[1mchosen_rewards=tensor([0.0024], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:57.028\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdpo_loss\u001b[0m:\u001b[36m297\u001b[0m - \u001b[31m\u001b[1mchosen_rewards.shape=torch.Size([1])\u001b[0m\n\u001b[32m2023-08-29 08:29:57.029\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m339\u001b[0m - \u001b[31m\u001b[1mhere chosen_rewards=tensor([0.0024], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:57.030\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m340\u001b[0m - \u001b[31m\u001b[1mhere rejected_rewards=tensor([0.0206], device='cuda:0')\u001b[0m\n\u001b[32m2023-08-29 08:29:57.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_batch_metrics\u001b[0m:\u001b[36m342\u001b[0m - \u001b[1mhere reward_accuracies=tensor([0.], device='cuda:0')\u001b[0m\n^C\nTraceback (most recent call last):\n  File \"/kaggle/working/MedicalGPT/dpo_only_for_show_acc.py\", line 661, in <module>\n    main()\n  File \"/kaggle/working/MedicalGPT/dpo_only_for_show_acc.py\", line 638, in main\n    train_result = trainer.train()\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1645, in train\n    return inner_training_loop(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1938, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2759, in training_step\n    loss = self.compute_loss(model, inputs)\n  File \"/opt/conda/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py\", line 396, in compute_loss\n    loss, metrics = self.get_batch_metrics(model, inputs, train_eval=\"train\")\n  File \"/kaggle/working/MedicalGPT/dpo_only_for_show_acc.py\", line 331, in get_batch_metrics\n    ) = self.concatenated_forward(self.ref_model, batch)\n  File \"/opt/conda/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py\", line 317, in concatenated_forward\n    all_logits = model(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 581, in forward\n    return model_forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/operations.py\", line 569, in __call__\n    return convert_to_fp32(self.model_forward(*args, **kwargs))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\", line 932, in forward\n    transformer_outputs = self.transformer(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\", line 828, in forward\n    hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\", line 638, in forward\n    layer_ret = layer(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\", line 542, in forward\n    attention_output, kv_cache = self.self_attention(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 165, in new_forward\n    output = old_forward(*args, **kwargs)\n  File \"/root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm2-6b/b1502f4f75c71499a3d566b14463edd62620ce9f/modeling_chatglm.py\", line 407, in forward\n    key_layer = apply_rotary_pos_emb(key_layer, rotary_pos_emb)\nKeyboardInterrupt\n","output_type":"stream"}]}]}