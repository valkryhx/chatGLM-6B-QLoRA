# 参考 从20230707开始记录

from_pretrained()中 empty_init=Fasle 
    https://github.com/THUDM/ChatGLM-6B/issues/530
    https://blog.csdn.net/BIT_666/article/details/131433393

deepspeed config 使用
    https://huggingface.co/docs/transformers/main_classes/deepspeed#constructing-massive-models

ds_zero2.json 参考
    https://huggingface.co/docs/transformers/main_classes/deepspeed#zero2-example

ds_zero3.json 参考
   https://huggingface.co/docs/transformers/main_classes/deepspeed#zero3-example

zero级别选择
   https://huggingface.co/docs/transformers/main_classes/deepspeed#how-to-choose-which-zero-stage-and-offloads-to-use-for-best-performance

模型加载过程
    https://github.com/beyondguo/LLM-Tuning/blob/master/chatglm2_lora_tuning.py#L96

模型qlora
    fork 后少量修改 https://github.com/shuxueslpi/chatGLM-6B-QLoRA/blob/main/train_qlora.py#L139

模型lora找出所有全连接层，为所有全连接添加adapter ,find_all_linear_names(model):
    https://github.com/yangjianxin1/Firefly/blob/master/train_qlora.py#L61

学习
   https://github.com/shibing624/MedicalGPT
   https://github.com/shibing624/textgen

chatglm-tuning项目 最早的lora finetune  使用alpaca 52k数据
   https://github.com/mymusise/ChatGLM-Tuning

 使用alpaca-gpt4 数据的建议
    https://github.com/THUDM/ChatGLM2-6B/issues/51

 全量微调显存占用 最好是8XA100 
    https://github.com/SpongebBob/Finetune-ChatGLM2-6B/issues/1

 二次预训练语料规模建议 ：50万中文裁判文书  sft用30万语料规模
    https://github.com/pengxiao-song/LaWGPT/tree/main

 大语言模型知识问答系统 腾讯  向量数据库方案
    https://mp.weixin.qq.com/s?__biz=MjM5ODYwMjI2MA==&mid=2649779506&idx=1&sn=d6406a827808b2887b959bb6b2b05976&chksm=beccfe4989bb775fea0aaa0d538787f3e7c06307eee75353287467da9234444b44b11099dec9&scene=126&sessionid=1687860304&key=4a31cc73eef27e0ce03c6d1f1ed0ce5bcce1eaf7f16bf696a0e810685324578bc50eb3f77b652dd45837694300d3edd78a72e8bd1eedb3fc60cc975408d8b4ca30ae2d3dd3f8e55599a7b0e67a6b6caf768de1525fa89e92208219c97c3d77657fa0903e0b3b646d183a8deec43ef9ba1ad48f736f062c38cd19fcedc9b54f57&ascene=15&uin=NTY2NTA4NjQ%3D&devicetype=Windows+10+x64&version=63060012&lang=zh_CN&session_us=gh_d14465b5ce6c&countrycode=AL&exportkey=n_ChQIAhIQYm7qwB%2F65GguyHWf4eIYphLuAQIE97dBBAEAAAAAADtYCh8zJ3wAAAAOpnltbLcz9gKNyK89dVj0p4WDZXVb2hIHwqQrI%2B%2BpfiCGP9knqn6fyfZ1g7J%2Bp%2BpAMLJ9BV0BeVCTkZzHscxnzQ99ViKu%2F6UmTVP7bS9k9N2N%2Fb065AbCsqgZTB71HJNnKWV9ECdaVF0xg7lBSejJT%2F7gURhMiDwnnj4thRav0M%2FHCfG7hdVbkWnYYbHPnCBzfCdnGuW3i7TAJahrFpHSRvtPJNNBi3EYfmlFEjwDwIAMzPDxO39AyHglVM3X0N%2Fi60sHSi15%2BihcgI9GqUlrS2hXEDYPzVQ%3D&acct

 adgen 数据集下载
    https://huggingface.co/datasets/RUCAIBox/Chinese-Generation/tree/main

  单条知识注入chatglm2 AdaLora  共7条语料
    https://www.zhihu.com/question/596950521/answer/3109759716
  
  微调cahtglm2保姆级教程
    https://zhuanlan.zhihu.com/p/641047705 
  
  NLP 大模型各类资料
    https://github.com/fighting41love/funNLP
 
 多轮对话SFT 语料组织 可以是拼接后的alpaca inst-inp-out三元组 也可以是 inst-inp-out-history 四元组  四元组就按照清华的prompt拼接成三元组
    https://github.com/shibing624/MedicalGPT/issues/46#issuecomment-1614807346
    https://blog.csdn.net/mingzai624/article/details/130735366?spm=1001.2014.3001.5502  这是一些常识 里面就是有4元组的例子
 清华官方的prompt拼接 就是history对话循环拼接 不难
    https://github.com/shibing624/MedicalGPT/issues/67

 好文章
    https://blog.csdn.net/mingzai624/article/details/130735366?spm=1001.2014.3001.5502

#----------------20230712------------------#

预训练中语料格式问题和一直输出不停止的问题  以及试用blocksize划分数据sampl个数的问题 以及设置max_len就停止的问题
    https://github.com/shibing624/MedicalGPT/issues/61

加载数据（比如用户上传的文件夹）可以先验证，也就是先glob读取所有文件名字 然后循环加载每一个文件 循环外面用try/catch接住异常 这样就能判断哪个具体的文件有问题
不过我感觉其实正常的load_dataset 其实已经支持加载文件名list了 直接在真正的load_dataset之前的验证阶段提前pre_load_dataset ，说不定仅仅需要在pre_load_dataset外面
搞一个try/catch就可以确定有问题的数据了。

二次预训练后推理极慢很可能是陷入输出循环不会停止了 建议用stream chat的方式输出看看
   https://github.com/shuxueslpi/chatGLM-6B-QLoRA/issues/23

一个stream chat的colab例子 三种方式调用stream chat 但是 其中的def ask_and_ans 函数好像使用报错
   https://colab.research.google.com/github/WSH032/ChatGLM-webui/blob/main/Colab_ChatGLM.ipynb#scrollTo=akGK5YpRjcu_
