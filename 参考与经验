# 参考 从20230707开始记录

from_pretrained()中 empty_init=Fasle 
    https://github.com/THUDM/ChatGLM-6B/issues/530
    https://blog.csdn.net/BIT_666/article/details/131433393

deepspeed config 使用
    https://huggingface.co/docs/transformers/main_classes/deepspeed#constructing-massive-models

ds_zero2.json 参考
    https://huggingface.co/docs/transformers/main_classes/deepspeed#zero2-example

ds_zero3.json 参考
   https://huggingface.co/docs/transformers/main_classes/deepspeed#zero3-example

zero级别选择
   https://huggingface.co/docs/transformers/main_classes/deepspeed#how-to-choose-which-zero-stage-and-offloads-to-use-for-best-performance

模型加载过程
    https://github.com/beyondguo/LLM-Tuning/blob/master/chatglm2_lora_tuning.py#L96

模型qlora
    fork 后少量修改 https://github.com/shuxueslpi/chatGLM-6B-QLoRA/blob/main/train_qlora.py#L139

模型lora找出所有全连接层，为所有全连接添加adapter ,find_all_linear_names(model):
    https://github.com/yangjianxin1/Firefly/blob/master/train_qlora.py#L61

学习
   https://github.com/shibing624/MedicalGPT
   https://github.com/shibing624/textgen

chatglm-tuning项目 最早的lora finetune  使用alpaca 52k数据
   https://github.com/mymusise/ChatGLM-Tuning

 使用alpaca-gpt4 数据的建议
    https://github.com/THUDM/ChatGLM2-6B/issues/51

 全量微调显存占用 最好是8XA100 
    https://github.com/SpongebBob/Finetune-ChatGLM2-6B/issues/1

 二次预训练语料规模建议 ：50万中文裁判文书  sft用30万语料规模
    https://github.com/pengxiao-song/LaWGPT/tree/main

 大语言模型知识问答系统 腾讯  向量数据库方案
    https://mp.weixin.qq.com/s?__biz=MjM5ODYwMjI2MA==&mid=2649779506&idx=1&sn=d6406a827808b2887b959bb6b2b05976&chksm=beccfe4989bb775fea0aaa0d538787f3e7c06307eee75353287467da9234444b44b11099dec9&scene=126&sessionid=1687860304&key=4a31cc73eef27e0ce03c6d1f1ed0ce5bcce1eaf7f16bf696a0e810685324578bc50eb3f77b652dd45837694300d3edd78a72e8bd1eedb3fc60cc975408d8b4ca30ae2d3dd3f8e55599a7b0e67a6b6caf768de1525fa89e92208219c97c3d77657fa0903e0b3b646d183a8deec43ef9ba1ad48f736f062c38cd19fcedc9b54f57&ascene=15&uin=NTY2NTA4NjQ%3D&devicetype=Windows+10+x64&version=63060012&lang=zh_CN&session_us=gh_d14465b5ce6c&countrycode=AL&exportkey=n_ChQIAhIQYm7qwB%2F65GguyHWf4eIYphLuAQIE97dBBAEAAAAAADtYCh8zJ3wAAAAOpnltbLcz9gKNyK89dVj0p4WDZXVb2hIHwqQrI%2B%2BpfiCGP9knqn6fyfZ1g7J%2Bp%2BpAMLJ9BV0BeVCTkZzHscxnzQ99ViKu%2F6UmTVP7bS9k9N2N%2Fb065AbCsqgZTB71HJNnKWV9ECdaVF0xg7lBSejJT%2F7gURhMiDwnnj4thRav0M%2FHCfG7hdVbkWnYYbHPnCBzfCdnGuW3i7TAJahrFpHSRvtPJNNBi3EYfmlFEjwDwIAMzPDxO39AyHglVM3X0N%2Fi60sHSi15%2BihcgI9GqUlrS2hXEDYPzVQ%3D&acct

 adgen 数据集下载
    https://huggingface.co/datasets/RUCAIBox/Chinese-Generation/tree/main

  单条知识注入chatglm2 AdaLora  共7条语料
    https://www.zhihu.com/question/596950521/answer/3109759716
  
  微调cahtglm2保姆级教程
    https://zhuanlan.zhihu.com/p/641047705 
  
  NLP 大模型各类资料
    https://github.com/fighting41love/funNLP
 
 多轮对话SFT 语料组织 可以是拼接后的alpaca inst-inp-out三元组 也可以是 inst-inp-out-history 四元组  四元组就按照清华的prompt拼接成三元组
    https://github.com/shibing624/MedicalGPT/issues/46#issuecomment-1614807346
    https://blog.csdn.net/mingzai624/article/details/130735366?spm=1001.2014.3001.5502  这是一些常识 里面就是有4元组的例子
 清华官方的prompt拼接 就是history对话循环拼接 不难
    https://github.com/shibing624/MedicalGPT/issues/67

 好文章
    https://blog.csdn.net/mingzai624/article/details/130735366?spm=1001.2014.3001.5502

#----------------20230712------------------#

预训练中语料格式问题和一直输出不停止的问题  以及试用blocksize划分数据sampl个数的问题 以及设置max_len就停止的问题
    https://github.com/shibing624/MedicalGPT/issues/61

加载数据（比如用户上传的文件夹）可以先验证，也就是先glob读取所有文件名字 然后循环加载每一个文件 循环外面用try/catch接住异常 这样就能判断哪个具体的文件有问题
不过我感觉其实正常的load_dataset 其实已经支持加载文件名list了 直接在真正的load_dataset之前的验证阶段提前pre_load_dataset ，说不定仅仅需要在pre_load_dataset外面
搞一个try/catch就可以确定有问题的数据了。

二次预训练后推理极慢很可能是陷入输出循环不会停止了 建议用stream chat的方式输出看看
   https://github.com/shuxueslpi/chatGLM-6B-QLoRA/issues/23

一个stream chat的colab例子 三种方式调用stream chat 但是 其中的def ask_and_ans 函数好像使用报错
   https://colab.research.google.com/github/WSH032/ChatGLM-webui/blob/main/Colab_ChatGLM.ipynb#scrollTo=akGK5YpRjcu_

#--------------------20230714----------------#

huggingface 官网API docs
      https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/trainer#transformers.Trainer

训练样本能记住邮箱 
训练样本中如果数据不平衡 比如员工邮箱samples有250条  FAQ有70条 那么有可能训练时着重拟合邮箱数据 FAQ数据就训练的弱一些
此时我尝试直接把FAQ数据复制2份 一共3份 达到210条 加入到训练集合中 强行在数量上平衡

#-------------------20230718---------------------#
一个6B模型 为何需要6×16GB的显存才能训练 因为显存占用分成3部分：模型参数24G + 梯度24G + AdamW优化器 48G=96GB 
而推理时由于没有梯度和优化器参数占用 所以仅仅需要24G
以上计算先不考虑CUDA程序和计算中间冗余 仅仅考虑模型本身所需的最小显存
量化+lora 可以大幅减少训练时的显存占用。
   https://redian.news/wxnews/348240 
   https://zhuanlan.zhihu.com/p/616858352

多轮对话的数据格式组织
   https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning#%E5%AF%B9%E8%AF%9D%E6%95%B0%E6%8D%AE%E9%9B%86
   https://github.com/THUDM/ChatGLM-6B/issues/410

图解大模型训练之：数据并行（DP、DDP、ZeRO、零冗余优化)
   https://www.cvmart.net/community/detail/7638

ZeRO零冗余优化器详解
   https://hub.baai.ac.cn/view/18162

图解大模型训练之：数据并行下篇( DeepSpeed ZeRO，零冗余优化)
   https://zhuanlan.zhihu.com/p/618865052

qlora 通俗介绍
   https://www.51cto.com/article/757269.html
   https://zhuanlan.zhihu.com/p/634256206

一文搞懂模型量化基础
   https://bbs.huaweicloud.com/blogs/390419

优化器参数为何显存占用是2倍的梯度 因为AdamW优化器（带weight decayL2正则化的Adam）包含一阶动量+二阶动量信息
   https://zhuanlan.zhihu.com/p/256393726
   https://www.cvmart.net/community/detail/5247

英伟达TensorRT int8 原理
   https://yunyang1994.gitee.io/2021/06/06/%E7%90%86%E8%A7%A3%E8%8B%B1%E4%BC%9F%E8%BE%BE-TensorRT-%E7%9A%84-INT8-%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86/

大模型统一后端 支持很多类的大模型展示
   https://github.com/xusenlinzy/api-for-open-llm

langchain 介绍
   https://foresightnews.pro/article/detail/31307
   https://zhuanlan.zhihu.com/p/620529542

幻觉的来源
  https://juejin.cn/post/7241861929412460600
  https://www.kuxai.com/article/829

对话机器人 行业资讯报告
  https://pdf.dfcfw.com/pdf/H3_AP202107041501676711_1.pdf
  https://www.infoq.cn/article/eo156vst0p4admrw76mv
  https://www.jiqizhixin.com/articles/2020-12-29-2
  https://redian.news/wxnews/289054
  https://www.ccita.net/news/204.html

chatglm-6B的api调用修改  Update api.py 同步方法异步化,防止请求之间在服务层相互阻塞,增加api调用的并发性能
  https://github.com/THUDM/ChatGLM-6B/pull/1340
  https://github.com/aleimu/ChatGLM-6B/tree/main  api.py 今天刚update  我也fork了该项目

自回归或者GPT2中的位移 lm_logits 与label 是怎么对应计算loss的  比如第1，2位的token如何预测第3位的label
我在看firefly项目的loss计算时  https://github.com/yangjianxin1/Firefly/blob/master/component/loss.py#L40
发现了
shift_logits = logits[..., :-1, :].contiguous()
shift_labels = labels[..., 1:].contiguous()
经过调研发现这段代码就是gpt-2自回归训练时，用于logits与label对齐的代码
http://mingchao.wang/5QjuqLH5/#12  解释的很好：
# move labels to correct device to enable model parallelism
            labels = labels.to(lm_logits.device)
            # Shift so that tokens < n predict n
            shift_logits = lm_logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
上述代码中，lm_logits 表示模型前向传播输出的结果，其shape为[batch_size, seq_len, vocab_size]；
labels 就是 copy 自 "input_ids" 的，其shape为[batch_size, seq_len]。
主要看上述代码的第6行和第7行可知，其对 labels 去掉了第一个token，对 lm_logits 去掉了最后一个token，
这样也就实现了根据前面的token预测下一个token了。 
也就是说lm_logits[3]本身就是利用到了前1、2位的信息生成的 所以可以直接用这个logits[3]的分布来预测[3]位置对应的label，也就是可以计算交叉熵loss。
类似的解释：
https://brightliao.com/2023/05/20/chatgpt-training/  代码第二十一行
https://www.cnblogs.com/phyger/p/14188608.html  代码中【# 此时的shift_labels为将输入的labels张量的切片labels[..., 1:].contiguous(), 即取(2, n)的label值；
        # 因此利用(1, n-1)的lm_logits值与(2, n)的label值即可计算此时自回归预训练的交叉熵损失值.】

huggingface 提供的训练gpt2样例
  https://huggingface.co/learn/nlp-course/zh-CN/chapter7/6?fw=pt
这个博客系列写的很好 可以能力提升
  https://brightliao.com/2023/05/20/chatgpt-training/
  http://mingchao.wang/

百川模型github baichuan 推荐的LLM effective Finetuning 项目
  https://github.com/hiyouga/LLaMA-Efficient-Tuning  支持不少模型 更新快 而且也有三阶段训练代码

#----------------------20230719----------------------#
8比特整型量化技术 llm.int8 中文介绍
  https://huggingface.co/blog/zh/hf-bitsandbytes-integration
  https://huggingface.co/blog/hf-bitsandbytes-integration

4bit 浮点数量化技术   FP4 NF4 也是bitsandbytes的
  https://huggingface.co/blog/4bit-transformers-bitsandbytes
  注意不管是4bit 还是8bit量化 都是对参数的storage data type存储类型进行量化 
  而计算类型computation data type 仍然是fp16 或者fp32 所以使用到lora来减少参与计算的参数量
  4-bit模型参数本身不能再微调 但是可以配合lora技术来微调lora矩阵的参数
  【QLoRA has one storage data type (usually 4-bit NormalFloat) for the base model weights and a computation data type (16-bit BrainFloat) used to perform computations.
   QLoRA dequantizes weights from the storage data type to the computation data type to perform the forward and backward passes, 
   but only computes weight gradients for the LoRA parameters which use 16-bit bfloat. 
   The weights are decompressed only when they are needed, therefore the memory usage stays low during training and inference.】
  【Can we train 4bit/8bit models?
     It is not possible to perform pure 4bit training on these models. However, you can train these models by leveraging parameter efficient fine tuning methods (PEFT) 
     and train for example adapters on top of them. That is what is done in the paper and is officially supported by the PEFT library from Hugging Face. 
     We also provide a training notebook and recommend users to check the QLoRA repository if they are interested in replicating the results from the paper.】
    4bit 的1-3-0 和1-2-1 相比 使用哪一种格式还没有定论 ，都行吧。
  【For FP4 there is no fixed format and as such one can try combinations of different mantissa/exponent combinations. In general, 3 exponent bits do a bit better in most cases. 
      But sometimes 2 exponent bits and a mantissa bit yield better performance.】
浮点数的表示 fp32 以及fp8 的原理  NP 浮点数的原理 跟理解FP4量化很有帮助 浮点数结构为sign	exponent	mantissa三部分
  http://www.cburch.com/books/float/ 
4bit sign	exponent	mantissa = 1-3-0 或者1-2-1 一般来说1-3-0用的更多
  [sign,exponent,mantissa] = [1,3,0]  https://proceedings.neurips.cc/paper/2020/file/13b919438259814cd5be8cb45877d577-Paper.pdf

load_in_4bit 和load_in_8bit 后，模型的显存占用会变小 使用print(model.get_memory_footprint()/1024/1024/1024) 单位GB来检查
一般来说在fp16作为原始精度下 4bit加载后模型大小为原始1/4 8bit加载为原始的1/2 。
https://huggingface.co/docs/transformers/main_classes/quantization#load-a-large-model-in-4bit

从每个模型的hf主页的config.json中可以看到torch.dtype 比如baichuan7B是float32 难怪需要7*4=28G以上的RAM才能加载
  https://huggingface.co/baichuan-inc/Baichuan-7B/blob/main/config.json
而清华的chatglm2-6B 是"torch_dtype": "float16", 所以仅需6.2*2=12.4G的RAM就可以load
  https://huggingface.co/THUDM/chatglm2-6b/blob/main/config.json

lora微调chatglm2-6B后出现重复的情况 可能是最早发布的版本。
在该人使用的版本中出问题的关键原因是，eos_token_id和pad_token_id一样，都是2。
代码中在140在对label进行处理的时候，将pad_token_id全部替换为ignore（-100）的时候 将eos_token_id也替换成了-100。
从而导致label最终没有eos_id从而不能停止。
   https://github.com/THUDM/ChatGLM2-6B/issues/228
