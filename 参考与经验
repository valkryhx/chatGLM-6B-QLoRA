# 参考 从20230707开始记录

from_pretrained()中 empty_init=Fasle 
    https://github.com/THUDM/ChatGLM-6B/issues/530
    https://blog.csdn.net/BIT_666/article/details/131433393

deepspeed config 使用
    https://huggingface.co/docs/transformers/main_classes/deepspeed#constructing-massive-models

ds_zero2.json 参考
    https://huggingface.co/docs/transformers/main_classes/deepspeed#zero2-example

ds_zero3.json 参考
   https://huggingface.co/docs/transformers/main_classes/deepspeed#zero3-example

zero级别选择
   https://huggingface.co/docs/transformers/main_classes/deepspeed#how-to-choose-which-zero-stage-and-offloads-to-use-for-best-performance

模型加载过程
    https://github.com/beyondguo/LLM-Tuning/blob/master/chatglm2_lora_tuning.py#L96

模型qlora
    fork 后少量修改 https://github.com/shuxueslpi/chatGLM-6B-QLoRA/blob/main/train_qlora.py#L139

模型lora找出所有全连接层，为所有全连接添加adapter ,find_all_linear_names(model):
    https://github.com/yangjianxin1/Firefly/blob/master/train_qlora.py#L61

学习
   https://github.com/shibing624/MedicalGPT
   https://github.com/shibing624/textgen

chatglm-tuning项目 最早的lora finetune  使用alpaca 52k数据
   https://github.com/mymusise/ChatGLM-Tuning

 使用alpaca-gpt4 数据的建议
    https://github.com/THUDM/ChatGLM2-6B/issues/51

 全量微调显存占用 最好是8XA100 
    https://github.com/SpongebBob/Finetune-ChatGLM2-6B/issues/1

 二次预训练语料规模建议 ：50万中文裁判文书  sft用30万语料规模
    https://github.com/pengxiao-song/LaWGPT/tree/main

 大语言模型知识问答系统 腾讯  向量数据库方案
    https://mp.weixin.qq.com/s?__biz=MjM5ODYwMjI2MA==&mid=2649779506&idx=1&sn=d6406a827808b2887b959bb6b2b05976&chksm=beccfe4989bb775fea0aaa0d538787f3e7c06307eee75353287467da9234444b44b11099dec9&scene=126&sessionid=1687860304&key=4a31cc73eef27e0ce03c6d1f1ed0ce5bcce1eaf7f16bf696a0e810685324578bc50eb3f77b652dd45837694300d3edd78a72e8bd1eedb3fc60cc975408d8b4ca30ae2d3dd3f8e55599a7b0e67a6b6caf768de1525fa89e92208219c97c3d77657fa0903e0b3b646d183a8deec43ef9ba1ad48f736f062c38cd19fcedc9b54f57&ascene=15&uin=NTY2NTA4NjQ%3D&devicetype=Windows+10+x64&version=63060012&lang=zh_CN&session_us=gh_d14465b5ce6c&countrycode=AL&exportkey=n_ChQIAhIQYm7qwB%2F65GguyHWf4eIYphLuAQIE97dBBAEAAAAAADtYCh8zJ3wAAAAOpnltbLcz9gKNyK89dVj0p4WDZXVb2hIHwqQrI%2B%2BpfiCGP9knqn6fyfZ1g7J%2Bp%2BpAMLJ9BV0BeVCTkZzHscxnzQ99ViKu%2F6UmTVP7bS9k9N2N%2Fb065AbCsqgZTB71HJNnKWV9ECdaVF0xg7lBSejJT%2F7gURhMiDwnnj4thRav0M%2FHCfG7hdVbkWnYYbHPnCBzfCdnGuW3i7TAJahrFpHSRvtPJNNBi3EYfmlFEjwDwIAMzPDxO39AyHglVM3X0N%2Fi60sHSi15%2BihcgI9GqUlrS2hXEDYPzVQ%3D&acct

 adgen 数据集下载
    https://huggingface.co/datasets/RUCAIBox/Chinese-Generation/tree/main

  单条知识注入chatglm2 AdaLora  共7条语料
    https://www.zhihu.com/question/596950521/answer/3109759716
  
  微调cahtglm2保姆级教程
    https://zhuanlan.zhihu.com/p/641047705 
  
  NLP 大模型各类资料
    https://github.com/fighting41love/funNLP
 
 多轮对话SFT 语料组织 可以是拼接后的alpaca inst-inp-out三元组 也可以是 inst-inp-out-history 四元组  四元组就按照清华的prompt拼接成三元组
    https://github.com/shibing624/MedicalGPT/issues/46#issuecomment-1614807346
    https://blog.csdn.net/mingzai624/article/details/130735366?spm=1001.2014.3001.5502  这是一些常识 里面就是有4元组的例子
 清华官方的prompt拼接 就是history对话循环拼接 不难
    https://github.com/shibing624/MedicalGPT/issues/67

 好文章
    https://blog.csdn.net/mingzai624/article/details/130735366?spm=1001.2014.3001.5502

#----------------20230712------------------#

预训练中语料格式问题和一直输出不停止的问题  以及试用blocksize划分数据sampl个数的问题 以及设置max_len就停止的问题
    https://github.com/shibing624/MedicalGPT/issues/61

加载数据（比如用户上传的文件夹）可以先验证，也就是先glob读取所有文件名字 然后循环加载每一个文件 循环外面用try/catch接住异常 这样就能判断哪个具体的文件有问题
不过我感觉其实正常的load_dataset 其实已经支持加载文件名list了 直接在真正的load_dataset之前的验证阶段提前pre_load_dataset ，说不定仅仅需要在pre_load_dataset外面
搞一个try/catch就可以确定有问题的数据了。

二次预训练后推理极慢很可能是陷入输出循环不会停止了 建议用stream chat的方式输出看看
   https://github.com/shuxueslpi/chatGLM-6B-QLoRA/issues/23

一个stream chat的colab例子 三种方式调用stream chat 但是 其中的def ask_and_ans 函数好像使用报错
   https://colab.research.google.com/github/WSH032/ChatGLM-webui/blob/main/Colab_ChatGLM.ipynb#scrollTo=akGK5YpRjcu_

#--------------------20230714----------------#

huggingface 官网API docs
      https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/trainer#transformers.Trainer

训练样本能记住邮箱 
训练样本中如果数据不平衡 比如员工邮箱samples有250条  FAQ有70条 那么有可能训练时着重拟合邮箱数据 FAQ数据就训练的弱一些
此时我尝试直接把FAQ数据复制2份 一共3份 达到210条 加入到训练集合中 强行在数量上平衡

#-------------------20230718---------------------#
一个6B模型 为何需要6×16GB的显存才能训练 因为显存占用分成3部分：模型参数24G + 梯度24G + AdamW优化器 48G=96GB 
而推理时由于没有梯度和优化器参数占用 所以仅仅需要24G
以上计算先不考虑CUDA程序和计算中间冗余 仅仅考虑模型本身所需的最小显存
量化+lora 可以大幅减少训练时的显存占用。
   https://redian.news/wxnews/348240 
   https://zhuanlan.zhihu.com/p/616858352

多轮对话的数据格式组织
   https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning#%E5%AF%B9%E8%AF%9D%E6%95%B0%E6%8D%AE%E9%9B%86
   https://github.com/THUDM/ChatGLM-6B/issues/410

图解大模型训练之：数据并行（DP、DDP、ZeRO、零冗余优化)
   https://www.cvmart.net/community/detail/7638

ZeRO零冗余优化器详解
   https://hub.baai.ac.cn/view/18162

图解大模型训练之：数据并行下篇( DeepSpeed ZeRO，零冗余优化)
   https://zhuanlan.zhihu.com/p/618865052

qlora 通俗介绍
   https://www.51cto.com/article/757269.html
   https://zhuanlan.zhihu.com/p/634256206

一文搞懂模型量化基础
   https://bbs.huaweicloud.com/blogs/390419

优化器参数为何显存占用是2倍的梯度 因为AdamW优化器（带weight decayL2正则化的Adam）包含一阶动量+二阶动量信息
   https://zhuanlan.zhihu.com/p/256393726
   https://www.cvmart.net/community/detail/5247

英伟达TensorRT int8 原理
   https://yunyang1994.gitee.io/2021/06/06/%E7%90%86%E8%A7%A3%E8%8B%B1%E4%BC%9F%E8%BE%BE-TensorRT-%E7%9A%84-INT8-%E5%8A%A0%E9%80%9F%E5%8E%9F%E7%90%86/

大模型统一后端 支持很多类的大模型展示
   https://github.com/xusenlinzy/api-for-open-llm

langchain 介绍
   https://foresightnews.pro/article/detail/31307
   https://zhuanlan.zhihu.com/p/620529542

幻觉的来源
  https://juejin.cn/post/7241861929412460600
  https://www.kuxai.com/article/829

对话机器人 行业资讯报告
  https://pdf.dfcfw.com/pdf/H3_AP202107041501676711_1.pdf
  https://www.infoq.cn/article/eo156vst0p4admrw76mv
  https://www.jiqizhixin.com/articles/2020-12-29-2
  https://redian.news/wxnews/289054
  https://www.ccita.net/news/204.html

chatglm-6B的api调用修改  Update api.py 同步方法异步化,防止请求之间在服务层相互阻塞,增加api调用的并发性能
  https://github.com/THUDM/ChatGLM-6B/pull/1340
  https://github.com/aleimu/ChatGLM-6B/tree/main  api.py 今天刚update  我也fork了该项目

自回归或者GPT2中的位移 lm_logits 与label 是怎么对应计算loss的  比如第1，2位的token如何预测第3位的label
我在看firefly项目的loss计算时  https://github.com/yangjianxin1/Firefly/blob/master/component/loss.py#L40
发现了
shift_logits = logits[..., :-1, :].contiguous()
shift_labels = labels[..., 1:].contiguous()
经过调研发现这段代码就是gpt-2自回归训练时，用于logits与label对齐的代码
http://mingchao.wang/5QjuqLH5/#12  解释的很好：
# move labels to correct device to enable model parallelism
            labels = labels.to(lm_logits.device)
            # Shift so that tokens < n predict n
            shift_logits = lm_logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
上述代码中，lm_logits 表示模型前向传播输出的结果，其shape为[batch_size, seq_len, vocab_size]；
labels 就是 copy 自 "input_ids" 的，其shape为[batch_size, seq_len]。
主要看上述代码的第6行和第7行可知，
其对 labels 去掉了第一个token对应的标签，
对 lm_logits 去掉了最后一个token对应的logit（注意logit的shape和label不同，logit是一个向量，label是一个int值），
然后对齐，这样也就实现了根据前面n-1的token的logtis（第n-1的logit本来就是前n-1个logit逐步forward得到的）
预测下一个token的label了。 
也就是说lm_logits[3]本身就是利用到了前1、2位的信息生成的 所以可以直接用这个logits[3]的分布来预测[3]位置对应的label，也就是可以计算交叉熵loss。
类似的解释：
https://brightliao.com/2023/05/20/chatgpt-training/  代码第二十一行
https://www.cnblogs.com/phyger/p/14188608.html  代码中【# 此时的shift_labels为将输入的labels张量的切片labels[..., 1:].contiguous(), 即取(2, n)的label值；
        # 因此利用(1, n-1)的lm_logits值与(2, n)的label值即可计算此时自回归预训练的交叉熵损失值.】

bs =2
seq =3
vocab_size=4
lm_logits=torch.rand(bs,seq,vocab_size).contiguous()

lm_logits
tensor([[[0.8853, 0.3273, 0.4089, 0.4020],
         [0.1083, 0.1012, 0.7600, 0.9405],
         [0.7811, 0.8414, 0.5911, 0.1856]],

        [[0.1678, 0.7493, 0.9916, 0.0696],
         [0.4782, 0.3564, 0.3803, 0.0190],
         [0.9489, 0.7898, 0.9645, 0.4280]]])

shift_logits = lm_logit[..., :-1, :].contiguous()

labels=torch.randint(1,10,(bs,seq))  # 注意是bs,seq
labels
shift_labels=labels[:,1:].contiguous()

shift_logits.view(-1, shift_logits.size(-1))
tensor([[0.5092, 0.5452, 0.4049, 0.9787],
        [0.4267, 0.3846, 0.0835, 0.6209],
        [0.8420, 0.6930, 0.6335, 0.9912],
        [0.3892, 0.8665, 0.8490, 0.9345]])
a=shift_labels.view(-1)
tensor([1, 5, 5, 3])  
然后计算crossEntropy(shift_logits.view(-1, shift_logits.size(-1))，shift_labels.view(-1))
http://mingchao.wang/5QjuqLH5/#12  解释的很好
我们也看到modeling_chatglm2.py源码中出现了shift_logit/shift_label操作
完全是一模一样https://huggingface.co/THUDM/chatglm2-6b/blob/main/modeling_chatglm.py#L953

huggingface 提供的训练gpt2样例
  https://huggingface.co/learn/nlp-course/zh-CN/chapter7/6?fw=pt
这个博客系列写的很好 可以能力提升
  https://brightliao.com/2023/05/20/chatgpt-training/
  http://mingchao.wang/

百川模型github baichuan 推荐的LLM effective Finetuning 项目
  https://github.com/hiyouga/LLaMA-Efficient-Tuning  支持不少模型 更新快 而且也有三阶段训练代码

#----------------------20230719----------------------#
8比特整型量化技术 llm.int8 中文介绍
  https://huggingface.co/blog/zh/hf-bitsandbytes-integration
  https://huggingface.co/blog/hf-bitsandbytes-integration

4bit qlora 官网例子 
   https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing&pli=1

4bit 浮点数量化技术   FP4 NF4 也是bitsandbytes的
  https://huggingface.co/blog/4bit-transformers-bitsandbytes
  注意不管是4bit 还是8bit量化 都是对参数的storage data type存储类型进行量化 
  而计算类型computation data type 仍然是fp16 或者fp32 所以使用到lora来减少参与计算的参数量
  4-bit模型参数本身不能再微调 但是可以配合lora技术来微调lora矩阵的参数
  【QLoRA has one storage data type (usually 4-bit NormalFloat) for the base model weights and a computation data type (16-bit BrainFloat) used to perform computations.
   QLoRA dequantizes weights from the storage data type to the computation data type to perform the forward and backward passes, 
   but only computes weight gradients for the LoRA parameters which use 16-bit bfloat. 
   The weights are decompressed only when they are needed, therefore the memory usage stays low during training and inference.】
  【Can we train 4bit/8bit models?
     It is not possible to perform pure 4bit training on these models. However, you can train these models by leveraging parameter efficient fine tuning methods (PEFT) 
     and train for example adapters on top of them. That is what is done in the paper and is officially supported by the PEFT library from Hugging Face. 
     We also provide a training notebook and recommend users to check the QLoRA repository if they are interested in replicating the results from the paper.】
    4bit 的1-3-0 和1-2-1 相比 使用哪一种格式还没有定论 ，都行吧。
  【For FP4 there is no fixed format and as such one can try combinations of different mantissa/exponent combinations. In general, 3 exponent bits do a bit better in most cases. 
      But sometimes 2 exponent bits and a mantissa bit yield better performance.】
浮点数的表示 fp32 以及fp8 的原理  NP 浮点数的原理 跟理解FP4量化很有帮助 浮点数结构为sign	exponent	mantissa三部分
  http://www.cburch.com/books/float/ 
4bit sign	exponent	mantissa = 1-3-0 或者1-2-1 一般来说1-3-0用的更多
  [sign,exponent,mantissa] = [1,3,0]  https://proceedings.neurips.cc/paper/2020/file/13b919438259814cd5be8cb45877d577-Paper.pdf

load_in_4bit 和load_in_8bit 后，模型的显存占用会变小 使用print(model.get_memory_footprint()/1024/1024/1024) 单位GB来检查
一般来说在fp16作为原始精度下 4bit加载后模型大小为原始1/4 8bit加载为原始的1/2 。
https://huggingface.co/docs/transformers/main_classes/quantization#load-a-large-model-in-4bit

从每个模型的hf主页的config.json中可以看到torch.dtype 比如baichuan7B是float32 难怪需要7*4=28G以上的RAM才能加载
  https://huggingface.co/baichuan-inc/Baichuan-7B/blob/main/config.json
而清华的chatglm2-6B 是"torch_dtype": "float16", 所以仅需6.2*2=12.4G的RAM就可以load
  https://huggingface.co/THUDM/chatglm2-6b/blob/main/config.json

lora微调chatglm2-6B后出现重复的情况 可能是最早发布的版本。
在该人使用的版本中出问题的关键原因是，eos_token_id和pad_token_id一样，都是2。
代码中在140在对label进行处理的时候，将pad_token_id全部替换为ignore（-100）的时候 将eos_token_id也替换成了-100。
从而导致label最终没有eos_id从而不能停止。
   https://github.com/THUDM/ChatGLM2-6B/issues/228

介绍了qlora的几个技术点 解释的不错 值得看
  https://zhuanlan.zhihu.com/p/634256206

训练你的大模型！低资源下的模型轻量化
  提到了gptq
  https://zhuanlan.zhihu.com/p/626701575

 量化AWQ GGMKL
    https://www.reddit.com/r/LocalLLaMA/comments/13yehfn/new_quantization_method_awq_outperforms_gptq_in/

很多微调训练代码中都会传入 --fp16 True  这是transformers.trainer的标准参数  默认是False 所以lora adapter里面也会存fp32 qlora也是 全量sft也是 
fp32是默认值 这样训练loss稳定 如果想减小adapter 可以将fp设置为True
   https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments.fp16
  [[[[fp16 (bool, optional, defaults to False) — Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.]]]]]

GPTQ+ LORA 好像并没有节省bit的优势 甚至不如bitsandbytes的FP4
  https://www.reddit.com/r/LocalLLaMA/comments/13r7pzg/gptqlora_efficient_finetuning_of_quantized_llms/

https://colab.research.google.com/drive/1T1pOgewAWVpR9gKpaEWw4orOrzPFb3yM?usp=sharing
https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing#scrollTo=XIyP_0r6zuVc
https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing

#------------20230720------------#
baichuan lora 数学题微调  我也fork了文章中提到的项目
    https://zhuanlan.zhihu.com/p/643935822

baichuan-7b vicuna 项目 用shareGPT leetcode和cot数据SFT 这个项目也有gptq版本的模型 可以试试在这个版本上微调
    https://huggingface.co/fireballoon/baichuan-vicuna-7b

fireballoon/baichuan-vicuna-chinese-7b-gptq  一个baichuan7b sft后的chat model  等会试试在这个上面做sft  原始模型也可以在hf找找

torchrun deepspeed accelerate分布式DDP训练时报错 Finding the cause of RuntimeError: Expected to mark a variable ready only once.....
需要把trainingArguments中的ddp_find_unused_parameters设置为False  在命令行输入后 记得在程序中打印出来检查  不然可能配置未生效
注意 parser.add_argument("--ddp_find_unused_parameters", type=str, default=False)  type是str 不是bool  如果是bool 只要这个参数写了 后面无论跟的什么值  这个参数都是True  那就不符合我们的目的了 而且这个bug很诡异
   一定要参考 https://github.com/valkryhx/Firefly/blob/master/gptq_lora_baichuan7b_vicuna_v2_%E6%94%AF%E6%8C%81%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83.ipynb   的说明
   https://discuss.pytorch.org/t/finding-the-cause-of-runtimeerror-expected-to-mark-a-variable-ready-only-once/124428
   https://blog.csdn.net/qq_45717425/article/details/130088045
   https://stackoverflow.com/questions/68000761/pytorch-ddp-finding-the-cause-of-expected-to-mark-a-variable-ready-only-once
   https://github.com/Lightning-AI/lightning/issues/14533
   https://huggingface.co/transformers/v4.9.0/main_classes/trainer.html
   注意 parser.add_argument("--ddp_find_unused_parameters", type=str, default=False)  type是str 不是bool  如果是bool 只要这个参数写了 后面无论跟的什么值  这个参数都是True  那就不符合我们的目的了 而且这个bug很诡异
   一定要参考 https://github.com/valkryhx/Firefly/blob/master/gptq_lora_baichuan7b_vicuna_v2_%E6%94%AF%E6%8C%81%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83.ipynb   的说明

#-------20230721--------#
langchain  langchain-chatglm 的介绍
   https://www.langchain.asia/
   https://github.com/imClumsyPanda/langchain-ChatGLM
   https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide
   https://python.langchain.com/docs/get_started/introduction.html
   https://python.langchain.com.cn/docs/

#-------20230723--------#
  关于argparse.ArgumentParser的用法中type=bool 类型参数的特殊之处
  如果参数A type=bool  那么不能使用 --A True/False的方式传参 而是直接 --A  这样A表示出现 就会自动读取action='store_false'或者store_true的值 所以其实很有迷惑之处
  有个好的办法 用eval 强行执行的方式作为type  讨论和比较好的带有--A True的解决方案见下：
  https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse
  

  Simplest. It's not flexible, but I prefer simplicity.

  parser.add_argument('--boolean_flag',
                      help='This is a boolean flag.',
                      type=eval, 
                      choices=[True, False], 
                      default='True')
  EDIT: If you don't trust the input, don't use eval.


#-----20230724------#
LLAMA 多轮对话的语料处理 好像是把所有history拼接后 形成 [Q1  A1 Q2 A2 ....Qn An]
https://github.com/hiyouga/LLaMA-Efficient-Tuning/blob/182b42504399d2755897b9737db1d36655a0fa50/src/llmtuner/dsets/preprocess.py#L31C17-L31C99
上面的代码用到了from llmtuner.extras.template import get_template
在第57 84 103行都用到了 get_dialog(examples) 而这个函数定义在25行 def get_dialog(examples): 用到了22行prompt_template和31行的prompt_template.get_dialog
而prompt_template 对象又是https://github.com/hiyouga/LLaMA-Efficient-Tuning/blob/182b42504399d2755897b9737db1d36655a0fa50/src/llmtuner/extras/template.py#L21
定义的 它的get_dialog方法又参考这里的29行
https://github.com/hiyouga/LLaMA-Efficient-Tuning/blob/182b42504399d2755897b9737db1d36655a0fa50/src/llmtuner/extras/template.py#L29
这就可以看出 将history处理成Q1，A1，Q2，A2的过程了 也就是conv.append操作

然后回到
https://github.com/hiyouga/LLaMA-Efficient-Tuning/blob/182b42504399d2755897b9737db1d36655a0fa50/src/llmtuner/dsets/preprocess.py#L50
从最后的input_ids += 和labels += 可以看出 +=是将history转化的QxAx全部考虑进去拼接成一个了
输入也就是input_ids 是包含Q1A1Q2A2...QnAn的 tokenized id  ，比如其中一部分是Q1A1+[tokenizer.eos_token_id]
需要预测的label则是将所有的Q部分用ignore token id掩盖 后面接上一个eos_id 而且正好长度等于Q1(masked)A1+[tokenizer.eos_token_id] 
也就是只用预测未被掩盖的A1部分
整个预测相当于并行预测多个A ，即问1答____问2答_____ 同时计算多个答案位置的loss   这和firefly项目的思路是一致的 
参考 
注意这个函数的line72和line73
【注意对比chatglm的多轮对话处理】https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/src/glmtuner/dsets/preprocess.py#L29
   明显看出就是按照QAQAQ拼起来的作为input 而label是最后一轮的answer，也就是说 多轮对话其实只是用到最后一轮来训练。语料利用效率不高，但是效果可能还行。
   可以考虑将多轮对话拆分成多个语料Q1-A1  Q1A1Q2-A2 Q1A1Q2A2Q3-A3 。。。这样虽然麻烦但是每一轮都是一个sample

向langchain-chatglm加载1G txt faiss报错
  https://github.com/imClumsyPanda/langchain-ChatGLM/issues/884

LLaMA类项目的4步骤微调项目 
  https://github.com/hiyouga/LLaMA-Efficient-Tuning
chatglm项目的微调项目 是同一个作者  哪些参数可以作为入参通过命令行传入 见https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/5ec41e2e74097d9863a41fec034c70b045b82a01/src/glmtuner/tuner/core/parser.py#L76
  https://github.com/hiyouga/ChatGLM-Efficient-Tuning
  提到RHLF 数据标注项目
  https://github.com/SupritYoung/RLHF-Label-Tool/tree/master

langflow flowise 两个低代码拖拽式的。Flowise就像Langflow一样是一个伟大的无代码工具，可以让你在大型语言模型的基础上建立强大的应用程序
   langflow基于python flowise基于typescript 都属于langchain上层UI的部分 github星数都在10K以上 很棒！
   https://github.com/FlowiseAI/Flowise
   https://github.com/logspace-ai/langflow

LLaMA类模型和chatglm模型 两类模型的多轮对话语料处理的区别 
   见下：llama模型是属于考虑了所有Q1A1Q2A2Q3A3...作为input_ids 而把所有的Q都掩盖成ignore token，label就是 __A1__A2__A3_... 属于考虑每个A的loss
   chatglm模型是用Q1A1Q2A2Qn 拼接起来作为Q_final ，最后一个An作为A，用Q_final当做input_ids ,A作为label。所以实际上只用了最后一轮对话来训练，作者说可以将多轮Q-A拆分成多个对话来训练，增加语料。
   https://github.com/hiyouga/LLaMA-Efficient-Tuning/issues/226

ChatGLM-efficient-Tuning 项目的几个传参例子： 
   glm2版本要传入--use_v2  
   还可以修改data_dir位置--dataset_dir data/car
   --load_best_model_at_end 也可以传
   --dev_ratio 0.085 可以传入 直接在train_dataset上切分出eval 不用传do_eval 而是在train的过程中利用该部分数据做eval
   看第三个link 好像所有training args都是可以传的 比如--save_total_limit
   
   https://github.com/hiyouga/ChatGLM-Efficient-Tuning/issues/163
   https://github.com/hiyouga/ChatGLM-Efficient-Tuning/issues/202
   https://github.com/hiyouga/ChatGLM-Efficient-Tuning/issues/212
   https://github.com/hiyouga/ChatGLM-Efficient-Tuning/issues/288  这个有deepspeed config
   用zero_optimization 搜issue 能看到很多deepspeed的案例 config
   https://github.com/search?q=repo%3Ahiyouga%2FChatGLM-Efficient-Tuning+zero_optimization&type=issues

#---------20230725----------#
   chatglm-efficeint-tuning项目中 使用qlora-4bit量化微调后 如果使用--fp16 则会出现deepspeed overflow的情况 这个时候不使用--fp16 默认fp32 就没这个情况了

   chatglm-efficient-tuning项目中 使用deepspeed 不管单gpu还是多gpu 都会出现到--save_steps 10 比如第10步 就开始CPU RAM OOM的情况
   经过定位 发现是deepspeed.TorchCheckpointEngine 在到save_steps的时候比如10 20 居然也会使用torch.load 把mp_rank_00_model_states.pt
   zero_pp_rank_0_mp_rank_00_optim_states.pt.  这种中间状态的模型states和优化器states 先load到cpu 再存到磁盘 所以多卡情况下ram直接oom（因为是2个卡的进程都在load到 ram）
   单卡时ram倒是没有马上oom 而是能正常接住load过来的各类.pt 成功保存到磁盘 而且ram也从10g下降到5g 但是每次保存文件巨大 都有3G-6G 存个几次 磁盘就满了。
   参考 https://github.com/hiyouga/ChatGLM-Efficient-Tuning/issues/342 
   zhihu上也有一套类似的代码 https://zhuanlan.zhihu.com/p/638994113  也有这个保存的操作 但是实际上我并不需要！！！这不知道谁抄谁的

   也有关于checkpoint文件很大的bug的讨论 但是看chatglm-efficient-tuning的项目实现 其save其实是使用了k:v.clone()的 现在无非就是不知道为啥会启动deepspeed save_checkpoint()这个动作
   这个checkpoint文件过大的bug讨论见：
   https://github.com/microsoft/DeepSpeed/issues/3303
   https://github.com/huggingface/transformers/issues/22822
   https://github.com/hiyouga/ChatGLM-Efficient-Tuning/issues/342

#--------20230726---------#
   MedicalGPT 的gradio不知道为何 即使基座模型不做量化 +qlora adapter 效果也很差 而且似乎在gradio.py中选择history为空 结果使用时似乎还是会被历史信息影响
   注意启动时的 --template_name chatglm2

   在不使用AutoModel和AutoTokenizer的情况下 要import ChatGLMForConditionalGeneration 模型和ChatGLMTokeniizer 这个tokenizer 
   那么在transformers库中是没有的 我们需要直接从hf上下载THUDM/chatglm2-6b项目中的各类py和json小文件 放到一个单独的目录
   （模型文件+各类json = 模型加载目录） （py文件是model类文件 各种tokenizer chat streamchat方法从这些类文件中来）
    为了简便 我们直接在这个类文件目录中新建我们的python 比如训练的python 加载的python 并且在我们自己的python
    import时 要写成  from  modeling_chatglm  import ChatGLMForConditionalGeneration
                     from  tokenizer_chatglm import ChatGLmTokenizer
                     并且要修改modeling_chatglm.py 的26行 import configuration_chatglm 把相对导入的 .去掉

    MedicalGPT的bug modeling_chatglm.py error: `-` operator, with a bool tensor is not supported是清华模型代码bug
    参见https://user-images.githubusercontent.com/121851857/256195899-9e0d7d65-9d44-494c-acac-682e205c37df.png
    可以这么改 688行：full_attention_mask -= padding_mask.unsqueeze(-1) - 1 -》 full_attention_mask -= padding_mask.unsqueeze(-1).int() - 1
    参考 https://huggingface.co/THUDM/chatglm2-6b/discussions/67

    alpaca_gpt4_data_zh是alpaca格式，是单轮对话 需要通过https://github.com/shibing624/MedicalGPT/blob/main/convert_alpaca.py 转换为ShareGPT格式
    这样就是让MedicalGPT 兼容单轮和多轮对话格式
    参考 https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/blob/main/data/alpaca_gpt4_data_zh.json
    再查查shareGPT 在这个过程中 我们发现 https://finance.sina.com.cn/tech/roll/2023-04-21/doc-imyrcpar4710479.shtml
    https://github.com/thunlp/UltraChat     UltraLM-13B 模型 
    UltraLM 项目上的AlpacaEval Leaderboard  排行 https://tatsu-lab.github.io/alpaca_eval/

    https://zhuanlan.zhihu.com/p/646072687  声称是模型如何理解表格 用markdown的 | 分割表格数据 效果好 用的是天池金融大模型的chatglm-6B 说明6B微调很有潜力
    作者写了很多很多很好的文章 特别是提到了PDF数据等非结构化数据的处理 可以参考 https://www.zhihu.com/people/yan-yan-qing-15
    天池上有一篇金融数据的处理流程的参赛笔记  可以参考里面提到的向量数据库 FAISS 等  很有指导意义
    https://tianchi.aliyun.com/forum/post/571708
    https://tianchi.aliyun.com/competition/entrance/532126/introduction

    关于使用预训练语料在已经指令微调的model上做增量训练的效果 大佬的结论是 正向好效果
    参见 https://www.zhihu.com/people/yan-yan-qing-15
    问：我想请问下，增量预训练在类似chatglm和baichuan-chat这种已经做过指令微调的模型上，效果是不是不好 甚至有副作用？增量预训练是否适合用在pretrain的模型上更合适呢？
    答：效果好，需要自己增量预训练的数据少一些；有较多的领域数据（无监督的数据），建议用base model，训练专门模型，如果没有，就用chat类的。

    chatGLM的预训练方式不是自回归 而是多个空白快填充 这个跟LLaMA好像差不多 难怪好多代码都改成了这样
    https://zhuanlan.zhihu.com/p/641499380

    从零训练一个多模态LLM：预训练+指令微调+对齐+融合多模态+链接外部系统  无代码 只有流程指导 很好！
    https://redian.news/wxnews/497220
    https://zhuanlan.zhihu.com/p/643611622

#----------20230727----------#
   tensorflow mnist 加载本地离线的mnist数据集案例 包括下载地址
   参考 https://zhuanlan.zhihu.com/p/351180890
   mnist下载和本地加载 https://stackoverflow.com/a/40693405


#--------20230730---------#
https://github.com/valkryhx/MedicalGPT/tree/MedicalGPT_0728  这个是medicalGPT_0728 branch
里面的pretraining.py 和 0730_sft_test.py 都是最好的代码 我自己改的
0730_pt_test.py 也测试通过 但是model_class.from_pretrained 没有更新到上两种的写法
pretrain_0729_test.py 这是测试 也通过的 

注意看0730_pt_test.py 最下面的说明是关于deepspeed的大文件bug解决方法 就是从json导入trainingargs 而非从命令行导入 因为命令行导入save steps 会被ds也读到 从而也会让ds在savesteps的频次保存很大的中间文件
注意看0730_sft_test.py 最上面的说明 是1.修改了chatglm的ChatGLMForConditionalGeneration的导入 直接从修改了的本地文件读 而不能是automodel 因为automodel会从hf官网加载 而官网文件有bug
不然会报错 RuntimeError: Subtraction, the `-` operator, with a bool tensor is not 
supported. If you are trying to invert a mask, use the `~` or `logical_not()` 
operator instead.
参考 https://huggingface.co/THUDM/chatglm2-6b/discussions/67
     https://github.com/shibing624/MedicalGPT/issues/124

2.修改了cur_len 针对chatglm 增加了代码 在817行左右
 if model_args.model_type =='chatglm':  ## add 0730
                cur_len +=2                        ## add 0730
3. 针对trainging args的处理方式和pt代码一致 从json读取  


注意 model.from_pretrained()中empty_init参数只是chatglm才有的 参考它的modeling_chatglm.py
其他模型没有这个参数 所以pretraining.py 和 0730_sft_test.py  的model.from_pretrained方法之前加了if modeltype = chatglm判断

eval loss= nan 但是train loss 正常下降 shi大佬说是跟torch_dtype=float16有关 但是实际上我测试发现因为 tokenized 之后cur_len mismatch导致的 部分samples相当于是空  所以为nan
我自己导入了一份 finetun_1_turn_conv目录下354.json 就全是1轮对话  这样就没有mismatch 就让evalloss正常了
但是小问题或者不是问题：1.eval acc这个值停留在0 然后上升到0.18就不变了 可能是acc参数有问题 所以sh大佬自己sft代码中没有向pt代码一样使用这个comput metric acc了
第二 这个多轮对话就mismatch的bug  还是等大佬自己修改
第三 针对chatglm  我把cur_len 手动 加了2 这样才能训练--template_name chatglm2 \ 参考 0730_sft_test.py 的817行左右
这个多轮对话的mismatch很有问题 目前不知道怎么解决

#---------20230731 本项目的pretrain_chatglm2_qlora.py 基本完成----------#
注意这个空白文件 不能有 不然会有4个token长的空白sample  影响准确率 并且很难排查 记住是空白内容text引起的 tokenizer会把他们视为四个token ：64790, 64792, 30910, 13
def group_texts(examples,block_size):
        # Concatenate all texts.
        #concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}
        logger.error(f"examples.keys()={examples.keys()}")
        total_length = len(examples[list(examples.keys())[0]])
        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can
        # customize this part to your needs.
        print(f"每个txt文件对应的samples个数 total_length={total_length}")
        logger.error(f"注意如果出现空白内容txt 则会出现异常短的单个sample 长度为4 类似[64790, 64792, 30910, 13, 0, 0, 0, 0, 0,0]，很难排查 所以数据集目录中不要出现空白内容txt。")

#----------20230801----------#
查看 训练后 lora_A/B 部分的weight model.base_model.model.transformer.encoder.layers[20].self_attention.query_key_value.lora_A.default.weight
查看 原始模型的参数也可以以此类推

SFT的数据格式一定要注意和chatglm2的格式保持一致 不然微调和似乎学到了只是但是输出都是胡说¶
[Round {可以填1}]\n\n问：{}\n\n答：
https://huggingface.co/THUDM/chatglm2-6b/blob/main/tokenization_chatglm.py#L168
参考本项目主页

#----------20230805----------#
多轮对话sft 成功  格式为sharegpt 使用的是sharegpt_1K + medi_1k 对话效果还行 能识别出继续 这种 使用的是chat 而非stream chat
经验 lr=1.8e-5 跑了15个小时 loss从8下降到0.06 
注意多轮对话格式一定要跟官网format一致：注意Q_temp A_temp 以及Q A的处理
 Q_temp = "[Round {}]\n\n问：{}"
 A_temp = "\n\n答：{}\n\n"
    input_ids =[]
    labels =[]
    for turn_number,conversation in enumerate(example['history']):
        q = conversation[0]
        a = conversation[1]
        Q = Q_temp.format(turn_number+1,q)  ## modify 20230803
        A = A_temp.format(a)


项目在https://www.kaggle.com/code/shxhuang/0804-sft/edit/run/138881384
peft_model_path= "/kaggle/working/chatGLM-6B-QLoRA/output-sharegpt-2k-sft-0805-v1/checkpoint-250 
另外这个是从./output-sharegpt-2k-sft-0804-v4/checkpoint-3980 继续sft得到 这个2980ckpt loss也很低 到0.1了
脚本
# ./data/sharegpt_multi_turn_data 目录不能有空白json文件
!git pull --all --force 
#!pip install  -U git+https://github.com/huggingface/peft.git   # 20230717 peft==0.4.0正式发布了 不用调版本了推理完后再训练需要重新升级到0.4.0dev 所以有这句
!deepspeed --include localhost:0,1  sft_multi_turn_qlora_chatglm2.py  \
  --train_args_json luzi.json \
  --model_name_or_path THUDM/chatglm2-6b \
  --output_dir output-sharegpt-2k-sft-0805-v1 \
  --num_train_samples -1 \
  --num_eval_samples 2 \
  --resume_from_checkpoint ./output-sharegpt-2k-sft-0804-v4/checkpoint-3980 \
  --train_data_path ./data/sharegpt_data  \
  --eval_data_path  ./data/sharegpt_data    \
  --data_type sharegpt  \
  --max_length 1800 \
  --lora_rank 64 \
  --lora_dropout 0.05 \
  --compute_dtype fp16 \
  --per_device_train_batch_size 2 \
  --per_device_eval_batch_size 2  \
  --gradient_accumulation_steps 1 \
  --learning_rate  1.8e-5 \
  --num_train_epochs  40  \
  --save_total_limit 2 \
  --load_in_4bit True \
--deepspeed ds_zero2_config.json

#------20230808-------#
https://blog.csdn.net/weixin_38145317/article/details/104917218
hasattr(model, "lora_A")
https://huggingface.co/datasets/Anthropic/hh-rlhf/viewer/Anthropic--hh-rlhf/train?row=98
https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/src/glmtuner/tuner/rm/trainer.py#L43
https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/d22a77eb87c36bb353b1e0e7caf064cb7fdf32ac/src/glmtuner/tuner/core/trainer.py#L12
dpo:https://github.com/lvwerra/trl/blob/main/examples/dpo.py
dpo:https://huggingface.co/blog/dpo-trl


#-------20230810-------#
查看model中某一层的参数 ，例如model结构如下，print(model)显示下面的结构
PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): ChatGLMForConditionalGeneration(
      (transformer): ChatGLMModel(
        (embedding): Embedding(
          (word_embeddings): Embedding(65024, 4096)
        )
        (rotary_pos_emb): RotaryEmbedding()
        (encoder): GLMTransformer(
          (layers): ModuleList(
            (0-27): 28 x GLMBlock(
              (input_layernorm): RMSNorm()
              (self_attention): SelfAttention(
                (query_key_value): Linear4bit(
                  in_features=4096, out_features=4608, bias=True
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=64, out_features=4608, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (core_attention): CoreAttention(
                  (attention_dropout): Dropout(p=0.0, inplace=False)
                )
                (dense): Linear4bit(
                  in_features=4096, out_features=4096, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=64, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
              )
              (post_attention_layernorm): RMSNorm()
              (mlp): MLP(
                (dense_h_to_4h): Linear4bit(
                  in_features=4096, out_features=27392, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=4096, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=64, out_features=27392, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
                (dense_4h_to_h): Linear4bit(
                  in_features=13696, out_features=4096, bias=False
                  (lora_dropout): ModuleDict(
                    (default): Dropout(p=0.05, inplace=False)
                  )
                  (lora_A): ModuleDict(
                    (default): Linear(in_features=13696, out_features=64, bias=False)
                  )
                  (lora_B): ModuleDict(
                    (default): Linear(in_features=64, out_features=4096, bias=False)
                  )
                  (lora_embedding_A): ParameterDict()
                  (lora_embedding_B): ParameterDict()
                )
              )
            )
          )
          (final_layernorm): RMSNorm()
        )
        (output_layer): Linear(in_features=4096, out_features=65024, bias=False)
      )
    )
  )
)
查看print(model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.lora_A.default.weight)
    print(model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.weight)
    print(model.base_model.model.transformer.encoder.layers[27].self_attention.dense.weight)


#------20230811------#
参考本项目 rewardmodel_qlora_chatglm2.py
# 注意 如果训练出现 key_error :'eval_loss' 那么说明collator传入的dict中没有labels这个key ，的确我们这个奖励模型的collator确实没有labels（因为后面才计算reward）
# 此时 参考 原始train()中training_args的定义 对比发现其中有个参数 label_names=[] 而我们的hf_train_args一开始是没有写这个参数的 打印后可以看到默认情况下label_names=None
# 参考 https://discuss.huggingface.co/t/keyerror-eval-loss-when-using-trainer-with-bertforqa/1920
# 这个Q-A类型的数据的 dataset 也是非常态化的 而是包含了'start_positions', 'end_positions' ，见  self.encodings.keys() = ['input_ids', 'attention_mask', 'start_positions', 'end_positions']
# 这种不包含labels但是又有自定义标签的数据（也就是collator输出的） 要在trainingargs中加入label_names = ["start_positions", "end_positions"] 将其作为标签信息
# 而奖励模型就没有标签信息 因此要显式的写明 label_names=[] 这样后面才能正常的在eval阶段计算eval_loss 不然在eval阶段发现eval结果中很神奇的没有eval_loss
# 另外参考 https://stackoverflow.com/questions/74239556/keyerror-eval-loss-in-hugginface-trainer
# 这种非标准化的collator输出（也就是没有labels） 需要在collator输出的dict中明显加上 "return_loss":True .代码中确实加了。
# 参考 https://huggingface.co/transformers/v4.2.2/_modules/transformers/training_args.html 提到了 label_names
#  label_names (:obj:`List[str]`, `optional`):
#              The list of keys in your dictionary of inputs that correspond to the labels.

#             Will eventually default to :obj:`["labels"]` except if the model used is one of the
#             :obj:`XxxForQuestionAnswering` in which case it will default to :obj:`["start_positions",
#             "end_positions"]`.

# TRL trl使用要点 PPOtrainer PPOConfig参数设置  经常性的清理memory   https://huggingface.co/docs/trl/v0.5.0/en/customization#use-the-cuda-cache-optimizer
 config = PPOConfig(..., optimize_cuda_cache=True)
 同样是上面的页面 trl的ppo训练似乎需要AutoModelForCausalLMWithValueHead类型的model才能训练
 from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
# 1. load a pretrained model
model = AutoModelForCausalLMWithValueHead.from_pretrained('bigscience/bloom-560m')

https://huggingface.co/docs/trl/trainer#trl.RewardTrainer
提到了奖励模型的选择 推荐是AutoModelForSequenceClassification 
也提到了RewardDataCollatorWithPadding data collator的输出是至少以下4个字段 难怪很多人都这么写
input_ids_chosen
attention_mask_chosen
input_ids_rejected
attention_mask_rejected

https://medium.com/@mlblogging.k/reinforcement-learning-for-tuning-language-models-how-chatgpt-is-trained-9ecf23518302
解释了ppo过程中奖励模型为何不更新 怕过拟合
While training only the policy is updated and the reward model is not optimised. The reason is that the reward model is only a ‘proxy’ for the human preferences which is trained using a handful of examples and optimising it along with the policy causes overfitting.

https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/src/glmtuner/tuner/core/loader.py#L126
这里单独为chatglm系列增加了lm_head 才能满足AutoModelForCausalLMWithValueHead的要求 见 https://huggingface.co/docs/trl/models#trl.AutoModelForCausalLMWithValueHead
这里说必须有（"lm_head", "embed_out"）之一 见lm_head_namings (tuple) — A tuple of strings that are used to identify the language model head of the wrapped model. This is set to ("lm_head", "embed_out") for this class but can be changed for other models in the future


#-----20230818----#
关于langchain 与chatglm 结合 做本地知识库的事
主要可以参考langchain-chatchat / localGPT 项目  其中前一个代码比较多 更新的快 后一个基本不更新但是代码结构更清晰
另外从学习的角度我更看好https://zhuanlan.zhihu.com/p/622418308 和 https://zhuanlan.zhihu.com/p/630857325
这里面提到了使用chatglm官网进行api方式启动的代码 ，也就是https://github.com/THUDM/ChatGLM-6B#api%E9%83%A8%E7%BD%B2
我这里换成了chatglm2-6b的api.py 启动  
启动后就可以将这个url作为参数（endpoint_url）传给langchain.llms.ChatGLM
参考这里 https://python.langchain.com/docs/integrations/llms/chatglm
这样会返回一个llm 这个llm就可以传给localGPT的run_localGPT.py 代码 211行
https://github.com/PromtEngineer/localGPT/blob/main/run_localGPT.py#L211C10-L212C17 虽然localGPT用的是pipeline
但是目前pipeline不支持chatglm 所以还是用langchain.llms提供的ChatGLM类就行 这个类实际上没有加载模型 而是提供一个chatglm服务的入口 从endpoint_url参数就可以猜到
我搜索了互联网 RetrievalQA.from_chain_type( llm=ChatGLM() 的这种写法有很多 所以确实是支持的
参见 https://juejin.cn/post/7236028062873550908
     https://www.langchain.cn/t/topic/361/4
     https://python.langchain.com/docs/integrations/llms/chatglm
     https://developer.volcengine.com/articles/7257721812740472890
     重点参考 https://zhuanlan.zhihu.com/p/644619003
     重点参考 https://zhuanlan.zhihu.com/p/644167758
     重点参考的文章提到了项目 https://github.com/liangwq/Chatglm_lora_multi-gpu/blob/main/APP_example/chat_langchain/chatglm_llm.py#L3
     里面有重写ChatGLM类的代码 Chatglm_lora_multi-gpu/APP_example/chatglm_agent
/llm_model.py  这是chatglm2的 上面一行是chatglm的
     ChatGLM类用法见https://github.com/liangwq/Chatglm_lora_multi-gpu/blob/1646b5b4db8ec9df74b4d68e9a9865d27e0bd9ba/APP_example/langchain_ChatGLM/chains/local_doc_qa.py#L35
      其35-38行

使用FastAPI在kaggle上启动uvicorn服务
https://www.kaggle.com/code/ahmedshahriarsakib/fastapi-on-kaggle-notebook-ngrok-postman
      
#------20230821------#
dpo 训练后generate的代码
https://github.com/huggingface/trl/issues/621#issuecomment-1672778405
dpo 训练后的各项指标wandb图
https://wandb.ai/krasul/huggingface/runs/c54lmder?workspace=user- 
来自https://huggingface.co/blog/dpo-trl的conclusion部分

dpo相关代码demo
https://github.com/huggingface/trl/tree/main/examples/research_projects/stack_llama_2/scripts
https://huggingface.co/docs/trl/main/en/dpo_trainer
peft的autopeftmodel使用demo  这个autopeftmodelforcausalLM似乎直接从sft后的lora目录加载模型
https://github.com/huggingface/blog/blob/main/dpo-trl.md
https://github.com/huggingface/blog/blob/main/dpo-trl.md
https://medium.com/@anchen.li/fine-tune-llama-2-with-sft-and-dpo-8b57cf3ec69#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImMzYWZlN2E5YmRhNDZiYWU2ZWY5N2U0NmM5NWNkYTQ4OTEyZTU5NzkiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMTI2MTQwMTcxNzkwMzE3MTgxNTUiLCJlbWFpbCI6Imh1Z2dpbmdmYWNhaUBnbWFpbC5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmJmIjoxNjkyODU5MTIyLCJuYW1lIjoiaHVnZ2luZyBmYWNhaSIsInBpY3R1cmUiOiJodHRwczovL2xoMy5nb29nbGV1c2VyY29udGVudC5jb20vYS9BQWNIVHRldmtsaktmaEJGU0w3TGR4Q3dka0owSGNFYkRRMl9JWmdZZjcxV1NxN1Q9czk2LWMiLCJnaXZlbl9uYW1lIjoiaHVnZ2luZyIsImZhbWlseV9uYW1lIjoiZmFjYWkiLCJsb2NhbGUiOiJ6aC1DTiIsImlhdCI6MTY5Mjg1OTQyMiwiZXhwIjoxNjkyODYzMDIyLCJqdGkiOiJlYzA2ZGIzYzcwMjY2NmVhODZlYTFlOTZlMjdhZmUwNzQ3MmVhYmRlIn0.C1lCLm9QhilUrUT4qCJh81j6hdIij_RnCGfE9T2aKr-86jQ91x4C9X9pz4TWQU1JESKjNVGSHxFdkANHn-wfnOfe6N9d1eMCj0F44NqFM7dDdSXn1whJRb085Nhd-zSHtEU6nAt16b-hyMY1h6VucxDel3ack1BJAh1rHAeinAI8tBwyYhZOuwCbEDWJV89Nj1dIORLGA35TP3HcMcBlB71zl_TBXz6H2Y4MsSTgYCexj6ZseYYJrI0_3XxRkEkb5onDSD1bbGxxCO-2-yCRZ2zXKDKmmQrLQbf5UKKVEJSulPmDdQ2AKFuXKruyCvcL0vm0UXd1rJ0_YEoet2P8jA


dpo trainer 中labels and logits shift 位移的代码 https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py#L295
        labels = labels[:, 1:].clone()
        logits = logits[:, :-1, :]
        loss_mask = labels != self.label_pad_token_id


from peft import AutoPeftModelForCausalLM
下面这个好像merge过了 ？
model = AutoPeftModelForCausalLM.from_pretrained(
    "dpo/final_checkpoint",
    low_cpu_mem_usage=True,
    torch_dtype=torch.float16,
    load_in_4bit=True,
)
model.generate(...)
参考用法
https://note.com/npaka/n/na506c63b8cc9
https://github.com/edumunozsala/llama-2-7B-4bit-python-coder/blob/main/Llama-2-finetune-qlora-python-coder.ipynb
https://github.com/edumunozsala/llama-2-7B-4bit-python-coder
看文章末尾链接 https://towardsai.net/p/machine-learning/fine-tuning-a-llama-2-7b-model-for-python-code-generation
https://ukey.co/blog/finetune-llama-2-peft-qlora-huggingface/
https://github.com/huggingface/notebooks/blob/main/sagemaker/28_train_llms_with_qlora/sagemaker-notebook.ipynb

automodelforcausallm 似乎支持chatglm？那不就直接可以用dpo算法了？我测一下
https://blog.51cto.com/u_15746412/7015314
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained("THUDM/chatglm2-6b")
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b")
input_ids = tokenizer.encode("你好，我是大帅哥 ", return_tensors="pt")
output_ids = model.generate(input_ids)
output_text = tokenizer.decode(output_ids[0])

fasttransformer 推理加速 一般不支持chatglm  现在看看似乎有支持的
https://github.com/THUDM/ChatGLM-6B/issues/109
https://huggingface.co/TMElyralab/lyraChatGLM
https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/PROJECT.md
https://huggingface.co/TMElyralab/lyraChatGLM/discussions  腾讯为chatglm搞的9000token/s 吹？
https://raw.githubusercontent.com/THUDM/ChatGLM-6B/main/PROJECT.md

#----20230823----#
关于peft的AutoPeftModelForCausalLM 直接从adapter目录加载出完整的base+adapter model的测试
1.结论是 确实可用 这个语法糖还不错 用法和transformers.AutoModel的from_pretrained方法差不多
2.如果之前的adapter是qlora训练的 那么需要在from_pretrained方法中加入quantization_config 
  而from_pretrained方法中load_in_4bit=True可以写也可以不写 此时的量化方法是qlora的  不是模型自身的（比如chatglm自身的quantize.py）
3. 如果前面使用了qlora但是后面使用autopeftmodelforcausllm.from_pretrained加载时没有加入quantization_config 那么
   写load_in_4bit=True时会使用模型自带的量化方法比如chatglm的quantize.py量化方法 我们会发现量化后的模型层参数与qlora量化完全对不上
   说明不能这样 必须带上quantization_config
4. 上面的结论是在和原始加载方法的对比上获得的  原始加载方法就是先automodel.from_pretrained 加入quantization_config 
   加载basemodel
   再使用peft的model = PeftModel.from_pretrained(model, peft_model_path)加载adapter路径  无论如何原始方法是标准结果
   所以一定注意在qlora模型加载时 autopeftmodelforcausallm.from_pretrained中要加上quantization_config

dataset.map中preprocess函数的作用 map中batched=True/False的区别
# map函数中batched=False 时 preprocess(sample)函数相当于横着看dataset 按照一行一行的sample传参
# 参数sample=｛q:str , chosen:str , rejected:str｝每个key对应一个str而非list
# 而当map函数中batched=True 可以这么理解 相当于是preprocess(samples)竖着看dataset 传参samples时一次性传原dataset一个列 比如
# samples ={question:list[str] , chosen: list[str],rejected:list[str]}
# 所以samples每个key对应的都是个list 
# 此时get_stack_exchange_paired 这个preprocess函数返回的也是个字典 其中每个key对应的也是list[str]
# 注意 经过map之后 其实也是一样的Arrow结构，竖着看也是一个key对应一个list[str] 比如train_dataset ["prompt"]是一个list 包含所有prompt  
# 而横着看是一个个完整样本的dict 比如train_dataset[0]={prompt:p,r1:str1,r2:str2}
# 这横着看竖着看还真能帮助理解
# 不管map的batched=True/False 经过dataset.map获取的结果都是一个Arrow，是一样的，仍然能横着看和竖着看。

dpo算法的奖励信号怎么来的？
# 看 dpo_trainer 源码 https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py#L302
# 似乎就是拿logits做了一次softmax变成概率的list 然后每个token的概率list取对应indxe=labels的概率值作为奖励 所有奖励求平均作为最终的奖励信号 
# 训练的目标是让chosen回答的平均奖励 比 reject回答的奖励信号大 大的越多越好
这样的话 如果先不在特定语料上做SFT 直接dpo 那不管是chosen还是reject 奖励信号（输出该token的概率）都基本接近0 
所以必须要在特定的语料上做SFT 才能让模型起码能够输出相关的token 初始的奖励信号不至于太小 后面才能有效训练

list[dict]结构的数据怎么load
# 构造一个list[dict]的obj 存到文件
a=[ {"text":"a sentence example."} for _ in range(5)  ]
import json
with open("a.x","w",encoding="utf-8") as fw:
    json.dump(a,fw)

# list[dict]形式的数据集加载方式 使用Dataset.from_list
# 要求datasets>=2.5.0 kaggle上是2.1.0使用pip install datasets -U 升级版本
from datasets import Dataset 
#my_list = [{"a": 1}, {"a": 2}, {"a": 3}] 
import json
my_list = json.load(open("a.x","r"))  # 
dataset = Dataset.from_list(my_list)
print(dataset)


#----20230829----#
在使用trl的dpo算法时 如果是qlora训练注意 optim一定要optim="paged_adamw_8bit", 不能像medicalGPT那样使用optim="paged_adamw_32bit",
否则loss=0 其他指标为NaN
我发现如果正常传入base model（或者合并后的sft 其实也是不带adapter的base model 形式） 使用lora而非qlora 则收敛的很快 
即使参考medicalGPT项目的dpo  qlora还是收敛的很慢
参考 https://colab.research.google.com/drive/1kMIe3pTec2snQvLBA00Br8ND1_zwy3Gr?usp=sharing#scrollTo=Xmltp4ILwSfu
lora收敛的巨快 我自己fork了一个项目也发现了这个现象
https://www.kaggle.com/huggingfacai/dpo-medgpt/edit
optim一定要optim="paged_adamw_8bit"之外 看看下面还有哪些optimizer可以用 trl和medicalGPT默认用的paged_adamw_32bit
我之前运气好用了optim="paged_adamw_8bit"  但是发现qlora 配合dpo 不收敛 。。。
https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py#L138

使用medicalGPT这个项目的dpo实现时 注意 我fork的medicalGPT/medGPT_0828
1. 普通lora（非qlora）时 可以将optim从adamw_hf 换成paged_adamw_32bit 收敛快 换成 paged_lion_32bit 收敛也快 而且根据lion优化器只存储一阶动量的特点 显存消耗更少
2. gradient_accumulation_steps 默认是4 这是写在训练代码中的dpo_training.py 这个值4 会影响reward/accuracies的计算 我改写了一份dpo_only_for_show_acc.py(medGPT_0828 这个分支)
查看一下policy_chosen_logps 到底是啥
这个reward_accuracies跟gradient_accumulation_steps有关 代码中默认是4 目前gradient_accumulation_steps=4 我在下面调用时显式的写出来
所以每次是4对chosen_rewards与rejected_rewards比较 那难怪会在 [0 , 0.25 , 0.5 ,0.75] 之间跳 很离散 而且这样的话train过程中的acc其实没有多大指示性 毕竟只有4对
还是要看eval_acc才有用 理解了！
参考 https://github.com/valkryhx/MedicalGPT/blob/medGPT_0828/dpo_3.py#L327 和 L334
求了 reward_acc的mean() 因为reward_acc是类似[1,0,0,1]这种比较结果后得到的True/False 再float()转成1./0.的tensor！参考下面的代码：
import torch
a=torch.tensor([1])
b=torch.tensor([5])
c=(a>b)
print(c)
print(c.float())
输出
tensor([False])
tensor([0.])

3.medicalGPT的 data/reward/中的test.json只有100条数据 使用普通版本的lora确实很容易就收敛了 但是我这里data/reward_yunguan有2800多条  train_loss其实有波动
 从0.3-1.x都出现过 这里注意 train的acc只是一个gradient_accumulation_steps=4 的[1,0,1,1]的mean 比如这就0.75了 所以其实经常在0 ，0.25 ， 0.5 ， 0.75这几个值之间跳
意义不大 主要还是要看eval_loss和eval_acc 以及 reward/margin  正常情况下 这个reward/margin是越来越大的 至少到0.4以上 不会在0.01徘徊

4 .目前 trl的dpo使用到的关键库
!pip install bitsandbytes==0.41.1
!pip install peft==0.5.0 
!pip install accelerate==0.21.0 
!pip install trl==0.6.0

#---20230830---#
参见https://www.tensorflow.org/datasets/splits
load_dataset 支持 split=train[:1%]这种写法 split=f"train[:{args.validation_split_percentage}%]  666
if "validation" not in raw_datasets.keys():
        raw_datasets["validation"] = load_dataset(
            'json',
            data_files=data_files,
            split=f"train[:{args.validation_split_percentage}%]",  # 注意train[:1%] 写法是支持的！参见https://www.tensorflow.org/datasets/splits
            cache_dir=args.cache_dir,
            )

使用命令行参数覆盖默认参数  注意赋值语句后千万不要加逗号 就是这样hf_train_args.per_device_train_batch_size=args.per_device_train_batch_size【,】
这个逗号会让赋值成为元组tuple 也就是让hf_train_args.xxx 的type成为tuple  这个错误贼奇特 第一次见 元组居然确实可以不加括号 我以为末尾是逗号，就会报错 结果语法没问题 
a = 3  # a is int
a = 3, # 语法居然不会报错 此时a = tuple ！！！真奇葩的写法 居然不小心在末尾加逗号遇到了


#---20230905---#
关于在训练chatglm2-6b时 数据是否需要format成 【问:{question}\n\n答:{answer}\n\n】的研究 
在hf上托管的chatglm2-6b源码的modeling_chatglm.py的849行的class ChatGLMForConditionalGeneration(ChatGLMPreTrainedModel)中查看forward方法
其中涉及到transformer_outputs = self.transformer 而transformer_outputs 之后就是loss计算了
这个self.transformer 又在854行找到  self.transformer = ChatGLMModel(config, empty_init=empty_init, device=device)
也就是实例化ChatGLMModel 并调用其forward 
在731行class ChatGLMModel(ChatGLMPreTrainedModel)的784行找到了forward 其中调用了get_prompt
这个get_prompt 没有使用【问:{question}\n\n答:{answer}\n\n】  可以检查代码

反过来看infer的时候 在1018行找到def chat 在1036行找到def stream_chat  这2个方法都是带有 @torch.inference_mode()上下文管理器标记的
def chat中用到了self.build_inputs 
build_inputs用到了tokenizer.build_prompt
而tokenizer.build_prompt在tokenzization_chatglm.py 的 162行 很明显格式化了prompt
 def build_prompt(self, query, history=None):
        if history is None:
            history = []
        prompt = ""
        for i, (old_query, response) in enumerate(history):
            prompt += "[Round {}]\n\n问：{}\n\n答：{}\n\n".format(i + 1, old_query, response)
        prompt += "[Round {}]\n\n问：{}\n\n答：".format(len(history) + 1, query)
        return prompt
再说这个def stream_chat
其中用到了build_stream_inputs 而build_stream_inputs中明显格式化了prompt prompt = "[Round {}]\n\n问：{}\n\n答：".format(len(history) + 1, query)
**** 综上
在训练时 chatglm2没有隐含的给语料加上格式化操作 但是在使用model.chat 和model.stream_chat时又在后台用了格式化
为了让训练和推理时保持一致（这样的一致会减少胡说的概率 相当于强制让训练过程符合推理过程的格式化要求）
所以要显式的让训练的样本做【问:{question}\n\n答:{answer}\n\n】格式化


qwen的格式化和baichuan的格式化 可以参考MedicalGPT的supervised_finetuning.py 里面都有 
其中qwen需要手动设置好tokenizer.eos_token / tokenizer.pad_token = tokenizer.eos_token 设置好以后tokenizer会自动扩展size 
medicalGPT使用的是<|im_end|> 但是我看qwen的tokenizer config里面实际约定的eos_toekn_id 是151643 也就是 <|endoftext|> ?
可能不影响吧 还有个特殊符号标记开头 是 <|im_start|> 
目前 qwen的防注入攻击默认是不开启的 也就是 <|endoftext|>会被tokenizer标记为单一的151643 符合社区大众用法 
而不是最最开始的 <  |  end of text |  > 这样的零散的好几个词符

qwen模型的训练是否在forward中format了input
可以看到modeling_qwen.py的class qwenlmheadmodel中 （config的automdoel指定为这个模型类）
的forward中并没有相关的format代码
但是在def chat和def stream_chat 中可以看到
995行 assert generation_config.chat_format == 'chatml'
1004行的make_context 是from .qwen_generation_utils import (
    HistoryType,
    make_context,
...)
我们在qwen_generation_utils.py 中看到119行的def make_context的171行
context_tokens += (
            nl_tokens
            + im_start_tokens
            + _tokenize_str("user", query)[1]
            + im_end_tokens
            + nl_tokens
            + im_start_tokens
            + tokenizer.encode("assistant")
            + nl_tokens
        )
这里nl_tokens 是\n的token 也就是 context_tokens的格式是 \n<|im_start|>user+问题<|im_end|>\nassistant\n
实际的多轮对话是prev_chat = (
                f"\n{im_start}{query_text}{im_end}\n{im_start}{response_text}{im_end}"
            )
其中query_text 是_tokenize_str("user", turn_query)  也就是f"{role}\n{content}"  user\n 提问
response_text 是 _tokenize_str("assistant", turn_repsonse) 也就是f"{role}\n{content}" assistant\n 回答
\n{im_start}{query_text}{im_end}\n{im_start}{response_text}{im_end} 也就是
\n<|im_start|>user\n提问<|im_end|>\n<|im_start|>assistant\n回答<|im_end|>


LORA 微调 embedding 层  参考https://github.com/QwenLM/Qwen-7B/issues/240#issuecomment-1695354408
注意加完新的token之后 要model.resize_token_embeddings(len(tokenizer))  不然报错
> 对于`<|extra_0|>`至`<|extra_204|>`，无论是预训练中还是Chat模型微调中，它们都没有被使用，相当于是“空白”概念。因而通过后续微调可以赋予全新的意义，且不与之前训练过程中使用的特殊token冲突。未微调新加入的token的话，确实也会导致生成异常。
> 
> 对于该场景，以下方案供您参考：
> 
> 1. 建议微调embedding，可开启peft中微调embedding的功能，参见[Add nn.Embedding Support to Lora huggingface/peft#337 (comment)](https://github.com/huggingface/peft/pull/337#issuecomment-1527412343)
> 2. 复用类似功能的特殊token，如chat模型中的`<|endoftext|>`表示文档结束、`<|im_start|>`表示一轮开始、`<|im_end|>`表示一轮结束，但可能难以修正其原有含义。
> 3. 采用纯文本表达，但可能不够鲁棒。

有个老旧但是完整的参考例子 里面将module_to_save=[wte,lmhead]我估计这个就是targe_modules=[wte,lmhead]的老旧写法
https://github.com/huggingface/peft/pull/337#issuecomment-1527412343
https://github.com/huggingface/peft/pull/337#issuecomment-1527412343

wte 就是GPTJmodel的word to embedding 层  #参考 https://huggingface.co/transformers/v4.11.3/_modules/transformers/models/gptj/modeling_gptj.html
class GPTJModel(GPTJPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)

        self.embed_dim = config.n_embd
        self.vocab_size = config.vocab_size
        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)

 ConversationBufferWindowMemory的memroy 加上return_messages=True 那么返回的语句
memory.load_memory_variables({})

 是个lsit 可以直接slice 或者 处理成标准history   {'history': [HumanMessage(content='not much you', additional_kwargs={}),
      AIMessage(content='not much', additional_kwargs={})]}

HumanMessage.content 直接得到 问题 string
AIMessage.conten 直接得到回答string

参考  我在kaggle上也实现了
https://python.langchain.com/docs/modules/memory/types/buffer_window


#---20230906---#
FastAPI的多文件上传 参考
https://www.tutorialsbuddy.com/python-fastapi-upload-files?expand_article=1   这个有上传和保存文件的代码 包括单一文件和多个文件上传
https://github.com/tiangolo/fastapi/issues/2694#issuecomment-769030865  这是多文件上传的调用写法
https://cloud.tencent.com/developer/ask/sof/107266271   多文件上传的调用写法
https://stackoverflow.com/questions/63138497/uploading-multiple-files-uploadfiles-fastapi
https://fastapi.tiangolo.com/zh/tutorial/request-files/
https://stackoverflow.com/questions/68981869/how-to-upload-a-single-file-to-fastapi-server-using-curl     curl的调用写法
https://github.com/tiangolo/fastapi/issues/2694#issue-792976892   curl分多次上传多个文件的写法

#---20230908---#
model = AutoModel.from_pretrained(global_args.model_name_or_path,
                                          trust_remote_code=True,                           
                                          load_in_4bit=【True if global_args.use_qlora else False】 ,   
                                          # 如果用qlora的话 load_in_4bit这里写True/False都不影响 因为都会被q_config的load_in_4bit=True覆盖   
                                          # 通过True/False条件下的logger.error("加载完基座模型layers[27].self_attention.query_key_value.weight")确认layer是一样的unit8值

在基座模型加载完后 （也就是model=AutoModel.from_pretrained）
直接写model.config.use_cache= False  也就是基座模型就设置use_cache=False 这样就不用在加载完lora层的peftmodel再写一次model.config.use_cache= False
# base_model 先设置use_cache=False 后面peftModel也会设置一次 在Trainer.train之前
# 不使用old的past_query_key_value  设置为True的话是为了加快训练 但是用的是旧的qkv
model.config.use_cache= False
logger.debug(f"base model.config.use_cache={model.config.use_cache}")
如果基座模型加载后没写 也可以在peftmodel后面写 可以移到trainer=LoraTrainer(xxx)之前
logger.debug(f"model.config.use_cache={model.config.use_cache}")  #  如果basemodel没写 这里就是True
model.config.use_cache = False   #设置False
logger.debug(f"model.config.use_cache={model.config.use_cache}")  #  现在就是False

# hf_train_args.report_to=args.report_to
参考   https://github.com/valkryhx/MedicalGPT/blob/medGPT_0828/dpo_peftmodel_my_style_0830.py
    ## 关于这个report_to 参数 我看了之前的sft/reward_model代码 都是不在命令行里面传的 直接使用从luzi.json配置文件中读取的"report_to":"tensorboard"配置 
        ##  这个配置在hf_train_args.report_to会被转成 ['tensorboard'] 这是符合这个参数list[str]的格式的 因为相关的处理程序会遍历这个list 
        ## 如果直接将ScriptArguments获取的args.report_to 赋值给hf_train_args 那么会导致hf_train_args.report_to= 'tensorboard' 而处理程序会遍历这个str 导致获取 t e n 。。。这一系列char来作为存放log的变量 
        ## 所以会报错 t is not valid  please use tensorboard，wandb xxx等 这种诡异的错误
        ## 为了使用这个变量 我在后面赋值时加一个[] 将 'tensorboard' 转成['tensorboard'],即hf_train_args.report_to=[args.report_to]
    hf_train_args.report_to=[args.report_to]

reward和ppo rlhf能跑的代码 rm_3.py ppo_chatglm2.py
https://github.com/valkryhx/chatGLM-6B-QLoRA/blob/main/chatglm-rewardmodel-qlora-rm_3%E6%88%90%E5%8A%9F_run_ppo%E6%88%90%E5%8A%9F.ipynb

#---20230914---#
chatglm2-6b中能让回答产生多样性（同一个问题 在top-k / temperature设置不变的情况下多种回答）的因素：
1.beam_search开启>1 ，默认是beam_search ==1 所以是关着的 
2.do_sample 默认是True（见model.chat源码）
这个平时一致忽略了 
在do_sample =True 时 有如下代码 【https://huggingface.co/THUDM/chatglm2-6b/blob/main/modeling_chatglm.py#L1161】
if generation_config.do_sample:
    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)
else:
    next_tokens = torch.argmax(probs, dim=-1)
上面的multinomial 是多项式采样，比如 probs=[0.1, 0.2,0.4,0.3] 先按照概率值大小排序成[0.4,0.3,0.2,0.1]然后将这些概率值当成[0,1)区间的线段
取一个随机值random random依次对比区间累计值落在哪个区间就取对应的区间idx。
例如random=0.85 第0累计区间为0.4 < 0.85
               第1累计区间为0.4+0.3=0.7 <0.85
               第2累计区间为0.4+0.3+0.2 = 0.9 > 0.85 
               则取idx = 2 （0.7<=0.85<0.9）
               返回idx=2的token
排序是为了让大的概率值排在前面的idx 从而优先被采样

在huggingface的 【https://huggingface.co/transformers/v3.2.0/_modules/transformers/generation_utils.html】 中也有类似的代码
比如
if do_sample:
                # Temperature (higher temperature => more likely to sample low probability tokens)
                if temperature != 1.0:
                    scores = scores / temperature
                # Top-p/top-k filtering
                next_token_logscores = top_k_top_p_filtering(scores, top_k=top_k, top_p=top_p)
                # Sample
                probs = F.softmax(next_token_logscores, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1).squeeze(1)

今天偶然发现chatglm2-6b也有 sequence classification头了  参考https://huggingface.co/THUDM/chatglm2-6b/blob/main/modeling_chatglm.py#L1198
查看ChatGLMForSequenceClassification
查看相关的classifier_head 代码 其实就是做了线性变换 将hidden_size 变换成num_labels维度
self.classifier_head = nn.Linear(config.hidden_size, config.num_labels, bias=True, dtype=torch.half)


