{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-11T10:51:26.245193Z","iopub.execute_input":"2023-08-11T10:51:26.245750Z","iopub.status.idle":"2023-08-11T10:51:26.252539Z","shell.execute_reply.started":"2023-08-11T10:51:26.245696Z","shell.execute_reply":"2023-08-11T10:51:26.251518Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/valkryhx/chatGLM-6B-QLoRA","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:27:03.974099Z","iopub.execute_input":"2023-08-11T16:27:03.974411Z","iopub.status.idle":"2023-08-11T16:27:04.963233Z","shell.execute_reply.started":"2023-08-11T16:27:03.974382Z","shell.execute_reply":"2023-08-11T16:27:04.961925Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"fatal: destination path 'chatGLM-6B-QLoRA' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-09T13:43:24.155248Z","iopub.execute_input":"2023-08-09T13:43:24.156012Z","iopub.status.idle":"2023-08-09T13:43:25.216333Z","shell.execute_reply.started":"2023-08-09T13:43:24.155960Z","shell.execute_reply":"2023-08-09T13:43:25.214930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd chatGLM-6B-QLoRA\n!git pull --all --force \n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-08-11T16:27:51.229453Z","iopub.execute_input":"2023-08-11T16:27:51.229934Z","iopub.status.idle":"2023-08-11T16:28:33.909552Z","shell.execute_reply.started":"2023-08-11T16:27:51.229890Z","shell.execute_reply":"2023-08-11T16:28:33.908255Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/chatGLM-6B-QLoRA\nFetching origin\nremote: Enumerating objects: 17, done.\u001b[K\nremote: Counting objects: 100% (17/17), done.\u001b[K\nremote: Compressing objects: 100% (14/14), done.\u001b[K\nremote: Total 14 (delta 7), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (14/14), 122.77 KiB | 3.07 MiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   3d73fa8..4c06287  main       -> origin/main\nUpdating 3d73fa8..4c06287\nFast-forward\n chatglm-rewardmodel-qlora.ipynb                    |    2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n rm_3.py                                            | 1602 \u001b[32m++++++++++++++++++++\u001b[m\n ...50\\200\\203\\344\\270\\216\\347\\273\\217\\351\\252\\214\" |   23 \u001b[32m+\u001b[m\n 3 files changed, 1626 insertions(+), 1 deletion(-)\n create mode 100644 rm_3.py\nCollecting peft==0.4.0 (from -r requirements.txt (line 1))\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.30.2)\nCollecting datasets==2.12.0 (from -r requirements.txt (line 3))\n  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm==4.65.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.65.0)\nCollecting loguru==0.7.0 (from -r requirements.txt (line 5))\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fire==0.5.0 (from -r requirements.txt (line 6))\n  Downloading fire-0.5.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting bitsandbytes==0.39.0 (from -r requirements.txt (line 7))\n  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting wandb==0.15.3 (from -r requirements.txt (line 8))\n  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting cpm_kernels==1.0.11 (from -r requirements.txt (line 9))\n  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.20.3)\nRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.1.99)\nCollecting deepspeed==0.9.5 (from -r requirements.txt (line 12))\n  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting evaluate==0.4.0 (from -r requirements.txt (line 13))\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting trl==0.4.7 (from -r requirements.txt (line 14))\n  Downloading trl-0.4.7-py3-none-any.whl (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (2.0.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (0.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (11.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (2.3.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.1.31)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.20.3)\nCollecting hjson (from deepspeed==0.9.5->-r requirements.txt (line 12))\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.11.1)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (9.0.0)\nRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.10.10)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (4.0.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2->-r requirements.txt (line 2)) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0->-r requirements.txt (line 1)) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2023.5.7)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2023.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.3.0)\nBuilding wheels for collected packages: fire, deepspeed\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=1cf9d83165ff32d6c11a707ff8108d9b49f6f2588d30bcc97c3e558da1f931ca\n  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844547 sha256=524c2e3003df8b074a8bba2829c90ccc982d3b0c230afb9327ca1b1664101c42\n  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\nSuccessfully built fire deepspeed\nInstalling collected packages: hjson, cpm_kernels, bitsandbytes, loguru, fire, wandb, deepspeed, peft, datasets, trl, evaluate\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.5\n    Uninstalling wandb-0.15.5:\n      Successfully uninstalled wandb-0.15.5\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed bitsandbytes-0.39.0 cpm_kernels-1.0.11 datasets-2.12.0 deepspeed-0.9.5 evaluate-0.4.0 fire-0.5.0 hjson-3.1.0 loguru-0.7.0 peft-0.4.0 trl-0.4.7 wandb-0.15.3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:13:52.884914Z","iopub.execute_input":"2023-08-10T14:13:52.885305Z","iopub.status.idle":"2023-08-10T14:13:54.440350Z","shell.execute_reply.started":"2023-08-10T14:13:52.885271Z","shell.execute_reply":"2023-08-10T14:13:54.438905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!CUDA_VISIBLE_DEVICES=0 python  rm_trl.py \\\n--model_name 'THUDM/chatglm2-6b' \\\n--resume_from_checkpoint /kaggle/working/chatGLM-6B-QLoRA/reward_model_0809_v1/checkpoint-50 \\\n--num_train_epochs 2 \\\n--gradient_accumulation_steps 1 \\\n--per_device_train_batch_size 4 \\\n--per_device_eval_batch_size  4 \\\n--max_length 512 \\\n--output_dir ./reward_model_0810_v2 \\\n--train_subset 80 \\\n--eval_subset 20 \\\n--local_rank 0  \\\n--bf16 False","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:36:48.134969Z","iopub.execute_input":"2023-08-10T15:36:48.135458Z","iopub.status.idle":"2023-08-10T15:40:03.300811Z","shell.execute_reply.started":"2023-08-10T15:36:48.135418Z","shell.execute_reply":"2023-08-10T15:40:03.299460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 上面的warning There were missing keys in the checkpoint model loaded: 'transformer.embedding.word_embeddings.weight', 'transformer.rotary_pos_emb.inv_freq', 'transformer.encoder.layers.0.input_layernorm.weight', ....\n# 是 trainingArguments中设置 load_best_model_at_end = True 后出现的\n# 说明 trainer的确在最后把最好的那个adapters的参数做了load（只不过load时strict=True要求严格匹配了）\n# 这说明保存在output_dir中的pytorch_model.bin和vhead.bin参数都是最佳的 我们来验证下","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-11T13:20:02.990258Z","iopub.execute_input":"2023-08-11T13:20:02.990655Z","iopub.status.idle":"2023-08-11T13:20:04.059182Z","shell.execute_reply.started":"2023-08-11T13:20:02.990619Z","shell.execute_reply":"2023-08-11T13:20:04.057829Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"验证成功  跑的是py中的test_load_best()\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# deepspeed 单机多卡从头训练 reward model","metadata":{}},{"cell_type":"code","source":"!git pull --all --force \n!deepspeed --include localhost:0,1  rewardmodel_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-rm-1k-0811-v2 \\\n  --num_train_samples 200 \\\n  --num_eval_samples 100 \\\n  --train_data_path ./data/rm_data  \\\n  --eval_data_path  ./data/rm_data    \\\n  --data_type sharegpt  \\\n  --max_length 800 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 4 \\\n  --per_device_eval_batch_size 4  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  2e-5 \\\n  --num_train_epochs  2  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json\n","metadata":{"execution":{"iopub.status.busy":"2023-08-11T12:40:22.920460Z","iopub.execute_input":"2023-08-11T12:40:22.920924Z","iopub.status.idle":"2023-08-11T13:06:52.328475Z","shell.execute_reply.started":"2023-08-11T12:40:22.920884Z","shell.execute_reply":"2023-08-11T13:06:52.327240Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Fetching origin\nremote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (3/3), 1.80 KiB | 461.00 KiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   91e90a0..7a781c3  main       -> origin/main\nUpdating 91e90a0..7a781c3\nFast-forward\n ...2\\350\\200\\203\\344\\270\\216\\347\\273\\217\\351\\252\\214\" | 19 \u001b[32m+++++++++++++++++++\u001b[m\n 1 file changed, 19 insertions(+)\n[2023-08-11 12:40:30,370] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-11 12:40:47,113] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n[2023-08-11 12:40:47,130] [INFO] [runner.py:555:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None rewardmodel_qlora_chatglm2.py --train_args_json luzi.json --model_name_or_path THUDM/chatglm2-6b --output_dir output-rm-1k-0811-v2 --num_train_samples 200 --num_eval_samples 100 --train_data_path ./data/rm_data --eval_data_path ./data/rm_data --data_type sharegpt --max_length 800 --lora_rank 64 --lora_dropout 0.05 --compute_dtype fp16 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --learning_rate 2e-5 --num_train_epochs 2 --save_total_limit 2 --load_in_4bit True --deepspeed ds_zero2_config.json\n[2023-08-11 12:40:48,936] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-11 12:40:54,938] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n[2023-08-11 12:40:54,938] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n[2023-08-11 12:40:54,938] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n[2023-08-11 12:40:54,938] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n[2023-08-11 12:40:54,938] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n[2023-08-11 12:40:54,938] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n[2023-08-11 12:40:54,938] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n[2023-08-11 12:40:54,938] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n[2023-08-11 12:40:54,938] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n[2023-08-11 12:40:54,938] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n[2023-08-11 12:40:54,938] [INFO] [launch.py:163:main] dist_world_size=2\n[2023-08-11 12:40:54,938] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-11 12:41:04,155] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-08-11 12:41:04,274] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n\u001b[32m2023-08-11 12:41:08.835\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1272\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n\u001b[32m2023-08-11 12:41:08.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1272\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n\u001b[32m2023-08-11 12:41:09.095\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1318\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-11 12:41:09.097\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n\u001b[32m2023-08-11 12:41:09.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1318\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-11 12:41:09.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 461.93it/s]\n\u001b[32m2023-08-11 12:41:09.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m596\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-11 12:41:09.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1m在取样之后 data len =200\u001b[0m\n\u001b[32m2023-08-11 12:41:09.334\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m604\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 459.85it/s]\n\u001b[32m2023-08-11 12:41:09.383\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m596\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-11 12:41:09.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1m在取样之后 data len =200\u001b[0m\n\u001b[32m2023-08-11 12:41:09.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m604\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n\u001b[32m2023-08-11 12:41:10.488\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n\u001b[32m2023-08-11 12:41:10.498\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 345.98it/s]\n\u001b[32m2023-08-11 12:41:10.678\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m596\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 545.28it/s]\n\u001b[32m2023-08-11 12:41:10.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m596\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-11 12:41:10.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1m在取样之后 data len =100\u001b[0m\n\u001b[32m2023-08-11 12:41:10.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m604\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n\u001b[32m2023-08-11 12:41:10.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1m在取样之后 data len =100\u001b[0m\n\u001b[32m2023-08-11 12:41:10.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m604\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\nMap: 100%|████████████████████████████| 100/100 [00:00<00:00, 176.43 examples/s]number_train_samples=200\nnumber_of_eval_samples=100\nnumber_train_samples=200                                                        \nnumber_of_eval_samples=100\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:49<00:00, 15.67s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:49<00:00, 15.67s/it]\nmemory footprint of model: 3.6520424485206604 GB\nmemory footprint of model: 3.6520424485206604 GB\n\u001b[32m2023-08-11 12:43:02.171\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1428\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\n\u001b[32m2023-08-11 12:43:02.170\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1428\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\nbelow trainable paramters only contains peft lora params.\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nmodel_type:  chatglm\nRewardModel(\n  (transformer): ChatGLMModel(\n    (embedding): Embedding(\n      (word_embeddings): Embedding(65024, 4096)\n    )\n    (rotary_pos_emb): RotaryEmbedding()\n    (encoder): GLMTransformer(\n      (layers): ModuleList(\n        (0-27): 28 x GLMBlock(\n          (input_layernorm): RMSNorm()\n          (self_attention): SelfAttention(\n            (query_key_value): Linear4bit(\n              in_features=4096, out_features=4608, bias=True\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4608, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n            (core_attention): CoreAttention(\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n            )\n            (dense): Linear4bit(\n              in_features=4096, out_features=4096, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4096, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n          )\n          (post_attention_layernorm): RMSNorm()\n          (mlp): MLP(\n            (dense_h_to_4h): Linear4bit(\n              in_features=4096, out_features=27392, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=27392, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n            (dense_4h_to_h): Linear4bit(\n              in_features=13696, out_features=4096, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=13696, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4096, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n          )\n        )\n      )\n      (final_layernorm): RMSNorm()\n    )\n    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n  )\n  (v_head): Linear(in_features=4096, out_features=1, bias=False)\n  (loss_fn): PairWiseLoss()\n)\nbelow trainable params include v_head\ntransformer_trainable_param = f118587392\nadd v_head params\ntrainable params: 118591488 || all params: 3506903040 || trainable%: 3.3817\nFinished loading model and tokenizer\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=[],\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-rm-1k-0811-v2,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=2.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-rm-1k-0811-v2,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=4,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\nbelow trainable paramters only contains peft lora params.\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nmodel_type:  chatglm\nRewardModel(\n  (transformer): ChatGLMModel(\n    (embedding): Embedding(\n      (word_embeddings): Embedding(65024, 4096)\n    )\n    (rotary_pos_emb): RotaryEmbedding()\n    (encoder): GLMTransformer(\n      (layers): ModuleList(\n        (0-27): 28 x GLMBlock(\n          (input_layernorm): RMSNorm()\n          (self_attention): SelfAttention(\n            (query_key_value): Linear4bit(\n              in_features=4096, out_features=4608, bias=True\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4608, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n            (core_attention): CoreAttention(\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n            )\n            (dense): Linear4bit(\n              in_features=4096, out_features=4096, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4096, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n          )\n          (post_attention_layernorm): RMSNorm()\n          (mlp): MLP(\n            (dense_h_to_4h): Linear4bit(\n              in_features=4096, out_features=27392, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=27392, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n            (dense_4h_to_h): Linear4bit(\n              in_features=13696, out_features=4096, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=13696, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4096, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n          )\n        )\n      )\n      (final_layernorm): RMSNorm()\n    )\n    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n  )\n  (v_head): Linear(in_features=4096, out_features=1, bias=False)\n  (loss_fn): PairWiseLoss()\n)\nbelow trainable params include v_head\ntransformer_trainable_param = f118587392\nadd v_head params\ntrainable params: 118591488 || all params: 3506903040 || trainable%: 3.3817\nFinished loading model and tokenizer\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=[],\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=1,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-rm-1k-0811-v2,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=2.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-rm-1k-0811-v2,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=4,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\n  0%|                                                    | 0/50 [00:00<?, ?it/s]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.2451, 1.1934, 1.0693, 1.1387], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([0.5264, 0.3171, 0.9897, 2.5508], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.4971, 1.4717, 1.5117, 1.4990], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 1.0938,  2.6035, -1.0859,  3.7793], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n  2%|▉                                           | 1/50 [00:21<17:17, 21.17s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([2.1855, 1.9971, 1.9014, 2.1895], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.7305, 1.4385, 1.2617, 5.8906], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.6631, 1.5625, 1.5283, 1.8496], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.2588, 1.7676, 2.1172, 3.7949], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n  4%|█▊                                          | 2/50 [00:37<14:41, 18.37s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.6182, 1.4590, 1.6152, 1.7871], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.2500, 1.6230, 1.9912, 4.6484], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.1133, 1.2598, 1.3154, 1.3633], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.1553, 3.3965, 1.7676, 1.9688], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n  6%|██▋                                         | 3/50 [00:54<13:50, 17.68s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.6318, 1.7725, 1.4766, 1.9434], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.4854, 1.9785, 0.6943, 2.7070], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.0938, 1.4033, 1.2119, 1.4795], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([0.9927, 1.4180, 2.9746, 0.1272], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n  8%|███▌                                        | 4/50 [01:11<13:23, 17.47s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.3496, 1.2646, 1.0889, 1.2158], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 2.2012,  1.7070, -2.6406,  3.5801], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([ 0.5361, -0.1332,  0.3821,  0.4392], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 0.6748, -2.3594,  0.3933,  4.9766], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 10%|████▍                                       | 5/50 [01:29<13:05, 17.46s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.7959, 1.3652, 1.4570, 1.7090], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.2549, 1.4736, 0.3752, 5.1445], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.3037, 1.0713, 1.1504, 1.4805], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 0.7617,  2.2363, -0.3801,  6.0000], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 12%|█████▎                                      | 6/50 [01:45<12:39, 17.26s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([ 0.9214,  0.6167,  1.4072, -1.3047], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 0.3210, -2.7266,  0.7632,  5.8984], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([0.7510, 0.6240, 0.8311, 3.9004], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([-3.3008, -0.8052, -0.6685,  3.5957], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 14%|██████▏                                     | 7/50 [02:02<12:17, 17.15s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([0.7388, 0.8228, 0.8901, 0.3367], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 0.3254, -0.1087,  0.9946,  3.4512], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([ 1.0928,  0.8643,  1.1504, -0.8125], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([-1.3350,  1.3350, -0.2668,  3.4160], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 16%|███████                                     | 8/50 [02:19<12:00, 17.15s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.7109, 1.3496, 1.3691, 1.8027], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 1.1895,  1.2803, -0.1792,  3.2461], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.8457, 1.7979, 1.4326, 1.8857], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.0859, 1.3965, 0.6001, 3.3418], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 18%|███████▉                                    | 9/50 [02:37<11:43, 17.17s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.7949, 1.5879, 1.5127, 1.7598], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.3760, 1.7588, 0.1981, 3.4570], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.9541, 1.6416, 1.6289, 2.0430], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.4902, 2.2812, 0.6294, 3.5527], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'loss': 1.0993, 'learning_rate': 1e-05, 'epoch': 0.4}                          \n 20%|████████▌                                  | 10/50 [02:54<11:26, 17.17s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 15%|██████▊                                     | 2/13 [00:06<00:36,  3.31s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 23%|██████████▏                                 | 3/13 [00:13<00:47,  4.71s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 31%|█████████████▌                              | 4/13 [00:19<00:49,  5.45s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 38%|████████████████▉                           | 5/13 [00:26<00:46,  5.87s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 46%|████████████████████▎                       | 6/13 [00:33<00:42,  6.11s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 54%|███████████████████████▋                    | 7/13 [00:39<00:37,  6.31s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 62%|███████████████████████████                 | 8/13 [00:46<00:32,  6.41s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 69%|██████████████████████████████▍             | 9/13 [00:53<00:25,  6.50s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 77%|█████████████████████████████████          | 10/13 [00:59<00:19,  6.55s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 85%|████████████████████████████████████▍      | 11/13 [01:06<00:13,  6.58s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 92%|███████████████████████████████████████▋   | 12/13 [01:13<00:06,  6.60s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n100%|███████████████████████████████████████████| 13/13 [01:19<00:00,  6.60s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n                                                                                \n\u001b[A{'eval_loss': 1.045790433883667, 'eval_accuracy': 0.52, 'eval_runtime': 87.305, 'eval_samples_per_second': 1.145, 'eval_steps_per_second': 0.149, 'epoch': 0.4}\n 20%|████████▌                                  | 10/50 [04:21<11:26, 17.17s/it]\n100%|███████████████████████████████████████████| 13/13 [01:22<00:00,  6.60s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 12:49:08.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0811-v2/checkpoint-10\u001b[0m\n\u001b[32m2023-08-11 12:49:08.907\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m802\u001b[0m - \u001b[31m\u001b[1m333 backbone_model, PreTrainedModel\u001b[0m\nTRAIN_TYPE ==lora\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.3281, 1.1025, 1.2920, 1.4404], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.8994, 2.9551, 0.6870, 4.2344], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.5488, 1.3047, 1.4961, 1.4033], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([0.8179, 0.9365, 2.0469, 4.8125], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 22%|█████████▍                                 | 11/50 [04:40<28:56, 44.52s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([2.0078, 2.1875, 1.9531, 2.1680], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([3.3145, 2.6621, 0.7681, 1.4609], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([2.2949, 2.2988, 1.9463, 2.3086], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.4736, 1.5088, 0.0867, 4.7188], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 24%|██████████▎                                | 12/50 [04:57<22:54, 36.16s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.4043, 1.2178, 1.2373, 1.9531], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 1.1914,  1.9814, -1.2412,  2.8223], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([2.6133, 2.3496, 1.9727, 2.3730], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.5059, 1.7139, 2.1250, 3.0195], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 26%|███████████▏                               | 13/50 [05:15<18:44, 30.41s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.8721, 1.7832, 2.1348, 0.0335], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([0.3369, 1.0166, 1.7842, 2.8672], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([2.6875, 3.0840, 3.2969, 2.9668], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([3.4102, 0.2666, 2.2520, 3.7773], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 28%|████████████                               | 14/50 [05:32<15:50, 26.41s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([2.9785, 2.8359, 3.3379, 3.3301], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([2.3477, 1.4434, 2.6309, 3.5859], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([3.6641, 3.3828, 3.2012, 3.4531], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([2.4824, 2.1719, 0.6562, 3.6445], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 30%|████████████▉                              | 15/50 [05:49<13:46, 23.62s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([4.0742, 3.8184, 3.6406, 3.8809], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([2.5547, 2.2207, 1.0371, 3.0996], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([3.8145, 3.8965, 4.3164, 3.5703], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([2.5781, 0.5415, 0.0583, 4.5820], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 32%|█████████████▊                             | 16/50 [06:06<12:18, 21.71s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([4.9453, 4.7773, 4.6719, 4.6758], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([3.7109, 3.5137, 0.7344, 5.1719], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([4.2188, 4.4531, 4.5703, 4.1523], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 4.6875,  2.6230,  1.2949, -0.2377], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 34%|██████████████▌                            | 17/50 [06:23<11:11, 20.36s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([ 3.2129,  3.7207, -0.7192,  0.6748], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 1.3818, -1.0244, -0.0414,  1.3945], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([5.9336, 5.0820, 5.3633, 4.9336], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([3.8047, 3.0605, 1.7598, 3.0059], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 36%|███████████████▍                           | 18/50 [06:41<10:20, 19.39s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([5.7773, 5.8203, 4.5195, 5.1992], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([3.4531, 2.9297, 0.8086, 2.3730], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([5.3633, 5.3516, 6.2031, 5.6016], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([-0.6812, -1.7734,  1.3496,  1.4150], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 38%|████████████████▎                          | 19/50 [06:58<09:40, 18.72s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([8.3672, 8.5859, 8.5859, 8.4375], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 6.0703,  4.7188, -0.8574,  3.5840], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([6.2891, 6.8672, 5.9961, 5.6641], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([3.6191, 2.8379, 3.1738, 3.2305], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'loss': 0.5277, 'learning_rate': 1.7500000000000002e-05, 'epoch': 0.8}         \n 40%|█████████████████▏                         | 20/50 [07:15<09:07, 18.26s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 15%|██████▊                                     | 2/13 [00:06<00:36,  3.31s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 23%|██████████▏                                 | 3/13 [00:13<00:46,  4.68s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 31%|█████████████▌                              | 4/13 [00:19<00:48,  5.42s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 38%|████████████████▉                           | 5/13 [00:26<00:46,  5.86s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 46%|████████████████████▎                       | 6/13 [00:33<00:42,  6.12s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 54%|███████████████████████▋                    | 7/13 [00:39<00:37,  6.31s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 62%|███████████████████████████                 | 8/13 [00:46<00:32,  6.41s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 69%|██████████████████████████████▍             | 9/13 [00:53<00:25,  6.47s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 77%|█████████████████████████████████          | 10/13 [00:59<00:19,  6.54s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 85%|████████████████████████████████████▍      | 11/13 [01:06<00:13,  6.56s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 92%|███████████████████████████████████████▋   | 12/13 [01:13<00:06,  6.59s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n100%|███████████████████████████████████████████| 13/13 [01:19<00:00,  6.62s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n                                                                                \n\u001b[A{'eval_loss': 0.09284980595111847, 'eval_accuracy': 0.98, 'eval_runtime': 86.4762, 'eval_samples_per_second': 1.156, 'eval_steps_per_second': 0.15, 'epoch': 0.8}\n 40%|█████████████████▏                         | 20/50 [08:41<09:07, 18.26s/it]\n100%|███████████████████████████████████████████| 13/13 [01:22<00:00,  6.62s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 12:53:29.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0811-v2/checkpoint-20\u001b[0m\n\u001b[32m2023-08-11 12:53:29.104\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m802\u001b[0m - \u001b[31m\u001b[1m333 backbone_model, PreTrainedModel\u001b[0m\nTRAIN_TYPE ==lora\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([7.4102, 7.3633, 7.8477, 6.3906], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 1.1494,  2.7520, -1.5742,  3.3594], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([9.2109, 7.4766, 7.6016, 7.5469], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([4.6797, 3.8496, 1.9766, 3.0352], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 42%|██████████████████                         | 21/50 [09:01<21:31, 44.52s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([5.8516, 0.8496, 0.2664, 0.7285], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 1.4834,  0.2561, -1.1572,  0.1085], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([10.0469, 10.5156, 10.6406,  9.3281], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([4.4688, 0.8481, 1.0830, 1.7754], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 44%|██████████████████▉                        | 22/50 [09:18<16:56, 36.29s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([ 9.4766, 11.2891, 11.5156,  7.8398], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 5.3047,  1.0098, -0.6953,  1.1738], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([8.7188, 9.7031, 8.5625, 6.3750], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.8984, 2.2773, 2.1875, 3.3145], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 46%|███████████████████▊                       | 23/50 [09:35<13:45, 30.56s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([10.9375, 11.0000, 11.7344, 11.8047], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([9.7500, 6.1016, 2.9414, 1.2842], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([ 9.4062,  9.4688,  9.1172, 10.4609], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([8.1172, 2.2188, 0.3313, 0.8760], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 48%|████████████████████▋                      | 24/50 [09:52<11:30, 26.56s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([11.5156, 12.0781, 12.6797, 12.5391], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([10.1406,  6.5664,  0.1650,  1.4746], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([11.4844, 11.3984, 11.8047,  8.4453], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 2.4688, -1.9434, -0.4546,  1.2510], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 50%|█████████████████████▌                     | 25/50 [10:09<09:54, 23.78s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([12.5547, 13.3203, 13.1172, 12.1328], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([8.2969, 5.0859, 1.4248, 0.1876], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([12.3047, 12.6016, 13.2734, 13.2422], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 8.0781,  6.2227, -0.9028,  1.4521], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 52%|██████████████████████▎                    | 26/50 [10:27<08:43, 21.80s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([14.5312, 14.4219, 13.1328, 10.5000], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([6.3789, 4.7695, 1.5518, 1.4473], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([12.7188, 13.1250, 11.5938,  9.6641], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([7.5039, 5.1250, 1.6191, 0.0490], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 54%|███████████████████████▏                   | 27/50 [10:44<07:49, 20.42s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([12.9141, 12.9922, 10.7422,  9.6406], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 7.1328,  5.7422, -0.2360,  3.1934], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([13.2734, 13.7422, 10.1016,  9.4844], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 7.6250,  6.0625, -0.8916,  1.5938], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 56%|████████████████████████                   | 28/50 [11:01<07:07, 19.44s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([9.5000, 9.6328, 9.1094, 8.2109], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([7.0664, 6.1055, 1.1279, 1.3916], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([9.6250, 9.8516, 8.8047, 8.5078], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 7.3242,  6.1875,  0.9043, -1.0801], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 58%|████████████████████████▉                  | 29/50 [11:18<06:33, 18.74s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([9.3359, 8.0469, 6.0078, 1.4209], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 4.5586, -0.5991, -0.4229,  2.2207], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([9.2734, 8.2266, 5.5547, 0.1798], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([0.3743, 2.4531, 0.4836, 1.3018], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'loss': 0.0613, 'learning_rate': 1.25e-05, 'epoch': 1.2}                       \n 60%|█████████████████████████▊                 | 30/50 [11:35<06:05, 18.28s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 15%|██████▊                                     | 2/13 [00:06<00:36,  3.32s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 23%|██████████▏                                 | 3/13 [00:13<00:46,  4.70s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 31%|█████████████▌                              | 4/13 [00:19<00:48,  5.44s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 38%|████████████████▉                           | 5/13 [00:26<00:46,  5.84s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 46%|████████████████████▎                       | 6/13 [00:33<00:42,  6.11s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 54%|███████████████████████▋                    | 7/13 [00:39<00:37,  6.30s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 62%|███████████████████████████                 | 8/13 [00:46<00:31,  6.40s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 69%|██████████████████████████████▍             | 9/13 [00:53<00:25,  6.47s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 77%|█████████████████████████████████          | 10/13 [00:59<00:19,  6.53s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 85%|████████████████████████████████████▍      | 11/13 [01:06<00:13,  6.57s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 92%|███████████████████████████████████████▋   | 12/13 [01:13<00:06,  6.58s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n100%|███████████████████████████████████████████| 13/13 [01:19<00:00,  6.62s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n                                                                                \n\u001b[A{'eval_loss': 0.052043382078409195, 'eval_accuracy': 0.98, 'eval_runtime': 86.5241, 'eval_samples_per_second': 1.156, 'eval_steps_per_second': 0.15, 'epoch': 1.2}\n 60%|█████████████████████████▊                 | 30/50 [13:02<06:05, 18.28s/it]\n100%|███████████████████████████████████████████| 13/13 [01:22<00:00,  6.62s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 12:57:49.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0811-v2/checkpoint-30\u001b[0m\n\u001b[32m2023-08-11 12:57:49.589\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m802\u001b[0m - \u001b[31m\u001b[1m333 backbone_model, PreTrainedModel\u001b[0m\nTRAIN_TYPE ==lora\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([9.6484, 8.8828, 7.7969, 6.5312], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 4.9766,  1.8398,  1.3096, -1.7637], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([9.8828, 9.4297, 8.9844, 8.1719], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([6.7148, 5.5625, 4.2617, 1.6172], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 62%|██████████████████████████▋                | 31/50 [13:22<14:11, 44.83s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([9.2891, 9.0391, 8.2188, 6.7227], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([5.3555, 4.4102, 0.5649, 0.4934], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([9.1094, 8.5625, 7.4492, 5.6914], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 5.0234,  0.3579, -0.5928, -0.0693], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 64%|███████████████████████████▌               | 32/50 [13:39<10:57, 36.53s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([9.8906, 9.2734, 8.5938, 7.3242], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 5.5391,  4.2891,  2.6309, -0.3215], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([10.0000,  8.9375,  7.8164,  6.3398], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 4.6250,  1.4541, -0.1809, -0.3188], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 66%|████████████████████████████▍              | 33/50 [13:56<08:42, 30.72s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([11.2266,  9.9766,  8.4844,  6.7266], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 5.1953,  2.6816,  3.2363, -0.3735], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([10.5781,  9.6094,  8.3281,  6.8242], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 4.5234,  1.7090, -0.3286, -0.0746], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 68%|█████████████████████████████▏             | 34/50 [14:14<07:06, 26.65s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([11.5859,  9.4219,  6.7188,  5.2109], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([-1.6123, -1.2705, -0.5776, -1.2100], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([9.5547, 8.0234, 5.6953, 0.9248], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 0.0506, -2.6738,  0.0728, -1.1533], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 70%|██████████████████████████████             | 35/50 [14:31<05:57, 23.81s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([13.5156, 11.5312,  8.3281,  4.8281], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([2.7559, 0.8804, 0.8267, 0.9355], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([13.9062, 11.8984,  9.0547,  6.4258], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 4.7695,  0.3987,  1.4941, -2.8730], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 72%|██████████████████████████████▉            | 36/50 [14:48<05:05, 21.81s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([16.1875, 14.9531, 11.5703,  6.7578], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 4.3008,  3.6094, -0.3975, -1.9219], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([13.2188, 11.0938,  5.4375,  0.5195], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([-1.2988,  0.4023, -0.6543, -0.5454], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 74%|███████████████████████████████▊           | 37/50 [15:05<04:25, 20.42s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([14.5156, 12.8047,  9.2500,  4.1016], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 2.4004,  2.3652,  4.0781, -1.0117], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([16.3125, 16.4062, 13.0625,  7.5977], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 3.6328,  2.7598,  0.0593, -0.6753], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 76%|████████████████████████████████▋          | 38/50 [15:22<03:53, 19.44s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([17.9688, 17.7500, 15.2891,  8.6094], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 4.5938,  4.1797,  0.9658, -0.2576], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([17.6719, 17.2500, 14.1328,  7.2656], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 3.8965,  3.2012,  1.3887, -0.2954], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 78%|█████████████████████████████████▌         | 39/50 [15:39<03:26, 18.74s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([19.0156, 17.6719, 15.8125,  9.2812], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 4.6797,  2.6875,  1.4092, -3.7695], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([18.1406, 17.7500, 15.1250,  8.3125], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 4.1094,  3.5000,  1.9355, -2.5762], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'loss': 0.0079, 'learning_rate': 7.500000000000001e-06, 'epoch': 1.6}          \n 80%|██████████████████████████████████▍        | 40/50 [15:56<03:02, 18.24s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 15%|██████▊                                     | 2/13 [00:06<00:36,  3.30s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 23%|██████████▏                                 | 3/13 [00:13<00:46,  4.69s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 31%|█████████████▌                              | 4/13 [00:19<00:48,  5.44s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 38%|████████████████▉                           | 5/13 [00:26<00:46,  5.83s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 46%|████████████████████▎                       | 6/13 [00:33<00:42,  6.11s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 54%|███████████████████████▋                    | 7/13 [00:39<00:37,  6.28s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 62%|███████████████████████████                 | 8/13 [00:46<00:31,  6.40s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 69%|██████████████████████████████▍             | 9/13 [00:53<00:25,  6.47s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 77%|█████████████████████████████████          | 10/13 [00:59<00:19,  6.54s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 85%|████████████████████████████████████▍      | 11/13 [01:06<00:13,  6.55s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 92%|███████████████████████████████████████▋   | 12/13 [01:12<00:06,  6.58s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n100%|███████████████████████████████████████████| 13/13 [01:19<00:00,  6.58s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n                                                                                \n\u001b[A{'eval_loss': 0.005298559553921223, 'eval_accuracy': 1.0, 'eval_runtime': 86.3055, 'eval_samples_per_second': 1.159, 'eval_steps_per_second': 0.151, 'epoch': 1.6}\n 80%|██████████████████████████████████▍        | 40/50 [17:23<03:02, 18.24s/it]\n100%|███████████████████████████████████████████| 13/13 [01:22<00:00,  6.58s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 13:02:10.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0811-v2/checkpoint-40\u001b[0m\n\u001b[32m2023-08-11 13:02:10.461\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m802\u001b[0m - \u001b[31m\u001b[1m333 backbone_model, PreTrainedModel\u001b[0m\nTRAIN_TYPE ==lora\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([18.0938, 17.2812, 15.2969,  6.5195], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 0.1674, -0.7734, -0.4009, -5.2109], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([17.2500, 17.3125, 16.8906, 11.6406], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 5.1445,  4.3555,  2.2402, -1.3633], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 82%|███████████████████████████████████▎       | 41/50 [17:43<06:42, 44.67s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([18.9688, 18.5625, 17.6719, 13.5859], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 8.3594,  1.0615, -3.4531, -6.5391], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([19.1562, 18.8594, 18.9219, 17.4375], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 9.5078,  5.5703,  4.7617, -4.7734], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 84%|████████████████████████████████████       | 42/50 [18:00<04:51, 36.42s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([20.8125, 20.8750, 20.4531, 20.3438], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([17.3125,  1.9424,  0.0595, -8.2812], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([21.1562, 20.5938, 21.2656, 20.7656], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 19.5938,  16.7656,   5.6562, -13.0156], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 86%|████████████████████████████████████▉      | 43/50 [18:17<03:34, 30.64s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([21.2031, 20.7031, 21.0312, 20.0000], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 17.8281,  15.0156,   1.6699, -14.5625], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([21.2500, 20.4688, 20.8594, 20.6875], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 18.3750,  15.1016,   4.2539, -14.5312], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 88%|█████████████████████████████████████▊     | 44/50 [18:34<02:39, 26.59s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([21.3281, 21.0781, 21.2500, 19.9844], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([16.5625, 14.1172, -4.1523, -7.8359], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([21.9219, 22.2656, 21.7344, 20.3750], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 17.2812,  13.9844,   0.6821, -17.6875], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 90%|██████████████████████████████████████▋    | 45/50 [18:51<01:58, 23.76s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([20.6250, 19.8438, 16.0312, -0.6895], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -3.3730,  -0.7754,  -1.2188, -13.0312], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([22.9062, 22.9531, 22.5000, 21.6250], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 17.8750,   0.8867,  -0.3420, -21.4531], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 92%|███████████████████████████████████████▌   | 46/50 [19:09<01:27, 21.77s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([23.4062, 23.0469, 23.2500, 21.9062], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 18.0156,  15.2891,   6.8945, -10.3359], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([23.8438, 23.4062, 23.4375, 21.9375], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 17.6250,  14.2578,   4.8359, -14.8047], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 94%|████████████████████████████████████████▍  | 47/50 [19:26<01:01, 20.41s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([24.8281, 24.5938, 23.5781, 21.0781], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 14.9844,  12.9062,   0.6758, -14.7422], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([25.0156, 24.5781, 24.2812, 20.8594], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 15.3594,  14.1484,   2.3945, -14.5156], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 96%|█████████████████████████████████████████▎ | 48/50 [19:43<00:38, 19.43s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([27.2031, 26.2656, 25.4531, 21.6562], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 14.6875,  11.7969,   7.5625, -14.0312], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([26.1406, 24.7656, 21.4062, 14.9453], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -0.4683,   1.6904,   0.8896, -13.7422], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 98%|██████████████████████████████████████████▏| 49/50 [20:00<00:18, 18.69s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([29.9688, 29.2500, 25.8594, 12.9844], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([  8.2578,   9.0625,   1.0703, -10.4922], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([25.4844, 21.8125, 14.2891, -1.2002], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([  0.3025,  -1.3311,  -1.2461, -19.5156], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'loss': 0.0046, 'learning_rate': 2.5e-06, 'epoch': 2.0}                        \n100%|███████████████████████████████████████████| 50/50 [20:17<00:00, 18.23s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 15%|██████▊                                     | 2/13 [00:06<00:36,  3.33s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 23%|██████████▏                                 | 3/13 [00:13<00:47,  4.71s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 31%|█████████████▌                              | 4/13 [00:20<00:49,  5.46s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 38%|████████████████▉                           | 5/13 [00:26<00:46,  5.87s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 46%|████████████████████▎                       | 6/13 [00:33<00:42,  6.12s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 54%|███████████████████████▋                    | 7/13 [00:39<00:37,  6.29s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 62%|███████████████████████████                 | 8/13 [00:46<00:31,  6.39s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 69%|██████████████████████████████▍             | 9/13 [00:53<00:25,  6.49s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 77%|█████████████████████████████████          | 10/13 [00:59<00:19,  6.54s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 85%|████████████████████████████████████▍      | 11/13 [01:06<00:13,  6.58s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 92%|███████████████████████████████████████▋   | 12/13 [01:13<00:06,  6.59s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n100%|███████████████████████████████████████████| 13/13 [01:19<00:00,  6.63s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n                                                                                \n\u001b[A{'eval_loss': 4.758995783049613e-05, 'eval_accuracy': 1.0, 'eval_runtime': 86.6415, 'eval_samples_per_second': 1.154, 'eval_steps_per_second': 0.15, 'epoch': 2.0}\n100%|███████████████████████████████████████████| 50/50 [21:44<00:00, 18.23s/it]\n100%|███████████████████████████████████████████| 13/13 [01:22<00:00,  6.63s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 13:06:31.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0811-v2/checkpoint-50\u001b[0m\n\u001b[32m2023-08-11 13:06:31.393\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m802\u001b[0m - \u001b[31m\u001b[1m333 backbone_model, PreTrainedModel\u001b[0m\nTRAIN_TYPE ==lora\nThere were missing keys in the checkpoint model loaded: ['transformer.embedding.word_embeddings.weight', 'transformer.rotary_pos_emb.inv_freq', 'transformer.encoder.layers.0.input_layernorm.weight', 'transformer.encoder.layers.0.self_attention.query_key_value.weight', 'transformer.encoder.layers.0.self_attention.query_key_value.bias', 'transformer.encoder.layers.0.self_attention.dense.weight', 'transformer.encoder.layers.0.post_attention_layernorm.weight', 'transformer.encoder.layers.0.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.0.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.1.input_layernorm.weight', 'transformer.encoder.layers.1.self_attention.query_key_value.weight', 'transformer.encoder.layers.1.self_attention.query_key_value.bias', 'transformer.encoder.layers.1.self_attention.dense.weight', 'transformer.encoder.layers.1.post_attention_layernorm.weight', 'transformer.encoder.layers.1.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.1.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.2.input_layernorm.weight', 'transformer.encoder.layers.2.self_attention.query_key_value.weight', 'transformer.encoder.layers.2.self_attention.query_key_value.bias', 'transformer.encoder.layers.2.self_attention.dense.weight', 'transformer.encoder.layers.2.post_attention_layernorm.weight', 'transformer.encoder.layers.2.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.2.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.3.input_layernorm.weight', 'transformer.encoder.layers.3.self_attention.query_key_value.weight', 'transformer.encoder.layers.3.self_attention.query_key_value.bias', 'transformer.encoder.layers.3.self_attention.dense.weight', 'transformer.encoder.layers.3.post_attention_layernorm.weight', 'transformer.encoder.layers.3.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.3.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.4.input_layernorm.weight', 'transformer.encoder.layers.4.self_attention.query_key_value.weight', 'transformer.encoder.layers.4.self_attention.query_key_value.bias', 'transformer.encoder.layers.4.self_attention.dense.weight', 'transformer.encoder.layers.4.post_attention_layernorm.weight', 'transformer.encoder.layers.4.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.4.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.5.input_layernorm.weight', 'transformer.encoder.layers.5.self_attention.query_key_value.weight', 'transformer.encoder.layers.5.self_attention.query_key_value.bias', 'transformer.encoder.layers.5.self_attention.dense.weight', 'transformer.encoder.layers.5.post_attention_layernorm.weight', 'transformer.encoder.layers.5.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.5.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.6.input_layernorm.weight', 'transformer.encoder.layers.6.self_attention.query_key_value.weight', 'transformer.encoder.layers.6.self_attention.query_key_value.bias', 'transformer.encoder.layers.6.self_attention.dense.weight', 'transformer.encoder.layers.6.post_attention_layernorm.weight', 'transformer.encoder.layers.6.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.6.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.7.input_layernorm.weight', 'transformer.encoder.layers.7.self_attention.query_key_value.weight', 'transformer.encoder.layers.7.self_attention.query_key_value.bias', 'transformer.encoder.layers.7.self_attention.dense.weight', 'transformer.encoder.layers.7.post_attention_layernorm.weight', 'transformer.encoder.layers.7.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.7.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.8.input_layernorm.weight', 'transformer.encoder.layers.8.self_attention.query_key_value.weight', 'transformer.encoder.layers.8.self_attention.query_key_value.bias', 'transformer.encoder.layers.8.self_attention.dense.weight', 'transformer.encoder.layers.8.post_attention_layernorm.weight', 'transformer.encoder.layers.8.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.8.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.9.input_layernorm.weight', 'transformer.encoder.layers.9.self_attention.query_key_value.weight', 'transformer.encoder.layers.9.self_attention.query_key_value.bias', 'transformer.encoder.layers.9.self_attention.dense.weight', 'transformer.encoder.layers.9.post_attention_layernorm.weight', 'transformer.encoder.layers.9.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.9.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.10.input_layernorm.weight', 'transformer.encoder.layers.10.self_attention.query_key_value.weight', 'transformer.encoder.layers.10.self_attention.query_key_value.bias', 'transformer.encoder.layers.10.self_attention.dense.weight', 'transformer.encoder.layers.10.post_attention_layernorm.weight', 'transformer.encoder.layers.10.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.10.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.11.input_layernorm.weight', 'transformer.encoder.layers.11.self_attention.query_key_value.weight', 'transformer.encoder.layers.11.self_attention.query_key_value.bias', 'transformer.encoder.layers.11.self_attention.dense.weight', 'transformer.encoder.layers.11.post_attention_layernorm.weight', 'transformer.encoder.layers.11.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.11.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.12.input_layernorm.weight', 'transformer.encoder.layers.12.self_attention.query_key_value.weight', 'transformer.encoder.layers.12.self_attention.query_key_value.bias', 'transformer.encoder.layers.12.self_attention.dense.weight', 'transformer.encoder.layers.12.post_attention_layernorm.weight', 'transformer.encoder.layers.12.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.12.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.13.input_layernorm.weight', 'transformer.encoder.layers.13.self_attention.query_key_value.weight', 'transformer.encoder.layers.13.self_attention.query_key_value.bias', 'transformer.encoder.layers.13.self_attention.dense.weight', 'transformer.encoder.layers.13.post_attention_layernorm.weight', 'transformer.encoder.layers.13.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.13.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.14.input_layernorm.weight', 'transformer.encoder.layers.14.self_attention.query_key_value.weight', 'transformer.encoder.layers.14.self_attention.query_key_value.bias', 'transformer.encoder.layers.14.self_attention.dense.weight', 'transformer.encoder.layers.14.post_attention_layernorm.weight', 'transformer.encoder.layers.14.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.14.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.15.input_layernorm.weight', 'transformer.encoder.layers.15.self_attention.query_key_value.weight', 'transformer.encoder.layers.15.self_attention.query_key_value.bias', 'transformer.encoder.layers.15.self_attention.dense.weight', 'transformer.encoder.layers.15.post_attention_layernorm.weight', 'transformer.encoder.layers.15.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.15.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.16.input_layernorm.weight', 'transformer.encoder.layers.16.self_attention.query_key_value.weight', 'transformer.encoder.layers.16.self_attention.query_key_value.bias', 'transformer.encoder.layers.16.self_attention.dense.weight', 'transformer.encoder.layers.16.post_attention_layernorm.weight', 'transformer.encoder.layers.16.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.16.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.17.input_layernorm.weight', 'transformer.encoder.layers.17.self_attention.query_key_value.weight', 'transformer.encoder.layers.17.self_attention.query_key_value.bias', 'transformer.encoder.layers.17.self_attention.dense.weight', 'transformer.encoder.layers.17.post_attention_layernorm.weight', 'transformer.encoder.layers.17.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.17.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.18.input_layernorm.weight', 'transformer.encoder.layers.18.self_attention.query_key_value.weight', 'transformer.encoder.layers.18.self_attention.query_key_value.bias', 'transformer.encoder.layers.18.self_attention.dense.weight', 'transformer.encoder.layers.18.post_attention_layernorm.weight', 'transformer.encoder.layers.18.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.18.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.19.input_layernorm.weight', 'transformer.encoder.layers.19.self_attention.query_key_value.weight', 'transformer.encoder.layers.19.self_attention.query_key_value.bias', 'transformer.encoder.layers.19.self_attention.dense.weight', 'transformer.encoder.layers.19.post_attention_layernorm.weight', 'transformer.encoder.layers.19.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.19.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.20.input_layernorm.weight', 'transformer.encoder.layers.20.self_attention.query_key_value.weight', 'transformer.encoder.layers.20.self_attention.query_key_value.bias', 'transformer.encoder.layers.20.self_attention.dense.weight', 'transformer.encoder.layers.20.post_attention_layernorm.weight', 'transformer.encoder.layers.20.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.20.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.21.input_layernorm.weight', 'transformer.encoder.layers.21.self_attention.query_key_value.weight', 'transformer.encoder.layers.21.self_attention.query_key_value.bias', 'transformer.encoder.layers.21.self_attention.dense.weight', 'transformer.encoder.layers.21.post_attention_layernorm.weight', 'transformer.encoder.layers.21.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.21.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.22.input_layernorm.weight', 'transformer.encoder.layers.22.self_attention.query_key_value.weight', 'transformer.encoder.layers.22.self_attention.query_key_value.bias', 'transformer.encoder.layers.22.self_attention.dense.weight', 'transformer.encoder.layers.22.post_attention_layernorm.weight', 'transformer.encoder.layers.22.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.22.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.23.input_layernorm.weight', 'transformer.encoder.layers.23.self_attention.query_key_value.weight', 'transformer.encoder.layers.23.self_attention.query_key_value.bias', 'transformer.encoder.layers.23.self_attention.dense.weight', 'transformer.encoder.layers.23.post_attention_layernorm.weight', 'transformer.encoder.layers.23.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.23.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.24.input_layernorm.weight', 'transformer.encoder.layers.24.self_attention.query_key_value.weight', 'transformer.encoder.layers.24.self_attention.query_key_value.bias', 'transformer.encoder.layers.24.self_attention.dense.weight', 'transformer.encoder.layers.24.post_attention_layernorm.weight', 'transformer.encoder.layers.24.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.24.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.25.input_layernorm.weight', 'transformer.encoder.layers.25.self_attention.query_key_value.weight', 'transformer.encoder.layers.25.self_attention.query_key_value.bias', 'transformer.encoder.layers.25.self_attention.dense.weight', 'transformer.encoder.layers.25.post_attention_layernorm.weight', 'transformer.encoder.layers.25.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.25.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.26.input_layernorm.weight', 'transformer.encoder.layers.26.self_attention.query_key_value.weight', 'transformer.encoder.layers.26.self_attention.query_key_value.bias', 'transformer.encoder.layers.26.self_attention.dense.weight', 'transformer.encoder.layers.26.post_attention_layernorm.weight', 'transformer.encoder.layers.26.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.26.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.27.input_layernorm.weight', 'transformer.encoder.layers.27.self_attention.query_key_value.weight', 'transformer.encoder.layers.27.self_attention.query_key_value.bias', 'transformer.encoder.layers.27.self_attention.dense.weight', 'transformer.encoder.layers.27.post_attention_layernorm.weight', 'transformer.encoder.layers.27.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.27.mlp.dense_4h_to_h.weight', 'transformer.encoder.final_layernorm.weight', 'transformer.output_layer.weight'].\nThere were missing keys in the checkpoint model loaded: ['transformer.embedding.word_embeddings.weight', 'transformer.rotary_pos_emb.inv_freq', 'transformer.encoder.layers.0.input_layernorm.weight', 'transformer.encoder.layers.0.self_attention.query_key_value.weight', 'transformer.encoder.layers.0.self_attention.query_key_value.bias', 'transformer.encoder.layers.0.self_attention.dense.weight', 'transformer.encoder.layers.0.post_attention_layernorm.weight', 'transformer.encoder.layers.0.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.0.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.1.input_layernorm.weight', 'transformer.encoder.layers.1.self_attention.query_key_value.weight', 'transformer.encoder.layers.1.self_attention.query_key_value.bias', 'transformer.encoder.layers.1.self_attention.dense.weight', 'transformer.encoder.layers.1.post_attention_layernorm.weight', 'transformer.encoder.layers.1.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.1.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.2.input_layernorm.weight', 'transformer.encoder.layers.2.self_attention.query_key_value.weight', 'transformer.encoder.layers.2.self_attention.query_key_value.bias', 'transformer.encoder.layers.2.self_attention.dense.weight', 'transformer.encoder.layers.2.post_attention_layernorm.weight', 'transformer.encoder.layers.2.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.2.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.3.input_layernorm.weight', 'transformer.encoder.layers.3.self_attention.query_key_value.weight', 'transformer.encoder.layers.3.self_attention.query_key_value.bias', 'transformer.encoder.layers.3.self_attention.dense.weight', 'transformer.encoder.layers.3.post_attention_layernorm.weight', 'transformer.encoder.layers.3.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.3.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.4.input_layernorm.weight', 'transformer.encoder.layers.4.self_attention.query_key_value.weight', 'transformer.encoder.layers.4.self_attention.query_key_value.bias', 'transformer.encoder.layers.4.self_attention.dense.weight', 'transformer.encoder.layers.4.post_attention_layernorm.weight', 'transformer.encoder.layers.4.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.4.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.5.input_layernorm.weight', 'transformer.encoder.layers.5.self_attention.query_key_value.weight', 'transformer.encoder.layers.5.self_attention.query_key_value.bias', 'transformer.encoder.layers.5.self_attention.dense.weight', 'transformer.encoder.layers.5.post_attention_layernorm.weight', 'transformer.encoder.layers.5.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.5.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.6.input_layernorm.weight', 'transformer.encoder.layers.6.self_attention.query_key_value.weight', 'transformer.encoder.layers.6.self_attention.query_key_value.bias', 'transformer.encoder.layers.6.self_attention.dense.weight', 'transformer.encoder.layers.6.post_attention_layernorm.weight', 'transformer.encoder.layers.6.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.6.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.7.input_layernorm.weight', 'transformer.encoder.layers.7.self_attention.query_key_value.weight', 'transformer.encoder.layers.7.self_attention.query_key_value.bias', 'transformer.encoder.layers.7.self_attention.dense.weight', 'transformer.encoder.layers.7.post_attention_layernorm.weight', 'transformer.encoder.layers.7.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.7.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.8.input_layernorm.weight', 'transformer.encoder.layers.8.self_attention.query_key_value.weight', 'transformer.encoder.layers.8.self_attention.query_key_value.bias', 'transformer.encoder.layers.8.self_attention.dense.weight', 'transformer.encoder.layers.8.post_attention_layernorm.weight', 'transformer.encoder.layers.8.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.8.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.9.input_layernorm.weight', 'transformer.encoder.layers.9.self_attention.query_key_value.weight', 'transformer.encoder.layers.9.self_attention.query_key_value.bias', 'transformer.encoder.layers.9.self_attention.dense.weight', 'transformer.encoder.layers.9.post_attention_layernorm.weight', 'transformer.encoder.layers.9.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.9.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.10.input_layernorm.weight', 'transformer.encoder.layers.10.self_attention.query_key_value.weight', 'transformer.encoder.layers.10.self_attention.query_key_value.bias', 'transformer.encoder.layers.10.self_attention.dense.weight', 'transformer.encoder.layers.10.post_attention_layernorm.weight', 'transformer.encoder.layers.10.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.10.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.11.input_layernorm.weight', 'transformer.encoder.layers.11.self_attention.query_key_value.weight', 'transformer.encoder.layers.11.self_attention.query_key_value.bias', 'transformer.encoder.layers.11.self_attention.dense.weight', 'transformer.encoder.layers.11.post_attention_layernorm.weight', 'transformer.encoder.layers.11.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.11.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.12.input_layernorm.weight', 'transformer.encoder.layers.12.self_attention.query_key_value.weight', 'transformer.encoder.layers.12.self_attention.query_key_value.bias', 'transformer.encoder.layers.12.self_attention.dense.weight', 'transformer.encoder.layers.12.post_attention_layernorm.weight', 'transformer.encoder.layers.12.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.12.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.13.input_layernorm.weight', 'transformer.encoder.layers.13.self_attention.query_key_value.weight', 'transformer.encoder.layers.13.self_attention.query_key_value.bias', 'transformer.encoder.layers.13.self_attention.dense.weight', 'transformer.encoder.layers.13.post_attention_layernorm.weight', 'transformer.encoder.layers.13.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.13.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.14.input_layernorm.weight', 'transformer.encoder.layers.14.self_attention.query_key_value.weight', 'transformer.encoder.layers.14.self_attention.query_key_value.bias', 'transformer.encoder.layers.14.self_attention.dense.weight', 'transformer.encoder.layers.14.post_attention_layernorm.weight', 'transformer.encoder.layers.14.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.14.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.15.input_layernorm.weight', 'transformer.encoder.layers.15.self_attention.query_key_value.weight', 'transformer.encoder.layers.15.self_attention.query_key_value.bias', 'transformer.encoder.layers.15.self_attention.dense.weight', 'transformer.encoder.layers.15.post_attention_layernorm.weight', 'transformer.encoder.layers.15.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.15.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.16.input_layernorm.weight', 'transformer.encoder.layers.16.self_attention.query_key_value.weight', 'transformer.encoder.layers.16.self_attention.query_key_value.bias', 'transformer.encoder.layers.16.self_attention.dense.weight', 'transformer.encoder.layers.16.post_attention_layernorm.weight', 'transformer.encoder.layers.16.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.16.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.17.input_layernorm.weight', 'transformer.encoder.layers.17.self_attention.query_key_value.weight', 'transformer.encoder.layers.17.self_attention.query_key_value.bias', 'transformer.encoder.layers.17.self_attention.dense.weight', 'transformer.encoder.layers.17.post_attention_layernorm.weight', 'transformer.encoder.layers.17.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.17.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.18.input_layernorm.weight', 'transformer.encoder.layers.18.self_attention.query_key_value.weight', 'transformer.encoder.layers.18.self_attention.query_key_value.bias', 'transformer.encoder.layers.18.self_attention.dense.weight', 'transformer.encoder.layers.18.post_attention_layernorm.weight', 'transformer.encoder.layers.18.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.18.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.19.input_layernorm.weight', 'transformer.encoder.layers.19.self_attention.query_key_value.weight', 'transformer.encoder.layers.19.self_attention.query_key_value.bias', 'transformer.encoder.layers.19.self_attention.dense.weight', 'transformer.encoder.layers.19.post_attention_layernorm.weight', 'transformer.encoder.layers.19.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.19.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.20.input_layernorm.weight', 'transformer.encoder.layers.20.self_attention.query_key_value.weight', 'transformer.encoder.layers.20.self_attention.query_key_value.bias', 'transformer.encoder.layers.20.self_attention.dense.weight', 'transformer.encoder.layers.20.post_attention_layernorm.weight', 'transformer.encoder.layers.20.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.20.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.21.input_layernorm.weight', 'transformer.encoder.layers.21.self_attention.query_key_value.weight', 'transformer.encoder.layers.21.self_attention.query_key_value.bias', 'transformer.encoder.layers.21.self_attention.dense.weight', 'transformer.encoder.layers.21.post_attention_layernorm.weight', 'transformer.encoder.layers.21.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.21.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.22.input_layernorm.weight', 'transformer.encoder.layers.22.self_attention.query_key_value.weight', 'transformer.encoder.layers.22.self_attention.query_key_value.bias', 'transformer.encoder.layers.22.self_attention.dense.weight', 'transformer.encoder.layers.22.post_attention_layernorm.weight', 'transformer.encoder.layers.22.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.22.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.23.input_layernorm.weight', 'transformer.encoder.layers.23.self_attention.query_key_value.weight', 'transformer.encoder.layers.23.self_attention.query_key_value.bias', 'transformer.encoder.layers.23.self_attention.dense.weight', 'transformer.encoder.layers.23.post_attention_layernorm.weight', 'transformer.encoder.layers.23.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.23.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.24.input_layernorm.weight', 'transformer.encoder.layers.24.self_attention.query_key_value.weight', 'transformer.encoder.layers.24.self_attention.query_key_value.bias', 'transformer.encoder.layers.24.self_attention.dense.weight', 'transformer.encoder.layers.24.post_attention_layernorm.weight', 'transformer.encoder.layers.24.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.24.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.25.input_layernorm.weight', 'transformer.encoder.layers.25.self_attention.query_key_value.weight', 'transformer.encoder.layers.25.self_attention.query_key_value.bias', 'transformer.encoder.layers.25.self_attention.dense.weight', 'transformer.encoder.layers.25.post_attention_layernorm.weight', 'transformer.encoder.layers.25.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.25.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.26.input_layernorm.weight', 'transformer.encoder.layers.26.self_attention.query_key_value.weight', 'transformer.encoder.layers.26.self_attention.query_key_value.bias', 'transformer.encoder.layers.26.self_attention.dense.weight', 'transformer.encoder.layers.26.post_attention_layernorm.weight', 'transformer.encoder.layers.26.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.26.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.27.input_layernorm.weight', 'transformer.encoder.layers.27.self_attention.query_key_value.weight', 'transformer.encoder.layers.27.self_attention.query_key_value.bias', 'transformer.encoder.layers.27.self_attention.dense.weight', 'transformer.encoder.layers.27.post_attention_layernorm.weight', 'transformer.encoder.layers.27.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.27.mlp.dense_4h_to_h.weight', 'transformer.encoder.final_layernorm.weight', 'transformer.output_layer.weight'].\n{'train_runtime': 1308.1516, 'train_samples_per_second': 0.306, 'train_steps_per_second': 0.038, 'train_loss': 0.34017853051424024, 'epoch': 2.0}\n\u001b[32m2023-08-11 13:06:34.716\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_only_lora_and_vhead\u001b[0m:\u001b[36m308\u001b[0m - \u001b[31m\u001b[1min func_save_only_lora_and_vhead\u001b[0m\n100%|███████████████████████████████████████████| 50/50 [21:47<00:00, 26.15s/it]\n\u001b[32m2023-08-11 13:06:34.724\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_only_lora_and_vhead\u001b[0m:\u001b[36m308\u001b[0m - \u001b[31m\u001b[1min func_save_only_lora_and_vhead\u001b[0m\nModel Saved. Only save lora layers and value_head.\nModel Saved. Only save lora layers and value_head.\n[2023-08-11 13:06:48,602] [INFO] [launch.py:347:main] Process 482 exits successfully.\n[2023-08-11 13:06:48,602] [INFO] [launch.py:347:main] Process 481 exits successfully.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"--deepspeed ds_zero2_config.json","metadata":{}},{"cell_type":"markdown","source":"# 单机多卡deepspeed 从ckpt继续训练 ","metadata":{}},{"cell_type":"code","source":"!git pull --all --force \n!deepspeed --include localhost:0,1  rewardmodel_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-rm-1k-0811-v3 \\\n  --resume_from_checkpoint output-rm-1k-0811-v2 \\\n  --num_train_samples 200 \\\n  --num_eval_samples 100 \\\n  --train_data_path ./data/rm_data  \\\n  --eval_data_path  ./data/rm_data    \\\n  --data_type sharegpt  \\\n  --max_length 800 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 4 \\\n  --per_device_eval_batch_size 4  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  2e-5 \\\n  --num_train_epochs  2  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"execution":{"iopub.status.busy":"2023-08-11T14:16:06.760852Z","iopub.execute_input":"2023-08-11T14:16:06.761319Z","iopub.status.idle":"2023-08-11T14:42:34.756018Z","shell.execute_reply.started":"2023-08-11T14:16:06.761278Z","shell.execute_reply":"2023-08-11T14:42:34.754676Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Fetching origin\nremote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (3/3), 696 bytes | 232.00 KiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   52b2f25..3d73fa8  main       -> origin/main\nUpdating 52b2f25..3d73fa8\nFast-forward\n rewardmodel_qlora_chatglm2.py | 5 \u001b[32m+\u001b[m\u001b[31m----\u001b[m\n 1 file changed, 1 insertion(+), 4 deletions(-)\n[2023-08-11 14:16:14,392] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-11 14:16:31,391] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n[2023-08-11 14:16:31,408] [INFO] [runner.py:555:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None rewardmodel_qlora_chatglm2.py --train_args_json luzi.json --model_name_or_path THUDM/chatglm2-6b --output_dir output-rm-1k-0811-v3 --resume_from_checkpoint output-rm-1k-0811-v2 --num_train_samples 200 --num_eval_samples 100 --train_data_path ./data/rm_data --eval_data_path ./data/rm_data --data_type sharegpt --max_length 800 --lora_rank 64 --lora_dropout 0.05 --compute_dtype fp16 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --learning_rate 2e-5 --num_train_epochs 2 --save_total_limit 2 --load_in_4bit True --deepspeed ds_zero2_config.json\n[2023-08-11 14:16:33,249] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-11 14:16:39,259] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n[2023-08-11 14:16:39,259] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n[2023-08-11 14:16:39,259] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n[2023-08-11 14:16:39,259] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n[2023-08-11 14:16:39,259] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n[2023-08-11 14:16:39,259] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n[2023-08-11 14:16:39,259] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n[2023-08-11 14:16:39,260] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n[2023-08-11 14:16:39,260] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n[2023-08-11 14:16:39,260] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n[2023-08-11 14:16:39,260] [INFO] [launch.py:163:main] dist_world_size=2\n[2023-08-11 14:16:39,260] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-11 14:16:49,214] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-08-11 14:16:49,229] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n\u001b[32m2023-08-11 14:16:54.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1274\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n\u001b[32m2023-08-11 14:16:54.019\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1274\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n\u001b[32m2023-08-11 14:16:54.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1320\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-11 14:16:54.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m591\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n\u001b[32m2023-08-11 14:16:54.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1320\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-11 14:16:54.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m591\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 531.46it/s]\n\u001b[32m2023-08-11 14:16:54.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m598\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-11 14:16:54.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m602\u001b[0m - \u001b[1m在取样之后 data len =200\u001b[0m\n\u001b[32m2023-08-11 14:16:54.508\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n\u001b[32m2023-08-11 14:16:54.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m591\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 440.44it/s]\n\u001b[32m2023-08-11 14:16:54.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m598\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-11 14:16:54.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m602\u001b[0m - \u001b[1m在取样之后 data len =200\u001b[0m\n\u001b[32m2023-08-11 14:16:54.575\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n\u001b[32m2023-08-11 14:16:54.599\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m591\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 688.27it/s]\n\u001b[32m2023-08-11 14:16:54.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m598\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-11 14:16:54.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m602\u001b[0m - \u001b[1m在取样之后 data len =100\u001b[0m\n\u001b[32m2023-08-11 14:16:54.713\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\nnumber_train_samples=200\nnumber_of_eval_samples=100\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 712.95it/s]\n\u001b[32m2023-08-11 14:16:54.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m598\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-11 14:16:54.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m602\u001b[0m - \u001b[1m在取样之后 data len =100\u001b[0m\n\u001b[32m2023-08-11 14:16:54.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m606\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\nnumber_train_samples=200\nnumber_of_eval_samples=100\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:49<00:00, 15.67s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:49<00:00, 15.68s/it]\nmemory footprint of model: 3.6520424485206604 GB\nmemory footprint of model: 3.6520424485206604 GB\n\u001b[32m2023-08-11 14:18:45.715\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1430\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\n\u001b[32m2023-08-11 14:18:45.720\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1430\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\nbelow trainable paramters only contains peft lora params.\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nmodel_type:  chatglm\nbelow trainable paramters only contains peft lora params.\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nmodel_type:  chatglm\n\u001b[32m2023-08-11 14:20:31.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1463\u001b[0m - \u001b[1madapter_weights={'transformer.encoder.layers.0.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0107, -0.0150,  0.0017,  ..., -0.0025, -0.0062, -0.0060],\n        [ 0.0147,  0.0098, -0.0099,  ...,  0.0022, -0.0090, -0.0094],\n        [ 0.0097,  0.0102,  0.0138,  ...,  0.0094, -0.0037,  0.0062],\n        ...,\n        [-0.0156,  0.0125,  0.0113,  ..., -0.0125,  0.0031,  0.0101],\n        [-0.0023,  0.0120,  0.0146,  ..., -0.0127, -0.0008,  0.0074],\n        [ 0.0093,  0.0104, -0.0121,  ...,  0.0098, -0.0030, -0.0007]]), 'transformer.encoder.layers.0.self_attention.query_key_value.lora_B.default.weight': tensor([[-9.1392e-05, -9.2385e-05, -7.2888e-05,  ..., -1.7385e-05,\n          1.0959e-05, -2.3269e-05],\n        [ 2.3189e-04,  2.5783e-04,  1.9757e-04,  ..., -8.3680e-05,\n          1.0515e-04,  6.6521e-05],\n        [ 1.1317e-04,  3.1648e-05, -7.4292e-05,  ...,  6.8238e-05,\n         -7.3030e-06, -1.1472e-04],\n        ...,\n        [-4.2388e-05, -2.7805e-04, -2.9434e-04,  ..., -2.0124e-04,\n          4.8057e-04, -4.5588e-04],\n        [-4.0577e-04, -5.3336e-04, -6.0664e-04,  ..., -4.9432e-04,\n          9.6858e-04, -5.9531e-04],\n        [ 3.4868e-04, -5.8953e-04, -3.2017e-04,  ..., -1.1294e-03,\n          1.0526e-03, -1.1466e-04]]), 'transformer.encoder.layers.0.self_attention.dense.lora_A.default.weight': tensor([[-0.0117,  0.0035, -0.0061,  ...,  0.0067,  0.0076,  0.0052],\n        [ 0.0059,  0.0105, -0.0121,  ...,  0.0152,  0.0123,  0.0039],\n        [-0.0056, -0.0037,  0.0067,  ..., -0.0029, -0.0140,  0.0098],\n        ...,\n        [-0.0007, -0.0081,  0.0076,  ..., -0.0086,  0.0002,  0.0036],\n        [-0.0038,  0.0151, -0.0027,  ..., -0.0054,  0.0113, -0.0060],\n        [ 0.0021, -0.0138,  0.0064,  ...,  0.0074, -0.0056,  0.0061]]), 'transformer.encoder.layers.0.self_attention.dense.lora_B.default.weight': tensor([[-1.1965e-03,  1.4480e-04,  8.8226e-04,  ..., -1.8866e-04,\n          1.3488e-03, -1.0850e-03],\n        [-3.8964e-04,  6.3681e-04,  4.0231e-04,  ..., -1.8807e-04,\n          3.5878e-04, -3.6624e-04],\n        [ 7.2240e-04, -6.7094e-05, -3.8705e-04,  ...,  1.7455e-04,\n         -6.9254e-04,  5.2197e-04],\n        ...,\n        [-4.2148e-04,  1.0947e-04,  2.0361e-04,  ..., -6.0091e-05,\n          5.3158e-04, -4.1960e-04],\n        [-6.6512e-04,  4.1957e-04,  4.3071e-04,  ..., -1.2933e-04,\n          6.3954e-04, -7.1288e-04],\n        [-1.6314e-04, -1.3906e-04,  8.7607e-05,  ...,  5.3411e-05,\n         -5.0098e-06, -7.6056e-05]]), 'transformer.encoder.layers.0.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0048,  0.0053, -0.0027,  ..., -0.0023,  0.0088, -0.0050],\n        [-0.0104, -0.0116, -0.0122,  ..., -0.0008,  0.0079,  0.0129],\n        [ 0.0066, -0.0071, -0.0114,  ..., -0.0010,  0.0143, -0.0012],\n        ...,\n        [-0.0014,  0.0109,  0.0039,  ..., -0.0075, -0.0028, -0.0124],\n        [-0.0116,  0.0113, -0.0071,  ..., -0.0072,  0.0131,  0.0056],\n        [-0.0062,  0.0052,  0.0108,  ..., -0.0145,  0.0133, -0.0067]]), 'transformer.encoder.layers.0.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 2.0481e-03, -1.8514e-03, -5.4739e-04,  ..., -1.4729e-03,\n          2.0894e-03, -1.4946e-03],\n        [ 1.6053e-03, -1.2336e-03,  9.0238e-05,  ..., -1.4112e-03,\n          1.6202e-03, -1.1312e-03],\n        [-4.5015e-04,  6.2350e-04,  5.7441e-04,  ...,  3.6895e-04,\n         -4.8282e-04,  3.1115e-04],\n        ...,\n        [ 7.9744e-04, -7.9104e-04, -3.1055e-04,  ..., -5.5636e-04,\n          8.1501e-04, -6.4725e-04],\n        [-2.0546e-04,  2.4347e-04,  2.3864e-04,  ...,  2.0709e-04,\n         -2.3876e-04,  1.7319e-04],\n        [ 5.8895e-04, -5.2807e-04, -3.1149e-04,  ..., -4.5742e-04,\n          6.6830e-04, -3.7791e-04]]), 'transformer.encoder.layers.0.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 0.0002,  0.0015,  0.0058,  ..., -0.0058, -0.0057,  0.0068],\n        [-0.0037, -0.0029, -0.0007,  ..., -0.0056, -0.0020, -0.0060],\n        [-0.0062, -0.0038,  0.0051,  ..., -0.0034, -0.0023,  0.0055],\n        ...,\n        [-0.0034, -0.0016,  0.0053,  ...,  0.0004,  0.0058,  0.0027],\n        [-0.0086,  0.0022, -0.0010,  ...,  0.0084,  0.0016, -0.0035],\n        [-0.0051,  0.0062, -0.0015,  ..., -0.0063,  0.0078,  0.0029]]), 'transformer.encoder.layers.0.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-5.7764e-04,  7.7479e-04, -8.8228e-04,  ...,  6.1373e-04,\n          7.6783e-05, -1.9260e-04],\n        [-5.4172e-04, -1.3321e-03, -2.9181e-04,  ...,  5.1719e-04,\n          1.8405e-03,  9.4169e-04],\n        [ 5.9705e-05,  8.8221e-04, -2.8406e-05,  ..., -1.9229e-04,\n         -9.6040e-04, -5.0410e-04],\n        ...,\n        [-1.9168e-03, -1.2745e-03, -2.2881e-03,  ...,  2.1121e-03,\n          3.0112e-03,  9.7206e-04],\n        [-4.2428e-04,  3.2990e-04, -7.0088e-04,  ...,  4.4781e-04,\n          3.2522e-04, -3.2057e-05],\n        [-1.4459e-04,  6.5754e-04, -6.0808e-04,  ...,  5.1269e-04,\n         -1.1299e-03, -4.5203e-04]]), 'transformer.encoder.layers.1.self_attention.query_key_value.lora_A.default.weight': tensor([[-8.0590e-04, -9.5482e-03, -6.0440e-03,  ..., -1.4955e-02,\n         -1.2496e-02,  3.8684e-03],\n        [-7.0838e-06,  2.3654e-03, -7.5290e-03,  ..., -1.0108e-02,\n          2.0806e-03, -2.8764e-03],\n        [ 1.3317e-02,  1.2516e-04,  1.6878e-03,  ..., -9.4807e-03,\n          2.9396e-03,  7.8167e-03],\n        ...,\n        [ 1.5289e-02,  3.8744e-03,  1.0777e-02,  ..., -7.0568e-03,\n          2.3444e-03,  1.1665e-02],\n        [-1.3252e-02,  1.0380e-02,  2.3957e-03,  ..., -1.1790e-02,\n         -1.2811e-02,  1.8812e-03],\n        [ 1.2411e-02, -2.1437e-03,  9.2814e-03,  ...,  1.8328e-03,\n          2.2927e-03, -1.2700e-02]]), 'transformer.encoder.layers.1.self_attention.query_key_value.lora_B.default.weight': tensor([[-2.5988e-04,  5.4453e-04,  2.7770e-04,  ...,  3.5234e-04,\n         -7.9655e-04, -8.6673e-04],\n        [ 4.9933e-04, -9.0353e-04, -4.3299e-04,  ..., -5.6816e-04,\n          1.4502e-03,  1.0899e-03],\n        [ 3.7987e-04, -6.6791e-04, -4.4429e-04,  ..., -2.2110e-04,\n          9.4394e-04,  6.2968e-04],\n        ...,\n        [-3.7852e-04,  1.0612e-04, -3.3604e-04,  ..., -4.9468e-04,\n         -5.5421e-05,  3.0145e-04],\n        [-4.3749e-04,  1.5011e-03,  1.1337e-03,  ...,  3.9833e-04,\n         -1.8603e-03, -3.0263e-04],\n        [-1.4466e-03,  1.0331e-03,  1.9687e-04,  ..., -1.6552e-03,\n         -1.2075e-03,  4.6883e-04]]), 'transformer.encoder.layers.1.self_attention.dense.lora_A.default.weight': tensor([[ 0.0095,  0.0133,  0.0008,  ..., -0.0086,  0.0118, -0.0145],\n        [-0.0077, -0.0073,  0.0107,  ...,  0.0053,  0.0123, -0.0005],\n        [-0.0032,  0.0125, -0.0004,  ..., -0.0003,  0.0064, -0.0004],\n        ...,\n        [ 0.0063,  0.0041,  0.0006,  ..., -0.0102,  0.0048, -0.0087],\n        [-0.0029, -0.0048,  0.0088,  ...,  0.0024,  0.0091, -0.0115],\n        [-0.0121, -0.0112,  0.0046,  ...,  0.0077,  0.0136,  0.0136]]), 'transformer.encoder.layers.1.self_attention.dense.lora_B.default.weight': tensor([[ 1.3932e-03, -2.0844e-04, -6.5071e-04,  ..., -1.4099e-03,\n          1.5980e-03, -1.7118e-03],\n        [-9.5875e-04,  6.0114e-04,  1.3225e-04,  ...,  4.3118e-04,\n         -1.1993e-03,  1.2900e-03],\n        [ 4.1944e-05,  5.4312e-04, -6.0617e-04,  ..., -6.2978e-04,\n          2.1985e-04, -4.8822e-04],\n        ...,\n        [-2.0708e-03,  4.6802e-05,  6.5355e-04,  ...,  1.2539e-03,\n         -1.7053e-03,  2.0039e-03],\n        [-3.3422e-04,  5.0489e-05, -1.1223e-04,  ...,  1.7368e-04,\n         -3.5169e-05,  2.9243e-04],\n        [ 1.1843e-03, -7.5603e-04,  3.8078e-04,  ..., -2.0953e-04,\n          7.0565e-04, -1.1152e-03]]), 'transformer.encoder.layers.1.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 4.2574e-05,  6.9095e-03,  9.4727e-03,  ...,  7.3516e-03,\n         -8.7387e-04,  1.2454e-02],\n        [-7.2360e-04, -9.5343e-03,  5.9918e-03,  ...,  1.5467e-03,\n         -7.8105e-03,  7.8826e-03],\n        [ 3.4805e-03,  7.7569e-03, -1.0221e-03,  ..., -3.1349e-04,\n          1.3530e-02,  6.1188e-03],\n        ...,\n        [ 9.3594e-03, -2.2341e-03, -1.1668e-02,  ...,  1.0388e-02,\n         -1.2172e-02,  1.1298e-02],\n        [-1.5105e-02, -7.4348e-03, -2.3848e-03,  ..., -4.4047e-03,\n         -7.7516e-04, -7.2527e-03],\n        [ 2.2427e-03, -1.0660e-02, -1.1513e-02,  ...,  1.1360e-02,\n          1.4848e-02, -1.5651e-02]]), 'transformer.encoder.layers.1.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-9.1027e-06,  1.1067e-04, -8.0557e-05,  ..., -1.9798e-04,\n         -9.5059e-05, -1.7003e-04],\n        [ 3.1255e-04,  1.9624e-03, -1.8978e-03,  ..., -1.2511e-03,\n         -1.2885e-03, -5.9977e-04],\n        [ 2.9808e-05, -4.4301e-04,  3.9645e-04,  ..., -1.0589e-04,\n          4.6105e-04,  1.2024e-04],\n        ...,\n        [-1.9787e-05,  2.3141e-04,  3.2328e-04,  ...,  3.5476e-04,\n         -1.3527e-04,  4.6960e-04],\n        [ 4.9983e-04, -8.9307e-04,  1.1447e-03,  ...,  8.9988e-05,\n          6.9178e-04,  1.1394e-03],\n        [-6.0436e-04,  1.3826e-03, -1.3548e-03,  ..., -1.8424e-04,\n         -9.7425e-04, -1.2190e-03]]), 'transformer.encoder.layers.1.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 0.0085, -0.0073,  0.0002,  ...,  0.0077, -0.0043, -0.0030],\n        [-0.0066, -0.0048, -0.0025,  ..., -0.0033,  0.0007, -0.0047],\n        [-0.0024, -0.0020,  0.0081,  ..., -0.0038,  0.0039,  0.0070],\n        ...,\n        [ 0.0026, -0.0080,  0.0014,  ..., -0.0004, -0.0073, -0.0055],\n        [-0.0011, -0.0038, -0.0034,  ..., -0.0045, -0.0063,  0.0038],\n        [ 0.0039, -0.0057, -0.0010,  ...,  0.0071,  0.0008,  0.0018]]), 'transformer.encoder.layers.1.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-3.3085e-04, -9.5911e-05, -4.1804e-04,  ..., -3.5538e-04,\n          1.2205e-03,  6.2754e-04],\n        [-5.9155e-04,  4.8538e-04,  5.5963e-04,  ..., -8.0710e-04,\n          7.3439e-05,  8.1326e-04],\n        [-1.5020e-03, -2.1482e-05, -6.7011e-04,  ..., -1.8633e-03,\n          2.8615e-03,  2.0933e-03],\n        ...,\n        [-3.0336e-04,  1.1291e-03,  2.4933e-03,  ..., -7.6965e-04,\n         -1.1890e-03,  1.0110e-03],\n        [-4.5049e-04, -2.7610e-04,  1.7397e-04,  ..., -6.1686e-04,\n          4.7767e-04,  1.5009e-03],\n        [ 1.1496e-03, -2.4218e-04, -5.3023e-04,  ...,  1.6775e-03,\n         -9.3569e-04, -1.7822e-03]]), 'transformer.encoder.layers.2.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0035,  0.0136, -0.0048,  ...,  0.0120,  0.0095, -0.0103],\n        [-0.0031, -0.0150,  0.0011,  ..., -0.0078, -0.0148,  0.0100],\n        [ 0.0071,  0.0041, -0.0047,  ...,  0.0024, -0.0153,  0.0098],\n        ...,\n        [ 0.0090,  0.0021,  0.0110,  ...,  0.0020, -0.0035,  0.0030],\n        [ 0.0068, -0.0017, -0.0076,  ..., -0.0004,  0.0032,  0.0128],\n        [ 0.0054, -0.0067,  0.0099,  ..., -0.0047, -0.0056, -0.0127]]), 'transformer.encoder.layers.2.self_attention.query_key_value.lora_B.default.weight': tensor([[ 1.3614e-03,  6.5751e-04, -8.6130e-04,  ...,  1.0904e-03,\n         -1.4313e-03,  9.6681e-04],\n        [-8.4969e-04, -6.9205e-04,  8.1563e-04,  ..., -8.3775e-04,\n          8.4680e-04, -2.1653e-04],\n        [-4.8386e-04,  2.1315e-05, -6.7721e-06,  ..., -3.6016e-04,\n          4.5396e-04, -6.6956e-04],\n        ...,\n        [-1.8163e-03,  5.1555e-04,  1.2787e-03,  ..., -7.2188e-04,\n          1.9863e-03, -2.9028e-03],\n        [-1.1747e-03, -7.7203e-04,  1.4138e-03,  ..., -1.0785e-03,\n          1.4972e-03, -1.1501e-03],\n        [ 3.7653e-04, -9.0919e-05,  3.4825e-04,  ...,  1.1447e-04,\n         -4.3005e-04, -2.1769e-05]]), 'transformer.encoder.layers.2.self_attention.dense.lora_A.default.weight': tensor([[-0.0013,  0.0112,  0.0008,  ..., -0.0082, -0.0094,  0.0153],\n        [-0.0031, -0.0033, -0.0007,  ..., -0.0153, -0.0007, -0.0093],\n        [-0.0100, -0.0020,  0.0015,  ..., -0.0110, -0.0068,  0.0005],\n        ...,\n        [-0.0037,  0.0069,  0.0041,  ...,  0.0109,  0.0083, -0.0120],\n        [-0.0158, -0.0110, -0.0018,  ..., -0.0082, -0.0073, -0.0035],\n        [ 0.0156,  0.0120, -0.0046,  ...,  0.0070, -0.0142, -0.0095]]), 'transformer.encoder.layers.2.self_attention.dense.lora_B.default.weight': tensor([[ 5.3298e-04, -1.0029e-03, -8.1091e-04,  ..., -6.6978e-04,\n          6.6700e-04, -1.7047e-04],\n        [-1.2372e-03,  2.1877e-03,  1.3997e-03,  ...,  1.4274e-03,\n         -1.5618e-03,  8.1648e-04],\n        [-3.1180e-05,  8.6280e-05, -1.5858e-04,  ..., -1.3977e-04,\n         -3.6186e-04,  1.2968e-04],\n        ...,\n        [-2.5874e-03,  4.0538e-03,  2.2620e-03,  ...,  2.9388e-03,\n         -2.0140e-03,  2.3446e-03],\n        [-1.4842e-04,  4.7374e-04, -3.7345e-05,  ...,  4.1066e-04,\n          5.5539e-04,  4.7820e-04],\n        [ 1.1349e-03, -1.4748e-03, -1.1807e-03,  ..., -1.0878e-03,\n          1.4222e-03, -6.8708e-04]]), 'transformer.encoder.layers.2.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0142, -0.0067, -0.0075,  ..., -0.0011,  0.0114,  0.0076],\n        [-0.0062, -0.0036, -0.0043,  ...,  0.0030, -0.0138, -0.0086],\n        [ 0.0025,  0.0036,  0.0031,  ...,  0.0148, -0.0127,  0.0078],\n        ...,\n        [ 0.0033, -0.0090, -0.0045,  ...,  0.0010,  0.0044, -0.0092],\n        [ 0.0020, -0.0141, -0.0112,  ..., -0.0037, -0.0112,  0.0062],\n        [-0.0022, -0.0067,  0.0150,  ...,  0.0112, -0.0016, -0.0044]]), 'transformer.encoder.layers.2.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-0.0008, -0.0003, -0.0008,  ..., -0.0011,  0.0011,  0.0011],\n        [ 0.0013,  0.0002, -0.0003,  ...,  0.0002,  0.0001, -0.0015],\n        [ 0.0007,  0.0005, -0.0009,  ..., -0.0005,  0.0006, -0.0003],\n        ...,\n        [-0.0004,  0.0002,  0.0020,  ...,  0.0020, -0.0021, -0.0003],\n        [ 0.0004,  0.0004,  0.0004,  ...,  0.0008, -0.0008, -0.0011],\n        [-0.0027, -0.0015,  0.0035,  ...,  0.0030, -0.0038,  0.0002]]), 'transformer.encoder.layers.2.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0075,  0.0025,  0.0010,  ...,  0.0034, -0.0066,  0.0025],\n        [ 0.0015,  0.0025,  0.0064,  ...,  0.0060,  0.0002, -0.0082],\n        [-0.0050, -0.0018,  0.0042,  ..., -0.0015, -0.0019,  0.0036],\n        ...,\n        [-0.0056, -0.0055, -0.0047,  ..., -0.0024,  0.0057,  0.0082],\n        [-0.0086,  0.0048, -0.0064,  ...,  0.0039,  0.0077,  0.0057],\n        [ 0.0059, -0.0067,  0.0007,  ..., -0.0041, -0.0002,  0.0086]]), 'transformer.encoder.layers.2.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-2.9556e-04, -1.0530e-03, -1.1850e-03,  ..., -1.6462e-04,\n         -6.3404e-04,  3.9213e-04],\n        [ 7.8705e-04,  2.3556e-03,  1.8171e-03,  ..., -5.2760e-04,\n          1.0361e-03, -1.5743e-03],\n        [-4.3855e-05,  2.7057e-04, -2.2261e-04,  ...,  5.0462e-04,\n          4.7290e-04,  4.0119e-05],\n        ...,\n        [ 1.5503e-03,  3.8899e-03,  3.4415e-03,  ..., -1.7694e-03,\n         -8.7097e-04, -3.2421e-03],\n        [-6.8173e-05,  2.8777e-04,  1.0375e-04,  ..., -4.5498e-04,\n         -7.9820e-04, -2.8194e-04],\n        [-4.9455e-04, -2.0769e-03, -1.5734e-03,  ...,  1.2308e-04,\n         -8.8601e-04,  5.0284e-04]]), 'transformer.encoder.layers.3.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0035, -0.0139,  0.0078,  ...,  0.0135,  0.0002, -0.0157],\n        [ 0.0058, -0.0069, -0.0119,  ...,  0.0127, -0.0073, -0.0050],\n        [-0.0086, -0.0015,  0.0119,  ...,  0.0021, -0.0103,  0.0119],\n        ...,\n        [ 0.0124,  0.0094,  0.0098,  ...,  0.0089,  0.0135,  0.0139],\n        [-0.0141,  0.0046,  0.0002,  ...,  0.0136, -0.0108, -0.0040],\n        [ 0.0063, -0.0098, -0.0116,  ..., -0.0120,  0.0078,  0.0004]]), 'transformer.encoder.layers.3.self_attention.query_key_value.lora_B.default.weight': tensor([[-7.7099e-04, -1.4911e-03, -8.8875e-04,  ...,  1.2193e-03,\n          1.3885e-05,  1.2949e-03],\n        [-5.1868e-04, -1.0539e-03, -5.2899e-04,  ...,  7.7394e-04,\n          1.0740e-04,  1.1023e-03],\n        [-2.4302e-04, -1.5688e-04, -1.3986e-05,  ...,  3.5951e-04,\n         -3.9522e-05, -3.5119e-05],\n        ...,\n        [-8.8802e-04, -1.3588e-03, -7.5765e-04,  ...,  4.2876e-04,\n         -4.2034e-04,  1.1926e-03],\n        [ 1.4840e-03,  2.9300e-03,  3.5715e-03,  ...,  1.9235e-03,\n          1.7294e-03, -3.6856e-03],\n        [ 6.9498e-04,  1.2170e-03,  7.5425e-04,  ..., -8.7167e-04,\n          2.5764e-04, -1.3399e-03]]), 'transformer.encoder.layers.3.self_attention.dense.lora_A.default.weight': tensor([[-0.0079, -0.0079, -0.0091,  ...,  0.0048,  0.0027,  0.0131],\n        [-0.0024,  0.0144, -0.0086,  ...,  0.0166, -0.0049, -0.0086],\n        [-0.0076,  0.0061, -0.0040,  ..., -0.0089, -0.0073, -0.0117],\n        ...,\n        [ 0.0123, -0.0092, -0.0139,  ...,  0.0007,  0.0080,  0.0117],\n        [ 0.0098,  0.0056,  0.0029,  ...,  0.0071, -0.0004, -0.0106],\n        [-0.0031,  0.0106,  0.0061,  ..., -0.0009,  0.0052, -0.0125]]), 'transformer.encoder.layers.3.self_attention.dense.lora_B.default.weight': tensor([[ 0.0009,  0.0012,  0.0012,  ...,  0.0011,  0.0009,  0.0012],\n        [-0.0010, -0.0014, -0.0017,  ..., -0.0019, -0.0017, -0.0013],\n        [-0.0001,  0.0004, -0.0002,  ..., -0.0004, -0.0007,  0.0001],\n        ...,\n        [-0.0034, -0.0045, -0.0049,  ..., -0.0041, -0.0048, -0.0039],\n        [ 0.0008,  0.0005,  0.0011,  ...,  0.0011,  0.0009,  0.0008],\n        [-0.0014, -0.0010, -0.0021,  ..., -0.0017, -0.0021, -0.0011]]), 'transformer.encoder.layers.3.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0071,  0.0141,  0.0057,  ..., -0.0155,  0.0095,  0.0153],\n        [-0.0009,  0.0006,  0.0033,  ...,  0.0012,  0.0034,  0.0131],\n        [-0.0132, -0.0019, -0.0159,  ..., -0.0073,  0.0090, -0.0053],\n        ...,\n        [ 0.0153,  0.0006,  0.0023,  ..., -0.0086,  0.0092, -0.0053],\n        [-0.0015,  0.0032, -0.0007,  ...,  0.0061, -0.0085, -0.0063],\n        [-0.0057, -0.0033,  0.0023,  ..., -0.0020,  0.0147, -0.0033]]), 'transformer.encoder.layers.3.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-1.3057e-03, -1.0205e-03,  1.3679e-03,  ..., -1.6525e-03,\n          6.9507e-04, -1.4372e-03],\n        [ 5.5304e-04,  8.1452e-05, -3.2280e-04,  ...,  6.1512e-04,\n         -3.1904e-04,  7.5497e-04],\n        [-1.2011e-03,  1.6269e-04, -5.8376e-05,  ..., -7.4828e-04,\n          8.1546e-04, -1.6878e-03],\n        ...,\n        [-1.5437e-03, -7.2963e-04,  1.2699e-03,  ..., -1.7635e-03,\n          4.3091e-04, -8.8266e-04],\n        [ 1.1791e-03,  1.6507e-03, -1.6027e-03,  ...,  1.7771e-03,\n          8.1688e-05,  6.3108e-04],\n        [-7.6837e-04, -4.4904e-04,  3.0336e-04,  ..., -7.4607e-04,\n          7.0796e-05, -9.1293e-04]]), 'transformer.encoder.layers.3.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 0.0028, -0.0051, -0.0042,  ...,  0.0046,  0.0025,  0.0082],\n        [ 0.0083,  0.0069, -0.0013,  ..., -0.0083,  0.0034,  0.0004],\n        [-0.0021,  0.0071,  0.0052,  ...,  0.0071,  0.0064,  0.0044],\n        ...,\n        [-0.0049,  0.0061,  0.0082,  ...,  0.0025,  0.0061,  0.0058],\n        [ 0.0021, -0.0002,  0.0007,  ..., -0.0017,  0.0083, -0.0076],\n        [ 0.0044,  0.0074, -0.0055,  ...,  0.0022, -0.0027, -0.0047]]), 'transformer.encoder.layers.3.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 9.3784e-04, -4.5998e-04, -9.2446e-04,  ..., -5.7726e-04,\n          1.2203e-04, -4.4953e-04],\n        [ 5.7920e-04,  1.9021e-04, -2.5086e-04,  ..., -5.4999e-05,\n         -7.2435e-04,  1.2140e-03],\n        [ 2.7027e-04, -7.0201e-04,  1.4860e-04,  ..., -1.2674e-04,\n         -7.4786e-04,  2.9005e-04],\n        ...,\n        [ 3.5949e-03, -2.5114e-03, -2.7545e-03,  ..., -2.5001e-03,\n         -2.3351e-03, -1.2062e-03],\n        [-3.9296e-04, -2.2709e-05,  3.9173e-04,  ...,  7.4954e-04,\n         -3.2673e-04,  4.4593e-04],\n        [ 1.6754e-03, -1.4404e-03, -1.2419e-03,  ..., -9.7007e-04,\n         -4.0013e-04, -1.4498e-03]]), 'transformer.encoder.layers.4.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0042,  0.0057, -0.0119,  ..., -0.0002,  0.0015,  0.0146],\n        [-0.0091, -0.0118, -0.0036,  ..., -0.0086,  0.0064,  0.0016],\n        [ 0.0071, -0.0005,  0.0070,  ..., -0.0136,  0.0088, -0.0135],\n        ...,\n        [-0.0129,  0.0016, -0.0024,  ..., -0.0080,  0.0018, -0.0013],\n        [-0.0058, -0.0151, -0.0070,  ..., -0.0026, -0.0124,  0.0051],\n        [ 0.0089,  0.0095,  0.0041,  ..., -0.0101, -0.0085,  0.0137]]), 'transformer.encoder.layers.4.self_attention.query_key_value.lora_B.default.weight': tensor([[ 3.8899e-04,  7.1604e-04,  3.7489e-04,  ..., -7.3902e-04,\n          1.1345e-03,  2.1496e-04],\n        [-5.8980e-04,  6.5585e-04, -5.8258e-04,  ...,  9.9152e-04,\n         -1.4964e-03,  1.7667e-04],\n        [-2.0282e-04,  4.5006e-04, -3.7025e-04,  ..., -1.3552e-05,\n          9.4815e-05,  2.4887e-04],\n        ...,\n        [ 1.3022e-04,  1.4978e-03,  3.1157e-04,  ..., -1.4410e-03,\n          1.5490e-03,  3.3675e-04],\n        [-1.3285e-03,  2.9267e-03, -9.0786e-04,  ..., -1.5237e-03,\n         -3.6255e-04,  7.9277e-04],\n        [ 4.8404e-04, -1.3335e-03,  1.9795e-04,  ...,  5.5391e-04,\n         -8.1289e-04, -1.4765e-03]]), 'transformer.encoder.layers.4.self_attention.dense.lora_A.default.weight': tensor([[ 0.0034,  0.0107,  0.0008,  ..., -0.0095,  0.0077, -0.0020],\n        [ 0.0048, -0.0085,  0.0026,  ..., -0.0103, -0.0020, -0.0114],\n        [ 0.0023,  0.0090,  0.0016,  ..., -0.0135, -0.0015,  0.0040],\n        ...,\n        [-0.0060,  0.0146,  0.0058,  ...,  0.0086, -0.0129,  0.0119],\n        [ 0.0111, -0.0027, -0.0037,  ..., -0.0103,  0.0056,  0.0134],\n        [-0.0086, -0.0118, -0.0142,  ..., -0.0031, -0.0009, -0.0119]]), 'transformer.encoder.layers.4.self_attention.dense.lora_B.default.weight': tensor([[-0.0001,  0.0014, -0.0003,  ..., -0.0002,  0.0009, -0.0004],\n        [ 0.0013, -0.0025,  0.0003,  ...,  0.0009,  0.0006,  0.0006],\n        [ 0.0007, -0.0012, -0.0003,  ...,  0.0005, -0.0001,  0.0002],\n        ...,\n        [ 0.0016, -0.0032,  0.0017,  ...,  0.0018,  0.0027,  0.0017],\n        [ 0.0011, -0.0008,  0.0009,  ...,  0.0005,  0.0002,  0.0010],\n        [ 0.0007, -0.0011,  0.0011,  ...,  0.0010,  0.0025,  0.0010]]), 'transformer.encoder.layers.4.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0034, -0.0068, -0.0112,  ..., -0.0093,  0.0125, -0.0070],\n        [-0.0017,  0.0110,  0.0094,  ...,  0.0110, -0.0016, -0.0023],\n        [ 0.0068, -0.0020, -0.0063,  ..., -0.0076,  0.0071,  0.0017],\n        ...,\n        [ 0.0095,  0.0006, -0.0050,  ..., -0.0057, -0.0136,  0.0059],\n        [-0.0028,  0.0149, -0.0063,  ..., -0.0010, -0.0098,  0.0005],\n        [ 0.0057,  0.0050,  0.0122,  ..., -0.0136,  0.0154,  0.0010]]), 'transformer.encoder.layers.4.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 2.3378e-04,  1.0649e-03,  5.4409e-04,  ...,  9.6847e-04,\n          9.4048e-05, -3.7533e-04],\n        [-3.0019e-04,  2.1588e-04, -2.0882e-04,  ...,  2.6311e-04,\n         -2.3875e-04, -1.6446e-04],\n        [-2.8573e-04,  4.2042e-04, -2.6946e-04,  ...,  4.3519e-05,\n         -3.3348e-04, -1.9729e-04],\n        ...,\n        [-4.3626e-04, -5.4377e-04, -8.7249e-04,  ..., -3.3167e-04,\n         -6.6911e-04, -6.0357e-04],\n        [-1.7779e-03,  2.0135e-03, -7.3531e-04,  ...,  1.4798e-03,\n         -2.0265e-03, -1.8602e-03],\n        [ 7.6866e-04, -5.3167e-04,  1.8335e-04,  ..., -5.3838e-04,\n          7.9464e-04,  4.1614e-04]]), 'transformer.encoder.layers.4.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 0.0081, -0.0013, -0.0034,  ...,  0.0030,  0.0019,  0.0065],\n        [-0.0013,  0.0034,  0.0076,  ...,  0.0071, -0.0045,  0.0024],\n        [-0.0003, -0.0057, -0.0004,  ..., -0.0077, -0.0067, -0.0053],\n        ...,\n        [ 0.0071,  0.0041,  0.0024,  ..., -0.0006,  0.0007,  0.0010],\n        [-0.0025, -0.0021, -0.0053,  ..., -0.0048,  0.0023,  0.0072],\n        [-0.0011, -0.0002, -0.0067,  ...,  0.0062, -0.0003, -0.0037]]), 'transformer.encoder.layers.4.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 8.5094e-04,  7.0783e-04, -1.0458e-03,  ..., -1.7031e-03,\n         -9.3356e-04,  2.8247e-04],\n        [-7.9222e-04, -5.8870e-04,  1.2754e-03,  ...,  1.5727e-03,\n          5.6560e-04,  2.4950e-04],\n        [-2.3153e-04, -5.3281e-04,  5.2459e-04,  ...,  9.1176e-04,\n          9.1010e-04, -1.5673e-04],\n        ...,\n        [-2.9593e-03, -2.3546e-03,  3.2004e-03,  ...,  2.2197e-03,\n          2.1051e-03, -1.6403e-03],\n        [-2.5257e-04, -2.4271e-04,  5.0754e-04,  ...,  5.5723e-04,\n          4.0237e-05, -6.9180e-04],\n        [-1.6875e-03, -9.0024e-04,  8.1975e-04,  ...,  5.7076e-04,\n          1.0781e-03, -1.4059e-03]]), 'transformer.encoder.layers.5.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0143, -0.0097,  0.0069,  ..., -0.0027,  0.0128,  0.0055],\n        [ 0.0078,  0.0145, -0.0135,  ..., -0.0013,  0.0061, -0.0047],\n        [-0.0046,  0.0066, -0.0060,  ...,  0.0147,  0.0106,  0.0149],\n        ...,\n        [ 0.0151,  0.0130,  0.0038,  ..., -0.0150, -0.0078,  0.0110],\n        [-0.0086,  0.0030,  0.0069,  ...,  0.0120,  0.0027, -0.0075],\n        [-0.0022,  0.0091,  0.0051,  ...,  0.0151,  0.0068, -0.0144]]), 'transformer.encoder.layers.5.self_attention.query_key_value.lora_B.default.weight': tensor([[ 5.2607e-04, -6.5003e-04,  2.3769e-04,  ..., -2.3105e-04,\n          5.2357e-05,  2.5540e-04],\n        [ 1.1306e-04, -2.0645e-04,  6.9574e-05,  ..., -2.0445e-05,\n          1.2724e-04,  3.3921e-04],\n        [ 5.1945e-05,  6.3758e-04, -5.9905e-04,  ..., -1.4194e-04,\n         -3.1760e-04, -4.2597e-04],\n        ...,\n        [ 4.9135e-04, -5.2964e-04,  6.9865e-04,  ...,  1.5736e-03,\n          1.9465e-03,  7.4437e-04],\n        [-6.5769e-04,  6.4800e-04, -9.0123e-04,  ..., -1.3382e-03,\n         -1.5778e-03, -1.0466e-03],\n        [ 8.3802e-04, -1.8204e-03,  1.2785e-03,  ...,  1.4930e-03,\n          2.1315e-03,  2.2253e-03]]), 'transformer.encoder.layers.5.self_attention.dense.lora_A.default.weight': tensor([[-0.0053,  0.0065, -0.0078,  ..., -0.0065, -0.0118, -0.0009],\n        [-0.0153, -0.0130, -0.0077,  ..., -0.0033, -0.0086, -0.0071],\n        [ 0.0146,  0.0131,  0.0030,  ..., -0.0033, -0.0103,  0.0024],\n        ...,\n        [-0.0097,  0.0104, -0.0133,  ..., -0.0141,  0.0076,  0.0053],\n        [ 0.0131,  0.0158, -0.0069,  ...,  0.0102, -0.0117, -0.0059],\n        [-0.0088,  0.0111, -0.0132,  ..., -0.0137, -0.0005, -0.0037]]), 'transformer.encoder.layers.5.self_attention.dense.lora_B.default.weight': tensor([[ 7.9292e-04, -1.4459e-04,  8.1628e-04,  ..., -7.4497e-04,\n          1.4922e-03, -1.9227e-03],\n        [-1.1619e-03, -4.3288e-06, -7.9592e-04,  ...,  4.5838e-04,\n         -9.8958e-04,  9.2415e-04],\n        [-1.3167e-04,  6.8927e-04, -2.1713e-04,  ...,  1.0859e-03,\n         -6.2147e-04,  1.5625e-03],\n        ...,\n        [-2.3497e-03, -1.2359e-03, -2.4201e-03,  ...,  1.0336e-03,\n          3.8217e-04,  4.0477e-04],\n        [ 2.9844e-04, -1.6794e-05, -2.6685e-04,  ...,  1.8968e-04,\n          5.6971e-04, -2.4999e-05],\n        [-1.8570e-03, -1.8663e-03, -8.3987e-04,  ...,  4.9193e-04,\n          1.4184e-03, -6.0230e-04]]), 'transformer.encoder.layers.5.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0074,  0.0022, -0.0030,  ..., -0.0010,  0.0047,  0.0068],\n        [-0.0013,  0.0093,  0.0019,  ...,  0.0063,  0.0101,  0.0104],\n        [ 0.0116, -0.0047, -0.0122,  ..., -0.0105,  0.0041,  0.0065],\n        ...,\n        [ 0.0048,  0.0110, -0.0105,  ..., -0.0128, -0.0136, -0.0121],\n        [-0.0108, -0.0086, -0.0155,  ...,  0.0083,  0.0149,  0.0025],\n        [-0.0145, -0.0084, -0.0068,  ..., -0.0120, -0.0020,  0.0057]]), 'transformer.encoder.layers.5.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-2.4933e-04, -1.7672e-04, -8.6593e-05,  ..., -1.9124e-04,\n         -3.1783e-04, -2.7167e-05],\n        [ 6.5058e-04, -8.1182e-04,  7.6235e-04,  ..., -4.9778e-04,\n         -8.8421e-04, -6.4972e-04],\n        [-1.6203e-03,  6.5142e-04, -7.9007e-04,  ...,  6.7747e-04,\n          8.0018e-04,  4.4228e-04],\n        ...,\n        [-2.5766e-04,  4.1227e-05, -2.3763e-04,  ...,  4.5338e-04,\n          4.0170e-04,  9.0051e-04],\n        [ 3.2066e-04, -1.0373e-04,  2.9009e-04,  ...,  1.1879e-04,\n         -3.0400e-04, -1.7471e-04],\n        [ 1.3662e-03, -9.0642e-04,  8.8100e-04,  ..., -1.3811e-04,\n         -1.0268e-03, -5.6059e-04]]), 'transformer.encoder.layers.5.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0010, -0.0072,  0.0085,  ..., -0.0064, -0.0072, -0.0030],\n        [ 0.0081, -0.0062,  0.0056,  ...,  0.0009, -0.0055,  0.0037],\n        [-0.0068, -0.0082, -0.0077,  ...,  0.0050, -0.0016,  0.0078],\n        ...,\n        [-0.0057, -0.0007, -0.0083,  ...,  0.0055,  0.0076, -0.0083],\n        [-0.0029, -0.0017,  0.0069,  ...,  0.0080, -0.0023, -0.0016],\n        [-0.0026, -0.0025,  0.0019,  ..., -0.0017,  0.0078, -0.0025]]), 'transformer.encoder.layers.5.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 0.0007, -0.0001,  0.0004,  ..., -0.0004, -0.0008, -0.0004],\n        [-0.0012, -0.0001, -0.0005,  ...,  0.0010,  0.0009,  0.0003],\n        [ 0.0008,  0.0004,  0.0003,  ..., -0.0003, -0.0006, -0.0007],\n        ...,\n        [-0.0015, -0.0011, -0.0008,  ...,  0.0013,  0.0010,  0.0010],\n        [ 0.0014,  0.0004,  0.0007,  ..., -0.0012, -0.0012, -0.0013],\n        [-0.0008, -0.0007, -0.0006,  ...,  0.0010,  0.0007,  0.0017]]), 'transformer.encoder.layers.6.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0020,  0.0060,  0.0033,  ..., -0.0138, -0.0140,  0.0075],\n        [-0.0086,  0.0150,  0.0097,  ...,  0.0032, -0.0148, -0.0083],\n        [ 0.0115,  0.0142,  0.0151,  ...,  0.0131,  0.0024,  0.0030],\n        ...,\n        [ 0.0102, -0.0148, -0.0075,  ..., -0.0023, -0.0043, -0.0026],\n        [-0.0006,  0.0086, -0.0056,  ..., -0.0137,  0.0108,  0.0031],\n        [ 0.0131,  0.0113, -0.0032,  ..., -0.0102, -0.0110,  0.0070]]), 'transformer.encoder.layers.6.self_attention.query_key_value.lora_B.default.weight': tensor([[ 2.8078e-05,  3.7125e-05, -8.7672e-05,  ...,  5.3629e-05,\n          4.5494e-05,  5.4597e-06],\n        [-3.7273e-05,  1.1302e-04, -1.0855e-04,  ...,  6.0215e-05,\n          2.1101e-04, -3.3241e-04],\n        [ 1.2945e-04, -1.5104e-04, -2.1526e-04,  ...,  6.5575e-05,\n          1.0019e-04, -3.3723e-05],\n        ...,\n        [-3.4891e-04, -8.1375e-04,  4.6395e-04,  ..., -6.0495e-04,\n         -8.8211e-04,  1.5329e-03],\n        [-4.4970e-04, -4.0897e-04,  4.5196e-04,  ..., -9.2292e-04,\n         -5.1154e-04,  1.4563e-03],\n        [ 6.9169e-05, -5.0146e-04, -3.3553e-04,  ...,  2.2370e-04,\n         -2.8447e-04,  2.8581e-04]]), 'transformer.encoder.layers.6.self_attention.dense.lora_A.default.weight': tensor([[ 4.4531e-05,  8.4152e-03,  8.4896e-03,  ..., -2.1782e-03,\n          1.2405e-02, -9.4109e-03],\n        [ 6.8290e-03, -1.1812e-03,  9.6986e-03,  ..., -6.6660e-03,\n          8.2449e-03,  1.1992e-02],\n        [-9.1001e-03, -6.4047e-03, -1.1337e-02,  ..., -6.4688e-03,\n         -1.3213e-02,  1.0440e-02],\n        ...,\n        [-9.2428e-03,  1.2099e-03,  7.6806e-03,  ..., -6.5981e-03,\n          4.6710e-03,  3.9324e-03],\n        [ 7.5670e-03,  1.5382e-02,  8.6050e-03,  ..., -1.5471e-02,\n         -7.4801e-03,  5.4316e-03],\n        [-1.0323e-02, -7.8930e-03, -1.0712e-02,  ...,  1.3100e-02,\n          1.0480e-02,  7.6062e-03]]), 'transformer.encoder.layers.6.self_attention.dense.lora_B.default.weight': tensor([[-3.4087e-04, -3.1922e-04, -6.3804e-05,  ..., -2.7171e-04,\n          3.8991e-04, -1.6649e-04],\n        [-3.6716e-05,  1.1127e-03,  9.4426e-04,  ...,  1.7305e-03,\n         -5.9514e-04,  1.1708e-03],\n        [ 1.7313e-05,  4.9003e-04,  9.6995e-05,  ...,  4.7001e-04,\n         -6.3891e-04,  4.3660e-04],\n        ...,\n        [ 7.3771e-04,  6.8412e-04,  1.4090e-03,  ...,  1.9912e-03,\n         -3.9764e-04,  7.4382e-04],\n        [-7.3119e-04, -7.2704e-04, -3.4238e-04,  ..., -8.3530e-04,\n          1.4420e-04,  3.0948e-04],\n        [ 3.0896e-04,  2.9216e-04, -1.1185e-04,  ...,  5.5641e-04,\n         -5.1594e-05, -3.6831e-04]]), 'transformer.encoder.layers.6.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 1.6583e-03, -3.9050e-05,  1.9899e-03,  ...,  4.3700e-03,\n          1.9875e-03,  1.3795e-02],\n        [ 7.8898e-03, -7.6347e-03,  7.5006e-03,  ..., -2.8661e-03,\n          7.7928e-03, -3.6805e-03],\n        [ 7.1951e-03, -8.5110e-03,  1.5840e-02,  ...,  8.1677e-03,\n          1.2966e-02, -3.1125e-03],\n        ...,\n        [ 4.4429e-03,  7.5799e-03,  9.9228e-03,  ...,  2.3507e-03,\n         -1.3939e-02,  1.4655e-03],\n        [-5.8848e-03, -5.2577e-03, -5.8326e-03,  ..., -9.1661e-03,\n          2.7608e-03, -2.1222e-03],\n        [ 7.2465e-03, -4.4450e-03, -1.2723e-02,  ..., -1.6004e-03,\n          1.3652e-02, -2.5448e-03]]), 'transformer.encoder.layers.6.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 4.1132e-05,  1.1611e-03, -1.1990e-03,  ...,  3.5749e-04,\n         -1.2255e-03, -9.6639e-04],\n        [-4.7965e-06,  3.0665e-04, -5.2274e-04,  ...,  8.8450e-05,\n         -8.1592e-04, -5.9422e-04],\n        [ 2.8546e-04, -1.4270e-04,  8.4536e-05,  ...,  2.5276e-05,\n          4.3521e-04,  2.4300e-04],\n        ...,\n        [ 5.0348e-04,  1.1181e-04,  1.3168e-04,  ..., -1.7020e-04,\n          4.2110e-05,  7.2996e-05],\n        [ 4.2983e-05, -5.2259e-04,  8.8524e-04,  ..., -3.7241e-04,\n          7.3379e-04,  6.9271e-04],\n        [-1.1626e-03, -1.8197e-03,  1.3626e-03,  ..., -1.4003e-03,\n          2.1479e-03,  1.5629e-03]]), 'transformer.encoder.layers.6.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0027,  0.0049, -0.0077,  ..., -0.0028, -0.0083,  0.0065],\n        [-0.0034, -0.0058,  0.0015,  ...,  0.0048,  0.0034, -0.0062],\n        [ 0.0077, -0.0055,  0.0053,  ..., -0.0070, -0.0013,  0.0049],\n        ...,\n        [ 0.0042,  0.0056,  0.0010,  ..., -0.0078, -0.0012,  0.0058],\n        [-0.0006,  0.0057, -0.0052,  ..., -0.0008,  0.0028,  0.0020],\n        [ 0.0044,  0.0058, -0.0013,  ...,  0.0021,  0.0087,  0.0085]]), 'transformer.encoder.layers.6.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-5.9063e-04, -3.8143e-04,  8.2192e-04,  ..., -8.0236e-04,\n          6.5689e-04, -3.2116e-04],\n        [ 9.7271e-04,  7.4861e-04, -9.0929e-04,  ...,  1.3038e-03,\n         -5.7977e-04,  2.3876e-04],\n        [ 5.3761e-04,  1.3689e-04, -5.3637e-04,  ...,  1.2739e-03,\n         -5.5635e-04,  8.7916e-04],\n        ...,\n        [ 2.5768e-04,  1.1322e-03, -9.6528e-04,  ...,  7.1644e-04,\n         -6.8627e-04,  4.1111e-04],\n        [-1.1440e-03, -2.1395e-03,  1.9740e-03,  ..., -1.2365e-03,\n          2.5745e-03, -1.7232e-04],\n        [ 4.0072e-05,  1.0681e-03, -1.4713e-03,  ..., -3.2465e-04,\n         -1.3167e-03,  6.3115e-04]]), 'transformer.encoder.layers.7.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0066, -0.0008, -0.0125,  ..., -0.0018,  0.0101, -0.0006],\n        [-0.0071,  0.0035,  0.0136,  ...,  0.0047, -0.0116,  0.0019],\n        [-0.0152, -0.0044, -0.0005,  ...,  0.0045,  0.0092,  0.0151],\n        ...,\n        [-0.0137, -0.0156, -0.0032,  ..., -0.0144,  0.0102, -0.0003],\n        [-0.0149, -0.0132, -0.0108,  ...,  0.0025,  0.0078,  0.0092],\n        [-0.0002,  0.0150, -0.0072,  ...,  0.0021, -0.0063,  0.0033]]), 'transformer.encoder.layers.7.self_attention.query_key_value.lora_B.default.weight': tensor([[ 1.4004e-04,  1.5589e-04,  1.9719e-06,  ..., -7.9450e-05,\n          1.5281e-04,  1.1020e-04],\n        [ 3.7914e-05, -1.0916e-04,  1.4928e-05,  ...,  1.7967e-04,\n         -8.5974e-05, -1.8833e-04],\n        [-2.3237e-04, -2.8970e-04, -2.4024e-04,  ...,  4.8477e-05,\n         -3.7585e-04, -1.8450e-04],\n        ...,\n        [ 1.9992e-04,  1.4724e-04, -7.2907e-04,  ...,  2.6152e-05,\n          8.0529e-05, -3.3309e-04],\n        [ 1.0827e-03,  3.1454e-04, -1.8808e-03,  ...,  9.2509e-04,\n          4.8152e-04,  1.1027e-04],\n        [ 1.1044e-03,  1.9262e-03,  1.2756e-03,  ..., -7.9333e-04,\n          1.9591e-03,  1.1991e-03]]), 'transformer.encoder.layers.7.self_attention.dense.lora_A.default.weight': tensor([[ 0.0078, -0.0150,  0.0063,  ..., -0.0082, -0.0077, -0.0135],\n        [-0.0019, -0.0043,  0.0049,  ...,  0.0067, -0.0097,  0.0007],\n        [ 0.0021,  0.0072,  0.0027,  ...,  0.0055,  0.0009,  0.0091],\n        ...,\n        [-0.0152,  0.0102,  0.0037,  ...,  0.0041, -0.0058,  0.0020],\n        [ 0.0121, -0.0051,  0.0099,  ..., -0.0098,  0.0056, -0.0003],\n        [ 0.0040,  0.0079, -0.0061,  ...,  0.0046, -0.0041, -0.0127]]), 'transformer.encoder.layers.7.self_attention.dense.lora_B.default.weight': tensor([[ 1.1372e-03,  8.3452e-04, -7.6600e-04,  ..., -3.3325e-04,\n         -7.2031e-04, -2.8001e-04],\n        [-5.0346e-04,  6.2417e-04,  2.8946e-04,  ...,  3.8298e-05,\n          2.7777e-04, -9.2754e-04],\n        [-2.4370e-04, -8.2578e-06,  3.0443e-04,  ...,  2.9871e-04,\n          2.3822e-04, -5.7357e-04],\n        ...,\n        [-7.5115e-04, -5.3623e-04,  9.8163e-04,  ...,  6.7999e-04,\n          1.0332e-03,  2.7265e-04],\n        [ 1.9797e-03,  9.9432e-04, -1.8126e-03,  ..., -7.4766e-04,\n         -2.3248e-03, -1.0943e-03],\n        [-7.5158e-04, -8.9026e-04,  6.6846e-04,  ...,  1.1768e-03,\n          1.1444e-03,  9.1815e-04]]), 'transformer.encoder.layers.7.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 1.1341e-02,  8.4588e-04,  3.0576e-03,  ..., -2.6712e-03,\n          1.5584e-02,  1.3342e-02],\n        [ 3.4135e-03,  2.2919e-04, -5.3761e-03,  ...,  1.0967e-02,\n         -1.1689e-02,  1.3615e-03],\n        [-5.4724e-04, -7.2596e-03,  9.6910e-04,  ...,  2.2222e-03,\n          1.7738e-03,  1.5420e-02],\n        ...,\n        [ 2.4349e-03, -1.2149e-02, -4.7288e-03,  ...,  1.1385e-02,\n         -1.2892e-02,  1.1675e-02],\n        [-1.5540e-02,  1.4729e-02, -1.0011e-02,  ..., -3.6304e-03,\n         -4.8853e-03,  1.1075e-02],\n        [-8.3447e-03,  7.8141e-03, -1.6721e-06,  ...,  1.0109e-03,\n         -1.1925e-02,  1.4996e-02]]), 'transformer.encoder.layers.7.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 1.0144e-05,  7.2045e-04, -9.4333e-04,  ...,  9.9568e-04,\n          6.5459e-04,  6.6484e-04],\n        [ 1.2947e-04,  3.1545e-05, -2.1677e-04,  ..., -1.2944e-04,\n          1.4813e-04,  2.3847e-04],\n        [ 1.2049e-04,  1.6545e-04,  1.7301e-04,  ...,  4.7114e-04,\n         -2.7544e-04, -2.2331e-05],\n        ...,\n        [ 2.9865e-04, -9.3751e-04,  4.4581e-04,  ..., -8.3803e-04,\n         -4.8836e-04, -7.1076e-04],\n        [-1.3696e-04,  3.9967e-04, -3.1127e-04,  ...,  3.3558e-05,\n          2.8598e-06,  1.7414e-04],\n        [-2.5321e-04,  3.0888e-04, -3.7770e-04,  ..., -2.1408e-06,\n          3.8067e-04,  3.6919e-04]]), 'transformer.encoder.layers.7.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0047,  0.0064,  0.0042,  ..., -0.0012,  0.0078,  0.0026],\n        [-0.0019,  0.0058, -0.0035,  ...,  0.0029,  0.0013,  0.0051],\n        [ 0.0070, -0.0057,  0.0021,  ..., -0.0042,  0.0043,  0.0009],\n        ...,\n        [ 0.0027, -0.0062,  0.0019,  ...,  0.0012, -0.0078, -0.0077],\n        [-0.0045, -0.0073, -0.0071,  ...,  0.0018,  0.0015,  0.0041],\n        [-0.0051, -0.0053, -0.0075,  ...,  0.0012, -0.0006,  0.0025]]), 'transformer.encoder.layers.7.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 1.2696e-03, -4.7649e-04,  1.4352e-03,  ...,  1.0728e-03,\n         -1.0880e-03, -7.9708e-04],\n        [ 1.3215e-04, -9.5470e-04,  1.2293e-03,  ...,  4.9273e-04,\n         -1.1904e-03, -1.5066e-03],\n        [-4.3471e-04, -5.3765e-04, -3.1625e-04,  ...,  8.5450e-05,\n         -3.7469e-05, -8.3350e-04],\n        ...,\n        [-7.4190e-04,  3.9979e-05, -1.0155e-03,  ..., -6.1571e-04,\n          5.8747e-04, -5.5652e-04],\n        [ 2.0233e-03, -8.3071e-04,  2.6014e-03,  ...,  1.3886e-03,\n         -1.8537e-03, -2.0618e-03],\n        [ 3.2211e-04,  6.0195e-04, -7.0313e-04,  ..., -9.1413e-04,\n          6.8179e-04,  5.3397e-04]]), 'transformer.encoder.layers.8.self_attention.query_key_value.lora_A.default.weight': tensor([[-1.0628e-04,  3.3814e-03, -1.0814e-02,  ...,  1.1947e-02,\n          5.3546e-03, -4.7397e-05],\n        [ 5.9759e-03, -1.1105e-02, -5.8469e-03,  ...,  5.7055e-03,\n         -1.4551e-02, -8.2163e-04],\n        [-9.8554e-03, -1.3487e-02, -7.8602e-03,  ...,  8.3770e-03,\n          1.2492e-02, -1.0227e-02],\n        ...,\n        [ 1.4215e-02,  7.5536e-03, -3.9159e-03,  ..., -5.3603e-03,\n          2.6833e-03,  1.5830e-02],\n        [-5.3438e-03, -4.9777e-03, -1.4437e-02,  ...,  1.5184e-02,\n          1.3205e-02,  5.4806e-03],\n        [-5.6004e-03, -7.6798e-03,  8.9419e-03,  ..., -6.1413e-03,\n         -7.7018e-03,  2.1028e-03]]), 'transformer.encoder.layers.8.self_attention.query_key_value.lora_B.default.weight': tensor([[-5.9572e-04,  6.1967e-04,  4.9788e-04,  ...,  2.3417e-04,\n         -9.7963e-04, -1.6977e-04],\n        [ 2.6418e-04, -1.2572e-04, -3.5155e-04,  ..., -2.3585e-04,\n          3.5757e-04, -7.0935e-05],\n        [ 2.9998e-06, -4.7616e-05, -4.8822e-05,  ..., -5.8709e-05,\n          1.6873e-04,  8.8409e-07],\n        ...,\n        [-4.7109e-04,  2.7700e-04,  4.8519e-04,  ..., -2.5307e-04,\n         -1.2984e-03, -3.6174e-04],\n        [ 3.3248e-04, -4.8606e-04, -8.0330e-04,  ..., -1.0825e-04,\n          1.0215e-03,  9.0506e-04],\n        [ 6.2988e-04,  6.4278e-04,  8.6353e-05,  ..., -1.3329e-03,\n         -1.1790e-03,  2.4693e-04]]), 'transformer.encoder.layers.8.self_attention.dense.lora_A.default.weight': tensor([[ 0.0050,  0.0017, -0.0155,  ..., -0.0143,  0.0007, -0.0135],\n        [ 0.0052,  0.0034,  0.0064,  ..., -0.0116, -0.0050, -0.0153],\n        [ 0.0109, -0.0154,  0.0050,  ..., -0.0001, -0.0092,  0.0119],\n        ...,\n        [-0.0150,  0.0097,  0.0115,  ...,  0.0071, -0.0082, -0.0121],\n        [ 0.0028,  0.0114,  0.0133,  ...,  0.0107,  0.0053,  0.0087],\n        [-0.0031,  0.0030, -0.0117,  ...,  0.0068, -0.0021,  0.0135]]), 'transformer.encoder.layers.8.self_attention.dense.lora_B.default.weight': tensor([[-2.5513e-03,  9.6844e-04, -1.2848e-04,  ..., -1.4271e-03,\n         -6.3580e-04,  1.2988e-03],\n        [ 5.0302e-04, -1.3591e-04,  5.1001e-04,  ...,  5.0618e-04,\n         -2.2059e-04, -4.7956e-04],\n        [-2.4558e-04, -2.0362e-04,  7.7475e-04,  ..., -1.3821e-05,\n         -2.8723e-04, -1.5392e-04],\n        ...,\n        [-1.1983e-03,  6.2428e-04,  3.4829e-04,  ..., -6.5524e-04,\n         -5.7637e-04,  3.3435e-04],\n        [-1.9612e-03,  8.4995e-04,  5.5203e-04,  ..., -1.1629e-03,\n         -1.1317e-03,  1.1068e-03],\n        [ 9.6506e-04, -2.9118e-04,  1.0015e-04,  ...,  4.6274e-05,\n         -1.1311e-04, -6.1442e-04]]), 'transformer.encoder.layers.8.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 2.9037e-03,  2.3184e-03, -1.0078e-02,  ...,  8.2785e-03,\n          1.1493e-02,  4.4805e-03],\n        [ 1.4173e-03,  1.7341e-03, -1.3671e-03,  ...,  6.0308e-03,\n          1.1780e-02, -4.3935e-03],\n        [ 6.8558e-04,  7.5164e-03,  1.2701e-02,  ..., -3.5290e-03,\n          1.7941e-03,  8.4370e-05],\n        ...,\n        [-1.4151e-02, -1.3786e-03,  1.0636e-02,  ...,  3.9552e-03,\n          5.1924e-03,  7.0125e-03],\n        [ 8.1848e-03, -1.6336e-03,  1.2478e-02,  ...,  3.0181e-03,\n         -9.3157e-03,  1.0336e-02],\n        [-1.2920e-02,  3.3443e-03, -4.6953e-03,  ..., -3.3558e-03,\n         -9.9686e-03, -1.0080e-02]]), 'transformer.encoder.layers.8.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-9.2077e-04,  1.4573e-03, -1.9237e-03,  ..., -2.1475e-03,\n          1.1663e-03, -1.0265e-03],\n        [ 1.5603e-04, -6.0081e-04,  6.2519e-04,  ...,  8.9749e-04,\n         -5.7978e-04,  3.1629e-04],\n        [ 7.4242e-05, -8.5533e-06, -1.7003e-04,  ..., -3.4674e-04,\n         -8.3229e-05,  1.0299e-04],\n        ...,\n        [-1.5116e-04,  5.6260e-05, -4.1067e-04,  ..., -5.2017e-04,\n          7.2712e-05, -4.6764e-04],\n        [ 1.2352e-03, -1.4209e-03,  3.2316e-03,  ...,  3.8090e-03,\n         -8.8279e-04,  1.4447e-03],\n        [ 8.9909e-04, -4.1134e-04,  5.7041e-04,  ...,  8.9120e-04,\n         -7.1759e-04,  6.6895e-04]]), 'transformer.encoder.layers.8.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0034,  0.0001,  0.0076,  ...,  0.0058,  0.0011, -0.0063],\n        [-0.0030, -0.0058,  0.0031,  ..., -0.0015,  0.0046,  0.0071],\n        [-0.0055, -0.0057,  0.0077,  ...,  0.0031, -0.0065, -0.0072],\n        ...,\n        [-0.0020,  0.0033,  0.0013,  ..., -0.0045,  0.0016,  0.0001],\n        [ 0.0011,  0.0056, -0.0041,  ...,  0.0035, -0.0047, -0.0060],\n        [ 0.0052, -0.0051,  0.0082,  ...,  0.0025,  0.0058, -0.0002]]), 'transformer.encoder.layers.8.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-1.8288e-04,  1.3821e-03,  6.1762e-04,  ...,  2.3273e-03,\n          1.2062e-04, -8.1162e-04],\n        [-1.8376e-04,  4.4578e-04, -1.0887e-04,  ..., -4.8908e-05,\n          4.3147e-04, -4.3777e-05],\n        [ 4.4202e-04,  2.0612e-04, -5.7573e-04,  ...,  1.7554e-04,\n         -2.4028e-04, -5.2152e-04],\n        ...,\n        [-1.3810e-04,  2.3621e-04,  2.6030e-04,  ...,  1.3193e-03,\n          2.8053e-04, -3.3754e-04],\n        [ 5.4233e-04,  9.5851e-04,  1.1596e-04,  ...,  2.0666e-03,\n          1.0425e-05, -1.2654e-03],\n        [-2.6097e-04, -7.6417e-04, -3.1450e-04,  ..., -1.1312e-03,\n         -9.1874e-04,  9.8295e-04]]), 'transformer.encoder.layers.9.self_attention.query_key_value.lora_A.default.weight': tensor([[ 2.8727e-03,  5.3104e-03,  7.4538e-03,  ..., -8.7071e-03,\n          1.0830e-02,  5.2967e-03],\n        [ 6.0455e-03, -4.2222e-03, -7.3085e-04,  ..., -8.1725e-05,\n          1.6033e-03,  2.8552e-03],\n        [ 1.0477e-02,  3.7103e-04, -5.7078e-03,  ...,  9.2611e-03,\n         -1.4555e-02,  8.5477e-03],\n        ...,\n        [ 6.7978e-03,  6.0945e-03,  8.7147e-03,  ...,  6.8687e-03,\n          1.4841e-02,  1.0171e-02],\n        [-3.1663e-03,  5.3601e-03, -3.8863e-03,  ...,  1.5077e-02,\n         -2.6833e-03,  1.4816e-02],\n        [-9.5675e-03, -1.1215e-02,  1.5269e-02,  ...,  1.2694e-03,\n         -9.1451e-03, -1.6391e-03]]), 'transformer.encoder.layers.9.self_attention.query_key_value.lora_B.default.weight': tensor([[ 2.4074e-04, -5.5052e-06,  2.3085e-05,  ..., -1.0122e-04,\n          1.3055e-04,  2.8998e-05],\n        [ 4.5869e-05,  1.7483e-04,  1.8155e-04,  ..., -1.0644e-04,\n         -1.4515e-04, -2.0691e-04],\n        [-1.2046e-04,  1.4083e-04, -5.5610e-05,  ..., -1.0485e-04,\n          1.6418e-05, -4.5711e-05],\n        ...,\n        [-9.5803e-04,  1.7145e-03,  1.8634e-03,  ..., -1.1150e-03,\n         -2.1863e-03, -1.2272e-03],\n        [ 1.2625e-03, -3.2071e-03, -3.4912e-03,  ...,  2.8049e-03,\n          4.1715e-03,  2.4035e-03],\n        [ 8.2478e-05,  1.7474e-05, -1.4192e-05,  ...,  1.6098e-05,\n          5.6406e-04, -1.3055e-04]]), 'transformer.encoder.layers.9.self_attention.dense.lora_A.default.weight': tensor([[ 0.0109,  0.0115,  0.0022,  ..., -0.0007,  0.0032, -0.0091],\n        [-0.0078, -0.0027, -0.0095,  ..., -0.0099,  0.0044,  0.0104],\n        [ 0.0104,  0.0136, -0.0098,  ..., -0.0051, -0.0113,  0.0141],\n        ...,\n        [ 0.0036,  0.0049,  0.0023,  ...,  0.0081, -0.0115,  0.0091],\n        [-0.0116,  0.0071,  0.0109,  ...,  0.0039,  0.0088, -0.0128],\n        [-0.0001, -0.0140, -0.0071,  ...,  0.0112, -0.0164,  0.0034]]), 'transformer.encoder.layers.9.self_attention.dense.lora_B.default.weight': tensor([[-5.3526e-04,  5.4186e-04,  7.1605e-04,  ..., -8.6077e-04,\n          8.5227e-04,  1.7794e-04],\n        [ 2.2995e-04,  9.1057e-05,  3.0638e-04,  ..., -1.3589e-04,\n          2.4406e-04, -5.0539e-05],\n        [-1.3139e-04, -1.9468e-04,  4.3890e-04,  ..., -5.9055e-04,\n          5.3427e-04,  2.1885e-04],\n        ...,\n        [-1.3336e-04,  1.6424e-05,  2.7854e-04,  ..., -4.0057e-04,\n          2.1139e-05, -1.6448e-04],\n        [ 4.8701e-05,  5.8961e-04,  4.8230e-04,  ..., -1.0358e-03,\n          1.0917e-03,  6.9415e-04],\n        [ 8.8699e-04, -4.8948e-04,  2.8281e-04,  ...,  7.2340e-04,\n         -4.7240e-04, -7.3369e-04]]), 'transformer.encoder.layers.9.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0056,  0.0007,  0.0116,  ...,  0.0123, -0.0123,  0.0156],\n        [ 0.0057,  0.0112, -0.0111,  ...,  0.0024, -0.0087,  0.0063],\n        [ 0.0086,  0.0015, -0.0029,  ..., -0.0068,  0.0116, -0.0001],\n        ...,\n        [-0.0095,  0.0080, -0.0133,  ..., -0.0005,  0.0007, -0.0048],\n        [ 0.0110, -0.0009,  0.0021,  ..., -0.0063,  0.0052,  0.0108],\n        [-0.0117, -0.0018,  0.0155,  ..., -0.0119,  0.0161, -0.0062]]), 'transformer.encoder.layers.9.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 4.7068e-04,  6.9773e-04, -4.7213e-04,  ...,  4.3688e-05,\n          8.9411e-04,  5.6907e-04],\n        [ 9.3958e-04, -6.1312e-04,  2.0803e-04,  ..., -1.2101e-04,\n         -1.5538e-03,  2.6183e-03],\n        [ 3.3743e-05,  2.3689e-04, -2.9789e-04,  ...,  2.7920e-04,\n         -2.0449e-04, -5.9301e-04],\n        ...,\n        [-8.5238e-04,  7.5691e-05,  4.5417e-04,  ..., -1.1031e-04,\n         -1.1391e-03,  1.0695e-03],\n        [-6.1559e-04,  1.6313e-04, -5.8741e-04,  ...,  1.0149e-04,\n         -1.2297e-05, -1.9446e-05],\n        [ 3.3353e-04, -7.2722e-04,  5.7434e-04,  ...,  5.3073e-04,\n         -4.1106e-05,  1.1065e-03]]), 'transformer.encoder.layers.9.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0024,  0.0022,  0.0079,  ...,  0.0035,  0.0041, -0.0022],\n        [-0.0028,  0.0065,  0.0005,  ..., -0.0061, -0.0031, -0.0066],\n        [-0.0024,  0.0021, -0.0037,  ...,  0.0043,  0.0049,  0.0026],\n        ...,\n        [ 0.0036,  0.0081,  0.0068,  ...,  0.0063, -0.0082, -0.0063],\n        [ 0.0015, -0.0074, -0.0077,  ...,  0.0066, -0.0078, -0.0034],\n        [-0.0047,  0.0061, -0.0051,  ..., -0.0051, -0.0005, -0.0061]]), 'transformer.encoder.layers.9.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-0.0008,  0.0014, -0.0008,  ...,  0.0003,  0.0010,  0.0007],\n        [ 0.0003, -0.0003,  0.0006,  ..., -0.0011, -0.0014, -0.0010],\n        [-0.0002,  0.0009, -0.0007,  ...,  0.0001,  0.0004,  0.0002],\n        ...,\n        [-0.0003,  0.0003,  0.0003,  ..., -0.0005, -0.0003,  0.0001],\n        [-0.0006,  0.0009, -0.0011,  ...,  0.0005,  0.0012,  0.0010],\n        [-0.0001,  0.0004, -0.0004,  ..., -0.0002, -0.0005,  0.0002]]), 'transformer.encoder.layers.10.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0115,  0.0107, -0.0048,  ..., -0.0076, -0.0037,  0.0032],\n        [-0.0139, -0.0054, -0.0098,  ...,  0.0071,  0.0019,  0.0108],\n        [-0.0117,  0.0075, -0.0054,  ...,  0.0136,  0.0085, -0.0064],\n        ...,\n        [ 0.0015,  0.0009, -0.0127,  ...,  0.0061, -0.0142,  0.0068],\n        [ 0.0035,  0.0039,  0.0018,  ..., -0.0019, -0.0102,  0.0034],\n        [-0.0058,  0.0100, -0.0149,  ...,  0.0131, -0.0003, -0.0054]]), 'transformer.encoder.layers.10.self_attention.query_key_value.lora_B.default.weight': tensor([[-1.0813e-04, -7.3336e-06, -1.5678e-04,  ..., -8.1089e-05,\n          9.5842e-05, -4.8617e-05],\n        [-1.8280e-04, -4.3355e-05, -4.2062e-05,  ...,  3.3964e-04,\n          3.3024e-05,  2.9494e-05],\n        [ 4.2085e-04,  4.0684e-04,  4.7139e-04,  ..., -2.3454e-05,\n          1.6948e-04, -1.9787e-04],\n        ...,\n        [ 4.5309e-04,  1.8642e-03,  4.2183e-04,  ..., -7.1908e-04,\n          1.8694e-03,  6.0237e-04],\n        [-2.5723e-04, -1.8519e-04,  3.4120e-04,  ..., -7.1432e-04,\n         -3.1194e-04, -7.7157e-04],\n        [ 6.8274e-04, -2.5385e-04,  9.7221e-04,  ..., -6.1259e-05,\n         -2.4765e-04, -2.8837e-04]]), 'transformer.encoder.layers.10.self_attention.dense.lora_A.default.weight': tensor([[ 0.0138, -0.0141,  0.0153,  ..., -0.0153, -0.0057, -0.0146],\n        [-0.0123,  0.0068, -0.0138,  ..., -0.0064, -0.0128,  0.0030],\n        [-0.0021,  0.0051, -0.0133,  ..., -0.0066,  0.0032,  0.0019],\n        ...,\n        [-0.0045, -0.0017,  0.0030,  ..., -0.0063,  0.0105,  0.0017],\n        [ 0.0064,  0.0027,  0.0107,  ...,  0.0088,  0.0062,  0.0127],\n        [-0.0091, -0.0145,  0.0015,  ...,  0.0020,  0.0062,  0.0028]]), 'transformer.encoder.layers.10.self_attention.dense.lora_B.default.weight': tensor([[-8.9569e-04, -9.6704e-04,  1.3464e-03,  ...,  5.8862e-04,\n         -4.4903e-04,  6.9103e-04],\n        [ 1.6929e-03,  6.9187e-04, -1.0208e-03,  ...,  5.1579e-04,\n          2.8948e-04, -1.0458e-03],\n        [-1.1274e-04, -1.9161e-04, -2.3390e-04,  ..., -2.8938e-04,\n          9.1791e-04, -1.5256e-04],\n        ...,\n        [ 8.3886e-04, -2.3098e-04, -5.2734e-04,  ..., -1.2872e-04,\n          1.8186e-04,  1.1132e-04],\n        [-6.3710e-04,  9.3826e-05, -1.8842e-04,  ..., -1.5500e-03,\n          2.8963e-03, -5.9342e-04],\n        [ 3.2606e-05, -4.9383e-04,  2.7232e-04,  ...,  9.9162e-04,\n          3.3221e-04,  4.1211e-04]]), 'transformer.encoder.layers.10.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0076,  0.0024, -0.0089,  ..., -0.0114, -0.0066, -0.0152],\n        [ 0.0111,  0.0025, -0.0147,  ...,  0.0118,  0.0020, -0.0062],\n        [-0.0084,  0.0045,  0.0073,  ..., -0.0084, -0.0106,  0.0084],\n        ...,\n        [ 0.0022,  0.0049, -0.0067,  ..., -0.0086,  0.0036, -0.0078],\n        [ 0.0143, -0.0100, -0.0133,  ...,  0.0137,  0.0026,  0.0146],\n        [-0.0120,  0.0117,  0.0081,  ...,  0.0061,  0.0056, -0.0143]]), 'transformer.encoder.layers.10.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-7.0520e-04,  5.6921e-04, -4.0145e-04,  ..., -5.1982e-04,\n         -2.5207e-04,  6.2511e-05],\n        [-6.6020e-05, -2.9683e-05, -4.5792e-05,  ..., -5.4060e-05,\n         -2.4262e-04,  9.4645e-05],\n        [ 6.0923e-05, -2.2492e-04, -2.4945e-05,  ..., -9.8477e-04,\n          8.6857e-04, -1.9406e-04],\n        ...,\n        [-1.3721e-03,  4.3571e-04,  4.7781e-04,  ...,  1.1588e-04,\n         -6.5050e-04,  1.3712e-04],\n        [ 7.2262e-05, -3.5402e-04,  6.1622e-05,  ...,  2.1787e-04,\n          1.9607e-05, -9.7296e-05],\n        [ 9.5875e-04, -8.8533e-04,  8.7888e-04,  ...,  1.4971e-03,\n         -4.9025e-04,  4.9391e-04]]), 'transformer.encoder.layers.10.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 4.4242e-05, -2.9668e-03, -4.7177e-03,  ..., -5.4083e-03,\n          6.7048e-03, -8.2868e-03],\n        [-2.7296e-04,  3.4927e-03, -5.1912e-04,  ...,  7.8118e-03,\n          2.8920e-03,  4.1102e-03],\n        [-4.3153e-03, -5.9994e-03,  2.3795e-03,  ..., -6.2460e-03,\n          5.0980e-03, -7.1136e-03],\n        ...,\n        [-6.2127e-04, -6.1981e-03, -6.3205e-03,  ...,  5.8443e-03,\n         -1.0742e-03,  3.6204e-03],\n        [ 7.9881e-04,  5.9869e-03,  5.3691e-03,  ..., -4.4791e-04,\n          2.5144e-03,  1.6272e-03],\n        [ 6.6367e-03, -6.5017e-03, -8.0360e-03,  ..., -6.9973e-03,\n          1.0307e-03,  2.8161e-03]]), 'transformer.encoder.layers.10.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-6.1413e-04,  1.2153e-03, -9.8811e-04,  ...,  1.6140e-03,\n          1.1339e-03,  5.2500e-05],\n        [ 1.3315e-04, -7.3840e-04,  3.2958e-04,  ..., -9.2811e-04,\n         -7.2118e-04, -4.2113e-04],\n        [-1.5520e-04,  1.0514e-03, -5.7167e-04,  ...,  4.1419e-04,\n         -1.5058e-04, -3.7431e-04],\n        ...,\n        [ 1.5935e-03, -9.4671e-04,  1.3675e-03,  ..., -2.1631e-03,\n         -3.3907e-04, -4.4608e-04],\n        [-8.1959e-04,  1.0827e-03, -1.2653e-03,  ...,  1.3586e-03,\n         -1.0075e-03, -7.6272e-04],\n        [ 7.8441e-04, -6.6949e-04,  8.7834e-04,  ..., -1.2117e-03,\n         -4.6454e-04,  5.1437e-05]]), 'transformer.encoder.layers.11.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0091,  0.0081, -0.0152,  ...,  0.0113, -0.0050,  0.0090],\n        [ 0.0041, -0.0038,  0.0143,  ..., -0.0003,  0.0117, -0.0027],\n        [-0.0092, -0.0058,  0.0104,  ...,  0.0124,  0.0147,  0.0138],\n        ...,\n        [-0.0006, -0.0104, -0.0109,  ...,  0.0022,  0.0002, -0.0150],\n        [ 0.0031,  0.0135,  0.0034,  ...,  0.0150, -0.0059, -0.0021],\n        [ 0.0010,  0.0044,  0.0133,  ..., -0.0039, -0.0089,  0.0075]]), 'transformer.encoder.layers.11.self_attention.query_key_value.lora_B.default.weight': tensor([[ 1.0848e-04,  7.2157e-05,  1.6923e-04,  ...,  9.1097e-05,\n         -7.2705e-06, -1.2108e-05],\n        [ 4.0074e-04, -1.9876e-04,  3.9745e-04,  ...,  4.1743e-04,\n          2.7273e-04,  8.6839e-05],\n        [ 5.2138e-05, -4.6418e-05,  4.8708e-04,  ...,  3.3294e-04,\n         -3.2150e-05,  8.1423e-06],\n        ...,\n        [ 6.5128e-04, -1.2411e-06,  8.6229e-04,  ...,  8.1840e-04,\n         -1.4001e-04, -1.1309e-04],\n        [-3.7765e-04,  3.0016e-03,  1.7174e-03,  ..., -1.0410e-03,\n          3.6485e-04,  1.6524e-04],\n        [-2.5835e-04, -1.4198e-04, -3.1708e-04,  ..., -1.1151e-04,\n          4.0384e-04, -3.8729e-05]]), 'transformer.encoder.layers.11.self_attention.dense.lora_A.default.weight': tensor([[ 0.0060, -0.0011,  0.0002,  ...,  0.0065, -0.0124,  0.0067],\n        [ 0.0166,  0.0064, -0.0129,  ...,  0.0127, -0.0141,  0.0098],\n        [ 0.0119, -0.0150,  0.0004,  ..., -0.0015,  0.0142,  0.0054],\n        ...,\n        [ 0.0078,  0.0025,  0.0116,  ...,  0.0020, -0.0119,  0.0044],\n        [-0.0122,  0.0115, -0.0037,  ...,  0.0107,  0.0115,  0.0155],\n        [ 0.0144,  0.0131, -0.0048,  ...,  0.0139,  0.0122, -0.0013]]), 'transformer.encoder.layers.11.self_attention.dense.lora_B.default.weight': tensor([[ 1.8605e-04, -8.7424e-04, -7.0889e-04,  ...,  1.5848e-03,\n          4.5695e-04, -2.1329e-03],\n        [-7.6609e-04,  1.8416e-04,  2.2378e-04,  ..., -4.4568e-04,\n         -1.6489e-04,  4.8741e-04],\n        [ 6.3205e-04,  4.1003e-05, -1.0729e-05,  ...,  5.7196e-04,\n          7.0691e-04, -1.0785e-03],\n        ...,\n        [-1.8720e-04,  6.6420e-04,  1.6800e-05,  ..., -5.3969e-04,\n         -8.0859e-04,  1.0153e-03],\n        [ 6.2339e-05, -2.9704e-04,  4.0034e-04,  ...,  6.8991e-04,\n          1.1787e-03, -1.3621e-03],\n        [-6.4194e-04,  5.1751e-04,  5.3585e-04,  ..., -1.3790e-04,\n          7.0517e-04, -5.5481e-04]]), 'transformer.encoder.layers.11.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 6.5214e-03,  4.2239e-03,  5.6670e-03,  ...,  3.8144e-03,\n         -9.8056e-03,  3.8474e-03],\n        [-5.1847e-03, -1.3331e-03, -1.1863e-02,  ...,  1.4989e-02,\n          1.5041e-02, -1.1156e-02],\n        [-2.0209e-03, -9.7961e-03,  7.1633e-03,  ..., -7.9175e-03,\n         -3.0037e-03, -1.3904e-02],\n        ...,\n        [-8.8619e-03, -9.1633e-04, -4.4367e-03,  ..., -3.9042e-05,\n          5.5635e-03,  7.1311e-03],\n        [ 8.6090e-03, -1.5261e-02,  6.8714e-03,  ...,  1.5728e-03,\n          5.1360e-03,  8.5415e-03],\n        [-2.9049e-03,  1.5844e-03,  2.2226e-03,  ..., -1.2840e-02,\n         -2.1936e-03,  3.0408e-03]]), 'transformer.encoder.layers.11.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 1.8477e-03, -2.3190e-03, -1.5610e-04,  ...,  1.5497e-03,\n          1.7988e-03, -1.2934e-03],\n        [-4.3005e-04,  3.8796e-04,  3.8969e-05,  ..., -4.8359e-04,\n         -1.0095e-03, -2.3008e-04],\n        [-8.9042e-05, -4.9546e-04, -3.0696e-04,  ..., -2.2516e-04,\n          4.6978e-04, -3.1132e-04],\n        ...,\n        [ 3.9982e-04, -7.5187e-05,  3.5928e-04,  ...,  2.1496e-04,\n          4.1932e-05, -4.4043e-05],\n        [ 1.9499e-03, -2.8939e-03,  4.5377e-04,  ...,  1.5264e-03,\n          1.4682e-03, -1.0462e-03],\n        [-1.1515e-03,  1.4263e-03, -3.9914e-04,  ..., -1.1218e-03,\n         -9.2404e-04,  2.8271e-04]]), 'transformer.encoder.layers.11.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0052, -0.0025,  0.0047,  ...,  0.0002,  0.0021, -0.0033],\n        [ 0.0072, -0.0068, -0.0065,  ..., -0.0054, -0.0060, -0.0013],\n        [ 0.0015, -0.0074,  0.0075,  ...,  0.0040, -0.0031, -0.0034],\n        ...,\n        [ 0.0027, -0.0069,  0.0018,  ...,  0.0051, -0.0061, -0.0044],\n        [ 0.0075, -0.0061,  0.0080,  ...,  0.0083,  0.0006,  0.0033],\n        [-0.0073, -0.0055,  0.0024,  ...,  0.0030,  0.0024, -0.0016]]), 'transformer.encoder.layers.11.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-3.0879e-04, -1.0852e-03,  6.7832e-04,  ..., -3.3646e-04,\n         -7.4861e-04, -8.5447e-04],\n        [-3.6938e-05,  6.7723e-04, -2.4713e-04,  ..., -2.3627e-04,\n          4.3788e-04,  3.8264e-05],\n        [ 4.6620e-04,  6.4297e-05, -1.1075e-04,  ..., -1.3456e-05,\n          8.0500e-04,  5.5148e-04],\n        ...,\n        [ 4.4750e-04,  5.5447e-04, -2.0268e-04,  ...,  4.4241e-04,\n          8.2749e-04,  5.8771e-04],\n        [-1.0450e-03, -7.9144e-04,  1.0128e-04,  ..., -1.0136e-03,\n         -1.6829e-03, -2.3792e-03],\n        [-4.9396e-04, -1.5989e-04, -6.6225e-05,  ..., -3.5726e-04,\n         -9.9511e-04, -2.8847e-04]]), 'transformer.encoder.layers.12.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0036, -0.0128,  0.0069,  ..., -0.0041,  0.0051, -0.0146],\n        [ 0.0082,  0.0071,  0.0106,  ..., -0.0053,  0.0017, -0.0014],\n        [ 0.0144, -0.0032,  0.0054,  ...,  0.0008, -0.0053,  0.0039],\n        ...,\n        [-0.0011,  0.0027, -0.0101,  ..., -0.0021,  0.0049, -0.0109],\n        [ 0.0024,  0.0010, -0.0145,  ...,  0.0084, -0.0005,  0.0137],\n        [-0.0013, -0.0070, -0.0029,  ...,  0.0002, -0.0038, -0.0100]]), 'transformer.encoder.layers.12.self_attention.query_key_value.lora_B.default.weight': tensor([[ 3.2000e-04,  2.2781e-05, -2.9615e-05,  ..., -2.7980e-04,\n         -3.0208e-04, -2.3555e-04],\n        [-1.9551e-04,  8.0422e-05,  1.0893e-04,  ...,  2.9667e-06,\n          4.0488e-04,  3.7941e-04],\n        [ 1.4518e-04, -3.3837e-06,  1.2644e-06,  ..., -7.1435e-05,\n          1.2080e-04,  1.2208e-04],\n        ...,\n        [ 8.4463e-04, -1.1179e-03, -3.2625e-04,  ..., -4.2292e-04,\n         -8.4372e-04, -1.2329e-04],\n        [ 1.7003e-04, -2.1339e-04, -1.1544e-04,  ...,  5.7868e-04,\n          1.1858e-03,  6.6647e-04],\n        [ 5.4299e-04, -1.9636e-03, -4.9098e-04,  ...,  9.3287e-04,\n          2.9525e-03,  2.4170e-03]]), 'transformer.encoder.layers.12.self_attention.dense.lora_A.default.weight': tensor([[ 0.0050,  0.0112, -0.0091,  ..., -0.0088, -0.0030,  0.0038],\n        [ 0.0044,  0.0142, -0.0010,  ...,  0.0056,  0.0125,  0.0102],\n        [-0.0007,  0.0133,  0.0089,  ...,  0.0109,  0.0054,  0.0107],\n        ...,\n        [ 0.0034,  0.0156,  0.0089,  ...,  0.0127, -0.0122,  0.0094],\n        [ 0.0033, -0.0038,  0.0056,  ...,  0.0077,  0.0066, -0.0064],\n        [-0.0133,  0.0012,  0.0103,  ..., -0.0066, -0.0010, -0.0106]]), 'transformer.encoder.layers.12.self_attention.dense.lora_B.default.weight': tensor([[-1.6127e-03, -8.4293e-04, -1.7213e-03,  ...,  1.1981e-03,\n         -1.2409e-03,  1.7983e-03],\n        [ 7.4363e-04,  1.1556e-03,  5.9411e-04,  ..., -2.3422e-04,\n          1.0230e-03, -1.1645e-03],\n        [-1.5669e-04, -1.0752e-04, -1.7222e-04,  ..., -1.3871e-04,\n         -5.9055e-04, -1.0418e-04],\n        ...,\n        [ 4.4053e-04, -7.1592e-05,  5.1954e-04,  ..., -8.0248e-04,\n          2.9245e-04,  1.1344e-04],\n        [-5.3647e-04, -1.0905e-03,  4.0136e-04,  ...,  1.9975e-03,\n         -1.2090e-04,  4.6093e-04],\n        [-3.7970e-04, -8.8846e-04, -1.0010e-03,  ...,  1.0748e-03,\n         -1.4360e-04,  7.4502e-04]]), 'transformer.encoder.layers.12.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0007,  0.0044, -0.0096,  ...,  0.0121,  0.0044, -0.0015],\n        [-0.0137,  0.0133,  0.0086,  ..., -0.0070,  0.0041, -0.0050],\n        [-0.0055, -0.0017, -0.0124,  ...,  0.0081, -0.0015,  0.0084],\n        ...,\n        [ 0.0083,  0.0139, -0.0120,  ...,  0.0046, -0.0007,  0.0090],\n        [-0.0147, -0.0041, -0.0057,  ...,  0.0102, -0.0116,  0.0022],\n        [ 0.0148, -0.0137, -0.0089,  ..., -0.0061,  0.0152, -0.0053]]), 'transformer.encoder.layers.12.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-5.4898e-04, -5.2503e-04, -2.5666e-04,  ...,  1.3941e-04,\n         -8.1629e-04,  5.7908e-05],\n        [-1.2347e-03,  2.2888e-03, -9.4833e-04,  ...,  1.4166e-03,\n          6.5712e-04, -3.9097e-04],\n        [ 5.8138e-04,  4.2122e-04,  9.6982e-05,  ...,  1.8444e-04,\n          1.2833e-04,  1.2580e-03],\n        ...,\n        [-1.6872e-04,  1.0286e-03, -3.5853e-04,  ...,  3.1332e-04,\n          8.8515e-04, -3.1105e-04],\n        [-7.3188e-05, -4.5258e-04, -2.3457e-04,  ...,  7.6854e-05,\n          5.5271e-05, -4.0835e-04],\n        [ 5.3644e-05,  1.7394e-04,  2.9979e-05,  ...,  1.4036e-05,\n          2.4168e-04,  1.5881e-05]]), 'transformer.encoder.layers.12.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0059, -0.0011,  0.0054,  ...,  0.0010,  0.0086, -0.0065],\n        [-0.0006,  0.0030, -0.0080,  ...,  0.0049,  0.0059,  0.0061],\n        [ 0.0007,  0.0038, -0.0088,  ...,  0.0086,  0.0067, -0.0059],\n        ...,\n        [-0.0013,  0.0043,  0.0041,  ..., -0.0050,  0.0014,  0.0030],\n        [ 0.0072,  0.0021,  0.0010,  ...,  0.0086, -0.0058, -0.0022],\n        [ 0.0030,  0.0034, -0.0055,  ...,  0.0055, -0.0085,  0.0038]]), 'transformer.encoder.layers.12.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-1.3915e-03,  1.5010e-03,  1.2697e-03,  ...,  2.0000e-03,\n         -1.7512e-04,  9.7962e-05],\n        [-6.4193e-04, -8.6304e-05,  5.8167e-04,  ...,  2.5865e-04,\n         -3.9799e-04,  8.1463e-05],\n        [-3.6306e-04,  3.2955e-04, -2.8343e-05,  ...,  3.3291e-04,\n         -1.2651e-04,  3.4828e-04],\n        ...,\n        [ 8.9839e-05,  2.7074e-04, -5.5634e-05,  ..., -2.4497e-04,\n          2.4338e-04,  2.4950e-04],\n        [-1.0401e-03,  1.2598e-03, -4.1941e-04,  ...,  1.5610e-03,\n         -1.1360e-03,  1.5003e-03],\n        [-1.0127e-03,  1.8705e-03, -2.3278e-04,  ...,  1.2066e-03,\n         -9.6354e-04,  1.0214e-03]]), 'transformer.encoder.layers.13.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0056,  0.0097,  0.0083,  ..., -0.0099, -0.0111, -0.0040],\n        [-0.0152, -0.0012, -0.0122,  ...,  0.0045, -0.0097,  0.0088],\n        [-0.0052, -0.0041,  0.0145,  ..., -0.0107, -0.0114, -0.0079],\n        ...,\n        [ 0.0101, -0.0135, -0.0028,  ...,  0.0046, -0.0120,  0.0083],\n        [ 0.0130,  0.0013, -0.0127,  ..., -0.0025,  0.0066, -0.0063],\n        [ 0.0147, -0.0034,  0.0130,  ...,  0.0109,  0.0062, -0.0101]]), 'transformer.encoder.layers.13.self_attention.query_key_value.lora_B.default.weight': tensor([[-1.2590e-04,  2.8156e-04,  2.6978e-04,  ...,  3.3096e-05,\n         -1.9421e-04,  2.0991e-04],\n        [ 9.7773e-05,  1.0933e-04,  1.0242e-04,  ...,  8.9280e-05,\n         -2.5635e-05, -1.7507e-04],\n        [-4.9048e-04, -3.9965e-04, -5.5369e-04,  ...,  9.0366e-05,\n          5.9240e-04, -4.1057e-04],\n        ...,\n        [-2.1120e-04, -3.4045e-04, -9.1165e-04,  ..., -1.0669e-04,\n          5.2132e-04,  1.1252e-04],\n        [-1.3031e-03,  1.3663e-03, -1.3848e-04,  ..., -1.2544e-03,\n         -8.1245e-04,  1.3184e-03],\n        [ 2.1178e-03,  4.6459e-04,  3.3945e-04,  ...,  1.1740e-03,\n         -6.9893e-04, -2.9440e-04]]), 'transformer.encoder.layers.13.self_attention.dense.lora_A.default.weight': tensor([[ 1.2780e-02, -1.3082e-02,  4.4972e-03,  ...,  6.6357e-04,\n         -3.1574e-03,  1.5189e-02],\n        [ 1.2392e-02, -1.7324e-03, -1.5597e-02,  ...,  9.5356e-03,\n         -1.4649e-02,  9.3123e-03],\n        [ 9.9283e-03, -1.1528e-02, -1.2183e-02,  ...,  5.7411e-03,\n         -3.5955e-03, -6.5192e-05],\n        ...,\n        [-5.4109e-03,  8.6725e-03,  2.0934e-03,  ..., -5.0641e-03,\n         -9.4143e-03, -6.0345e-03],\n        [-8.6943e-03, -1.2828e-02, -3.0427e-03,  ...,  4.3087e-03,\n          5.6443e-03, -7.5966e-03],\n        [ 1.0721e-02, -4.3211e-04,  1.3891e-02,  ..., -4.0435e-03,\n          1.4803e-03,  1.9459e-03]]), 'transformer.encoder.layers.13.self_attention.dense.lora_B.default.weight': tensor([[-1.4081e-03,  1.3479e-03, -1.2106e-03,  ..., -9.8592e-04,\n         -5.6313e-04,  8.0661e-04],\n        [-6.2985e-04, -6.3883e-05, -1.5019e-04,  ..., -7.5800e-05,\n          1.2898e-04,  1.9437e-04],\n        [ 8.8672e-05,  1.4146e-04, -2.5111e-04,  ..., -4.2681e-04,\n          1.5237e-04,  1.3879e-04],\n        ...,\n        [-3.3960e-04,  4.1638e-04, -3.5250e-04,  ..., -2.4376e-04,\n         -8.0411e-04,  4.3650e-04],\n        [-1.3784e-04, -3.5599e-04, -5.9171e-04,  ...,  5.8304e-04,\n          2.1985e-03, -4.6799e-04],\n        [-3.7174e-04, -3.4466e-04, -1.4341e-03,  ..., -9.7580e-05,\n          1.8198e-03, -7.0173e-04]]), 'transformer.encoder.layers.13.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-1.0325e-02, -3.5228e-03, -1.7983e-05,  ...,  1.2832e-02,\n          7.7525e-03,  5.8798e-03],\n        [-1.4882e-02,  1.0854e-02, -4.2380e-03,  ..., -1.1083e-02,\n          8.9289e-03,  7.2520e-03],\n        [-9.1195e-04,  1.4822e-02,  1.9048e-03,  ..., -3.8024e-03,\n         -1.2154e-02,  1.0339e-02],\n        ...,\n        [ 1.3515e-02, -1.4765e-02,  1.4157e-02,  ...,  1.4864e-02,\n         -6.5557e-03,  1.0160e-03],\n        [-7.8277e-03, -1.4534e-03, -1.9903e-03,  ..., -9.0941e-03,\n          1.4527e-03,  1.1365e-02],\n        [-8.7700e-03, -6.0932e-03,  1.1118e-04,  ...,  1.3449e-04,\n         -1.2936e-02,  1.1048e-02]]), 'transformer.encoder.layers.13.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-7.5498e-05, -4.3935e-04, -5.9866e-04,  ..., -3.5808e-04,\n          2.0044e-05, -1.8766e-04],\n        [-2.9564e-04, -6.3790e-04, -3.8582e-04,  ..., -2.2266e-04,\n         -3.4079e-04, -9.6392e-05],\n        [-2.4213e-04, -2.8322e-04, -1.4105e-04,  ..., -2.3694e-05,\n          3.2149e-05,  5.8067e-05],\n        ...,\n        [-4.7075e-04, -1.0080e-04, -2.9224e-04,  ...,  2.4024e-04,\n          1.9913e-04, -3.2333e-04],\n        [-1.6468e-04, -3.0763e-06,  1.5523e-04,  ...,  7.5331e-04,\n         -9.8892e-05, -6.8826e-04],\n        [-3.2019e-04, -6.2690e-04, -1.6927e-04,  ..., -1.3101e-03,\n          7.6799e-05,  2.6965e-04]]), 'transformer.encoder.layers.13.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 0.0054,  0.0015,  0.0002,  ..., -0.0043, -0.0049, -0.0017],\n        [ 0.0072, -0.0056,  0.0016,  ...,  0.0004, -0.0047, -0.0036],\n        [ 0.0080, -0.0079,  0.0064,  ...,  0.0011, -0.0019, -0.0008],\n        ...,\n        [-0.0050,  0.0063, -0.0031,  ...,  0.0051, -0.0048, -0.0041],\n        [-0.0073,  0.0032, -0.0066,  ...,  0.0059, -0.0080,  0.0088],\n        [-0.0080,  0.0086,  0.0004,  ..., -0.0058,  0.0035, -0.0015]]), 'transformer.encoder.layers.13.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-8.3805e-04, -6.9632e-04, -1.3282e-04,  ..., -8.1587e-06,\n          3.7996e-05,  3.7448e-04],\n        [-1.3931e-04,  1.6405e-04, -3.6481e-04,  ..., -5.3555e-04,\n          9.6881e-05,  4.1821e-04],\n        [-8.2041e-04,  2.0074e-04,  6.1100e-04,  ...,  1.4596e-04,\n          6.9947e-04,  4.9150e-04],\n        ...,\n        [ 8.0992e-04,  2.0901e-04, -2.1625e-04,  ..., -1.1266e-04,\n         -7.6813e-05, -1.2725e-03],\n        [-1.5762e-03,  2.3616e-05,  3.1819e-04,  ..., -8.1275e-04,\n          1.1228e-03,  6.8765e-04],\n        [-1.9616e-03,  6.0062e-04,  8.8563e-04,  ..., -4.4658e-05,\n          1.1646e-03,  1.3045e-03]]), 'transformer.encoder.layers.14.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0008,  0.0129, -0.0072,  ...,  0.0092, -0.0075, -0.0036],\n        [-0.0019, -0.0070,  0.0079,  ..., -0.0090,  0.0024, -0.0063],\n        [ 0.0061,  0.0144, -0.0030,  ..., -0.0157,  0.0142, -0.0011],\n        ...,\n        [-0.0070, -0.0028, -0.0056,  ...,  0.0154, -0.0112,  0.0137],\n        [ 0.0011, -0.0099,  0.0082,  ..., -0.0128, -0.0149, -0.0150],\n        [-0.0107, -0.0031,  0.0021,  ..., -0.0014, -0.0102, -0.0018]]), 'transformer.encoder.layers.14.self_attention.query_key_value.lora_B.default.weight': tensor([[ 1.8125e-04,  5.3087e-05,  7.5244e-05,  ..., -1.8669e-04,\n          2.2056e-04, -1.3803e-04],\n        [-9.6279e-05, -1.9539e-04, -6.0395e-05,  ...,  5.7935e-05,\n          1.7727e-04,  6.0118e-05],\n        [-2.0234e-04,  1.7012e-04, -3.4883e-04,  ...,  2.4673e-04,\n         -3.9576e-04,  4.9365e-04],\n        ...,\n        [ 1.8505e-05, -2.6453e-04, -3.1086e-04,  ...,  1.0067e-04,\n          2.3859e-04,  2.5845e-04],\n        [-3.0491e-04,  7.5653e-04, -3.4332e-04,  ...,  3.9646e-04,\n         -2.6484e-04, -9.3500e-05],\n        [ 1.0829e-03,  1.7614e-04,  2.4032e-03,  ..., -1.0829e-03,\n          1.1291e-03, -3.0203e-03]]), 'transformer.encoder.layers.14.self_attention.dense.lora_A.default.weight': tensor([[ 0.0043,  0.0020,  0.0035,  ...,  0.0064,  0.0020, -0.0122],\n        [ 0.0048, -0.0033,  0.0114,  ...,  0.0131, -0.0028,  0.0127],\n        [-0.0088, -0.0026, -0.0037,  ..., -0.0135, -0.0139,  0.0111],\n        ...,\n        [ 0.0032, -0.0147, -0.0027,  ...,  0.0002,  0.0134,  0.0118],\n        [ 0.0019, -0.0132,  0.0090,  ..., -0.0013, -0.0065, -0.0027],\n        [ 0.0125,  0.0067, -0.0073,  ..., -0.0012, -0.0099, -0.0015]]), 'transformer.encoder.layers.14.self_attention.dense.lora_B.default.weight': tensor([[-8.8037e-04, -7.9422e-04,  1.0056e-03,  ..., -2.3510e-04,\n         -1.4875e-03, -8.9987e-04],\n        [ 3.0244e-04,  3.8693e-04,  2.9719e-04,  ..., -1.3696e-04,\n         -6.2387e-04, -4.2795e-04],\n        [-7.4022e-04, -8.5571e-04,  9.8748e-04,  ...,  4.3851e-04,\n         -1.1016e-03, -1.0859e-03],\n        ...,\n        [ 9.3347e-04,  7.7861e-05, -1.1720e-03,  ...,  1.5845e-04,\n          8.0470e-04,  1.2068e-03],\n        [-9.7086e-04, -8.5828e-05,  1.0187e-03,  ..., -9.9792e-04,\n         -1.2141e-03, -1.7131e-03],\n        [-1.7478e-04, -4.7719e-04,  3.9943e-04,  ..., -4.2764e-05,\n         -1.1268e-03, -9.0943e-04]]), 'transformer.encoder.layers.14.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0050,  0.0143, -0.0014,  ...,  0.0089, -0.0084,  0.0154],\n        [ 0.0053,  0.0024, -0.0012,  ..., -0.0142,  0.0103, -0.0107],\n        [ 0.0079, -0.0105, -0.0145,  ..., -0.0095, -0.0123,  0.0035],\n        ...,\n        [ 0.0042,  0.0011,  0.0050,  ..., -0.0018,  0.0040, -0.0009],\n        [ 0.0079, -0.0018,  0.0024,  ..., -0.0038, -0.0001, -0.0068],\n        [ 0.0091, -0.0120,  0.0047,  ..., -0.0046, -0.0031, -0.0133]]), 'transformer.encoder.layers.14.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-4.0869e-04,  1.2682e-03, -5.6362e-04,  ...,  9.4863e-04,\n          6.9336e-04,  3.8678e-04],\n        [ 1.2768e-04, -2.5273e-04,  5.7676e-04,  ..., -4.9212e-04,\n         -2.3946e-04,  5.4046e-05],\n        [-5.6153e-04, -7.5596e-04,  2.9015e-04,  ..., -4.3431e-04,\n         -4.1562e-04, -2.3533e-04],\n        ...,\n        [-4.1062e-04, -1.5239e-03,  6.5178e-04,  ..., -1.3285e-03,\n         -1.1710e-03, -8.5083e-04],\n        [ 5.4355e-05,  2.4687e-04, -3.1510e-04,  ...,  2.2105e-04,\n         -2.2908e-04,  3.4498e-04],\n        [-6.0203e-04, -1.5414e-04, -2.0355e-04,  ...,  2.3316e-04,\n          1.9454e-05, -1.6742e-04]]), 'transformer.encoder.layers.14.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0046, -0.0047,  0.0061,  ...,  0.0037, -0.0069,  0.0047],\n        [ 0.0007,  0.0047,  0.0074,  ...,  0.0020,  0.0082, -0.0012],\n        [ 0.0021, -0.0023, -0.0011,  ...,  0.0016,  0.0021,  0.0022],\n        ...,\n        [ 0.0064,  0.0082, -0.0083,  ..., -0.0066,  0.0018, -0.0056],\n        [-0.0031,  0.0037,  0.0059,  ..., -0.0022,  0.0067, -0.0022],\n        [-0.0081, -0.0029,  0.0079,  ..., -0.0042, -0.0037,  0.0051]]), 'transformer.encoder.layers.14.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 3.3141e-04,  2.9802e-04, -6.9690e-04,  ..., -1.1555e-04,\n          5.4472e-04, -2.7626e-04],\n        [-3.1853e-04,  4.1703e-04,  1.9031e-04,  ...,  3.8083e-04,\n         -5.0445e-04, -1.3844e-04],\n        [ 1.0199e-03, -1.0751e-05, -1.0788e-03,  ..., -1.5948e-03,\n          1.5601e-03, -1.0563e-03],\n        ...,\n        [-1.9301e-04, -5.1746e-04,  3.7999e-04,  ...,  6.1888e-04,\n         -8.3858e-04, -1.7058e-05],\n        [-2.3496e-04,  6.4971e-04,  1.6095e-04,  ..., -1.9989e-03,\n          1.9223e-03, -1.1028e-03],\n        [ 2.7813e-04,  2.4763e-04, -2.1524e-04,  ..., -1.5689e-03,\n          1.3189e-03, -1.3071e-03]]), 'transformer.encoder.layers.15.self_attention.query_key_value.lora_A.default.weight': tensor([[ 1.0861e-02, -3.8414e-03,  4.1739e-05,  ..., -1.0445e-02,\n          2.7586e-03,  1.4249e-02],\n        [ 1.3201e-02, -1.5935e-03, -6.2083e-03,  ...,  1.1956e-03,\n         -6.8226e-03,  6.4703e-03],\n        [ 1.2630e-02, -9.1952e-05,  2.2986e-03,  ...,  1.3788e-02,\n         -3.5727e-03, -1.3945e-02],\n        ...,\n        [-3.9044e-03,  1.2422e-02, -1.2479e-02,  ..., -4.5678e-03,\n         -1.4844e-02,  7.5475e-03],\n        [ 7.7057e-03, -2.0382e-04,  1.3155e-02,  ..., -4.8490e-04,\n          4.6743e-03, -5.2782e-03],\n        [ 7.0024e-03, -4.2935e-03,  8.2443e-03,  ...,  9.7307e-03,\n         -1.3814e-02,  8.0695e-03]]), 'transformer.encoder.layers.15.self_attention.query_key_value.lora_B.default.weight': tensor([[ 1.3045e-05,  9.5139e-05,  1.0387e-04,  ...,  1.4369e-04,\n         -2.6891e-04,  1.3076e-05],\n        [ 3.1215e-05,  1.3433e-04,  2.8827e-04,  ...,  4.5982e-04,\n          1.1636e-04,  3.0680e-04],\n        [ 2.5880e-04, -1.7184e-04,  5.1995e-05,  ...,  4.3566e-05,\n          6.2420e-05,  4.0218e-05],\n        ...,\n        [ 9.0968e-04,  3.5050e-04, -5.7707e-04,  ..., -4.8715e-04,\n         -1.3037e-04,  6.5393e-04],\n        [-8.0880e-04, -8.3373e-04, -5.4567e-04,  ...,  1.0876e-03,\n          9.7188e-04,  6.3216e-04],\n        [-6.5644e-05, -8.4798e-04,  4.3825e-04,  ...,  5.0232e-04,\n          9.3637e-04,  1.2545e-03]]), 'transformer.encoder.layers.15.self_attention.dense.lora_A.default.weight': tensor([[ 0.0043, -0.0135, -0.0120,  ...,  0.0109,  0.0012, -0.0115],\n        [-0.0027,  0.0146,  0.0041,  ..., -0.0063,  0.0142,  0.0133],\n        [ 0.0026, -0.0013, -0.0070,  ...,  0.0136,  0.0119, -0.0035],\n        ...,\n        [ 0.0153, -0.0004, -0.0063,  ..., -0.0144, -0.0042, -0.0112],\n        [-0.0063, -0.0116,  0.0016,  ...,  0.0018,  0.0023,  0.0107],\n        [ 0.0050, -0.0038,  0.0086,  ...,  0.0061,  0.0148,  0.0135]]), 'transformer.encoder.layers.15.self_attention.dense.lora_B.default.weight': tensor([[-4.5400e-04, -6.5113e-04, -6.3984e-04,  ...,  5.8211e-04,\n          5.0807e-04,  7.2836e-04],\n        [ 7.7241e-04,  3.1742e-04,  8.7225e-04,  ..., -3.8018e-04,\n         -8.3924e-04, -3.9574e-04],\n        [ 2.7860e-04, -5.3940e-04, -9.3742e-04,  ...,  5.5527e-04,\n          7.0589e-04,  5.9615e-04],\n        ...,\n        [-4.9451e-05,  1.6681e-04,  5.5514e-04,  ...,  1.8318e-05,\n         -3.6974e-04, -1.3033e-04],\n        [ 1.1899e-03, -1.2572e-04, -4.5246e-04,  ...,  5.5359e-04,\n          2.4625e-04, -1.4421e-04],\n        [ 8.9455e-04,  2.9706e-04, -1.3899e-04,  ..., -8.8595e-05,\n         -3.0269e-04, -3.9423e-04]]), 'transformer.encoder.layers.15.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0058, -0.0060,  0.0046,  ...,  0.0075,  0.0158, -0.0011],\n        [ 0.0028, -0.0078, -0.0121,  ..., -0.0089,  0.0038,  0.0055],\n        [ 0.0101, -0.0038, -0.0062,  ..., -0.0132, -0.0116,  0.0094],\n        ...,\n        [ 0.0018,  0.0043,  0.0133,  ..., -0.0036, -0.0135,  0.0088],\n        [ 0.0012,  0.0007,  0.0077,  ...,  0.0151, -0.0121,  0.0006],\n        [-0.0141, -0.0134,  0.0137,  ...,  0.0040, -0.0039, -0.0054]]), 'transformer.encoder.layers.15.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-5.1040e-04, -2.4803e-04, -3.7858e-04,  ..., -2.3190e-04,\n          6.9739e-04, -8.2286e-04],\n        [-2.4974e-03,  9.5274e-06, -1.4681e-03,  ...,  7.1010e-04,\n          7.6595e-04, -6.2007e-04],\n        [-9.5570e-04, -1.0195e-04, -5.6199e-04,  ...,  4.1810e-04,\n          4.0359e-04, -3.1226e-04],\n        ...,\n        [ 4.4696e-05,  4.5028e-04,  1.3340e-04,  ...,  4.0933e-04,\n         -3.7662e-05,  7.3219e-04],\n        [-1.0409e-03,  1.1090e-05, -6.2006e-04,  ...,  6.1564e-04,\n          2.9828e-04,  3.0194e-04],\n        [-4.5557e-05, -2.8849e-04, -2.0907e-04,  ..., -4.2543e-04,\n          1.5926e-04,  2.2191e-04]]), 'transformer.encoder.layers.15.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 8.4576e-03,  5.1238e-03, -8.1360e-03,  ...,  2.2001e-03,\n         -3.8943e-03,  2.2179e-04],\n        [ 4.6608e-05,  3.3161e-03, -2.1354e-03,  ..., -2.1226e-03,\n         -4.1370e-03, -2.7710e-03],\n        [-2.4032e-03, -8.3577e-03,  2.0172e-03,  ..., -7.7269e-03,\n         -1.4999e-03,  1.9665e-03],\n        ...,\n        [ 8.2675e-03, -9.1231e-04,  4.7642e-03,  ..., -9.7609e-04,\n         -4.4655e-03, -1.2262e-03],\n        [-8.2113e-03,  1.8759e-03,  7.3398e-04,  ...,  7.9442e-03,\n          4.5019e-03, -5.2210e-03],\n        [-8.6304e-04,  5.2124e-03,  2.6622e-03,  ...,  4.4109e-03,\n         -5.8655e-03, -3.7641e-03]]), 'transformer.encoder.layers.15.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-1.3330e-03,  5.5479e-04, -7.8439e-04,  ...,  6.1433e-04,\n          1.0168e-03, -5.4933e-04],\n        [ 3.4607e-04, -9.9376e-05, -5.2542e-05,  ...,  3.1788e-05,\n         -4.7771e-05, -2.4397e-05],\n        [ 5.6808e-05,  7.8844e-04,  5.1267e-04,  ...,  5.3441e-04,\n          8.2304e-04, -9.8211e-04],\n        ...,\n        [ 7.8466e-04, -1.2300e-03, -1.2848e-04,  ..., -5.8122e-04,\n         -1.0640e-03,  8.5088e-04],\n        [ 1.0614e-04,  8.2441e-04,  3.8440e-04,  ..., -4.2748e-04,\n          1.0107e-04, -1.5459e-03],\n        [-2.6967e-04,  1.5756e-03, -3.3422e-04,  ...,  9.5960e-04,\n          1.3668e-03, -2.3843e-03]]), 'transformer.encoder.layers.16.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0014, -0.0132,  0.0119,  ..., -0.0112, -0.0090,  0.0123],\n        [-0.0001, -0.0149,  0.0117,  ..., -0.0084, -0.0148, -0.0052],\n        [-0.0091,  0.0061, -0.0141,  ...,  0.0122, -0.0013, -0.0109],\n        ...,\n        [ 0.0045,  0.0106, -0.0049,  ..., -0.0007, -0.0111, -0.0100],\n        [-0.0086, -0.0131, -0.0116,  ...,  0.0035, -0.0054,  0.0038],\n        [-0.0104,  0.0110,  0.0019,  ...,  0.0110,  0.0094,  0.0040]]), 'transformer.encoder.layers.16.self_attention.query_key_value.lora_B.default.weight': tensor([[ 2.5918e-04,  7.4498e-05, -1.9251e-04,  ..., -2.6075e-06,\n         -4.8598e-05, -7.5103e-06],\n        [ 1.1291e-04,  3.7955e-04, -2.8413e-04,  ..., -1.8256e-04,\n          3.3218e-04, -1.5253e-04],\n        [ 1.6030e-04,  2.9333e-04, -2.6651e-04,  ..., -2.2556e-04,\n          2.3338e-04, -2.3639e-04],\n        ...,\n        [ 1.4093e-03,  3.6970e-03, -4.3111e-03,  ..., -2.3112e-03,\n          2.5305e-03, -2.3818e-03],\n        [ 8.8547e-04,  1.6694e-03, -1.4002e-03,  ..., -8.6242e-05,\n          9.0770e-04, -1.2530e-03],\n        [ 1.2885e-03,  2.1860e-03, -2.9697e-03,  ..., -1.0353e-03,\n          1.8233e-03, -9.1245e-04]]), 'transformer.encoder.layers.16.self_attention.dense.lora_A.default.weight': tensor([[ 0.0053,  0.0104, -0.0105,  ...,  0.0107,  0.0102, -0.0106],\n        [-0.0014, -0.0149,  0.0014,  ..., -0.0119,  0.0062,  0.0014],\n        [-0.0081, -0.0051,  0.0126,  ...,  0.0094,  0.0061, -0.0073],\n        ...,\n        [ 0.0114,  0.0021,  0.0145,  ..., -0.0117,  0.0091, -0.0007],\n        [-0.0024,  0.0094,  0.0003,  ..., -0.0133,  0.0060,  0.0160],\n        [-0.0083, -0.0057, -0.0140,  ..., -0.0100, -0.0035, -0.0091]]), 'transformer.encoder.layers.16.self_attention.dense.lora_B.default.weight': tensor([[-1.0122e-03, -2.4985e-04,  9.6365e-04,  ..., -3.8635e-04,\n         -3.1764e-04, -1.1690e-03],\n        [ 9.3789e-04,  6.3615e-04, -3.5571e-04,  ..., -2.2845e-05,\n          7.9170e-04,  5.6075e-04],\n        [-8.3229e-04, -4.1235e-05,  6.2617e-04,  ..., -1.4096e-03,\n         -3.2962e-04, -8.4072e-04],\n        ...,\n        [ 8.5454e-05,  5.4960e-04, -2.7080e-04,  ...,  1.7054e-04,\n          4.5346e-05,  4.2918e-04],\n        [-1.3066e-04, -1.6973e-04, -3.1631e-05,  ..., -7.9304e-04,\n          7.6917e-05,  3.2972e-04],\n        [ 7.9479e-04,  1.1543e-04, -2.0146e-04,  ..., -7.4148e-04,\n          7.4729e-04,  5.9137e-04]]), 'transformer.encoder.layers.16.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0062, -0.0061, -0.0085,  ...,  0.0051, -0.0013, -0.0160],\n        [ 0.0051,  0.0152, -0.0108,  ..., -0.0013,  0.0024,  0.0111],\n        [ 0.0113, -0.0117,  0.0064,  ..., -0.0015, -0.0032,  0.0008],\n        ...,\n        [ 0.0114,  0.0066,  0.0066,  ..., -0.0110, -0.0020,  0.0014],\n        [-0.0025,  0.0100,  0.0025,  ..., -0.0040, -0.0111, -0.0006],\n        [ 0.0057, -0.0057,  0.0045,  ..., -0.0107,  0.0134,  0.0097]]), 'transformer.encoder.layers.16.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 1.9603e-05,  7.4594e-04, -3.5713e-04,  ...,  2.4643e-04,\n         -1.1158e-04,  2.9052e-04],\n        [ 6.5580e-07, -3.8752e-04, -1.3895e-04,  ...,  7.3352e-05,\n         -3.4637e-04,  1.0444e-05],\n        [ 1.1365e-04,  2.7316e-04, -5.7457e-05,  ...,  1.3068e-04,\n          5.7457e-06,  4.1834e-05],\n        ...,\n        [ 3.1983e-04, -1.0371e-06,  5.6812e-05,  ...,  2.8734e-04,\n          2.0160e-04,  2.8305e-04],\n        [ 3.8060e-04, -2.2376e-04,  1.0545e-04,  ...,  5.1651e-04,\n         -2.6785e-04, -4.9457e-04],\n        [ 7.3788e-04,  9.8060e-05,  7.2359e-04,  ...,  9.6660e-04,\n         -2.0728e-04, -2.7771e-04]]), 'transformer.encoder.layers.16.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-3.1120e-03,  3.1109e-03,  1.8735e-03,  ...,  1.7669e-03,\n         -3.8848e-03,  7.2550e-03],\n        [-1.3033e-03,  8.2686e-03,  4.7694e-03,  ...,  5.7733e-03,\n          3.5700e-03, -4.7395e-03],\n        [-5.5215e-03,  3.0996e-03,  7.6274e-03,  ...,  5.2444e-03,\n          7.4581e-03,  5.9598e-03],\n        ...,\n        [-6.0285e-03, -5.1201e-03,  3.8696e-03,  ..., -5.4784e-03,\n         -5.0521e-03,  6.4217e-03],\n        [ 1.2812e-03,  2.5515e-05,  5.0726e-03,  ..., -6.2907e-03,\n          3.1865e-03,  1.1215e-03],\n        [ 8.0663e-03, -4.3213e-03,  4.5397e-04,  ..., -6.9287e-03,\n         -4.8455e-03, -5.4257e-03]]), 'transformer.encoder.layers.16.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-4.6627e-04, -3.6109e-04, -2.2815e-04,  ..., -3.6442e-04,\n         -3.7990e-04, -5.3401e-04],\n        [-3.6297e-04, -1.9298e-04, -1.4621e-04,  ...,  1.8945e-05,\n          1.2970e-03, -5.6730e-04],\n        [ 3.3654e-04,  6.9176e-04, -1.0134e-03,  ...,  1.7848e-04,\n         -5.4204e-04, -6.2567e-04],\n        ...,\n        [-5.4762e-04, -1.6654e-04,  2.7231e-04,  ..., -8.0415e-04,\n          1.5698e-05,  6.2062e-04],\n        [ 5.4518e-04,  7.2712e-04, -5.1428e-04,  ..., -1.7611e-05,\n         -3.1313e-04, -1.6781e-04],\n        [ 9.6444e-05,  3.3716e-04, -7.1769e-05,  ...,  3.8551e-04,\n          6.3531e-04,  9.7017e-05]]), 'transformer.encoder.layers.17.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0016, -0.0090, -0.0080,  ..., -0.0146,  0.0130, -0.0124],\n        [ 0.0041,  0.0062, -0.0121,  ..., -0.0107,  0.0151, -0.0051],\n        [ 0.0149,  0.0142, -0.0110,  ...,  0.0022,  0.0098, -0.0053],\n        ...,\n        [ 0.0094,  0.0133,  0.0015,  ...,  0.0096,  0.0023, -0.0064],\n        [ 0.0111,  0.0021, -0.0064,  ...,  0.0075, -0.0118, -0.0012],\n        [-0.0136,  0.0091, -0.0010,  ...,  0.0019, -0.0080,  0.0007]]), 'transformer.encoder.layers.17.self_attention.query_key_value.lora_B.default.weight': tensor([[-6.2282e-04, -4.1987e-04,  4.5515e-04,  ..., -6.7975e-04,\n         -2.4844e-04,  3.8078e-04],\n        [ 9.6206e-05,  2.6893e-05, -4.1635e-04,  ...,  4.6244e-04,\n          5.1032e-06, -1.2591e-04],\n        [ 5.9293e-04,  1.0589e-04, -2.2097e-04,  ...,  2.2430e-04,\n          2.6841e-04, -5.9013e-04],\n        ...,\n        [ 5.4245e-04,  7.8409e-04, -4.1879e-04,  ...,  4.6850e-04,\n          1.1504e-03, -3.2397e-05],\n        [-1.1932e-03, -6.0026e-04, -1.2839e-03,  ..., -7.4927e-04,\n          2.9770e-04,  1.3436e-04],\n        [ 3.6907e-04, -1.8954e-04, -9.8352e-04,  ...,  5.5301e-04,\n          3.4896e-04, -2.9369e-05]]), 'transformer.encoder.layers.17.self_attention.dense.lora_A.default.weight': tensor([[-0.0041,  0.0064,  0.0072,  ...,  0.0075,  0.0041,  0.0113],\n        [ 0.0109,  0.0005, -0.0094,  ..., -0.0023,  0.0140,  0.0112],\n        [ 0.0026,  0.0084,  0.0011,  ...,  0.0008,  0.0014, -0.0109],\n        ...,\n        [-0.0116,  0.0058, -0.0120,  ...,  0.0119, -0.0057, -0.0102],\n        [-0.0109,  0.0154, -0.0086,  ...,  0.0122, -0.0085, -0.0081],\n        [ 0.0078,  0.0119, -0.0105,  ...,  0.0034,  0.0008, -0.0046]]), 'transformer.encoder.layers.17.self_attention.dense.lora_B.default.weight': tensor([[ 2.8566e-05,  3.1594e-04, -3.1093e-04,  ...,  6.1467e-04,\n          5.4561e-04, -3.6129e-04],\n        [-1.0749e-03, -9.1801e-04, -9.4529e-04,  ..., -8.5971e-04,\n         -1.5546e-04, -1.2241e-04],\n        [ 9.0407e-04,  1.1219e-03,  6.1740e-04,  ...,  1.2116e-03,\n         -1.2622e-03,  4.0271e-04],\n        ...,\n        [-7.3146e-04, -1.3681e-03, -6.8489e-04,  ..., -1.0357e-03,\n          1.0547e-03,  5.4013e-04],\n        [ 2.8019e-04,  3.7307e-04,  1.7772e-04,  ...,  2.0836e-05,\n         -4.6683e-04,  2.3372e-04],\n        [-6.5271e-05, -1.1616e-03, -8.8101e-04,  ..., -1.7006e-03,\n         -5.0049e-04, -8.5785e-05]]), 'transformer.encoder.layers.17.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0087, -0.0036, -0.0149,  ...,  0.0039, -0.0111, -0.0042],\n        [ 0.0126, -0.0148,  0.0095,  ..., -0.0045, -0.0153, -0.0049],\n        [ 0.0143, -0.0138,  0.0091,  ...,  0.0057, -0.0098,  0.0091],\n        ...,\n        [-0.0002,  0.0021,  0.0017,  ..., -0.0118,  0.0097, -0.0041],\n        [-0.0001,  0.0138, -0.0097,  ...,  0.0073, -0.0135, -0.0115],\n        [-0.0106, -0.0040, -0.0101,  ..., -0.0077,  0.0090,  0.0079]]), 'transformer.encoder.layers.17.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 7.9063e-05, -3.8642e-05,  2.9159e-04,  ..., -4.3502e-05,\n         -4.4121e-04, -1.5003e-04],\n        [-8.1002e-04,  1.9564e-04, -8.3484e-04,  ...,  3.8922e-04,\n          6.7558e-04,  2.5422e-04],\n        [ 3.2821e-04, -1.5262e-04, -5.9634e-05,  ...,  1.7440e-04,\n          4.5530e-04, -7.9814e-05],\n        ...,\n        [ 1.5744e-04,  8.1907e-04,  1.4701e-03,  ..., -9.1178e-04,\n         -6.0688e-04, -1.4027e-04],\n        [-5.3278e-05, -1.9526e-04,  2.4739e-04,  ...,  1.7083e-04,\n         -2.7500e-04,  9.2138e-05],\n        [-9.2686e-04, -1.0099e-03, -2.6352e-04,  ...,  2.6572e-04,\n          5.7458e-04,  6.8898e-04]]), 'transformer.encoder.layers.17.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 5.2980e-03, -7.2462e-03,  6.5741e-03,  ..., -1.0655e-03,\n          6.2529e-04,  4.6722e-03],\n        [-4.4602e-03,  4.6123e-03, -6.4101e-03,  ...,  7.8775e-03,\n         -6.5187e-03,  8.0162e-03],\n        [ 8.0320e-03, -2.2740e-03, -1.4145e-03,  ..., -3.4382e-03,\n         -6.6252e-03, -4.1997e-03],\n        ...,\n        [-3.8655e-03,  3.9737e-03,  7.7241e-03,  ...,  3.5457e-03,\n         -1.2845e-03, -9.2787e-05],\n        [-8.0487e-03,  6.6471e-03, -1.2536e-03,  ...,  2.6905e-03,\n          4.7536e-03, -7.8613e-03],\n        [-3.4491e-03, -5.1530e-04, -8.5859e-03,  ...,  6.9439e-03,\n          5.1908e-03, -1.4742e-03]]), 'transformer.encoder.layers.17.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-2.3051e-04,  7.9499e-05, -3.9989e-04,  ...,  8.7549e-04,\n         -2.0273e-04,  4.5430e-04],\n        [ 4.0215e-04,  2.3697e-05, -3.5718e-04,  ...,  1.9550e-04,\n         -7.1070e-05, -1.6608e-04],\n        [ 1.8109e-03,  8.3014e-04,  1.4954e-03,  ..., -3.6972e-05,\n          1.6477e-05, -7.0918e-04],\n        ...,\n        [-2.2272e-03, -1.5658e-03, -1.4418e-03,  ...,  7.3411e-04,\n          1.1951e-04,  1.1091e-03],\n        [ 1.2612e-03,  2.5884e-04,  1.2112e-03,  ...,  7.7357e-04,\n         -9.5321e-04, -5.7625e-04],\n        [ 8.9762e-04, -3.4365e-04, -6.2814e-04,  ...,  6.7574e-04,\n         -1.0650e-03, -1.4209e-04]]), 'transformer.encoder.layers.18.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0068, -0.0093,  0.0097,  ..., -0.0007,  0.0095, -0.0145],\n        [ 0.0031,  0.0152, -0.0097,  ...,  0.0098, -0.0105, -0.0076],\n        [ 0.0081,  0.0036,  0.0153,  ..., -0.0063, -0.0054,  0.0141],\n        ...,\n        [-0.0088, -0.0029,  0.0060,  ...,  0.0022,  0.0052,  0.0020],\n        [-0.0055, -0.0005, -0.0037,  ..., -0.0071, -0.0130,  0.0131],\n        [ 0.0096,  0.0058, -0.0105,  ..., -0.0130,  0.0155, -0.0097]]), 'transformer.encoder.layers.18.self_attention.query_key_value.lora_B.default.weight': tensor([[-2.9217e-04, -2.1535e-04, -3.8102e-05,  ...,  1.9308e-04,\n         -1.0366e-04, -9.2855e-05],\n        [ 5.1288e-05, -9.1696e-05, -3.3840e-05,  ..., -5.1330e-05,\n          1.6675e-05, -2.6816e-04],\n        [ 2.1419e-04,  7.8519e-05, -1.4131e-05,  ..., -7.1923e-05,\n          3.6174e-05,  3.4565e-05],\n        ...,\n        [ 2.2839e-04, -1.0051e-04,  3.1199e-04,  ...,  2.4018e-04,\n         -4.9485e-04,  7.9171e-05],\n        [-1.6809e-03, -1.6615e-03,  5.7654e-04,  ...,  7.1525e-04,\n          1.1050e-03, -3.9639e-05],\n        [ 1.0538e-03,  1.2461e-03,  6.2761e-04,  ..., -1.4040e-03,\n         -1.7147e-03,  4.1587e-06]]), 'transformer.encoder.layers.18.self_attention.dense.lora_A.default.weight': tensor([[ 0.0091, -0.0150, -0.0119,  ...,  0.0107,  0.0140, -0.0158],\n        [ 0.0112,  0.0022,  0.0153,  ..., -0.0030,  0.0062,  0.0002],\n        [ 0.0144, -0.0146, -0.0074,  ...,  0.0096,  0.0084, -0.0154],\n        ...,\n        [ 0.0075,  0.0121,  0.0072,  ...,  0.0153, -0.0130,  0.0049],\n        [-0.0016, -0.0099,  0.0031,  ..., -0.0113, -0.0122,  0.0075],\n        [-0.0026, -0.0133, -0.0096,  ...,  0.0095, -0.0125,  0.0145]]), 'transformer.encoder.layers.18.self_attention.dense.lora_B.default.weight': tensor([[-6.5150e-04,  3.7132e-04, -1.2638e-04,  ...,  3.0241e-04,\n          3.0214e-04, -1.9846e-04],\n        [-1.0313e-03,  6.6650e-04, -3.4357e-04,  ..., -4.4766e-05,\n          4.5723e-04, -3.4206e-04],\n        [ 1.9114e-04,  2.1685e-03, -1.0281e-03,  ..., -7.8783e-04,\n          1.5000e-03, -2.7214e-04],\n        ...,\n        [-1.7981e-04, -1.9042e-03,  9.0142e-04,  ...,  1.1033e-03,\n         -1.5623e-03, -1.6276e-04],\n        [ 2.7305e-04,  1.5618e-03, -6.2924e-04,  ..., -1.4703e-03,\n          9.0787e-04,  1.7059e-04],\n        [-7.1795e-04,  8.1724e-04, -5.7769e-04,  ...,  1.7725e-04,\n          3.0028e-04, -5.8863e-04]]), 'transformer.encoder.layers.18.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0163,  0.0097, -0.0006,  ...,  0.0131, -0.0080,  0.0003],\n        [ 0.0122, -0.0151, -0.0117,  ..., -0.0064, -0.0052, -0.0042],\n        [ 0.0133,  0.0114,  0.0101,  ...,  0.0023, -0.0113,  0.0040],\n        ...,\n        [-0.0127, -0.0002,  0.0011,  ..., -0.0010,  0.0114,  0.0064],\n        [ 0.0011,  0.0034, -0.0116,  ..., -0.0103, -0.0051, -0.0076],\n        [-0.0042, -0.0041,  0.0103,  ...,  0.0125,  0.0052,  0.0053]]), 'transformer.encoder.layers.18.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-2.5757e-05,  6.0640e-04,  2.0311e-04,  ...,  1.9811e-04,\n          4.4109e-04, -2.7798e-04],\n        [-1.4740e-04, -1.1080e-04, -2.9069e-04,  ...,  6.0782e-05,\n          3.0354e-04,  2.1484e-04],\n        [ 2.7468e-04, -4.7168e-04, -5.9162e-04,  ...,  4.7939e-04,\n          5.3200e-04, -1.4414e-04],\n        ...,\n        [-5.2452e-04,  1.6644e-03,  6.2266e-04,  ..., -1.4678e-03,\n         -1.3096e-03,  1.5319e-03],\n        [ 6.9643e-05,  1.3423e-04,  3.1834e-04,  ..., -1.8438e-04,\n         -5.0592e-04,  2.1503e-04],\n        [ 2.2760e-05,  3.0765e-04,  3.2949e-04,  ..., -1.5414e-04,\n         -4.9352e-04,  7.1681e-04]]), 'transformer.encoder.layers.18.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 4.9128e-04,  2.3900e-03, -1.3932e-03,  ...,  3.4318e-03,\n          6.0233e-03,  2.4322e-03],\n        [-5.8792e-03, -8.3126e-03, -5.9525e-03,  ..., -2.0436e-03,\n         -2.2721e-03, -6.8457e-03],\n        [-7.5811e-03,  7.3602e-03,  2.2649e-03,  ...,  3.6337e-03,\n          7.2726e-03, -8.1677e-03],\n        ...,\n        [ 2.2884e-04,  8.4863e-03, -2.4564e-03,  ...,  5.2845e-03,\n          5.0283e-03,  4.0557e-03],\n        [-8.5366e-03, -2.8004e-03,  3.9034e-03,  ...,  3.7556e-04,\n         -4.7083e-03,  1.9983e-03],\n        [ 3.3671e-05, -2.4665e-03, -6.5282e-03,  ..., -4.8930e-03,\n         -2.7279e-04,  1.2501e-03]]), 'transformer.encoder.layers.18.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 1.0117e-03,  2.8721e-04, -7.8639e-04,  ...,  9.8078e-04,\n          4.9954e-04, -2.6682e-04],\n        [-1.1915e-03, -7.3399e-04,  9.6643e-04,  ..., -1.4294e-03,\n         -7.6470e-04,  4.8185e-04],\n        [-7.4401e-04, -1.5165e-03,  1.7194e-03,  ..., -4.5319e-04,\n          8.7511e-04, -1.7728e-03],\n        ...,\n        [ 3.5819e-04,  1.3271e-03, -7.9353e-04,  ..., -5.8815e-04,\n         -7.1408e-04,  1.2071e-03],\n        [-7.8204e-04, -1.1083e-03,  1.2883e-03,  ..., -8.3650e-04,\n         -6.1431e-05, -9.0790e-04],\n        [-3.1246e-04,  1.0054e-03,  6.9168e-04,  ..., -1.2464e-03,\n         -4.6500e-04, -1.9877e-04]]), 'transformer.encoder.layers.19.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0057, -0.0149, -0.0010,  ...,  0.0024, -0.0004,  0.0089],\n        [ 0.0050,  0.0046, -0.0101,  ..., -0.0149, -0.0053, -0.0066],\n        [ 0.0061,  0.0019,  0.0018,  ..., -0.0026, -0.0074, -0.0146],\n        ...,\n        [ 0.0004,  0.0085,  0.0093,  ...,  0.0010, -0.0153,  0.0136],\n        [ 0.0104,  0.0099, -0.0052,  ..., -0.0097, -0.0133, -0.0024],\n        [ 0.0041, -0.0062, -0.0097,  ..., -0.0138,  0.0100,  0.0049]]), 'transformer.encoder.layers.19.self_attention.query_key_value.lora_B.default.weight': tensor([[-3.2843e-05,  1.9833e-05, -1.1825e-04,  ...,  8.2482e-05,\n          2.0090e-04,  3.3955e-05],\n        [-2.2094e-04,  3.2055e-05, -6.3699e-05,  ..., -8.3169e-05,\n         -1.9594e-04, -5.0996e-07],\n        [-2.4460e-04, -1.0287e-05, -1.1761e-04,  ..., -1.1386e-04,\n         -2.0679e-04,  1.7122e-04],\n        ...,\n        [-8.7166e-04, -5.5179e-04,  6.8716e-04,  ..., -3.9034e-04,\n         -1.7182e-03, -1.3553e-03],\n        [-7.7066e-05,  5.7950e-04, -1.2412e-04,  ...,  2.5256e-04,\n          9.6815e-04,  8.7418e-04],\n        [-4.0198e-04,  1.1632e-03, -1.1500e-03,  ...,  1.1294e-03,\n          1.8405e-03,  3.2535e-04]]), 'transformer.encoder.layers.19.self_attention.dense.lora_A.default.weight': tensor([[-0.0024,  0.0069, -0.0028,  ...,  0.0146, -0.0007,  0.0094],\n        [-0.0020,  0.0033, -0.0029,  ...,  0.0127, -0.0113,  0.0041],\n        [ 0.0143,  0.0144, -0.0014,  ...,  0.0078, -0.0007, -0.0031],\n        ...,\n        [ 0.0152, -0.0109,  0.0083,  ...,  0.0046, -0.0086, -0.0037],\n        [ 0.0013,  0.0100, -0.0113,  ...,  0.0098, -0.0008, -0.0119],\n        [ 0.0117, -0.0107,  0.0131,  ..., -0.0156, -0.0035, -0.0105]]), 'transformer.encoder.layers.19.self_attention.dense.lora_B.default.weight': tensor([[ 2.1647e-04, -8.4577e-04, -7.6620e-04,  ..., -1.6292e-04,\n         -2.8370e-04, -1.7130e-04],\n        [ 4.2803e-04,  1.2923e-03,  8.0248e-04,  ...,  8.7645e-05,\n          9.2071e-04, -8.3653e-05],\n        [ 7.9164e-04,  8.9611e-04,  1.1505e-03,  ...,  2.3528e-03,\n         -1.3856e-03, -1.1903e-03],\n        ...,\n        [-8.1564e-04, -1.0072e-03, -8.0740e-04,  ..., -1.9923e-03,\n          5.2955e-04,  9.0802e-04],\n        [ 4.7747e-04,  1.3814e-03, -1.4720e-06,  ...,  1.2648e-03,\n          4.8545e-04, -5.9169e-04],\n        [ 5.4062e-05,  5.3262e-04,  4.4075e-04,  ..., -4.0825e-04,\n          6.6577e-04,  3.0942e-04]]), 'transformer.encoder.layers.19.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0154,  0.0058,  0.0105,  ..., -0.0116, -0.0145, -0.0106],\n        [-0.0072,  0.0065, -0.0045,  ...,  0.0134,  0.0008,  0.0136],\n        [-0.0013,  0.0107,  0.0021,  ..., -0.0020,  0.0126, -0.0144],\n        ...,\n        [-0.0047, -0.0122, -0.0002,  ..., -0.0106, -0.0145,  0.0143],\n        [ 0.0135, -0.0098, -0.0131,  ...,  0.0144, -0.0073,  0.0074],\n        [ 0.0107,  0.0110,  0.0112,  ..., -0.0130, -0.0140,  0.0134]]), 'transformer.encoder.layers.19.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-1.1085e-03,  1.0820e-03, -3.8915e-04,  ...,  1.1544e-03,\n          1.1737e-03, -1.2557e-03],\n        [-2.3852e-05, -2.7092e-04,  7.4478e-05,  ..., -3.3833e-04,\n         -3.1971e-04, -2.8265e-04],\n        [ 5.1338e-04, -3.2222e-04,  2.5113e-04,  ..., -4.7351e-04,\n         -3.9278e-04, -1.7480e-04],\n        ...,\n        [-1.5594e-04,  1.5294e-04,  7.3754e-06,  ...,  1.8262e-05,\n         -8.9993e-06,  5.8172e-05],\n        [-5.9879e-06,  1.0098e-04,  8.2511e-05,  ...,  1.2409e-04,\n          1.0756e-05, -2.2160e-04],\n        [ 2.9660e-04, -1.2594e-04,  9.0536e-05,  ..., -1.8832e-04,\n         -9.3586e-05,  4.7693e-05]]), 'transformer.encoder.layers.19.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-5.4117e-03,  7.9220e-03,  1.5933e-05,  ...,  1.5838e-03,\n          6.6616e-03,  2.7299e-04],\n        [ 4.7969e-04,  5.1744e-03, -4.9950e-03,  ...,  4.0641e-03,\n          3.7230e-03, -5.2488e-03],\n        [ 6.1544e-03,  8.8356e-04, -4.4855e-03,  ..., -3.0033e-03,\n          4.8049e-04, -3.1042e-04],\n        ...,\n        [ 5.6400e-03,  8.7196e-04,  4.1919e-04,  ..., -1.5403e-03,\n         -2.6095e-03,  6.7340e-03],\n        [ 4.2044e-03,  6.6585e-03,  3.5444e-03,  ..., -2.8351e-03,\n          7.7022e-03,  2.2753e-03],\n        [ 5.1343e-03,  5.9398e-04, -3.8896e-03,  ..., -4.3190e-03,\n          8.2235e-03, -4.3230e-03]]), 'transformer.encoder.layers.19.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-1.0185e-03,  1.4573e-04,  8.1052e-04,  ..., -2.3700e-05,\n          4.2471e-04, -6.1096e-04],\n        [ 1.7467e-04, -9.6890e-04, -6.1367e-04,  ..., -1.0057e-03,\n          1.1073e-03,  3.1953e-04],\n        [-1.6151e-03, -5.9456e-04,  8.2880e-04,  ..., -1.4380e-03,\n          1.7352e-03, -4.5224e-04],\n        ...,\n        [ 8.5824e-04, -1.8240e-04, -9.8597e-04,  ...,  1.0727e-03,\n         -1.7029e-03,  6.7556e-04],\n        [-5.9283e-04, -4.4284e-04, -9.0910e-04,  ..., -7.9066e-04,\n          1.1169e-03, -4.5714e-04],\n        [ 3.1939e-04, -1.1218e-03, -8.4790e-04,  ..., -1.8158e-03,\n          1.3680e-03, -5.0772e-05]]), 'transformer.encoder.layers.20.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0039,  0.0009,  0.0103,  ..., -0.0145,  0.0022,  0.0095],\n        [ 0.0133,  0.0082, -0.0111,  ..., -0.0093, -0.0106, -0.0090],\n        [-0.0121,  0.0078, -0.0121,  ...,  0.0086, -0.0002, -0.0041],\n        ...,\n        [-0.0072, -0.0002,  0.0083,  ..., -0.0105, -0.0042, -0.0110],\n        [ 0.0019, -0.0033,  0.0069,  ...,  0.0140,  0.0102, -0.0074],\n        [ 0.0002,  0.0028, -0.0124,  ..., -0.0122,  0.0114, -0.0080]]), 'transformer.encoder.layers.20.self_attention.query_key_value.lora_B.default.weight': tensor([[-3.0266e-04,  7.6026e-06,  5.0524e-04,  ...,  1.5626e-04,\n          1.5958e-04, -2.3933e-04],\n        [-1.7470e-04, -4.6555e-05,  3.2142e-04,  ...,  4.8017e-04,\n          1.1095e-04, -2.6994e-04],\n        [-2.0530e-04, -4.5677e-05, -5.4982e-04,  ..., -4.7701e-04,\n          3.0455e-04,  1.5208e-04],\n        ...,\n        [ 5.8066e-04,  3.0264e-04, -5.0878e-04,  ...,  5.4825e-04,\n          3.0187e-04,  9.4579e-04],\n        [-1.0072e-03,  1.6854e-03,  1.0240e-03,  ..., -1.4124e-03,\n         -1.4206e-03,  6.7025e-04],\n        [ 1.5572e-03,  4.4485e-04, -6.4044e-04,  ..., -6.0543e-04,\n         -1.3097e-03, -2.3622e-04]]), 'transformer.encoder.layers.20.self_attention.dense.lora_A.default.weight': tensor([[ 3.3124e-03, -8.9069e-03,  9.6336e-03,  ...,  1.4172e-02,\n         -9.8840e-03, -8.4261e-03],\n        [-1.3516e-02, -1.2505e-02,  1.0533e-02,  ..., -6.9571e-03,\n          6.4218e-03,  1.5381e-02],\n        [-1.1074e-02, -3.9039e-04,  6.9830e-03,  ..., -2.3208e-05,\n         -1.0862e-02, -1.5406e-02],\n        ...,\n        [ 4.1384e-03, -3.2311e-03, -1.3691e-02,  ...,  5.2697e-03,\n          1.2635e-02,  5.9247e-03],\n        [-2.5344e-03,  8.5431e-03,  4.8564e-03,  ..., -1.8488e-03,\n         -1.3355e-02,  1.4058e-02],\n        [ 6.3440e-03, -1.5081e-02, -1.1012e-02,  ...,  4.9681e-03,\n         -8.8998e-03,  1.3949e-02]]), 'transformer.encoder.layers.20.self_attention.dense.lora_B.default.weight': tensor([[-4.4114e-04,  2.7694e-04, -3.5558e-04,  ...,  9.0929e-04,\n         -1.2988e-03,  2.3127e-04],\n        [ 4.9375e-04,  2.9892e-04,  2.2736e-04,  ..., -7.1557e-04,\n         -6.7468e-04, -2.4149e-04],\n        [-3.2765e-04, -4.5116e-04, -1.0325e-03,  ...,  1.9472e-03,\n         -2.8050e-03, -2.0237e-05],\n        ...,\n        [ 2.4028e-04,  1.3044e-03,  1.0554e-03,  ..., -2.0578e-03,\n          1.9944e-03, -1.7487e-04],\n        [ 7.7836e-04, -7.4931e-04, -3.3178e-04,  ...,  1.7077e-04,\n         -9.2141e-04, -1.1655e-04],\n        [-4.9106e-04, -3.5960e-04, -5.4560e-05,  ..., -2.2813e-04,\n         -4.8330e-04, -1.7757e-04]]), 'transformer.encoder.layers.20.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0104,  0.0013,  0.0002,  ..., -0.0028,  0.0143,  0.0066],\n        [ 0.0082, -0.0013,  0.0052,  ...,  0.0060,  0.0034, -0.0074],\n        [ 0.0037, -0.0085,  0.0028,  ..., -0.0103,  0.0089, -0.0148],\n        ...,\n        [-0.0094, -0.0076, -0.0096,  ...,  0.0027, -0.0115,  0.0005],\n        [ 0.0114, -0.0063,  0.0092,  ..., -0.0073,  0.0003, -0.0104],\n        [-0.0039,  0.0003, -0.0159,  ...,  0.0011,  0.0075,  0.0066]]), 'transformer.encoder.layers.20.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-4.7988e-05,  2.0245e-04,  2.1456e-04,  ...,  1.4660e-04,\n         -3.8255e-06, -3.9609e-04],\n        [-1.4657e-03,  2.0120e-03,  1.3196e-03,  ...,  3.9818e-03,\n         -1.9111e-03, -1.8526e-03],\n        [-4.7395e-04,  2.4128e-05, -7.6477e-04,  ..., -9.2483e-04,\n          3.8911e-04, -2.8376e-04],\n        ...,\n        [-3.3301e-04,  6.8616e-04,  4.0877e-04,  ...,  3.8037e-04,\n         -7.5187e-04, -2.9701e-04],\n        [ 2.4702e-04,  4.1479e-04, -3.6511e-05,  ...,  2.2542e-03,\n         -1.2426e-03, -2.7279e-04],\n        [ 3.2192e-04, -3.1275e-04, -4.2077e-05,  ..., -3.6080e-04,\n          3.4952e-04, -4.9261e-04]]), 'transformer.encoder.layers.20.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-7.7452e-03, -6.9239e-03, -1.2660e-03,  ...,  8.3618e-03,\n         -6.7358e-03,  5.2903e-03],\n        [ 6.7490e-03, -7.1386e-03,  5.1098e-03,  ..., -4.0980e-03,\n         -1.8720e-03, -8.3574e-03],\n        [-2.5018e-03,  4.9054e-03,  3.6359e-03,  ..., -3.7874e-03,\n         -2.7785e-03,  2.9212e-03],\n        ...,\n        [ 3.8905e-03,  6.7747e-03, -5.0861e-03,  ..., -5.3490e-03,\n         -2.8270e-03,  5.9177e-03],\n        [ 9.1057e-05,  8.6039e-04, -6.8791e-03,  ...,  1.4122e-03,\n         -7.0467e-03, -6.6334e-03],\n        [-6.5721e-03, -6.9322e-03,  1.8273e-03,  ...,  7.0916e-03,\n          7.2205e-03,  7.6552e-03]]), 'transformer.encoder.layers.20.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-1.3048e-03, -8.2307e-04,  6.4686e-04,  ...,  1.1121e-04,\n          4.1970e-04,  8.7736e-04],\n        [ 2.5709e-04,  2.5836e-05,  1.3290e-03,  ...,  6.5573e-04,\n          3.0707e-04,  1.8572e-04],\n        [-1.9886e-03, -6.5769e-04,  1.6125e-03,  ...,  7.0990e-04,\n          1.0031e-03,  8.3896e-04],\n        ...,\n        [ 1.5488e-03,  9.3162e-04, -1.3318e-03,  ..., -3.7868e-04,\n         -8.5973e-04, -1.1392e-03],\n        [-1.0868e-05, -9.6547e-04,  2.4798e-03,  ...,  1.1242e-03,\n          6.3094e-04,  1.9270e-03],\n        [-7.1563e-05,  3.7225e-04,  1.5079e-03,  ..., -3.4459e-04,\n          1.9676e-04, -2.3073e-05]]), 'transformer.encoder.layers.21.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0029,  0.0121, -0.0053,  ..., -0.0108, -0.0103,  0.0057],\n        [-0.0099,  0.0062,  0.0011,  ...,  0.0115,  0.0020,  0.0153],\n        [ 0.0030, -0.0122,  0.0044,  ..., -0.0007,  0.0155, -0.0086],\n        ...,\n        [-0.0150, -0.0043,  0.0034,  ..., -0.0148, -0.0138,  0.0083],\n        [ 0.0081, -0.0015,  0.0137,  ...,  0.0108,  0.0006,  0.0013],\n        [ 0.0126, -0.0045, -0.0146,  ...,  0.0113, -0.0124,  0.0106]]), 'transformer.encoder.layers.21.self_attention.query_key_value.lora_B.default.weight': tensor([[ 3.1513e-04, -2.7239e-04, -1.2170e-04,  ..., -9.0711e-05,\n          2.6609e-04,  3.4191e-04],\n        [ 4.7374e-05, -2.9114e-04, -2.0414e-04,  ..., -6.8923e-05,\n          2.4267e-04,  5.0671e-04],\n        [ 1.4809e-04,  1.0465e-04, -6.8486e-06,  ..., -8.3551e-05,\n         -1.6835e-04,  3.3939e-04],\n        ...,\n        [ 7.6358e-05,  7.1466e-04, -5.2815e-05,  ...,  5.9879e-04,\n         -2.7323e-05, -3.9232e-04],\n        [ 1.2863e-03,  1.6342e-04, -1.5612e-03,  ...,  5.0552e-04,\n         -1.8173e-03, -1.0672e-03],\n        [ 1.9130e-03,  1.0185e-03, -2.2672e-03,  ...,  1.3008e-03,\n         -3.4149e-03, -2.6268e-03]]), 'transformer.encoder.layers.21.self_attention.dense.lora_A.default.weight': tensor([[-7.7921e-03,  6.7425e-03, -5.1144e-03,  ...,  5.4258e-05,\n          1.6197e-02,  7.8187e-03],\n        [-4.1524e-03,  5.2251e-03,  8.5337e-03,  ...,  1.5360e-02,\n         -3.5200e-03, -6.1265e-03],\n        [ 9.2851e-04, -1.0871e-04,  1.2180e-03,  ...,  1.1531e-02,\n          4.7049e-03,  9.0298e-03],\n        ...,\n        [ 1.1254e-02, -1.2216e-02,  6.2034e-05,  ..., -5.5550e-03,\n         -6.9472e-04, -6.8806e-03],\n        [ 9.3424e-03, -4.1662e-03,  9.7010e-03,  ..., -1.4902e-02,\n          1.6233e-02,  1.3968e-03],\n        [ 3.7943e-03,  5.6097e-03,  7.9213e-03,  ...,  8.9123e-03,\n         -1.0896e-03, -1.1854e-02]]), 'transformer.encoder.layers.21.self_attention.dense.lora_B.default.weight': tensor([[-1.6038e-03,  1.6449e-05, -8.3135e-04,  ...,  6.2163e-04,\n         -6.6939e-04,  1.7288e-03],\n        [ 4.3001e-05,  3.0609e-04, -3.6415e-06,  ...,  2.5922e-04,\n          8.7917e-05,  2.2458e-04],\n        [-1.4569e-03,  9.2404e-04, -1.0133e-03,  ...,  1.2554e-03,\n         -3.0347e-04,  2.1345e-03],\n        ...,\n        [ 1.1963e-03, -1.0166e-03,  5.3371e-04,  ..., -8.0008e-04,\n          1.0872e-03, -1.8266e-03],\n        [-8.6790e-04,  2.1347e-04, -7.0377e-04,  ...,  1.8659e-04,\n         -1.2532e-03,  1.1924e-03],\n        [ 3.4877e-04,  5.5218e-04,  2.1616e-05,  ...,  7.5582e-05,\n          8.2209e-04,  2.3113e-04]]), 'transformer.encoder.layers.21.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0049,  0.0119, -0.0083,  ..., -0.0137, -0.0046, -0.0046],\n        [-0.0113,  0.0120,  0.0010,  ...,  0.0071, -0.0009,  0.0164],\n        [-0.0033, -0.0110,  0.0042,  ..., -0.0131, -0.0062,  0.0019],\n        ...,\n        [-0.0108, -0.0134, -0.0073,  ...,  0.0029, -0.0135, -0.0162],\n        [-0.0012, -0.0004,  0.0088,  ..., -0.0158, -0.0072, -0.0004],\n        [ 0.0159,  0.0057,  0.0074,  ..., -0.0134,  0.0027,  0.0065]]), 'transformer.encoder.layers.21.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 1.1017e-03, -1.5707e-03, -1.0691e-03,  ...,  2.9744e-04,\n          1.2352e-03,  1.9219e-03],\n        [-3.2712e-04,  6.4513e-04,  1.0774e-04,  ..., -4.0509e-04,\n          1.8476e-04, -3.8244e-04],\n        [-2.6018e-04,  3.0478e-05,  3.0973e-04,  ...,  2.2241e-04,\n         -2.9210e-04, -3.3850e-04],\n        ...,\n        [-7.0603e-04, -1.3315e-03, -8.3958e-04,  ...,  1.0068e-03,\n          8.5624e-04,  3.1388e-04],\n        [ 4.3094e-04,  4.1978e-05,  1.1208e-04,  ..., -9.3122e-05,\n          2.9375e-04,  4.4171e-04],\n        [ 2.7956e-04,  5.8584e-04,  5.1172e-04,  ..., -5.4745e-04,\n         -4.3685e-04, -7.5747e-04]]), 'transformer.encoder.layers.21.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 1.8218e-03,  1.9093e-03,  2.5079e-03,  ..., -1.8098e-04,\n         -2.8992e-03,  1.1203e-03],\n        [ 3.4386e-03,  7.4828e-03,  4.9597e-03,  ...,  3.8251e-03,\n          7.7508e-03, -1.7147e-03],\n        [-1.1010e-03,  8.8198e-03,  4.5354e-03,  ..., -4.6442e-03,\n          8.4015e-03,  1.2316e-03],\n        ...,\n        [-7.6160e-04, -4.8052e-03, -1.0932e-03,  ..., -4.2940e-03,\n          5.8933e-03, -3.6422e-03],\n        [-4.2478e-03, -4.4128e-03, -2.7090e-03,  ..., -6.9170e-03,\n          6.5535e-03,  5.1368e-03],\n        [ 2.7449e-03, -4.3916e-03,  6.5231e-04,  ...,  4.3907e-03,\n         -7.6171e-03,  7.8142e-05]]), 'transformer.encoder.layers.21.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 7.7594e-04, -6.2730e-04, -5.1604e-04,  ...,  1.3260e-03,\n          1.1691e-03,  7.5003e-04],\n        [ 6.8811e-04, -4.3270e-04, -7.0469e-04,  ...,  8.0657e-04,\n          8.2019e-05,  3.5840e-04],\n        [ 1.0562e-03, -2.0065e-03, -9.2875e-04,  ...,  1.2034e-03,\n          8.4097e-04,  1.5275e-03],\n        ...,\n        [-1.4785e-03,  1.5579e-03,  7.2671e-04,  ..., -1.5814e-03,\n         -1.6194e-03, -9.9647e-04],\n        [ 1.6832e-03, -5.1887e-04, -6.8741e-04,  ...,  1.0038e-03,\n          1.4446e-03,  6.5247e-04],\n        [ 1.0080e-03, -9.8583e-04, -1.1984e-03,  ...,  1.0343e-03,\n          9.4329e-04,  9.7656e-04]]), 'transformer.encoder.layers.22.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0015, -0.0110,  0.0063,  ...,  0.0094,  0.0080, -0.0043],\n        [-0.0130, -0.0038, -0.0103,  ...,  0.0123, -0.0041, -0.0049],\n        [-0.0068,  0.0134,  0.0065,  ...,  0.0053, -0.0012,  0.0091],\n        ...,\n        [-0.0007, -0.0102,  0.0109,  ..., -0.0125,  0.0082,  0.0052],\n        [-0.0090, -0.0044,  0.0092,  ...,  0.0037,  0.0113,  0.0025],\n        [-0.0089,  0.0107, -0.0007,  ..., -0.0154,  0.0144, -0.0096]]), 'transformer.encoder.layers.22.self_attention.query_key_value.lora_B.default.weight': tensor([[ 2.1475e-05,  1.2878e-03, -1.0689e-03,  ...,  1.2632e-04,\n          9.4430e-04, -1.6842e-05],\n        [-1.1266e-04, -3.8339e-04,  4.2999e-04,  ..., -4.6671e-05,\n         -6.7781e-04,  2.4540e-04],\n        [-2.8154e-04, -1.2524e-04, -4.6358e-05,  ...,  9.7751e-05,\n          2.5753e-04, -1.3386e-04],\n        ...,\n        [-1.0749e-03, -3.5563e-04,  8.4866e-05,  ...,  5.9792e-04,\n         -2.9470e-04,  4.9589e-04],\n        [ 2.9325e-03,  4.6420e-03, -3.5389e-03,  ..., -1.0471e-03,\n         -1.2972e-04, -1.7748e-03],\n        [ 1.0261e-03,  2.4453e-03, -2.2904e-03,  ...,  1.5817e-04,\n         -8.5312e-05, -1.0415e-03]]), 'transformer.encoder.layers.22.self_attention.dense.lora_A.default.weight': tensor([[-0.0074,  0.0152,  0.0017,  ...,  0.0121,  0.0039,  0.0129],\n        [ 0.0088, -0.0126,  0.0046,  ...,  0.0110, -0.0041,  0.0032],\n        [ 0.0095, -0.0089, -0.0092,  ..., -0.0098, -0.0041, -0.0084],\n        ...,\n        [ 0.0052,  0.0104,  0.0069,  ..., -0.0078, -0.0016, -0.0092],\n        [-0.0022, -0.0048,  0.0040,  ...,  0.0077,  0.0102,  0.0071],\n        [ 0.0067,  0.0106,  0.0047,  ...,  0.0010,  0.0069, -0.0094]]), 'transformer.encoder.layers.22.self_attention.dense.lora_B.default.weight': tensor([[-1.2811e-03,  8.2230e-04, -8.9002e-04,  ...,  2.7459e-04,\n         -1.4567e-03,  2.5326e-04],\n        [ 4.2396e-04,  3.3628e-04,  1.3189e-03,  ..., -6.0142e-05,\n         -7.1825e-04,  6.5448e-04],\n        [-1.1124e-04,  8.1478e-04,  8.8288e-04,  ...,  6.0710e-04,\n         -1.5887e-03,  9.0420e-04],\n        ...,\n        [ 1.2579e-03, -1.1464e-03,  4.4727e-04,  ..., -2.2807e-04,\n          1.5087e-03, -1.0311e-03],\n        [-2.4749e-04,  8.9547e-05, -1.9726e-04,  ..., -2.9867e-04,\n         -1.2399e-03,  7.7599e-04],\n        [ 1.9610e-03, -3.9337e-04,  2.6921e-03,  ..., -4.9167e-04,\n         -9.3435e-04,  1.2131e-03]]), 'transformer.encoder.layers.22.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0039,  0.0011,  0.0118,  ..., -0.0091,  0.0070,  0.0002],\n        [-0.0118, -0.0136,  0.0021,  ..., -0.0123,  0.0022,  0.0024],\n        [-0.0055,  0.0085,  0.0060,  ..., -0.0101, -0.0088, -0.0056],\n        ...,\n        [-0.0101, -0.0050, -0.0053,  ...,  0.0060, -0.0018, -0.0025],\n        [-0.0096, -0.0020,  0.0050,  ...,  0.0033, -0.0014, -0.0101],\n        [ 0.0074,  0.0141,  0.0126,  ..., -0.0081,  0.0031, -0.0140]]), 'transformer.encoder.layers.22.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 2.0250e-04,  9.9658e-05, -4.4209e-04,  ...,  1.9038e-04,\n         -2.6267e-04, -3.0604e-04],\n        [-4.7813e-06, -4.6635e-05,  3.2057e-04,  ..., -1.1633e-04,\n          7.0010e-05,  1.4701e-05],\n        [-1.1957e-04,  1.0673e-04,  2.3840e-04,  ..., -4.2984e-04,\n          5.3115e-04, -8.0841e-05],\n        ...,\n        [-1.5584e-04,  2.5354e-04,  9.1476e-05,  ..., -8.4038e-05,\n         -8.6427e-05, -2.7387e-04],\n        [-3.0261e-04, -3.0304e-04,  4.7967e-04,  ..., -3.5911e-04,\n          5.5951e-04, -1.6500e-04],\n        [ 6.8304e-04,  3.7494e-04,  9.7333e-05,  ..., -1.9757e-04,\n         -4.6172e-04, -1.2398e-03]]), 'transformer.encoder.layers.22.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 0.0022, -0.0029, -0.0064,  ..., -0.0037,  0.0042,  0.0086],\n        [ 0.0079,  0.0013,  0.0029,  ...,  0.0003, -0.0074,  0.0028],\n        [ 0.0044, -0.0044,  0.0013,  ...,  0.0072,  0.0044, -0.0070],\n        ...,\n        [ 0.0022, -0.0052,  0.0048,  ..., -0.0033, -0.0037,  0.0020],\n        [ 0.0058, -0.0033, -0.0056,  ...,  0.0008, -0.0004,  0.0032],\n        [-0.0063,  0.0072,  0.0006,  ..., -0.0071,  0.0072, -0.0004]]), 'transformer.encoder.layers.22.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 0.0005,  0.0004,  0.0012,  ...,  0.0005, -0.0010, -0.0022],\n        [ 0.0009,  0.0002,  0.0002,  ...,  0.0011, -0.0006, -0.0012],\n        [ 0.0013,  0.0006,  0.0012,  ...,  0.0008, -0.0011, -0.0018],\n        ...,\n        [-0.0009, -0.0008, -0.0015,  ..., -0.0014,  0.0019,  0.0019],\n        [ 0.0010,  0.0005,  0.0011,  ...,  0.0016, -0.0005, -0.0020],\n        [ 0.0010,  0.0004,  0.0007,  ...,  0.0012,  0.0002, -0.0013]]), 'transformer.encoder.layers.23.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0076, -0.0151,  0.0003,  ...,  0.0052, -0.0043, -0.0129],\n        [ 0.0140, -0.0034,  0.0142,  ..., -0.0015,  0.0005,  0.0149],\n        [-0.0117, -0.0087, -0.0116,  ..., -0.0101,  0.0027, -0.0018],\n        ...,\n        [ 0.0054, -0.0086,  0.0133,  ...,  0.0123, -0.0069, -0.0075],\n        [ 0.0046,  0.0123, -0.0009,  ...,  0.0041, -0.0017, -0.0085],\n        [ 0.0138, -0.0111,  0.0021,  ...,  0.0084, -0.0005, -0.0083]]), 'transformer.encoder.layers.23.self_attention.query_key_value.lora_B.default.weight': tensor([[ 5.9264e-05, -5.0612e-04,  3.0764e-04,  ..., -1.3152e-05,\n          1.7230e-04,  2.5223e-04],\n        [ 4.6152e-05,  2.7520e-04,  4.3350e-05,  ..., -1.4655e-04,\n          3.4385e-04, -1.4516e-04],\n        [ 1.1180e-04,  4.6493e-04, -3.4167e-04,  ..., -2.7673e-06,\n          1.5280e-04, -9.2737e-05],\n        ...,\n        [-1.3227e-03,  1.3175e-03, -6.1780e-04,  ...,  7.3271e-04,\n         -1.0882e-03, -1.3771e-04],\n        [-1.0514e-03,  1.7295e-03, -1.2026e-03,  ...,  1.7920e-03,\n         -1.0479e-03, -2.0312e-03],\n        [-1.2248e-03,  2.0045e-04, -2.4407e-04,  ...,  8.3626e-04,\n         -2.2342e-03,  2.0643e-04]]), 'transformer.encoder.layers.23.self_attention.dense.lora_A.default.weight': tensor([[ 5.0574e-03,  9.0719e-03,  6.2652e-03,  ...,  6.3878e-03,\n         -1.3955e-03, -4.1721e-03],\n        [-1.1490e-02,  9.6270e-03, -9.4617e-04,  ...,  1.3459e-02,\n         -1.0278e-02,  1.3253e-02],\n        [-7.3838e-03,  9.7601e-03, -2.0682e-03,  ...,  2.8911e-03,\n          3.5692e-05,  6.1360e-03],\n        ...,\n        [ 1.4692e-02, -1.1930e-02, -3.2423e-03,  ...,  3.0724e-03,\n          1.8895e-03,  7.4844e-03],\n        [-1.0657e-02, -1.2798e-02, -1.0879e-02,  ...,  1.1705e-02,\n          1.0590e-02, -1.0786e-02],\n        [ 5.1460e-03, -3.5261e-03, -7.9743e-03,  ...,  1.1326e-02,\n         -8.0854e-03, -1.3055e-02]]), 'transformer.encoder.layers.23.self_attention.dense.lora_B.default.weight': tensor([[-5.2635e-04, -3.3924e-04, -5.3199e-04,  ..., -5.7709e-04,\n          1.0377e-03,  1.1723e-04],\n        [-1.0711e-03, -1.1830e-04,  8.8242e-04,  ..., -4.9646e-04,\n          1.4864e-04,  4.7171e-04],\n        [-1.8800e-04, -1.2917e-03,  4.8546e-04,  ..., -7.7515e-04,\n          8.0072e-04, -1.7681e-04],\n        ...,\n        [ 1.0033e-03,  8.0428e-04, -1.5516e-04,  ...,  4.1827e-04,\n         -8.1165e-04,  8.5278e-04],\n        [-1.6740e-04, -1.1638e-03,  1.1904e-03,  ..., -3.9198e-04,\n          7.4707e-04, -4.5083e-04],\n        [-2.4830e-04, -1.4953e-03,  3.3534e-04,  ..., -1.5453e-04,\n          4.9877e-04, -2.3750e-05]]), 'transformer.encoder.layers.23.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-3.4471e-03, -4.2862e-03,  6.3190e-03,  ..., -5.8179e-05,\n         -7.7761e-03, -8.0660e-03],\n        [ 1.1320e-02, -1.0867e-02,  2.4893e-03,  ...,  1.6396e-03,\n         -1.1963e-02, -1.0615e-02],\n        [-1.1805e-02,  5.1745e-03,  7.4414e-04,  ..., -4.3719e-03,\n         -5.1858e-03,  1.5277e-02],\n        ...,\n        [-7.9487e-03,  3.7530e-03,  6.2427e-03,  ..., -1.3982e-02,\n          9.7878e-03,  3.0613e-03],\n        [ 3.5325e-03, -7.9815e-03, -1.5444e-02,  ...,  2.6136e-03,\n          1.7543e-03, -4.4048e-03],\n        [-1.3967e-02,  5.4879e-04,  3.5595e-03,  ...,  1.2967e-02,\n          3.9066e-03, -3.1029e-03]]), 'transformer.encoder.layers.23.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 2.5478e-04,  3.3947e-04,  3.0854e-05,  ..., -4.7630e-04,\n         -1.1153e-03, -1.2194e-03],\n        [ 3.1660e-04, -5.5912e-04,  8.7435e-05,  ...,  5.0827e-04,\n          4.0687e-04,  6.0776e-04],\n        [-1.8289e-04,  2.3488e-04,  1.7341e-04,  ..., -3.5429e-04,\n         -3.7340e-06, -7.1036e-04],\n        ...,\n        [-5.0488e-05, -1.2788e-04,  2.3706e-04,  ..., -1.8782e-04,\n         -4.4661e-04, -7.5154e-04],\n        [-1.8555e-04, -1.4394e-04,  7.2469e-05,  ..., -2.1824e-04,\n         -1.1910e-04, -1.7962e-04],\n        [ 1.3565e-03, -1.1555e-03,  8.4488e-04,  ...,  1.1318e-03,\n          1.6078e-03,  1.1677e-03]]), 'transformer.encoder.layers.23.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 3.8594e-03, -5.4022e-03,  4.3028e-03,  ...,  4.1801e-03,\n         -2.9463e-03, -2.6939e-04],\n        [ 5.9380e-03,  6.5701e-03, -3.8343e-03,  ...,  1.8101e-03,\n         -3.7568e-03,  4.5084e-03],\n        [ 1.2167e-03, -6.1101e-05,  4.0996e-03,  ..., -6.8665e-03,\n          2.1786e-03,  6.3046e-03],\n        ...,\n        [ 7.9866e-03, -6.3598e-03, -2.2654e-03,  ..., -6.9049e-03,\n         -6.2310e-03, -7.7381e-03],\n        [ 6.4663e-03, -1.4590e-03,  5.0508e-03,  ..., -7.7157e-03,\n          2.8622e-03,  4.3498e-03],\n        [-9.1447e-04,  2.5877e-03, -8.0105e-03,  ..., -1.4626e-03,\n          7.9912e-03,  4.2460e-03]]), 'transformer.encoder.layers.23.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 1.5983e-03, -9.9376e-04, -6.4944e-04,  ...,  8.6740e-04,\n         -1.1121e-03,  1.1789e-03],\n        [ 5.6263e-04, -1.2814e-03, -6.7344e-04,  ...,  2.9899e-04,\n         -6.7756e-04,  4.5203e-04],\n        [ 1.5239e-03, -3.1753e-05, -1.2833e-03,  ...,  2.4365e-03,\n         -2.0002e-03,  5.6631e-04],\n        ...,\n        [-1.3981e-03,  7.5653e-04,  1.2116e-03,  ..., -1.6442e-03,\n          1.1879e-03, -8.8872e-04],\n        [ 1.9046e-03, -5.1450e-04, -4.1026e-05,  ...,  1.1635e-03,\n         -1.8319e-03,  1.5940e-03],\n        [ 1.4577e-03, -1.6122e-04, -1.0529e-03,  ...,  1.0788e-03,\n         -1.2268e-03,  6.6034e-04]]), 'transformer.encoder.layers.24.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0103,  0.0135, -0.0146,  ...,  0.0014, -0.0147,  0.0096],\n        [ 0.0045, -0.0004,  0.0035,  ...,  0.0115,  0.0126,  0.0023],\n        [-0.0137, -0.0097, -0.0162,  ..., -0.0060,  0.0074, -0.0067],\n        ...,\n        [ 0.0004,  0.0082, -0.0156,  ..., -0.0087, -0.0106,  0.0097],\n        [-0.0120,  0.0075, -0.0086,  ...,  0.0090,  0.0031,  0.0083],\n        [-0.0050, -0.0067,  0.0125,  ...,  0.0055,  0.0005, -0.0104]]), 'transformer.encoder.layers.24.self_attention.query_key_value.lora_B.default.weight': tensor([[-3.6337e-04,  7.0750e-04,  6.1927e-04,  ...,  2.5368e-04,\n         -5.6337e-04,  6.6699e-04],\n        [-2.8170e-04,  1.4685e-04,  2.5118e-04,  ...,  3.9637e-04,\n         -2.7866e-04,  5.0650e-04],\n        [-2.8854e-06, -4.4362e-05, -4.3787e-06,  ...,  5.0950e-05,\n          9.2751e-05,  2.6989e-06],\n        ...,\n        [-1.0271e-03, -1.6166e-03,  1.9350e-04,  ..., -2.5608e-03,\n         -3.2332e-04,  1.9091e-04],\n        [-4.5102e-04, -4.2772e-04,  1.9435e-04,  ..., -7.8177e-04,\n         -1.2402e-03,  1.6440e-03],\n        [-4.7869e-04,  1.7353e-04,  3.5001e-04,  ..., -6.4461e-04,\n         -9.4728e-04,  1.9346e-04]]), 'transformer.encoder.layers.24.self_attention.dense.lora_A.default.weight': tensor([[ 0.0112, -0.0071,  0.0022,  ...,  0.0023, -0.0069,  0.0012],\n        [-0.0108, -0.0100,  0.0069,  ...,  0.0137,  0.0072, -0.0050],\n        [ 0.0146, -0.0023,  0.0076,  ..., -0.0156, -0.0138,  0.0018],\n        ...,\n        [-0.0004, -0.0140, -0.0096,  ..., -0.0029,  0.0070,  0.0076],\n        [ 0.0037,  0.0148, -0.0108,  ...,  0.0061,  0.0129, -0.0064],\n        [ 0.0017,  0.0112,  0.0095,  ..., -0.0153,  0.0005,  0.0045]]), 'transformer.encoder.layers.24.self_attention.dense.lora_B.default.weight': tensor([[ 4.3181e-04, -1.8313e-03,  9.7737e-04,  ..., -1.5311e-03,\n         -2.0971e-03, -1.0083e-04],\n        [-1.9680e-04, -9.1892e-04,  3.1225e-05,  ..., -4.6913e-04,\n         -2.8603e-04, -2.4373e-04],\n        [ 6.7173e-04, -1.4435e-03, -5.9281e-04,  ..., -1.3091e-03,\n         -4.4359e-05, -5.7810e-04],\n        ...,\n        [-3.3440e-04,  1.5939e-03, -6.2119e-04,  ...,  1.2768e-03,\n          1.3420e-03,  7.4301e-04],\n        [ 1.2657e-03, -7.0822e-04,  9.1993e-04,  ..., -1.0987e-03,\n         -5.6604e-04,  1.4867e-04],\n        [ 3.9891e-04, -1.1808e-03, -6.6251e-05,  ..., -6.6314e-04,\n         -8.7411e-04, -2.1404e-04]]), 'transformer.encoder.layers.24.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0132, -0.0113, -0.0029,  ..., -0.0069, -0.0088, -0.0163],\n        [ 0.0149,  0.0101, -0.0050,  ..., -0.0146, -0.0132,  0.0123],\n        [ 0.0016,  0.0027, -0.0011,  ..., -0.0027,  0.0158, -0.0116],\n        ...,\n        [-0.0163, -0.0075, -0.0089,  ..., -0.0085,  0.0041, -0.0162],\n        [-0.0102,  0.0079,  0.0109,  ...,  0.0112, -0.0106, -0.0113],\n        [ 0.0047, -0.0125, -0.0130,  ...,  0.0130,  0.0024, -0.0062]]), 'transformer.encoder.layers.24.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-1.3802e-03, -2.0797e-03,  1.4840e-03,  ..., -1.5778e-03,\n         -7.7636e-04,  2.1055e-03],\n        [-5.3605e-04, -8.9903e-04,  9.8748e-04,  ..., -1.0898e-03,\n         -2.8151e-05,  8.3794e-04],\n        [ 1.2199e-03,  8.9823e-04, -1.2451e-03,  ...,  1.0491e-03,\n          2.8740e-04, -1.2965e-03],\n        ...,\n        [-7.4223e-04, -9.6673e-04,  1.0253e-03,  ..., -8.3946e-04,\n         -2.0202e-05,  1.2430e-03],\n        [-6.0257e-05, -1.2970e-04,  3.2396e-04,  ..., -1.4866e-04,\n          9.2644e-05,  1.2754e-05],\n        [ 1.4917e-03,  1.6537e-03, -1.2376e-03,  ...,  1.2856e-03,\n          3.0550e-04, -1.4781e-03]]), 'transformer.encoder.layers.24.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0016, -0.0020, -0.0070,  ...,  0.0014,  0.0059,  0.0058],\n        [ 0.0047,  0.0003,  0.0056,  ...,  0.0046, -0.0074,  0.0033],\n        [ 0.0039, -0.0016,  0.0058,  ...,  0.0031,  0.0075, -0.0013],\n        ...,\n        [-0.0017,  0.0032, -0.0045,  ...,  0.0044,  0.0032,  0.0066],\n        [-0.0019,  0.0056, -0.0038,  ...,  0.0066, -0.0037, -0.0077],\n        [-0.0068, -0.0039, -0.0015,  ...,  0.0002,  0.0003,  0.0078]]), 'transformer.encoder.layers.24.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-8.7965e-04, -3.6477e-04,  1.5933e-03,  ..., -1.3332e-03,\n         -7.8934e-04,  2.1282e-03],\n        [-3.1561e-04, -5.6197e-04,  1.4379e-04,  ..., -5.6021e-04,\n         -6.6567e-04,  1.2136e-04],\n        [-2.2510e-03, -1.9253e-03,  5.0003e-04,  ..., -1.1303e-03,\n         -1.6573e-03,  2.1214e-03],\n        ...,\n        [ 1.4165e-03,  6.8343e-04, -1.2655e-03,  ...,  9.7578e-04,\n          1.2427e-03, -1.7354e-03],\n        [-6.2958e-05, -2.1044e-05,  6.3857e-04,  ..., -1.2624e-04,\n         -5.8461e-04,  1.7367e-03],\n        [-1.0337e-03, -7.6237e-04,  2.6720e-04,  ..., -7.4720e-04,\n         -8.0674e-04,  1.8811e-03]]), 'transformer.encoder.layers.25.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0120,  0.0104,  0.0132,  ..., -0.0122, -0.0149, -0.0034],\n        [-0.0113,  0.0144,  0.0027,  ...,  0.0105,  0.0094,  0.0013],\n        [ 0.0052, -0.0150, -0.0121,  ..., -0.0139,  0.0085, -0.0025],\n        ...,\n        [ 0.0070, -0.0052,  0.0052,  ...,  0.0080,  0.0129,  0.0113],\n        [-0.0025, -0.0105, -0.0036,  ...,  0.0114,  0.0094, -0.0093],\n        [ 0.0021, -0.0131,  0.0094,  ..., -0.0072, -0.0147, -0.0070]]), 'transformer.encoder.layers.25.self_attention.query_key_value.lora_B.default.weight': tensor([[ 2.9623e-04,  2.0007e-06, -1.2483e-05,  ...,  1.0683e-05,\n         -6.0667e-05,  2.6609e-04],\n        [ 4.4738e-04,  7.0108e-05,  3.6912e-04,  ...,  1.9147e-05,\n         -8.9399e-05,  6.0315e-04],\n        [-4.3204e-04,  1.9863e-04, -1.7678e-04,  ..., -2.0671e-05,\n          7.5929e-05, -4.3381e-04],\n        ...,\n        [-1.4771e-03,  2.7876e-04, -8.8191e-04,  ...,  2.1896e-04,\n          6.1257e-04, -2.3636e-04],\n        [ 2.9287e-03, -2.6330e-04,  2.3981e-03,  ..., -2.2206e-03,\n         -1.7540e-03,  2.4921e-03],\n        [ 6.6478e-04,  2.9899e-04,  1.4630e-03,  ..., -2.3701e-03,\n          1.4081e-03, -7.1958e-04]]), 'transformer.encoder.layers.25.self_attention.dense.lora_A.default.weight': tensor([[ 0.0128,  0.0108,  0.0122,  ..., -0.0082, -0.0080, -0.0019],\n        [-0.0059,  0.0053, -0.0009,  ..., -0.0031, -0.0100,  0.0024],\n        [ 0.0107, -0.0159,  0.0133,  ...,  0.0137, -0.0067,  0.0132],\n        ...,\n        [-0.0086, -0.0073, -0.0019,  ...,  0.0099, -0.0032, -0.0095],\n        [ 0.0099,  0.0153, -0.0091,  ...,  0.0018,  0.0003,  0.0051],\n        [-0.0097, -0.0048,  0.0082,  ...,  0.0087, -0.0004,  0.0101]]), 'transformer.encoder.layers.25.self_attention.dense.lora_B.default.weight': tensor([[-1.3610e-03,  1.5388e-03,  8.9876e-04,  ..., -1.0045e-03,\n         -3.3300e-04,  1.7756e-04],\n        [ 8.2925e-05,  1.9517e-04, -1.8347e-04,  ..., -4.2973e-04,\n         -4.7850e-04, -6.4119e-04],\n        [-8.8251e-04,  1.8741e-03,  5.5243e-04,  ..., -1.8446e-03,\n         -1.0407e-03, -2.0380e-03],\n        ...,\n        [ 1.5963e-03, -2.0039e-03, -1.5050e-03,  ...,  1.1999e-03,\n          5.1611e-04,  4.0859e-04],\n        [-3.3247e-04,  1.3682e-03, -7.8277e-05,  ..., -4.4702e-04,\n         -4.4661e-04, -3.2944e-04],\n        [-1.6032e-04,  7.0459e-04, -1.9959e-04,  ..., -6.2222e-04,\n         -2.4779e-04, -4.6721e-04]]), 'transformer.encoder.layers.25.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0145,  0.0102,  0.0012,  ..., -0.0143,  0.0036, -0.0147],\n        [ 0.0109, -0.0143, -0.0023,  ..., -0.0059, -0.0014,  0.0062],\n        [ 0.0046,  0.0045, -0.0038,  ..., -0.0158,  0.0059, -0.0015],\n        ...,\n        [-0.0117,  0.0042,  0.0166,  ..., -0.0067,  0.0084, -0.0019],\n        [-0.0159,  0.0071,  0.0041,  ..., -0.0152, -0.0073, -0.0064],\n        [ 0.0137, -0.0060,  0.0044,  ..., -0.0100, -0.0101, -0.0042]]), 'transformer.encoder.layers.25.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 9.6191e-04, -1.2582e-03, -1.3314e-03,  ..., -1.0580e-03,\n          5.2428e-04,  1.1407e-03],\n        [-3.4805e-04,  4.9594e-04, -3.0481e-04,  ..., -6.9922e-04,\n          1.1665e-04, -3.1816e-04],\n        [ 3.5149e-04, -2.2133e-04,  4.5477e-04,  ..., -3.0287e-04,\n         -3.4901e-04,  1.2715e-04],\n        ...,\n        [ 9.4555e-05, -4.9489e-04, -6.7861e-04,  ..., -4.0254e-04,\n          6.4373e-04,  1.3822e-04],\n        [ 2.5849e-04, -3.5493e-05, -5.6753e-04,  ..., -6.2858e-04,\n          4.3080e-04,  7.0163e-04],\n        [-6.3977e-04,  2.8929e-04,  1.6836e-04,  ...,  6.2174e-05,\n          2.2811e-04,  1.0895e-04]]), 'transformer.encoder.layers.25.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-1.6206e-03,  6.2556e-04, -5.0018e-03,  ..., -1.0627e-03,\n         -2.6039e-03, -2.9480e-03],\n        [ 8.6793e-03, -7.1307e-03, -4.8527e-03,  ..., -7.3632e-03,\n          2.4893e-03,  8.2633e-05],\n        [ 2.6330e-03, -1.3875e-03,  6.0298e-03,  ...,  3.7392e-03,\n          1.5656e-03, -7.4828e-03],\n        ...,\n        [-4.0878e-03, -1.0395e-03, -5.4854e-03,  ..., -8.0274e-03,\n          6.8506e-03,  7.4758e-03],\n        [ 6.8640e-04,  7.0552e-03, -1.1013e-03,  ...,  3.3685e-03,\n         -4.3212e-03,  3.0752e-03],\n        [-6.9084e-03,  8.1787e-03, -6.2060e-03,  ...,  4.1315e-03,\n         -4.7392e-03, -3.1748e-03]]), 'transformer.encoder.layers.25.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 0.0021, -0.0007, -0.0023,  ..., -0.0007,  0.0006, -0.0021],\n        [ 0.0010, -0.0003, -0.0011,  ..., -0.0004,  0.0003, -0.0012],\n        [ 0.0025, -0.0003, -0.0023,  ..., -0.0012,  0.0011, -0.0027],\n        ...,\n        [-0.0019, -0.0005,  0.0018,  ...,  0.0001, -0.0004,  0.0020],\n        [ 0.0013,  0.0006, -0.0007,  ...,  0.0004,  0.0003, -0.0006],\n        [ 0.0010, -0.0011, -0.0013,  ..., -0.0007,  0.0004, -0.0012]]), 'transformer.encoder.layers.26.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0054,  0.0068,  0.0126,  ..., -0.0099, -0.0141,  0.0076],\n        [-0.0054,  0.0110,  0.0104,  ..., -0.0006,  0.0147,  0.0014],\n        [-0.0123, -0.0005,  0.0028,  ...,  0.0150, -0.0147, -0.0070],\n        ...,\n        [ 0.0050,  0.0062, -0.0015,  ...,  0.0098, -0.0082, -0.0033],\n        [ 0.0003, -0.0055,  0.0036,  ...,  0.0008,  0.0147, -0.0037],\n        [-0.0092,  0.0090, -0.0071,  ...,  0.0023, -0.0101, -0.0085]]), 'transformer.encoder.layers.26.self_attention.query_key_value.lora_B.default.weight': tensor([[-9.9318e-05,  2.4462e-05, -2.7569e-04,  ...,  1.2182e-04,\n          1.6169e-04, -1.2048e-04],\n        [ 1.7063e-04,  6.5173e-05,  1.5745e-04,  ..., -1.5845e-04,\n         -1.4556e-04,  2.3111e-05],\n        [ 4.0745e-04, -4.0994e-04,  2.6850e-04,  ..., -3.1370e-04,\n         -1.5331e-04,  1.4193e-04],\n        ...,\n        [ 8.4138e-04, -1.9252e-03,  1.8983e-03,  ..., -1.0561e-03,\n          1.1966e-03,  2.2645e-03],\n        [ 7.0541e-04, -2.2594e-04,  2.2781e-04,  ...,  2.2823e-04,\n         -2.1954e-04,  2.4144e-04],\n        [ 1.6787e-03, -2.0266e-03, -6.8444e-05,  ..., -1.0562e-03,\n          1.0789e-03, -8.8447e-05]]), 'transformer.encoder.layers.26.self_attention.dense.lora_A.default.weight': tensor([[-1.8527e-03, -8.9172e-03,  1.1733e-02,  ..., -1.0371e-02,\n          9.3074e-03, -6.5122e-03],\n        [-2.6186e-03, -1.3186e-02, -4.0961e-04,  ..., -1.7411e-03,\n         -8.4110e-03,  1.3612e-04],\n        [-6.7963e-03, -1.2911e-02, -6.0569e-03,  ..., -2.5815e-03,\n          6.1713e-04, -1.3832e-02],\n        ...,\n        [-9.6925e-03,  5.8496e-05, -2.5731e-03,  ..., -8.6440e-03,\n         -4.2937e-03, -1.0639e-02],\n        [ 6.6464e-03, -1.5888e-02,  1.5174e-02,  ...,  1.0043e-02,\n          8.9426e-03,  9.9346e-03],\n        [-4.0691e-03, -8.2715e-03,  1.2849e-02,  ...,  2.6042e-03,\n          1.1124e-02,  9.0843e-03]]), 'transformer.encoder.layers.26.self_attention.dense.lora_B.default.weight': tensor([[ 1.4226e-03, -2.3779e-03, -2.0114e-03,  ...,  1.1426e-03,\n         -6.8704e-04, -2.0714e-03],\n        [ 1.0421e-03, -1.3539e-03, -1.1482e-03,  ...,  9.8845e-04,\n         -5.0293e-04, -1.0385e-03],\n        [ 4.9606e-04, -1.3381e-03, -1.4107e-03,  ...,  4.3180e-04,\n         -3.5191e-04, -6.0448e-04],\n        ...,\n        [-1.4603e-03,  2.6913e-03,  1.6264e-03,  ..., -1.4036e-03,\n          5.5722e-04,  2.0967e-03],\n        [ 1.6495e-04, -6.4212e-04, -8.3380e-04,  ...,  3.3623e-04,\n         -1.9073e-04, -7.9657e-06],\n        [ 4.6791e-04, -1.0361e-03, -1.5016e-03,  ...,  7.3213e-04,\n         -8.7298e-07, -1.1078e-03]]), 'transformer.encoder.layers.26.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0008,  0.0144,  0.0033,  ..., -0.0131,  0.0134, -0.0066],\n        [-0.0041, -0.0132,  0.0143,  ..., -0.0150, -0.0020,  0.0055],\n        [ 0.0141,  0.0087, -0.0020,  ...,  0.0061, -0.0073, -0.0093],\n        ...,\n        [ 0.0003,  0.0151, -0.0027,  ...,  0.0064, -0.0060,  0.0101],\n        [ 0.0079, -0.0160,  0.0026,  ...,  0.0005, -0.0090, -0.0092],\n        [ 0.0004,  0.0016, -0.0050,  ..., -0.0120,  0.0098, -0.0104]]), 'transformer.encoder.layers.26.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 3.5868e-04,  3.5120e-04, -3.4220e-04,  ...,  4.9439e-04,\n          4.4529e-04, -1.6743e-04],\n        [ 2.7112e-04, -1.9748e-05, -5.9139e-05,  ...,  3.5004e-04,\n          3.4818e-05,  2.4624e-04],\n        [ 1.6251e-04,  3.8343e-04, -6.9120e-04,  ...,  7.6427e-04,\n         -8.0929e-06,  6.2912e-04],\n        ...,\n        [-6.5107e-04, -8.4692e-05,  5.0399e-05,  ..., -4.5966e-04,\n          1.6512e-06, -4.0472e-04],\n        [-6.2746e-04, -3.4117e-04,  8.8390e-04,  ..., -9.7383e-04,\n         -5.7909e-04, -2.6127e-04],\n        [-1.2139e-04,  2.2829e-04, -2.8271e-04,  ...,  1.2044e-04,\n         -6.9118e-05, -2.0586e-05]]), 'transformer.encoder.layers.26.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 4.1176e-03,  3.1242e-03,  3.0370e-03,  ..., -8.4269e-03,\n         -7.1587e-03, -8.3351e-03],\n        [-1.0197e-03, -1.3190e-03, -1.5060e-03,  ..., -1.5692e-03,\n         -5.8144e-03,  2.6560e-03],\n        [-6.3304e-03, -3.8990e-03,  5.5998e-03,  ..., -1.3160e-03,\n         -4.7889e-03, -8.3046e-03],\n        ...,\n        [ 2.4128e-03, -3.3859e-05,  3.4016e-03,  ..., -7.6441e-03,\n         -7.7680e-04,  2.6199e-03],\n        [-3.5436e-03,  8.2226e-03,  2.5182e-03,  ..., -2.6289e-03,\n         -1.8365e-03, -1.3521e-03],\n        [-6.7619e-03, -6.8530e-03,  3.5372e-03,  ...,  3.0615e-03,\n         -5.7468e-03,  2.8854e-03]]), 'transformer.encoder.layers.26.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 2.5518e-03, -3.4964e-03,  3.4559e-03,  ...,  3.3964e-03,\n         -1.5436e-03,  3.1223e-03],\n        [ 1.3373e-03, -1.1995e-03,  1.1254e-03,  ...,  1.0747e-03,\n         -3.4588e-05,  6.3508e-04],\n        [ 2.0478e-03, -2.4139e-03,  3.3587e-03,  ...,  1.8724e-04,\n         -1.2226e-03,  2.5496e-03],\n        ...,\n        [-1.2963e-03,  1.1228e-03, -1.6540e-03,  ..., -2.8018e-04,\n          6.9539e-04, -9.2256e-04],\n        [ 1.5311e-03, -1.9451e-03,  2.6006e-03,  ...,  1.0063e-03,\n         -6.9539e-04,  1.9809e-03],\n        [ 1.3591e-03, -1.6864e-03,  2.3823e-03,  ...,  4.6305e-04,\n         -9.1644e-04,  1.7845e-03]]), 'transformer.encoder.layers.27.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0136,  0.0063,  0.0098,  ..., -0.0018,  0.0144, -0.0020],\n        [ 0.0083, -0.0060,  0.0151,  ..., -0.0148, -0.0126,  0.0095],\n        [-0.0084, -0.0104,  0.0111,  ..., -0.0020,  0.0136,  0.0018],\n        ...,\n        [-0.0141,  0.0004, -0.0141,  ..., -0.0027, -0.0015, -0.0071],\n        [-0.0141,  0.0013, -0.0073,  ...,  0.0121, -0.0009, -0.0001],\n        [-0.0056, -0.0126, -0.0016,  ..., -0.0084,  0.0131, -0.0048]]), 'transformer.encoder.layers.27.self_attention.query_key_value.lora_B.default.weight': tensor([[-4.2290e-04, -3.8932e-04,  5.9062e-04,  ...,  2.4591e-04,\n          2.4287e-04, -3.4639e-04],\n        [-9.3678e-05,  3.1147e-05,  2.3419e-05,  ...,  2.2625e-04,\n         -3.1529e-04,  9.3745e-05],\n        [ 2.9582e-05,  1.5474e-04,  1.5879e-04,  ...,  1.4989e-04,\n         -1.9811e-04, -1.2910e-04],\n        ...,\n        [ 1.0145e-03,  2.7504e-04, -1.8694e-04,  ..., -1.4149e-04,\n          5.4582e-04,  1.1210e-03],\n        [ 1.9857e-04,  4.4447e-04, -7.1615e-04,  ...,  6.8715e-05,\n         -2.3314e-05,  3.5005e-04],\n        [-1.4589e-03, -8.6309e-04,  3.0268e-03,  ...,  1.2867e-03,\n         -3.3752e-03,  1.0514e-04]]), 'transformer.encoder.layers.27.self_attention.dense.lora_A.default.weight': tensor([[-0.0137, -0.0065,  0.0081,  ..., -0.0123,  0.0049, -0.0046],\n        [ 0.0120,  0.0045, -0.0101,  ...,  0.0097, -0.0140,  0.0106],\n        [ 0.0034,  0.0073, -0.0114,  ...,  0.0131, -0.0066, -0.0069],\n        ...,\n        [ 0.0061,  0.0042, -0.0054,  ...,  0.0032,  0.0091,  0.0099],\n        [ 0.0025, -0.0058, -0.0153,  ...,  0.0079, -0.0085,  0.0096],\n        [-0.0126, -0.0033, -0.0101,  ..., -0.0026,  0.0081,  0.0030]]), 'transformer.encoder.layers.27.self_attention.dense.lora_B.default.weight': tensor([[-2.6935e-03,  1.9042e-03, -4.7136e-05,  ..., -3.2100e-03,\n         -2.6108e-03, -1.1066e-03],\n        [-1.1612e-03,  4.9296e-04,  2.5322e-05,  ..., -1.2694e-03,\n         -8.5980e-04, -4.8397e-04],\n        [-1.1803e-03,  5.4789e-04, -1.5858e-03,  ..., -5.9246e-04,\n         -2.5746e-03, -8.0055e-04],\n        ...,\n        [ 1.4507e-03, -5.9514e-04,  5.6751e-05,  ...,  1.1259e-03,\n          1.7304e-03,  4.2627e-04],\n        [-2.0124e-03,  1.4481e-03,  1.9736e-05,  ..., -1.5949e-03,\n         -1.7301e-03, -3.1909e-04],\n        [-1.3043e-03,  1.1072e-03, -2.1276e-04,  ..., -1.7443e-03,\n         -1.6993e-03, -8.9472e-04]]), 'transformer.encoder.layers.27.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0008,  0.0106, -0.0127,  ...,  0.0048,  0.0033,  0.0046],\n        [ 0.0033, -0.0113, -0.0035,  ..., -0.0106, -0.0126, -0.0080],\n        [-0.0110, -0.0007, -0.0134,  ..., -0.0024, -0.0117,  0.0123],\n        ...,\n        [-0.0074,  0.0054,  0.0116,  ...,  0.0058, -0.0107, -0.0067],\n        [-0.0149,  0.0115, -0.0099,  ..., -0.0049,  0.0107,  0.0134],\n        [ 0.0058,  0.0150,  0.0053,  ..., -0.0064, -0.0149,  0.0147]]), 'transformer.encoder.layers.27.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-1.0538e-03,  1.3764e-04,  4.5604e-05,  ..., -2.9443e-04,\n         -2.3670e-04,  3.6422e-04],\n        [-5.0206e-04, -4.3784e-04, -7.9406e-04,  ...,  1.1419e-03,\n          5.9054e-04,  1.2588e-03],\n        [-1.0223e-03, -1.6919e-04, -2.0172e-04,  ...,  1.0388e-04,\n          3.9523e-05,  4.9052e-04],\n        ...,\n        [-6.9423e-04, -3.6783e-04, -1.7526e-04,  ..., -2.0799e-04,\n         -2.8006e-05,  7.9407e-04],\n        [-1.0787e-03, -8.0028e-04,  5.1491e-04,  ..., -1.1075e-03,\n         -2.7492e-04,  9.4664e-04],\n        [-8.7975e-05, -3.8448e-05,  4.7156e-04,  ..., -4.0261e-04,\n         -3.2003e-04, -1.0737e-04]]), 'transformer.encoder.layers.27.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 0.0036,  0.0083,  0.0025,  ...,  0.0052,  0.0090,  0.0051],\n        [ 0.0072,  0.0051, -0.0011,  ..., -0.0061, -0.0070, -0.0045],\n        [ 0.0059,  0.0002,  0.0060,  ..., -0.0071, -0.0005,  0.0078],\n        ...,\n        [ 0.0052,  0.0039, -0.0062,  ..., -0.0034,  0.0001,  0.0045],\n        [ 0.0044,  0.0042, -0.0087,  ..., -0.0066,  0.0026, -0.0030],\n        [ 0.0086,  0.0009, -0.0031,  ...,  0.0020, -0.0003, -0.0020]]), 'transformer.encoder.layers.27.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-2.0886e-03,  2.5807e-03, -1.6624e-03,  ...,  1.8710e-03,\n         -1.8607e-03,  9.3332e-05],\n        [-1.0684e-03,  1.3237e-03, -9.8307e-04,  ...,  1.2632e-03,\n         -1.0300e-03, -1.3078e-04],\n        [-3.0255e-03,  3.8075e-03, -2.5327e-03,  ...,  2.9061e-03,\n         -2.8244e-03,  1.2787e-04],\n        ...,\n        [ 1.6453e-03, -1.8293e-03,  1.1745e-03,  ..., -1.3281e-03,\n          1.4509e-03, -8.9004e-05],\n        [-2.1625e-03,  2.3358e-03, -1.7079e-03,  ...,  1.7876e-03,\n         -1.8970e-03,  8.2095e-05],\n        [-2.1307e-03,  2.3656e-03, -1.6017e-03,  ...,  1.8221e-03,\n         -2.0079e-03,  6.8205e-05]]), 'v_head.weight': tensor([[ 0.0088,  0.0041,  0.0163,  ..., -0.0066,  0.0090,  0.0084]])}\u001b[0m\n\u001b[32m2023-08-11 14:20:31.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1463\u001b[0m - \u001b[1madapter_weights={'transformer.encoder.layers.0.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0107, -0.0150,  0.0017,  ..., -0.0025, -0.0062, -0.0060],\n        [ 0.0147,  0.0098, -0.0099,  ...,  0.0022, -0.0090, -0.0094],\n        [ 0.0097,  0.0102,  0.0138,  ...,  0.0094, -0.0037,  0.0062],\n        ...,\n        [-0.0156,  0.0125,  0.0113,  ..., -0.0125,  0.0031,  0.0101],\n        [-0.0023,  0.0120,  0.0146,  ..., -0.0127, -0.0008,  0.0074],\n        [ 0.0093,  0.0104, -0.0121,  ...,  0.0098, -0.0030, -0.0007]]), 'transformer.encoder.layers.0.self_attention.query_key_value.lora_B.default.weight': tensor([[-9.1392e-05, -9.2385e-05, -7.2888e-05,  ..., -1.7385e-05,\n          1.0959e-05, -2.3269e-05],\n        [ 2.3189e-04,  2.5783e-04,  1.9757e-04,  ..., -8.3680e-05,\n          1.0515e-04,  6.6521e-05],\n        [ 1.1317e-04,  3.1648e-05, -7.4292e-05,  ...,  6.8238e-05,\n         -7.3030e-06, -1.1472e-04],\n        ...,\n        [-4.2388e-05, -2.7805e-04, -2.9434e-04,  ..., -2.0124e-04,\n          4.8057e-04, -4.5588e-04],\n        [-4.0577e-04, -5.3336e-04, -6.0664e-04,  ..., -4.9432e-04,\n          9.6858e-04, -5.9531e-04],\n        [ 3.4868e-04, -5.8953e-04, -3.2017e-04,  ..., -1.1294e-03,\n          1.0526e-03, -1.1466e-04]]), 'transformer.encoder.layers.0.self_attention.dense.lora_A.default.weight': tensor([[-0.0117,  0.0035, -0.0061,  ...,  0.0067,  0.0076,  0.0052],\n        [ 0.0059,  0.0105, -0.0121,  ...,  0.0152,  0.0123,  0.0039],\n        [-0.0056, -0.0037,  0.0067,  ..., -0.0029, -0.0140,  0.0098],\n        ...,\n        [-0.0007, -0.0081,  0.0076,  ..., -0.0086,  0.0002,  0.0036],\n        [-0.0038,  0.0151, -0.0027,  ..., -0.0054,  0.0113, -0.0060],\n        [ 0.0021, -0.0138,  0.0064,  ...,  0.0074, -0.0056,  0.0061]]), 'transformer.encoder.layers.0.self_attention.dense.lora_B.default.weight': tensor([[-1.1965e-03,  1.4480e-04,  8.8226e-04,  ..., -1.8866e-04,\n          1.3488e-03, -1.0850e-03],\n        [-3.8964e-04,  6.3681e-04,  4.0231e-04,  ..., -1.8807e-04,\n          3.5878e-04, -3.6624e-04],\n        [ 7.2240e-04, -6.7094e-05, -3.8705e-04,  ...,  1.7455e-04,\n         -6.9254e-04,  5.2197e-04],\n        ...,\n        [-4.2148e-04,  1.0947e-04,  2.0361e-04,  ..., -6.0091e-05,\n          5.3158e-04, -4.1960e-04],\n        [-6.6512e-04,  4.1957e-04,  4.3071e-04,  ..., -1.2933e-04,\n          6.3954e-04, -7.1288e-04],\n        [-1.6314e-04, -1.3906e-04,  8.7607e-05,  ...,  5.3411e-05,\n         -5.0098e-06, -7.6056e-05]]), 'transformer.encoder.layers.0.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0048,  0.0053, -0.0027,  ..., -0.0023,  0.0088, -0.0050],\n        [-0.0104, -0.0116, -0.0122,  ..., -0.0008,  0.0079,  0.0129],\n        [ 0.0066, -0.0071, -0.0114,  ..., -0.0010,  0.0143, -0.0012],\n        ...,\n        [-0.0014,  0.0109,  0.0039,  ..., -0.0075, -0.0028, -0.0124],\n        [-0.0116,  0.0113, -0.0071,  ..., -0.0072,  0.0131,  0.0056],\n        [-0.0062,  0.0052,  0.0108,  ..., -0.0145,  0.0133, -0.0067]]), 'transformer.encoder.layers.0.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 2.0481e-03, -1.8514e-03, -5.4739e-04,  ..., -1.4729e-03,\n          2.0894e-03, -1.4946e-03],\n        [ 1.6053e-03, -1.2336e-03,  9.0238e-05,  ..., -1.4112e-03,\n          1.6202e-03, -1.1312e-03],\n        [-4.5015e-04,  6.2350e-04,  5.7441e-04,  ...,  3.6895e-04,\n         -4.8282e-04,  3.1115e-04],\n        ...,\n        [ 7.9744e-04, -7.9104e-04, -3.1055e-04,  ..., -5.5636e-04,\n          8.1501e-04, -6.4725e-04],\n        [-2.0546e-04,  2.4347e-04,  2.3864e-04,  ...,  2.0709e-04,\n         -2.3876e-04,  1.7319e-04],\n        [ 5.8895e-04, -5.2807e-04, -3.1149e-04,  ..., -4.5742e-04,\n          6.6830e-04, -3.7791e-04]]), 'transformer.encoder.layers.0.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 0.0002,  0.0015,  0.0058,  ..., -0.0058, -0.0057,  0.0068],\n        [-0.0037, -0.0029, -0.0007,  ..., -0.0056, -0.0020, -0.0060],\n        [-0.0062, -0.0038,  0.0051,  ..., -0.0034, -0.0023,  0.0055],\n        ...,\n        [-0.0034, -0.0016,  0.0053,  ...,  0.0004,  0.0058,  0.0027],\n        [-0.0086,  0.0022, -0.0010,  ...,  0.0084,  0.0016, -0.0035],\n        [-0.0051,  0.0062, -0.0015,  ..., -0.0063,  0.0078,  0.0029]]), 'transformer.encoder.layers.0.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-5.7764e-04,  7.7479e-04, -8.8228e-04,  ...,  6.1373e-04,\n          7.6783e-05, -1.9260e-04],\n        [-5.4172e-04, -1.3321e-03, -2.9181e-04,  ...,  5.1719e-04,\n          1.8405e-03,  9.4169e-04],\n        [ 5.9705e-05,  8.8221e-04, -2.8406e-05,  ..., -1.9229e-04,\n         -9.6040e-04, -5.0410e-04],\n        ...,\n        [-1.9168e-03, -1.2745e-03, -2.2881e-03,  ...,  2.1121e-03,\n          3.0112e-03,  9.7206e-04],\n        [-4.2428e-04,  3.2990e-04, -7.0088e-04,  ...,  4.4781e-04,\n          3.2522e-04, -3.2057e-05],\n        [-1.4459e-04,  6.5754e-04, -6.0808e-04,  ...,  5.1269e-04,\n         -1.1299e-03, -4.5203e-04]]), 'transformer.encoder.layers.1.self_attention.query_key_value.lora_A.default.weight': tensor([[-8.0590e-04, -9.5482e-03, -6.0440e-03,  ..., -1.4955e-02,\n         -1.2496e-02,  3.8684e-03],\n        [-7.0838e-06,  2.3654e-03, -7.5290e-03,  ..., -1.0108e-02,\n          2.0806e-03, -2.8764e-03],\n        [ 1.3317e-02,  1.2516e-04,  1.6878e-03,  ..., -9.4807e-03,\n          2.9396e-03,  7.8167e-03],\n        ...,\n        [ 1.5289e-02,  3.8744e-03,  1.0777e-02,  ..., -7.0568e-03,\n          2.3444e-03,  1.1665e-02],\n        [-1.3252e-02,  1.0380e-02,  2.3957e-03,  ..., -1.1790e-02,\n         -1.2811e-02,  1.8812e-03],\n        [ 1.2411e-02, -2.1437e-03,  9.2814e-03,  ...,  1.8328e-03,\n          2.2927e-03, -1.2700e-02]]), 'transformer.encoder.layers.1.self_attention.query_key_value.lora_B.default.weight': tensor([[-2.5988e-04,  5.4453e-04,  2.7770e-04,  ...,  3.5234e-04,\n         -7.9655e-04, -8.6673e-04],\n        [ 4.9933e-04, -9.0353e-04, -4.3299e-04,  ..., -5.6816e-04,\n          1.4502e-03,  1.0899e-03],\n        [ 3.7987e-04, -6.6791e-04, -4.4429e-04,  ..., -2.2110e-04,\n          9.4394e-04,  6.2968e-04],\n        ...,\n        [-3.7852e-04,  1.0612e-04, -3.3604e-04,  ..., -4.9468e-04,\n         -5.5421e-05,  3.0145e-04],\n        [-4.3749e-04,  1.5011e-03,  1.1337e-03,  ...,  3.9833e-04,\n         -1.8603e-03, -3.0263e-04],\n        [-1.4466e-03,  1.0331e-03,  1.9687e-04,  ..., -1.6552e-03,\n         -1.2075e-03,  4.6883e-04]]), 'transformer.encoder.layers.1.self_attention.dense.lora_A.default.weight': tensor([[ 0.0095,  0.0133,  0.0008,  ..., -0.0086,  0.0118, -0.0145],\n        [-0.0077, -0.0073,  0.0107,  ...,  0.0053,  0.0123, -0.0005],\n        [-0.0032,  0.0125, -0.0004,  ..., -0.0003,  0.0064, -0.0004],\n        ...,\n        [ 0.0063,  0.0041,  0.0006,  ..., -0.0102,  0.0048, -0.0087],\n        [-0.0029, -0.0048,  0.0088,  ...,  0.0024,  0.0091, -0.0115],\n        [-0.0121, -0.0112,  0.0046,  ...,  0.0077,  0.0136,  0.0136]]), 'transformer.encoder.layers.1.self_attention.dense.lora_B.default.weight': tensor([[ 1.3932e-03, -2.0844e-04, -6.5071e-04,  ..., -1.4099e-03,\n          1.5980e-03, -1.7118e-03],\n        [-9.5875e-04,  6.0114e-04,  1.3225e-04,  ...,  4.3118e-04,\n         -1.1993e-03,  1.2900e-03],\n        [ 4.1944e-05,  5.4312e-04, -6.0617e-04,  ..., -6.2978e-04,\n          2.1985e-04, -4.8822e-04],\n        ...,\n        [-2.0708e-03,  4.6802e-05,  6.5355e-04,  ...,  1.2539e-03,\n         -1.7053e-03,  2.0039e-03],\n        [-3.3422e-04,  5.0489e-05, -1.1223e-04,  ...,  1.7368e-04,\n         -3.5169e-05,  2.9243e-04],\n        [ 1.1843e-03, -7.5603e-04,  3.8078e-04,  ..., -2.0953e-04,\n          7.0565e-04, -1.1152e-03]]), 'transformer.encoder.layers.1.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 4.2574e-05,  6.9095e-03,  9.4727e-03,  ...,  7.3516e-03,\n         -8.7387e-04,  1.2454e-02],\n        [-7.2360e-04, -9.5343e-03,  5.9918e-03,  ...,  1.5467e-03,\n         -7.8105e-03,  7.8826e-03],\n        [ 3.4805e-03,  7.7569e-03, -1.0221e-03,  ..., -3.1349e-04,\n          1.3530e-02,  6.1188e-03],\n        ...,\n        [ 9.3594e-03, -2.2341e-03, -1.1668e-02,  ...,  1.0388e-02,\n         -1.2172e-02,  1.1298e-02],\n        [-1.5105e-02, -7.4348e-03, -2.3848e-03,  ..., -4.4047e-03,\n         -7.7516e-04, -7.2527e-03],\n        [ 2.2427e-03, -1.0660e-02, -1.1513e-02,  ...,  1.1360e-02,\n          1.4848e-02, -1.5651e-02]]), 'transformer.encoder.layers.1.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-9.1027e-06,  1.1067e-04, -8.0557e-05,  ..., -1.9798e-04,\n         -9.5059e-05, -1.7003e-04],\n        [ 3.1255e-04,  1.9624e-03, -1.8978e-03,  ..., -1.2511e-03,\n         -1.2885e-03, -5.9977e-04],\n        [ 2.9808e-05, -4.4301e-04,  3.9645e-04,  ..., -1.0589e-04,\n          4.6105e-04,  1.2024e-04],\n        ...,\n        [-1.9787e-05,  2.3141e-04,  3.2328e-04,  ...,  3.5476e-04,\n         -1.3527e-04,  4.6960e-04],\n        [ 4.9983e-04, -8.9307e-04,  1.1447e-03,  ...,  8.9988e-05,\n          6.9178e-04,  1.1394e-03],\n        [-6.0436e-04,  1.3826e-03, -1.3548e-03,  ..., -1.8424e-04,\n         -9.7425e-04, -1.2190e-03]]), 'transformer.encoder.layers.1.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 0.0085, -0.0073,  0.0002,  ...,  0.0077, -0.0043, -0.0030],\n        [-0.0066, -0.0048, -0.0025,  ..., -0.0033,  0.0007, -0.0047],\n        [-0.0024, -0.0020,  0.0081,  ..., -0.0038,  0.0039,  0.0070],\n        ...,\n        [ 0.0026, -0.0080,  0.0014,  ..., -0.0004, -0.0073, -0.0055],\n        [-0.0011, -0.0038, -0.0034,  ..., -0.0045, -0.0063,  0.0038],\n        [ 0.0039, -0.0057, -0.0010,  ...,  0.0071,  0.0008,  0.0018]]), 'transformer.encoder.layers.1.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-3.3085e-04, -9.5911e-05, -4.1804e-04,  ..., -3.5538e-04,\n          1.2205e-03,  6.2754e-04],\n        [-5.9155e-04,  4.8538e-04,  5.5963e-04,  ..., -8.0710e-04,\n          7.3439e-05,  8.1326e-04],\n        [-1.5020e-03, -2.1482e-05, -6.7011e-04,  ..., -1.8633e-03,\n          2.8615e-03,  2.0933e-03],\n        ...,\n        [-3.0336e-04,  1.1291e-03,  2.4933e-03,  ..., -7.6965e-04,\n         -1.1890e-03,  1.0110e-03],\n        [-4.5049e-04, -2.7610e-04,  1.7397e-04,  ..., -6.1686e-04,\n          4.7767e-04,  1.5009e-03],\n        [ 1.1496e-03, -2.4218e-04, -5.3023e-04,  ...,  1.6775e-03,\n         -9.3569e-04, -1.7822e-03]]), 'transformer.encoder.layers.2.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0035,  0.0136, -0.0048,  ...,  0.0120,  0.0095, -0.0103],\n        [-0.0031, -0.0150,  0.0011,  ..., -0.0078, -0.0148,  0.0100],\n        [ 0.0071,  0.0041, -0.0047,  ...,  0.0024, -0.0153,  0.0098],\n        ...,\n        [ 0.0090,  0.0021,  0.0110,  ...,  0.0020, -0.0035,  0.0030],\n        [ 0.0068, -0.0017, -0.0076,  ..., -0.0004,  0.0032,  0.0128],\n        [ 0.0054, -0.0067,  0.0099,  ..., -0.0047, -0.0056, -0.0127]]), 'transformer.encoder.layers.2.self_attention.query_key_value.lora_B.default.weight': tensor([[ 1.3614e-03,  6.5751e-04, -8.6130e-04,  ...,  1.0904e-03,\n         -1.4313e-03,  9.6681e-04],\n        [-8.4969e-04, -6.9205e-04,  8.1563e-04,  ..., -8.3775e-04,\n          8.4680e-04, -2.1653e-04],\n        [-4.8386e-04,  2.1315e-05, -6.7721e-06,  ..., -3.6016e-04,\n          4.5396e-04, -6.6956e-04],\n        ...,\n        [-1.8163e-03,  5.1555e-04,  1.2787e-03,  ..., -7.2188e-04,\n          1.9863e-03, -2.9028e-03],\n        [-1.1747e-03, -7.7203e-04,  1.4138e-03,  ..., -1.0785e-03,\n          1.4972e-03, -1.1501e-03],\n        [ 3.7653e-04, -9.0919e-05,  3.4825e-04,  ...,  1.1447e-04,\n         -4.3005e-04, -2.1769e-05]]), 'transformer.encoder.layers.2.self_attention.dense.lora_A.default.weight': tensor([[-0.0013,  0.0112,  0.0008,  ..., -0.0082, -0.0094,  0.0153],\n        [-0.0031, -0.0033, -0.0007,  ..., -0.0153, -0.0007, -0.0093],\n        [-0.0100, -0.0020,  0.0015,  ..., -0.0110, -0.0068,  0.0005],\n        ...,\n        [-0.0037,  0.0069,  0.0041,  ...,  0.0109,  0.0083, -0.0120],\n        [-0.0158, -0.0110, -0.0018,  ..., -0.0082, -0.0073, -0.0035],\n        [ 0.0156,  0.0120, -0.0046,  ...,  0.0070, -0.0142, -0.0095]]), 'transformer.encoder.layers.2.self_attention.dense.lora_B.default.weight': tensor([[ 5.3298e-04, -1.0029e-03, -8.1091e-04,  ..., -6.6978e-04,\n          6.6700e-04, -1.7047e-04],\n        [-1.2372e-03,  2.1877e-03,  1.3997e-03,  ...,  1.4274e-03,\n         -1.5618e-03,  8.1648e-04],\n        [-3.1180e-05,  8.6280e-05, -1.5858e-04,  ..., -1.3977e-04,\n         -3.6186e-04,  1.2968e-04],\n        ...,\n        [-2.5874e-03,  4.0538e-03,  2.2620e-03,  ...,  2.9388e-03,\n         -2.0140e-03,  2.3446e-03],\n        [-1.4842e-04,  4.7374e-04, -3.7345e-05,  ...,  4.1066e-04,\n          5.5539e-04,  4.7820e-04],\n        [ 1.1349e-03, -1.4748e-03, -1.1807e-03,  ..., -1.0878e-03,\n          1.4222e-03, -6.8708e-04]]), 'transformer.encoder.layers.2.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0142, -0.0067, -0.0075,  ..., -0.0011,  0.0114,  0.0076],\n        [-0.0062, -0.0036, -0.0043,  ...,  0.0030, -0.0138, -0.0086],\n        [ 0.0025,  0.0036,  0.0031,  ...,  0.0148, -0.0127,  0.0078],\n        ...,\n        [ 0.0033, -0.0090, -0.0045,  ...,  0.0010,  0.0044, -0.0092],\n        [ 0.0020, -0.0141, -0.0112,  ..., -0.0037, -0.0112,  0.0062],\n        [-0.0022, -0.0067,  0.0150,  ...,  0.0112, -0.0016, -0.0044]]), 'transformer.encoder.layers.2.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-0.0008, -0.0003, -0.0008,  ..., -0.0011,  0.0011,  0.0011],\n        [ 0.0013,  0.0002, -0.0003,  ...,  0.0002,  0.0001, -0.0015],\n        [ 0.0007,  0.0005, -0.0009,  ..., -0.0005,  0.0006, -0.0003],\n        ...,\n        [-0.0004,  0.0002,  0.0020,  ...,  0.0020, -0.0021, -0.0003],\n        [ 0.0004,  0.0004,  0.0004,  ...,  0.0008, -0.0008, -0.0011],\n        [-0.0027, -0.0015,  0.0035,  ...,  0.0030, -0.0038,  0.0002]]), 'transformer.encoder.layers.2.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0075,  0.0025,  0.0010,  ...,  0.0034, -0.0066,  0.0025],\n        [ 0.0015,  0.0025,  0.0064,  ...,  0.0060,  0.0002, -0.0082],\n        [-0.0050, -0.0018,  0.0042,  ..., -0.0015, -0.0019,  0.0036],\n        ...,\n        [-0.0056, -0.0055, -0.0047,  ..., -0.0024,  0.0057,  0.0082],\n        [-0.0086,  0.0048, -0.0064,  ...,  0.0039,  0.0077,  0.0057],\n        [ 0.0059, -0.0067,  0.0007,  ..., -0.0041, -0.0002,  0.0086]]), 'transformer.encoder.layers.2.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-2.9556e-04, -1.0530e-03, -1.1850e-03,  ..., -1.6462e-04,\n         -6.3404e-04,  3.9213e-04],\n        [ 7.8705e-04,  2.3556e-03,  1.8171e-03,  ..., -5.2760e-04,\n          1.0361e-03, -1.5743e-03],\n        [-4.3855e-05,  2.7057e-04, -2.2261e-04,  ...,  5.0462e-04,\n          4.7290e-04,  4.0119e-05],\n        ...,\n        [ 1.5503e-03,  3.8899e-03,  3.4415e-03,  ..., -1.7694e-03,\n         -8.7097e-04, -3.2421e-03],\n        [-6.8173e-05,  2.8777e-04,  1.0375e-04,  ..., -4.5498e-04,\n         -7.9820e-04, -2.8194e-04],\n        [-4.9455e-04, -2.0769e-03, -1.5734e-03,  ...,  1.2308e-04,\n         -8.8601e-04,  5.0284e-04]]), 'transformer.encoder.layers.3.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0035, -0.0139,  0.0078,  ...,  0.0135,  0.0002, -0.0157],\n        [ 0.0058, -0.0069, -0.0119,  ...,  0.0127, -0.0073, -0.0050],\n        [-0.0086, -0.0015,  0.0119,  ...,  0.0021, -0.0103,  0.0119],\n        ...,\n        [ 0.0124,  0.0094,  0.0098,  ...,  0.0089,  0.0135,  0.0139],\n        [-0.0141,  0.0046,  0.0002,  ...,  0.0136, -0.0108, -0.0040],\n        [ 0.0063, -0.0098, -0.0116,  ..., -0.0120,  0.0078,  0.0004]]), 'transformer.encoder.layers.3.self_attention.query_key_value.lora_B.default.weight': tensor([[-7.7099e-04, -1.4911e-03, -8.8875e-04,  ...,  1.2193e-03,\n          1.3885e-05,  1.2949e-03],\n        [-5.1868e-04, -1.0539e-03, -5.2899e-04,  ...,  7.7394e-04,\n          1.0740e-04,  1.1023e-03],\n        [-2.4302e-04, -1.5688e-04, -1.3986e-05,  ...,  3.5951e-04,\n         -3.9522e-05, -3.5119e-05],\n        ...,\n        [-8.8802e-04, -1.3588e-03, -7.5765e-04,  ...,  4.2876e-04,\n         -4.2034e-04,  1.1926e-03],\n        [ 1.4840e-03,  2.9300e-03,  3.5715e-03,  ...,  1.9235e-03,\n          1.7294e-03, -3.6856e-03],\n        [ 6.9498e-04,  1.2170e-03,  7.5425e-04,  ..., -8.7167e-04,\n          2.5764e-04, -1.3399e-03]]), 'transformer.encoder.layers.3.self_attention.dense.lora_A.default.weight': tensor([[-0.0079, -0.0079, -0.0091,  ...,  0.0048,  0.0027,  0.0131],\n        [-0.0024,  0.0144, -0.0086,  ...,  0.0166, -0.0049, -0.0086],\n        [-0.0076,  0.0061, -0.0040,  ..., -0.0089, -0.0073, -0.0117],\n        ...,\n        [ 0.0123, -0.0092, -0.0139,  ...,  0.0007,  0.0080,  0.0117],\n        [ 0.0098,  0.0056,  0.0029,  ...,  0.0071, -0.0004, -0.0106],\n        [-0.0031,  0.0106,  0.0061,  ..., -0.0009,  0.0052, -0.0125]]), 'transformer.encoder.layers.3.self_attention.dense.lora_B.default.weight': tensor([[ 0.0009,  0.0012,  0.0012,  ...,  0.0011,  0.0009,  0.0012],\n        [-0.0010, -0.0014, -0.0017,  ..., -0.0019, -0.0017, -0.0013],\n        [-0.0001,  0.0004, -0.0002,  ..., -0.0004, -0.0007,  0.0001],\n        ...,\n        [-0.0034, -0.0045, -0.0049,  ..., -0.0041, -0.0048, -0.0039],\n        [ 0.0008,  0.0005,  0.0011,  ...,  0.0011,  0.0009,  0.0008],\n        [-0.0014, -0.0010, -0.0021,  ..., -0.0017, -0.0021, -0.0011]]), 'transformer.encoder.layers.3.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0071,  0.0141,  0.0057,  ..., -0.0155,  0.0095,  0.0153],\n        [-0.0009,  0.0006,  0.0033,  ...,  0.0012,  0.0034,  0.0131],\n        [-0.0132, -0.0019, -0.0159,  ..., -0.0073,  0.0090, -0.0053],\n        ...,\n        [ 0.0153,  0.0006,  0.0023,  ..., -0.0086,  0.0092, -0.0053],\n        [-0.0015,  0.0032, -0.0007,  ...,  0.0061, -0.0085, -0.0063],\n        [-0.0057, -0.0033,  0.0023,  ..., -0.0020,  0.0147, -0.0033]]), 'transformer.encoder.layers.3.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-1.3057e-03, -1.0205e-03,  1.3679e-03,  ..., -1.6525e-03,\n          6.9507e-04, -1.4372e-03],\n        [ 5.5304e-04,  8.1452e-05, -3.2280e-04,  ...,  6.1512e-04,\n         -3.1904e-04,  7.5497e-04],\n        [-1.2011e-03,  1.6269e-04, -5.8376e-05,  ..., -7.4828e-04,\n          8.1546e-04, -1.6878e-03],\n        ...,\n        [-1.5437e-03, -7.2963e-04,  1.2699e-03,  ..., -1.7635e-03,\n          4.3091e-04, -8.8266e-04],\n        [ 1.1791e-03,  1.6507e-03, -1.6027e-03,  ...,  1.7771e-03,\n          8.1688e-05,  6.3108e-04],\n        [-7.6837e-04, -4.4904e-04,  3.0336e-04,  ..., -7.4607e-04,\n          7.0796e-05, -9.1293e-04]]), 'transformer.encoder.layers.3.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 0.0028, -0.0051, -0.0042,  ...,  0.0046,  0.0025,  0.0082],\n        [ 0.0083,  0.0069, -0.0013,  ..., -0.0083,  0.0034,  0.0004],\n        [-0.0021,  0.0071,  0.0052,  ...,  0.0071,  0.0064,  0.0044],\n        ...,\n        [-0.0049,  0.0061,  0.0082,  ...,  0.0025,  0.0061,  0.0058],\n        [ 0.0021, -0.0002,  0.0007,  ..., -0.0017,  0.0083, -0.0076],\n        [ 0.0044,  0.0074, -0.0055,  ...,  0.0022, -0.0027, -0.0047]]), 'transformer.encoder.layers.3.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 9.3784e-04, -4.5998e-04, -9.2446e-04,  ..., -5.7726e-04,\n          1.2203e-04, -4.4953e-04],\n        [ 5.7920e-04,  1.9021e-04, -2.5086e-04,  ..., -5.4999e-05,\n         -7.2435e-04,  1.2140e-03],\n        [ 2.7027e-04, -7.0201e-04,  1.4860e-04,  ..., -1.2674e-04,\n         -7.4786e-04,  2.9005e-04],\n        ...,\n        [ 3.5949e-03, -2.5114e-03, -2.7545e-03,  ..., -2.5001e-03,\n         -2.3351e-03, -1.2062e-03],\n        [-3.9296e-04, -2.2709e-05,  3.9173e-04,  ...,  7.4954e-04,\n         -3.2673e-04,  4.4593e-04],\n        [ 1.6754e-03, -1.4404e-03, -1.2419e-03,  ..., -9.7007e-04,\n         -4.0013e-04, -1.4498e-03]]), 'transformer.encoder.layers.4.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0042,  0.0057, -0.0119,  ..., -0.0002,  0.0015,  0.0146],\n        [-0.0091, -0.0118, -0.0036,  ..., -0.0086,  0.0064,  0.0016],\n        [ 0.0071, -0.0005,  0.0070,  ..., -0.0136,  0.0088, -0.0135],\n        ...,\n        [-0.0129,  0.0016, -0.0024,  ..., -0.0080,  0.0018, -0.0013],\n        [-0.0058, -0.0151, -0.0070,  ..., -0.0026, -0.0124,  0.0051],\n        [ 0.0089,  0.0095,  0.0041,  ..., -0.0101, -0.0085,  0.0137]]), 'transformer.encoder.layers.4.self_attention.query_key_value.lora_B.default.weight': tensor([[ 3.8899e-04,  7.1604e-04,  3.7489e-04,  ..., -7.3902e-04,\n          1.1345e-03,  2.1496e-04],\n        [-5.8980e-04,  6.5585e-04, -5.8258e-04,  ...,  9.9152e-04,\n         -1.4964e-03,  1.7667e-04],\n        [-2.0282e-04,  4.5006e-04, -3.7025e-04,  ..., -1.3552e-05,\n          9.4815e-05,  2.4887e-04],\n        ...,\n        [ 1.3022e-04,  1.4978e-03,  3.1157e-04,  ..., -1.4410e-03,\n          1.5490e-03,  3.3675e-04],\n        [-1.3285e-03,  2.9267e-03, -9.0786e-04,  ..., -1.5237e-03,\n         -3.6255e-04,  7.9277e-04],\n        [ 4.8404e-04, -1.3335e-03,  1.9795e-04,  ...,  5.5391e-04,\n         -8.1289e-04, -1.4765e-03]]), 'transformer.encoder.layers.4.self_attention.dense.lora_A.default.weight': tensor([[ 0.0034,  0.0107,  0.0008,  ..., -0.0095,  0.0077, -0.0020],\n        [ 0.0048, -0.0085,  0.0026,  ..., -0.0103, -0.0020, -0.0114],\n        [ 0.0023,  0.0090,  0.0016,  ..., -0.0135, -0.0015,  0.0040],\n        ...,\n        [-0.0060,  0.0146,  0.0058,  ...,  0.0086, -0.0129,  0.0119],\n        [ 0.0111, -0.0027, -0.0037,  ..., -0.0103,  0.0056,  0.0134],\n        [-0.0086, -0.0118, -0.0142,  ..., -0.0031, -0.0009, -0.0119]]), 'transformer.encoder.layers.4.self_attention.dense.lora_B.default.weight': tensor([[-0.0001,  0.0014, -0.0003,  ..., -0.0002,  0.0009, -0.0004],\n        [ 0.0013, -0.0025,  0.0003,  ...,  0.0009,  0.0006,  0.0006],\n        [ 0.0007, -0.0012, -0.0003,  ...,  0.0005, -0.0001,  0.0002],\n        ...,\n        [ 0.0016, -0.0032,  0.0017,  ...,  0.0018,  0.0027,  0.0017],\n        [ 0.0011, -0.0008,  0.0009,  ...,  0.0005,  0.0002,  0.0010],\n        [ 0.0007, -0.0011,  0.0011,  ...,  0.0010,  0.0025,  0.0010]]), 'transformer.encoder.layers.4.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0034, -0.0068, -0.0112,  ..., -0.0093,  0.0125, -0.0070],\n        [-0.0017,  0.0110,  0.0094,  ...,  0.0110, -0.0016, -0.0023],\n        [ 0.0068, -0.0020, -0.0063,  ..., -0.0076,  0.0071,  0.0017],\n        ...,\n        [ 0.0095,  0.0006, -0.0050,  ..., -0.0057, -0.0136,  0.0059],\n        [-0.0028,  0.0149, -0.0063,  ..., -0.0010, -0.0098,  0.0005],\n        [ 0.0057,  0.0050,  0.0122,  ..., -0.0136,  0.0154,  0.0010]]), 'transformer.encoder.layers.4.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 2.3378e-04,  1.0649e-03,  5.4409e-04,  ...,  9.6847e-04,\n          9.4048e-05, -3.7533e-04],\n        [-3.0019e-04,  2.1588e-04, -2.0882e-04,  ...,  2.6311e-04,\n         -2.3875e-04, -1.6446e-04],\n        [-2.8573e-04,  4.2042e-04, -2.6946e-04,  ...,  4.3519e-05,\n         -3.3348e-04, -1.9729e-04],\n        ...,\n        [-4.3626e-04, -5.4377e-04, -8.7249e-04,  ..., -3.3167e-04,\n         -6.6911e-04, -6.0357e-04],\n        [-1.7779e-03,  2.0135e-03, -7.3531e-04,  ...,  1.4798e-03,\n         -2.0265e-03, -1.8602e-03],\n        [ 7.6866e-04, -5.3167e-04,  1.8335e-04,  ..., -5.3838e-04,\n          7.9464e-04,  4.1614e-04]]), 'transformer.encoder.layers.4.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 0.0081, -0.0013, -0.0034,  ...,  0.0030,  0.0019,  0.0065],\n        [-0.0013,  0.0034,  0.0076,  ...,  0.0071, -0.0045,  0.0024],\n        [-0.0003, -0.0057, -0.0004,  ..., -0.0077, -0.0067, -0.0053],\n        ...,\n        [ 0.0071,  0.0041,  0.0024,  ..., -0.0006,  0.0007,  0.0010],\n        [-0.0025, -0.0021, -0.0053,  ..., -0.0048,  0.0023,  0.0072],\n        [-0.0011, -0.0002, -0.0067,  ...,  0.0062, -0.0003, -0.0037]]), 'transformer.encoder.layers.4.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 8.5094e-04,  7.0783e-04, -1.0458e-03,  ..., -1.7031e-03,\n         -9.3356e-04,  2.8247e-04],\n        [-7.9222e-04, -5.8870e-04,  1.2754e-03,  ...,  1.5727e-03,\n          5.6560e-04,  2.4950e-04],\n        [-2.3153e-04, -5.3281e-04,  5.2459e-04,  ...,  9.1176e-04,\n          9.1010e-04, -1.5673e-04],\n        ...,\n        [-2.9593e-03, -2.3546e-03,  3.2004e-03,  ...,  2.2197e-03,\n          2.1051e-03, -1.6403e-03],\n        [-2.5257e-04, -2.4271e-04,  5.0754e-04,  ...,  5.5723e-04,\n          4.0237e-05, -6.9180e-04],\n        [-1.6875e-03, -9.0024e-04,  8.1975e-04,  ...,  5.7076e-04,\n          1.0781e-03, -1.4059e-03]]), 'transformer.encoder.layers.5.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0143, -0.0097,  0.0069,  ..., -0.0027,  0.0128,  0.0055],\n        [ 0.0078,  0.0145, -0.0135,  ..., -0.0013,  0.0061, -0.0047],\n        [-0.0046,  0.0066, -0.0060,  ...,  0.0147,  0.0106,  0.0149],\n        ...,\n        [ 0.0151,  0.0130,  0.0038,  ..., -0.0150, -0.0078,  0.0110],\n        [-0.0086,  0.0030,  0.0069,  ...,  0.0120,  0.0027, -0.0075],\n        [-0.0022,  0.0091,  0.0051,  ...,  0.0151,  0.0068, -0.0144]]), 'transformer.encoder.layers.5.self_attention.query_key_value.lora_B.default.weight': tensor([[ 5.2607e-04, -6.5003e-04,  2.3769e-04,  ..., -2.3105e-04,\n          5.2357e-05,  2.5540e-04],\n        [ 1.1306e-04, -2.0645e-04,  6.9574e-05,  ..., -2.0445e-05,\n          1.2724e-04,  3.3921e-04],\n        [ 5.1945e-05,  6.3758e-04, -5.9905e-04,  ..., -1.4194e-04,\n         -3.1760e-04, -4.2597e-04],\n        ...,\n        [ 4.9135e-04, -5.2964e-04,  6.9865e-04,  ...,  1.5736e-03,\n          1.9465e-03,  7.4437e-04],\n        [-6.5769e-04,  6.4800e-04, -9.0123e-04,  ..., -1.3382e-03,\n         -1.5778e-03, -1.0466e-03],\n        [ 8.3802e-04, -1.8204e-03,  1.2785e-03,  ...,  1.4930e-03,\n          2.1315e-03,  2.2253e-03]]), 'transformer.encoder.layers.5.self_attention.dense.lora_A.default.weight': tensor([[-0.0053,  0.0065, -0.0078,  ..., -0.0065, -0.0118, -0.0009],\n        [-0.0153, -0.0130, -0.0077,  ..., -0.0033, -0.0086, -0.0071],\n        [ 0.0146,  0.0131,  0.0030,  ..., -0.0033, -0.0103,  0.0024],\n        ...,\n        [-0.0097,  0.0104, -0.0133,  ..., -0.0141,  0.0076,  0.0053],\n        [ 0.0131,  0.0158, -0.0069,  ...,  0.0102, -0.0117, -0.0059],\n        [-0.0088,  0.0111, -0.0132,  ..., -0.0137, -0.0005, -0.0037]]), 'transformer.encoder.layers.5.self_attention.dense.lora_B.default.weight': tensor([[ 7.9292e-04, -1.4459e-04,  8.1628e-04,  ..., -7.4497e-04,\n          1.4922e-03, -1.9227e-03],\n        [-1.1619e-03, -4.3288e-06, -7.9592e-04,  ...,  4.5838e-04,\n         -9.8958e-04,  9.2415e-04],\n        [-1.3167e-04,  6.8927e-04, -2.1713e-04,  ...,  1.0859e-03,\n         -6.2147e-04,  1.5625e-03],\n        ...,\n        [-2.3497e-03, -1.2359e-03, -2.4201e-03,  ...,  1.0336e-03,\n          3.8217e-04,  4.0477e-04],\n        [ 2.9844e-04, -1.6794e-05, -2.6685e-04,  ...,  1.8968e-04,\n          5.6971e-04, -2.4999e-05],\n        [-1.8570e-03, -1.8663e-03, -8.3987e-04,  ...,  4.9193e-04,\n          1.4184e-03, -6.0230e-04]]), 'transformer.encoder.layers.5.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0074,  0.0022, -0.0030,  ..., -0.0010,  0.0047,  0.0068],\n        [-0.0013,  0.0093,  0.0019,  ...,  0.0063,  0.0101,  0.0104],\n        [ 0.0116, -0.0047, -0.0122,  ..., -0.0105,  0.0041,  0.0065],\n        ...,\n        [ 0.0048,  0.0110, -0.0105,  ..., -0.0128, -0.0136, -0.0121],\n        [-0.0108, -0.0086, -0.0155,  ...,  0.0083,  0.0149,  0.0025],\n        [-0.0145, -0.0084, -0.0068,  ..., -0.0120, -0.0020,  0.0057]]), 'transformer.encoder.layers.5.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-2.4933e-04, -1.7672e-04, -8.6593e-05,  ..., -1.9124e-04,\n         -3.1783e-04, -2.7167e-05],\n        [ 6.5058e-04, -8.1182e-04,  7.6235e-04,  ..., -4.9778e-04,\n         -8.8421e-04, -6.4972e-04],\n        [-1.6203e-03,  6.5142e-04, -7.9007e-04,  ...,  6.7747e-04,\n          8.0018e-04,  4.4228e-04],\n        ...,\n        [-2.5766e-04,  4.1227e-05, -2.3763e-04,  ...,  4.5338e-04,\n          4.0170e-04,  9.0051e-04],\n        [ 3.2066e-04, -1.0373e-04,  2.9009e-04,  ...,  1.1879e-04,\n         -3.0400e-04, -1.7471e-04],\n        [ 1.3662e-03, -9.0642e-04,  8.8100e-04,  ..., -1.3811e-04,\n         -1.0268e-03, -5.6059e-04]]), 'transformer.encoder.layers.5.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0010, -0.0072,  0.0085,  ..., -0.0064, -0.0072, -0.0030],\n        [ 0.0081, -0.0062,  0.0056,  ...,  0.0009, -0.0055,  0.0037],\n        [-0.0068, -0.0082, -0.0077,  ...,  0.0050, -0.0016,  0.0078],\n        ...,\n        [-0.0057, -0.0007, -0.0083,  ...,  0.0055,  0.0076, -0.0083],\n        [-0.0029, -0.0017,  0.0069,  ...,  0.0080, -0.0023, -0.0016],\n        [-0.0026, -0.0025,  0.0019,  ..., -0.0017,  0.0078, -0.0025]]), 'transformer.encoder.layers.5.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 0.0007, -0.0001,  0.0004,  ..., -0.0004, -0.0008, -0.0004],\n        [-0.0012, -0.0001, -0.0005,  ...,  0.0010,  0.0009,  0.0003],\n        [ 0.0008,  0.0004,  0.0003,  ..., -0.0003, -0.0006, -0.0007],\n        ...,\n        [-0.0015, -0.0011, -0.0008,  ...,  0.0013,  0.0010,  0.0010],\n        [ 0.0014,  0.0004,  0.0007,  ..., -0.0012, -0.0012, -0.0013],\n        [-0.0008, -0.0007, -0.0006,  ...,  0.0010,  0.0007,  0.0017]]), 'transformer.encoder.layers.6.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0020,  0.0060,  0.0033,  ..., -0.0138, -0.0140,  0.0075],\n        [-0.0086,  0.0150,  0.0097,  ...,  0.0032, -0.0148, -0.0083],\n        [ 0.0115,  0.0142,  0.0151,  ...,  0.0131,  0.0024,  0.0030],\n        ...,\n        [ 0.0102, -0.0148, -0.0075,  ..., -0.0023, -0.0043, -0.0026],\n        [-0.0006,  0.0086, -0.0056,  ..., -0.0137,  0.0108,  0.0031],\n        [ 0.0131,  0.0113, -0.0032,  ..., -0.0102, -0.0110,  0.0070]]), 'transformer.encoder.layers.6.self_attention.query_key_value.lora_B.default.weight': tensor([[ 2.8078e-05,  3.7125e-05, -8.7672e-05,  ...,  5.3629e-05,\n          4.5494e-05,  5.4597e-06],\n        [-3.7273e-05,  1.1302e-04, -1.0855e-04,  ...,  6.0215e-05,\n          2.1101e-04, -3.3241e-04],\n        [ 1.2945e-04, -1.5104e-04, -2.1526e-04,  ...,  6.5575e-05,\n          1.0019e-04, -3.3723e-05],\n        ...,\n        [-3.4891e-04, -8.1375e-04,  4.6395e-04,  ..., -6.0495e-04,\n         -8.8211e-04,  1.5329e-03],\n        [-4.4970e-04, -4.0897e-04,  4.5196e-04,  ..., -9.2292e-04,\n         -5.1154e-04,  1.4563e-03],\n        [ 6.9169e-05, -5.0146e-04, -3.3553e-04,  ...,  2.2370e-04,\n         -2.8447e-04,  2.8581e-04]]), 'transformer.encoder.layers.6.self_attention.dense.lora_A.default.weight': tensor([[ 4.4531e-05,  8.4152e-03,  8.4896e-03,  ..., -2.1782e-03,\n          1.2405e-02, -9.4109e-03],\n        [ 6.8290e-03, -1.1812e-03,  9.6986e-03,  ..., -6.6660e-03,\n          8.2449e-03,  1.1992e-02],\n        [-9.1001e-03, -6.4047e-03, -1.1337e-02,  ..., -6.4688e-03,\n         -1.3213e-02,  1.0440e-02],\n        ...,\n        [-9.2428e-03,  1.2099e-03,  7.6806e-03,  ..., -6.5981e-03,\n          4.6710e-03,  3.9324e-03],\n        [ 7.5670e-03,  1.5382e-02,  8.6050e-03,  ..., -1.5471e-02,\n         -7.4801e-03,  5.4316e-03],\n        [-1.0323e-02, -7.8930e-03, -1.0712e-02,  ...,  1.3100e-02,\n          1.0480e-02,  7.6062e-03]]), 'transformer.encoder.layers.6.self_attention.dense.lora_B.default.weight': tensor([[-3.4087e-04, -3.1922e-04, -6.3804e-05,  ..., -2.7171e-04,\n          3.8991e-04, -1.6649e-04],\n        [-3.6716e-05,  1.1127e-03,  9.4426e-04,  ...,  1.7305e-03,\n         -5.9514e-04,  1.1708e-03],\n        [ 1.7313e-05,  4.9003e-04,  9.6995e-05,  ...,  4.7001e-04,\n         -6.3891e-04,  4.3660e-04],\n        ...,\n        [ 7.3771e-04,  6.8412e-04,  1.4090e-03,  ...,  1.9912e-03,\n         -3.9764e-04,  7.4382e-04],\n        [-7.3119e-04, -7.2704e-04, -3.4238e-04,  ..., -8.3530e-04,\n          1.4420e-04,  3.0948e-04],\n        [ 3.0896e-04,  2.9216e-04, -1.1185e-04,  ...,  5.5641e-04,\n         -5.1594e-05, -3.6831e-04]]), 'transformer.encoder.layers.6.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 1.6583e-03, -3.9050e-05,  1.9899e-03,  ...,  4.3700e-03,\n          1.9875e-03,  1.3795e-02],\n        [ 7.8898e-03, -7.6347e-03,  7.5006e-03,  ..., -2.8661e-03,\n          7.7928e-03, -3.6805e-03],\n        [ 7.1951e-03, -8.5110e-03,  1.5840e-02,  ...,  8.1677e-03,\n          1.2966e-02, -3.1125e-03],\n        ...,\n        [ 4.4429e-03,  7.5799e-03,  9.9228e-03,  ...,  2.3507e-03,\n         -1.3939e-02,  1.4655e-03],\n        [-5.8848e-03, -5.2577e-03, -5.8326e-03,  ..., -9.1661e-03,\n          2.7608e-03, -2.1222e-03],\n        [ 7.2465e-03, -4.4450e-03, -1.2723e-02,  ..., -1.6004e-03,\n          1.3652e-02, -2.5448e-03]]), 'transformer.encoder.layers.6.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 4.1132e-05,  1.1611e-03, -1.1990e-03,  ...,  3.5749e-04,\n         -1.2255e-03, -9.6639e-04],\n        [-4.7965e-06,  3.0665e-04, -5.2274e-04,  ...,  8.8450e-05,\n         -8.1592e-04, -5.9422e-04],\n        [ 2.8546e-04, -1.4270e-04,  8.4536e-05,  ...,  2.5276e-05,\n          4.3521e-04,  2.4300e-04],\n        ...,\n        [ 5.0348e-04,  1.1181e-04,  1.3168e-04,  ..., -1.7020e-04,\n          4.2110e-05,  7.2996e-05],\n        [ 4.2983e-05, -5.2259e-04,  8.8524e-04,  ..., -3.7241e-04,\n          7.3379e-04,  6.9271e-04],\n        [-1.1626e-03, -1.8197e-03,  1.3626e-03,  ..., -1.4003e-03,\n          2.1479e-03,  1.5629e-03]]), 'transformer.encoder.layers.6.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0027,  0.0049, -0.0077,  ..., -0.0028, -0.0083,  0.0065],\n        [-0.0034, -0.0058,  0.0015,  ...,  0.0048,  0.0034, -0.0062],\n        [ 0.0077, -0.0055,  0.0053,  ..., -0.0070, -0.0013,  0.0049],\n        ...,\n        [ 0.0042,  0.0056,  0.0010,  ..., -0.0078, -0.0012,  0.0058],\n        [-0.0006,  0.0057, -0.0052,  ..., -0.0008,  0.0028,  0.0020],\n        [ 0.0044,  0.0058, -0.0013,  ...,  0.0021,  0.0087,  0.0085]]), 'transformer.encoder.layers.6.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-5.9063e-04, -3.8143e-04,  8.2192e-04,  ..., -8.0236e-04,\n          6.5689e-04, -3.2116e-04],\n        [ 9.7271e-04,  7.4861e-04, -9.0929e-04,  ...,  1.3038e-03,\n         -5.7977e-04,  2.3876e-04],\n        [ 5.3761e-04,  1.3689e-04, -5.3637e-04,  ...,  1.2739e-03,\n         -5.5635e-04,  8.7916e-04],\n        ...,\n        [ 2.5768e-04,  1.1322e-03, -9.6528e-04,  ...,  7.1644e-04,\n         -6.8627e-04,  4.1111e-04],\n        [-1.1440e-03, -2.1395e-03,  1.9740e-03,  ..., -1.2365e-03,\n          2.5745e-03, -1.7232e-04],\n        [ 4.0072e-05,  1.0681e-03, -1.4713e-03,  ..., -3.2465e-04,\n         -1.3167e-03,  6.3115e-04]]), 'transformer.encoder.layers.7.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0066, -0.0008, -0.0125,  ..., -0.0018,  0.0101, -0.0006],\n        [-0.0071,  0.0035,  0.0136,  ...,  0.0047, -0.0116,  0.0019],\n        [-0.0152, -0.0044, -0.0005,  ...,  0.0045,  0.0092,  0.0151],\n        ...,\n        [-0.0137, -0.0156, -0.0032,  ..., -0.0144,  0.0102, -0.0003],\n        [-0.0149, -0.0132, -0.0108,  ...,  0.0025,  0.0078,  0.0092],\n        [-0.0002,  0.0150, -0.0072,  ...,  0.0021, -0.0063,  0.0033]]), 'transformer.encoder.layers.7.self_attention.query_key_value.lora_B.default.weight': tensor([[ 1.4004e-04,  1.5589e-04,  1.9719e-06,  ..., -7.9450e-05,\n          1.5281e-04,  1.1020e-04],\n        [ 3.7914e-05, -1.0916e-04,  1.4928e-05,  ...,  1.7967e-04,\n         -8.5974e-05, -1.8833e-04],\n        [-2.3237e-04, -2.8970e-04, -2.4024e-04,  ...,  4.8477e-05,\n         -3.7585e-04, -1.8450e-04],\n        ...,\n        [ 1.9992e-04,  1.4724e-04, -7.2907e-04,  ...,  2.6152e-05,\n          8.0529e-05, -3.3309e-04],\n        [ 1.0827e-03,  3.1454e-04, -1.8808e-03,  ...,  9.2509e-04,\n          4.8152e-04,  1.1027e-04],\n        [ 1.1044e-03,  1.9262e-03,  1.2756e-03,  ..., -7.9333e-04,\n          1.9591e-03,  1.1991e-03]]), 'transformer.encoder.layers.7.self_attention.dense.lora_A.default.weight': tensor([[ 0.0078, -0.0150,  0.0063,  ..., -0.0082, -0.0077, -0.0135],\n        [-0.0019, -0.0043,  0.0049,  ...,  0.0067, -0.0097,  0.0007],\n        [ 0.0021,  0.0072,  0.0027,  ...,  0.0055,  0.0009,  0.0091],\n        ...,\n        [-0.0152,  0.0102,  0.0037,  ...,  0.0041, -0.0058,  0.0020],\n        [ 0.0121, -0.0051,  0.0099,  ..., -0.0098,  0.0056, -0.0003],\n        [ 0.0040,  0.0079, -0.0061,  ...,  0.0046, -0.0041, -0.0127]]), 'transformer.encoder.layers.7.self_attention.dense.lora_B.default.weight': tensor([[ 1.1372e-03,  8.3452e-04, -7.6600e-04,  ..., -3.3325e-04,\n         -7.2031e-04, -2.8001e-04],\n        [-5.0346e-04,  6.2417e-04,  2.8946e-04,  ...,  3.8298e-05,\n          2.7777e-04, -9.2754e-04],\n        [-2.4370e-04, -8.2578e-06,  3.0443e-04,  ...,  2.9871e-04,\n          2.3822e-04, -5.7357e-04],\n        ...,\n        [-7.5115e-04, -5.3623e-04,  9.8163e-04,  ...,  6.7999e-04,\n          1.0332e-03,  2.7265e-04],\n        [ 1.9797e-03,  9.9432e-04, -1.8126e-03,  ..., -7.4766e-04,\n         -2.3248e-03, -1.0943e-03],\n        [-7.5158e-04, -8.9026e-04,  6.6846e-04,  ...,  1.1768e-03,\n          1.1444e-03,  9.1815e-04]]), 'transformer.encoder.layers.7.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 1.1341e-02,  8.4588e-04,  3.0576e-03,  ..., -2.6712e-03,\n          1.5584e-02,  1.3342e-02],\n        [ 3.4135e-03,  2.2919e-04, -5.3761e-03,  ...,  1.0967e-02,\n         -1.1689e-02,  1.3615e-03],\n        [-5.4724e-04, -7.2596e-03,  9.6910e-04,  ...,  2.2222e-03,\n          1.7738e-03,  1.5420e-02],\n        ...,\n        [ 2.4349e-03, -1.2149e-02, -4.7288e-03,  ...,  1.1385e-02,\n         -1.2892e-02,  1.1675e-02],\n        [-1.5540e-02,  1.4729e-02, -1.0011e-02,  ..., -3.6304e-03,\n         -4.8853e-03,  1.1075e-02],\n        [-8.3447e-03,  7.8141e-03, -1.6721e-06,  ...,  1.0109e-03,\n         -1.1925e-02,  1.4996e-02]]), 'transformer.encoder.layers.7.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 1.0144e-05,  7.2045e-04, -9.4333e-04,  ...,  9.9568e-04,\n          6.5459e-04,  6.6484e-04],\n        [ 1.2947e-04,  3.1545e-05, -2.1677e-04,  ..., -1.2944e-04,\n          1.4813e-04,  2.3847e-04],\n        [ 1.2049e-04,  1.6545e-04,  1.7301e-04,  ...,  4.7114e-04,\n         -2.7544e-04, -2.2331e-05],\n        ...,\n        [ 2.9865e-04, -9.3751e-04,  4.4581e-04,  ..., -8.3803e-04,\n         -4.8836e-04, -7.1076e-04],\n        [-1.3696e-04,  3.9967e-04, -3.1127e-04,  ...,  3.3558e-05,\n          2.8598e-06,  1.7414e-04],\n        [-2.5321e-04,  3.0888e-04, -3.7770e-04,  ..., -2.1408e-06,\n          3.8067e-04,  3.6919e-04]]), 'transformer.encoder.layers.7.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0047,  0.0064,  0.0042,  ..., -0.0012,  0.0078,  0.0026],\n        [-0.0019,  0.0058, -0.0035,  ...,  0.0029,  0.0013,  0.0051],\n        [ 0.0070, -0.0057,  0.0021,  ..., -0.0042,  0.0043,  0.0009],\n        ...,\n        [ 0.0027, -0.0062,  0.0019,  ...,  0.0012, -0.0078, -0.0077],\n        [-0.0045, -0.0073, -0.0071,  ...,  0.0018,  0.0015,  0.0041],\n        [-0.0051, -0.0053, -0.0075,  ...,  0.0012, -0.0006,  0.0025]]), 'transformer.encoder.layers.7.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 1.2696e-03, -4.7649e-04,  1.4352e-03,  ...,  1.0728e-03,\n         -1.0880e-03, -7.9708e-04],\n        [ 1.3215e-04, -9.5470e-04,  1.2293e-03,  ...,  4.9273e-04,\n         -1.1904e-03, -1.5066e-03],\n        [-4.3471e-04, -5.3765e-04, -3.1625e-04,  ...,  8.5450e-05,\n         -3.7469e-05, -8.3350e-04],\n        ...,\n        [-7.4190e-04,  3.9979e-05, -1.0155e-03,  ..., -6.1571e-04,\n          5.8747e-04, -5.5652e-04],\n        [ 2.0233e-03, -8.3071e-04,  2.6014e-03,  ...,  1.3886e-03,\n         -1.8537e-03, -2.0618e-03],\n        [ 3.2211e-04,  6.0195e-04, -7.0313e-04,  ..., -9.1413e-04,\n          6.8179e-04,  5.3397e-04]]), 'transformer.encoder.layers.8.self_attention.query_key_value.lora_A.default.weight': tensor([[-1.0628e-04,  3.3814e-03, -1.0814e-02,  ...,  1.1947e-02,\n          5.3546e-03, -4.7397e-05],\n        [ 5.9759e-03, -1.1105e-02, -5.8469e-03,  ...,  5.7055e-03,\n         -1.4551e-02, -8.2163e-04],\n        [-9.8554e-03, -1.3487e-02, -7.8602e-03,  ...,  8.3770e-03,\n          1.2492e-02, -1.0227e-02],\n        ...,\n        [ 1.4215e-02,  7.5536e-03, -3.9159e-03,  ..., -5.3603e-03,\n          2.6833e-03,  1.5830e-02],\n        [-5.3438e-03, -4.9777e-03, -1.4437e-02,  ...,  1.5184e-02,\n          1.3205e-02,  5.4806e-03],\n        [-5.6004e-03, -7.6798e-03,  8.9419e-03,  ..., -6.1413e-03,\n         -7.7018e-03,  2.1028e-03]]), 'transformer.encoder.layers.8.self_attention.query_key_value.lora_B.default.weight': tensor([[-5.9572e-04,  6.1967e-04,  4.9788e-04,  ...,  2.3417e-04,\n         -9.7963e-04, -1.6977e-04],\n        [ 2.6418e-04, -1.2572e-04, -3.5155e-04,  ..., -2.3585e-04,\n          3.5757e-04, -7.0935e-05],\n        [ 2.9998e-06, -4.7616e-05, -4.8822e-05,  ..., -5.8709e-05,\n          1.6873e-04,  8.8409e-07],\n        ...,\n        [-4.7109e-04,  2.7700e-04,  4.8519e-04,  ..., -2.5307e-04,\n         -1.2984e-03, -3.6174e-04],\n        [ 3.3248e-04, -4.8606e-04, -8.0330e-04,  ..., -1.0825e-04,\n          1.0215e-03,  9.0506e-04],\n        [ 6.2988e-04,  6.4278e-04,  8.6353e-05,  ..., -1.3329e-03,\n         -1.1790e-03,  2.4693e-04]]), 'transformer.encoder.layers.8.self_attention.dense.lora_A.default.weight': tensor([[ 0.0050,  0.0017, -0.0155,  ..., -0.0143,  0.0007, -0.0135],\n        [ 0.0052,  0.0034,  0.0064,  ..., -0.0116, -0.0050, -0.0153],\n        [ 0.0109, -0.0154,  0.0050,  ..., -0.0001, -0.0092,  0.0119],\n        ...,\n        [-0.0150,  0.0097,  0.0115,  ...,  0.0071, -0.0082, -0.0121],\n        [ 0.0028,  0.0114,  0.0133,  ...,  0.0107,  0.0053,  0.0087],\n        [-0.0031,  0.0030, -0.0117,  ...,  0.0068, -0.0021,  0.0135]]), 'transformer.encoder.layers.8.self_attention.dense.lora_B.default.weight': tensor([[-2.5513e-03,  9.6844e-04, -1.2848e-04,  ..., -1.4271e-03,\n         -6.3580e-04,  1.2988e-03],\n        [ 5.0302e-04, -1.3591e-04,  5.1001e-04,  ...,  5.0618e-04,\n         -2.2059e-04, -4.7956e-04],\n        [-2.4558e-04, -2.0362e-04,  7.7475e-04,  ..., -1.3821e-05,\n         -2.8723e-04, -1.5392e-04],\n        ...,\n        [-1.1983e-03,  6.2428e-04,  3.4829e-04,  ..., -6.5524e-04,\n         -5.7637e-04,  3.3435e-04],\n        [-1.9612e-03,  8.4995e-04,  5.5203e-04,  ..., -1.1629e-03,\n         -1.1317e-03,  1.1068e-03],\n        [ 9.6506e-04, -2.9118e-04,  1.0015e-04,  ...,  4.6274e-05,\n         -1.1311e-04, -6.1442e-04]]), 'transformer.encoder.layers.8.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 2.9037e-03,  2.3184e-03, -1.0078e-02,  ...,  8.2785e-03,\n          1.1493e-02,  4.4805e-03],\n        [ 1.4173e-03,  1.7341e-03, -1.3671e-03,  ...,  6.0308e-03,\n          1.1780e-02, -4.3935e-03],\n        [ 6.8558e-04,  7.5164e-03,  1.2701e-02,  ..., -3.5290e-03,\n          1.7941e-03,  8.4370e-05],\n        ...,\n        [-1.4151e-02, -1.3786e-03,  1.0636e-02,  ...,  3.9552e-03,\n          5.1924e-03,  7.0125e-03],\n        [ 8.1848e-03, -1.6336e-03,  1.2478e-02,  ...,  3.0181e-03,\n         -9.3157e-03,  1.0336e-02],\n        [-1.2920e-02,  3.3443e-03, -4.6953e-03,  ..., -3.3558e-03,\n         -9.9686e-03, -1.0080e-02]]), 'transformer.encoder.layers.8.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-9.2077e-04,  1.4573e-03, -1.9237e-03,  ..., -2.1475e-03,\n          1.1663e-03, -1.0265e-03],\n        [ 1.5603e-04, -6.0081e-04,  6.2519e-04,  ...,  8.9749e-04,\n         -5.7978e-04,  3.1629e-04],\n        [ 7.4242e-05, -8.5533e-06, -1.7003e-04,  ..., -3.4674e-04,\n         -8.3229e-05,  1.0299e-04],\n        ...,\n        [-1.5116e-04,  5.6260e-05, -4.1067e-04,  ..., -5.2017e-04,\n          7.2712e-05, -4.6764e-04],\n        [ 1.2352e-03, -1.4209e-03,  3.2316e-03,  ...,  3.8090e-03,\n         -8.8279e-04,  1.4447e-03],\n        [ 8.9909e-04, -4.1134e-04,  5.7041e-04,  ...,  8.9120e-04,\n         -7.1759e-04,  6.6895e-04]]), 'transformer.encoder.layers.8.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0034,  0.0001,  0.0076,  ...,  0.0058,  0.0011, -0.0063],\n        [-0.0030, -0.0058,  0.0031,  ..., -0.0015,  0.0046,  0.0071],\n        [-0.0055, -0.0057,  0.0077,  ...,  0.0031, -0.0065, -0.0072],\n        ...,\n        [-0.0020,  0.0033,  0.0013,  ..., -0.0045,  0.0016,  0.0001],\n        [ 0.0011,  0.0056, -0.0041,  ...,  0.0035, -0.0047, -0.0060],\n        [ 0.0052, -0.0051,  0.0082,  ...,  0.0025,  0.0058, -0.0002]]), 'transformer.encoder.layers.8.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-1.8288e-04,  1.3821e-03,  6.1762e-04,  ...,  2.3273e-03,\n          1.2062e-04, -8.1162e-04],\n        [-1.8376e-04,  4.4578e-04, -1.0887e-04,  ..., -4.8908e-05,\n          4.3147e-04, -4.3777e-05],\n        [ 4.4202e-04,  2.0612e-04, -5.7573e-04,  ...,  1.7554e-04,\n         -2.4028e-04, -5.2152e-04],\n        ...,\n        [-1.3810e-04,  2.3621e-04,  2.6030e-04,  ...,  1.3193e-03,\n          2.8053e-04, -3.3754e-04],\n        [ 5.4233e-04,  9.5851e-04,  1.1596e-04,  ...,  2.0666e-03,\n          1.0425e-05, -1.2654e-03],\n        [-2.6097e-04, -7.6417e-04, -3.1450e-04,  ..., -1.1312e-03,\n         -9.1874e-04,  9.8295e-04]]), 'transformer.encoder.layers.9.self_attention.query_key_value.lora_A.default.weight': tensor([[ 2.8727e-03,  5.3104e-03,  7.4538e-03,  ..., -8.7071e-03,\n          1.0830e-02,  5.2967e-03],\n        [ 6.0455e-03, -4.2222e-03, -7.3085e-04,  ..., -8.1725e-05,\n          1.6033e-03,  2.8552e-03],\n        [ 1.0477e-02,  3.7103e-04, -5.7078e-03,  ...,  9.2611e-03,\n         -1.4555e-02,  8.5477e-03],\n        ...,\n        [ 6.7978e-03,  6.0945e-03,  8.7147e-03,  ...,  6.8687e-03,\n          1.4841e-02,  1.0171e-02],\n        [-3.1663e-03,  5.3601e-03, -3.8863e-03,  ...,  1.5077e-02,\n         -2.6833e-03,  1.4816e-02],\n        [-9.5675e-03, -1.1215e-02,  1.5269e-02,  ...,  1.2694e-03,\n         -9.1451e-03, -1.6391e-03]]), 'transformer.encoder.layers.9.self_attention.query_key_value.lora_B.default.weight': tensor([[ 2.4074e-04, -5.5052e-06,  2.3085e-05,  ..., -1.0122e-04,\n          1.3055e-04,  2.8998e-05],\n        [ 4.5869e-05,  1.7483e-04,  1.8155e-04,  ..., -1.0644e-04,\n         -1.4515e-04, -2.0691e-04],\n        [-1.2046e-04,  1.4083e-04, -5.5610e-05,  ..., -1.0485e-04,\n          1.6418e-05, -4.5711e-05],\n        ...,\n        [-9.5803e-04,  1.7145e-03,  1.8634e-03,  ..., -1.1150e-03,\n         -2.1863e-03, -1.2272e-03],\n        [ 1.2625e-03, -3.2071e-03, -3.4912e-03,  ...,  2.8049e-03,\n          4.1715e-03,  2.4035e-03],\n        [ 8.2478e-05,  1.7474e-05, -1.4192e-05,  ...,  1.6098e-05,\n          5.6406e-04, -1.3055e-04]]), 'transformer.encoder.layers.9.self_attention.dense.lora_A.default.weight': tensor([[ 0.0109,  0.0115,  0.0022,  ..., -0.0007,  0.0032, -0.0091],\n        [-0.0078, -0.0027, -0.0095,  ..., -0.0099,  0.0044,  0.0104],\n        [ 0.0104,  0.0136, -0.0098,  ..., -0.0051, -0.0113,  0.0141],\n        ...,\n        [ 0.0036,  0.0049,  0.0023,  ...,  0.0081, -0.0115,  0.0091],\n        [-0.0116,  0.0071,  0.0109,  ...,  0.0039,  0.0088, -0.0128],\n        [-0.0001, -0.0140, -0.0071,  ...,  0.0112, -0.0164,  0.0034]]), 'transformer.encoder.layers.9.self_attention.dense.lora_B.default.weight': tensor([[-5.3526e-04,  5.4186e-04,  7.1605e-04,  ..., -8.6077e-04,\n          8.5227e-04,  1.7794e-04],\n        [ 2.2995e-04,  9.1057e-05,  3.0638e-04,  ..., -1.3589e-04,\n          2.4406e-04, -5.0539e-05],\n        [-1.3139e-04, -1.9468e-04,  4.3890e-04,  ..., -5.9055e-04,\n          5.3427e-04,  2.1885e-04],\n        ...,\n        [-1.3336e-04,  1.6424e-05,  2.7854e-04,  ..., -4.0057e-04,\n          2.1139e-05, -1.6448e-04],\n        [ 4.8701e-05,  5.8961e-04,  4.8230e-04,  ..., -1.0358e-03,\n          1.0917e-03,  6.9415e-04],\n        [ 8.8699e-04, -4.8948e-04,  2.8281e-04,  ...,  7.2340e-04,\n         -4.7240e-04, -7.3369e-04]]), 'transformer.encoder.layers.9.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0056,  0.0007,  0.0116,  ...,  0.0123, -0.0123,  0.0156],\n        [ 0.0057,  0.0112, -0.0111,  ...,  0.0024, -0.0087,  0.0063],\n        [ 0.0086,  0.0015, -0.0029,  ..., -0.0068,  0.0116, -0.0001],\n        ...,\n        [-0.0095,  0.0080, -0.0133,  ..., -0.0005,  0.0007, -0.0048],\n        [ 0.0110, -0.0009,  0.0021,  ..., -0.0063,  0.0052,  0.0108],\n        [-0.0117, -0.0018,  0.0155,  ..., -0.0119,  0.0161, -0.0062]]), 'transformer.encoder.layers.9.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 4.7068e-04,  6.9773e-04, -4.7213e-04,  ...,  4.3688e-05,\n          8.9411e-04,  5.6907e-04],\n        [ 9.3958e-04, -6.1312e-04,  2.0803e-04,  ..., -1.2101e-04,\n         -1.5538e-03,  2.6183e-03],\n        [ 3.3743e-05,  2.3689e-04, -2.9789e-04,  ...,  2.7920e-04,\n         -2.0449e-04, -5.9301e-04],\n        ...,\n        [-8.5238e-04,  7.5691e-05,  4.5417e-04,  ..., -1.1031e-04,\n         -1.1391e-03,  1.0695e-03],\n        [-6.1559e-04,  1.6313e-04, -5.8741e-04,  ...,  1.0149e-04,\n         -1.2297e-05, -1.9446e-05],\n        [ 3.3353e-04, -7.2722e-04,  5.7434e-04,  ...,  5.3073e-04,\n         -4.1106e-05,  1.1065e-03]]), 'transformer.encoder.layers.9.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0024,  0.0022,  0.0079,  ...,  0.0035,  0.0041, -0.0022],\n        [-0.0028,  0.0065,  0.0005,  ..., -0.0061, -0.0031, -0.0066],\n        [-0.0024,  0.0021, -0.0037,  ...,  0.0043,  0.0049,  0.0026],\n        ...,\n        [ 0.0036,  0.0081,  0.0068,  ...,  0.0063, -0.0082, -0.0063],\n        [ 0.0015, -0.0074, -0.0077,  ...,  0.0066, -0.0078, -0.0034],\n        [-0.0047,  0.0061, -0.0051,  ..., -0.0051, -0.0005, -0.0061]]), 'transformer.encoder.layers.9.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-0.0008,  0.0014, -0.0008,  ...,  0.0003,  0.0010,  0.0007],\n        [ 0.0003, -0.0003,  0.0006,  ..., -0.0011, -0.0014, -0.0010],\n        [-0.0002,  0.0009, -0.0007,  ...,  0.0001,  0.0004,  0.0002],\n        ...,\n        [-0.0003,  0.0003,  0.0003,  ..., -0.0005, -0.0003,  0.0001],\n        [-0.0006,  0.0009, -0.0011,  ...,  0.0005,  0.0012,  0.0010],\n        [-0.0001,  0.0004, -0.0004,  ..., -0.0002, -0.0005,  0.0002]]), 'transformer.encoder.layers.10.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0115,  0.0107, -0.0048,  ..., -0.0076, -0.0037,  0.0032],\n        [-0.0139, -0.0054, -0.0098,  ...,  0.0071,  0.0019,  0.0108],\n        [-0.0117,  0.0075, -0.0054,  ...,  0.0136,  0.0085, -0.0064],\n        ...,\n        [ 0.0015,  0.0009, -0.0127,  ...,  0.0061, -0.0142,  0.0068],\n        [ 0.0035,  0.0039,  0.0018,  ..., -0.0019, -0.0102,  0.0034],\n        [-0.0058,  0.0100, -0.0149,  ...,  0.0131, -0.0003, -0.0054]]), 'transformer.encoder.layers.10.self_attention.query_key_value.lora_B.default.weight': tensor([[-1.0813e-04, -7.3336e-06, -1.5678e-04,  ..., -8.1089e-05,\n          9.5842e-05, -4.8617e-05],\n        [-1.8280e-04, -4.3355e-05, -4.2062e-05,  ...,  3.3964e-04,\n          3.3024e-05,  2.9494e-05],\n        [ 4.2085e-04,  4.0684e-04,  4.7139e-04,  ..., -2.3454e-05,\n          1.6948e-04, -1.9787e-04],\n        ...,\n        [ 4.5309e-04,  1.8642e-03,  4.2183e-04,  ..., -7.1908e-04,\n          1.8694e-03,  6.0237e-04],\n        [-2.5723e-04, -1.8519e-04,  3.4120e-04,  ..., -7.1432e-04,\n         -3.1194e-04, -7.7157e-04],\n        [ 6.8274e-04, -2.5385e-04,  9.7221e-04,  ..., -6.1259e-05,\n         -2.4765e-04, -2.8837e-04]]), 'transformer.encoder.layers.10.self_attention.dense.lora_A.default.weight': tensor([[ 0.0138, -0.0141,  0.0153,  ..., -0.0153, -0.0057, -0.0146],\n        [-0.0123,  0.0068, -0.0138,  ..., -0.0064, -0.0128,  0.0030],\n        [-0.0021,  0.0051, -0.0133,  ..., -0.0066,  0.0032,  0.0019],\n        ...,\n        [-0.0045, -0.0017,  0.0030,  ..., -0.0063,  0.0105,  0.0017],\n        [ 0.0064,  0.0027,  0.0107,  ...,  0.0088,  0.0062,  0.0127],\n        [-0.0091, -0.0145,  0.0015,  ...,  0.0020,  0.0062,  0.0028]]), 'transformer.encoder.layers.10.self_attention.dense.lora_B.default.weight': tensor([[-8.9569e-04, -9.6704e-04,  1.3464e-03,  ...,  5.8862e-04,\n         -4.4903e-04,  6.9103e-04],\n        [ 1.6929e-03,  6.9187e-04, -1.0208e-03,  ...,  5.1579e-04,\n          2.8948e-04, -1.0458e-03],\n        [-1.1274e-04, -1.9161e-04, -2.3390e-04,  ..., -2.8938e-04,\n          9.1791e-04, -1.5256e-04],\n        ...,\n        [ 8.3886e-04, -2.3098e-04, -5.2734e-04,  ..., -1.2872e-04,\n          1.8186e-04,  1.1132e-04],\n        [-6.3710e-04,  9.3826e-05, -1.8842e-04,  ..., -1.5500e-03,\n          2.8963e-03, -5.9342e-04],\n        [ 3.2606e-05, -4.9383e-04,  2.7232e-04,  ...,  9.9162e-04,\n          3.3221e-04,  4.1211e-04]]), 'transformer.encoder.layers.10.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0076,  0.0024, -0.0089,  ..., -0.0114, -0.0066, -0.0152],\n        [ 0.0111,  0.0025, -0.0147,  ...,  0.0118,  0.0020, -0.0062],\n        [-0.0084,  0.0045,  0.0073,  ..., -0.0084, -0.0106,  0.0084],\n        ...,\n        [ 0.0022,  0.0049, -0.0067,  ..., -0.0086,  0.0036, -0.0078],\n        [ 0.0143, -0.0100, -0.0133,  ...,  0.0137,  0.0026,  0.0146],\n        [-0.0120,  0.0117,  0.0081,  ...,  0.0061,  0.0056, -0.0143]]), 'transformer.encoder.layers.10.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-7.0520e-04,  5.6921e-04, -4.0145e-04,  ..., -5.1982e-04,\n         -2.5207e-04,  6.2511e-05],\n        [-6.6020e-05, -2.9683e-05, -4.5792e-05,  ..., -5.4060e-05,\n         -2.4262e-04,  9.4645e-05],\n        [ 6.0923e-05, -2.2492e-04, -2.4945e-05,  ..., -9.8477e-04,\n          8.6857e-04, -1.9406e-04],\n        ...,\n        [-1.3721e-03,  4.3571e-04,  4.7781e-04,  ...,  1.1588e-04,\n         -6.5050e-04,  1.3712e-04],\n        [ 7.2262e-05, -3.5402e-04,  6.1622e-05,  ...,  2.1787e-04,\n          1.9607e-05, -9.7296e-05],\n        [ 9.5875e-04, -8.8533e-04,  8.7888e-04,  ...,  1.4971e-03,\n         -4.9025e-04,  4.9391e-04]]), 'transformer.encoder.layers.10.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 4.4242e-05, -2.9668e-03, -4.7177e-03,  ..., -5.4083e-03,\n          6.7048e-03, -8.2868e-03],\n        [-2.7296e-04,  3.4927e-03, -5.1912e-04,  ...,  7.8118e-03,\n          2.8920e-03,  4.1102e-03],\n        [-4.3153e-03, -5.9994e-03,  2.3795e-03,  ..., -6.2460e-03,\n          5.0980e-03, -7.1136e-03],\n        ...,\n        [-6.2127e-04, -6.1981e-03, -6.3205e-03,  ...,  5.8443e-03,\n         -1.0742e-03,  3.6204e-03],\n        [ 7.9881e-04,  5.9869e-03,  5.3691e-03,  ..., -4.4791e-04,\n          2.5144e-03,  1.6272e-03],\n        [ 6.6367e-03, -6.5017e-03, -8.0360e-03,  ..., -6.9973e-03,\n          1.0307e-03,  2.8161e-03]]), 'transformer.encoder.layers.10.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-6.1413e-04,  1.2153e-03, -9.8811e-04,  ...,  1.6140e-03,\n          1.1339e-03,  5.2500e-05],\n        [ 1.3315e-04, -7.3840e-04,  3.2958e-04,  ..., -9.2811e-04,\n         -7.2118e-04, -4.2113e-04],\n        [-1.5520e-04,  1.0514e-03, -5.7167e-04,  ...,  4.1419e-04,\n         -1.5058e-04, -3.7431e-04],\n        ...,\n        [ 1.5935e-03, -9.4671e-04,  1.3675e-03,  ..., -2.1631e-03,\n         -3.3907e-04, -4.4608e-04],\n        [-8.1959e-04,  1.0827e-03, -1.2653e-03,  ...,  1.3586e-03,\n         -1.0075e-03, -7.6272e-04],\n        [ 7.8441e-04, -6.6949e-04,  8.7834e-04,  ..., -1.2117e-03,\n         -4.6454e-04,  5.1437e-05]]), 'transformer.encoder.layers.11.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0091,  0.0081, -0.0152,  ...,  0.0113, -0.0050,  0.0090],\n        [ 0.0041, -0.0038,  0.0143,  ..., -0.0003,  0.0117, -0.0027],\n        [-0.0092, -0.0058,  0.0104,  ...,  0.0124,  0.0147,  0.0138],\n        ...,\n        [-0.0006, -0.0104, -0.0109,  ...,  0.0022,  0.0002, -0.0150],\n        [ 0.0031,  0.0135,  0.0034,  ...,  0.0150, -0.0059, -0.0021],\n        [ 0.0010,  0.0044,  0.0133,  ..., -0.0039, -0.0089,  0.0075]]), 'transformer.encoder.layers.11.self_attention.query_key_value.lora_B.default.weight': tensor([[ 1.0848e-04,  7.2157e-05,  1.6923e-04,  ...,  9.1097e-05,\n         -7.2705e-06, -1.2108e-05],\n        [ 4.0074e-04, -1.9876e-04,  3.9745e-04,  ...,  4.1743e-04,\n          2.7273e-04,  8.6839e-05],\n        [ 5.2138e-05, -4.6418e-05,  4.8708e-04,  ...,  3.3294e-04,\n         -3.2150e-05,  8.1423e-06],\n        ...,\n        [ 6.5128e-04, -1.2411e-06,  8.6229e-04,  ...,  8.1840e-04,\n         -1.4001e-04, -1.1309e-04],\n        [-3.7765e-04,  3.0016e-03,  1.7174e-03,  ..., -1.0410e-03,\n          3.6485e-04,  1.6524e-04],\n        [-2.5835e-04, -1.4198e-04, -3.1708e-04,  ..., -1.1151e-04,\n          4.0384e-04, -3.8729e-05]]), 'transformer.encoder.layers.11.self_attention.dense.lora_A.default.weight': tensor([[ 0.0060, -0.0011,  0.0002,  ...,  0.0065, -0.0124,  0.0067],\n        [ 0.0166,  0.0064, -0.0129,  ...,  0.0127, -0.0141,  0.0098],\n        [ 0.0119, -0.0150,  0.0004,  ..., -0.0015,  0.0142,  0.0054],\n        ...,\n        [ 0.0078,  0.0025,  0.0116,  ...,  0.0020, -0.0119,  0.0044],\n        [-0.0122,  0.0115, -0.0037,  ...,  0.0107,  0.0115,  0.0155],\n        [ 0.0144,  0.0131, -0.0048,  ...,  0.0139,  0.0122, -0.0013]]), 'transformer.encoder.layers.11.self_attention.dense.lora_B.default.weight': tensor([[ 1.8605e-04, -8.7424e-04, -7.0889e-04,  ...,  1.5848e-03,\n          4.5695e-04, -2.1329e-03],\n        [-7.6609e-04,  1.8416e-04,  2.2378e-04,  ..., -4.4568e-04,\n         -1.6489e-04,  4.8741e-04],\n        [ 6.3205e-04,  4.1003e-05, -1.0729e-05,  ...,  5.7196e-04,\n          7.0691e-04, -1.0785e-03],\n        ...,\n        [-1.8720e-04,  6.6420e-04,  1.6800e-05,  ..., -5.3969e-04,\n         -8.0859e-04,  1.0153e-03],\n        [ 6.2339e-05, -2.9704e-04,  4.0034e-04,  ...,  6.8991e-04,\n          1.1787e-03, -1.3621e-03],\n        [-6.4194e-04,  5.1751e-04,  5.3585e-04,  ..., -1.3790e-04,\n          7.0517e-04, -5.5481e-04]]), 'transformer.encoder.layers.11.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 6.5214e-03,  4.2239e-03,  5.6670e-03,  ...,  3.8144e-03,\n         -9.8056e-03,  3.8474e-03],\n        [-5.1847e-03, -1.3331e-03, -1.1863e-02,  ...,  1.4989e-02,\n          1.5041e-02, -1.1156e-02],\n        [-2.0209e-03, -9.7961e-03,  7.1633e-03,  ..., -7.9175e-03,\n         -3.0037e-03, -1.3904e-02],\n        ...,\n        [-8.8619e-03, -9.1633e-04, -4.4367e-03,  ..., -3.9042e-05,\n          5.5635e-03,  7.1311e-03],\n        [ 8.6090e-03, -1.5261e-02,  6.8714e-03,  ...,  1.5728e-03,\n          5.1360e-03,  8.5415e-03],\n        [-2.9049e-03,  1.5844e-03,  2.2226e-03,  ..., -1.2840e-02,\n         -2.1936e-03,  3.0408e-03]]), 'transformer.encoder.layers.11.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 1.8477e-03, -2.3190e-03, -1.5610e-04,  ...,  1.5497e-03,\n          1.7988e-03, -1.2934e-03],\n        [-4.3005e-04,  3.8796e-04,  3.8969e-05,  ..., -4.8359e-04,\n         -1.0095e-03, -2.3008e-04],\n        [-8.9042e-05, -4.9546e-04, -3.0696e-04,  ..., -2.2516e-04,\n          4.6978e-04, -3.1132e-04],\n        ...,\n        [ 3.9982e-04, -7.5187e-05,  3.5928e-04,  ...,  2.1496e-04,\n          4.1932e-05, -4.4043e-05],\n        [ 1.9499e-03, -2.8939e-03,  4.5377e-04,  ...,  1.5264e-03,\n          1.4682e-03, -1.0462e-03],\n        [-1.1515e-03,  1.4263e-03, -3.9914e-04,  ..., -1.1218e-03,\n         -9.2404e-04,  2.8271e-04]]), 'transformer.encoder.layers.11.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0052, -0.0025,  0.0047,  ...,  0.0002,  0.0021, -0.0033],\n        [ 0.0072, -0.0068, -0.0065,  ..., -0.0054, -0.0060, -0.0013],\n        [ 0.0015, -0.0074,  0.0075,  ...,  0.0040, -0.0031, -0.0034],\n        ...,\n        [ 0.0027, -0.0069,  0.0018,  ...,  0.0051, -0.0061, -0.0044],\n        [ 0.0075, -0.0061,  0.0080,  ...,  0.0083,  0.0006,  0.0033],\n        [-0.0073, -0.0055,  0.0024,  ...,  0.0030,  0.0024, -0.0016]]), 'transformer.encoder.layers.11.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-3.0879e-04, -1.0852e-03,  6.7832e-04,  ..., -3.3646e-04,\n         -7.4861e-04, -8.5447e-04],\n        [-3.6938e-05,  6.7723e-04, -2.4713e-04,  ..., -2.3627e-04,\n          4.3788e-04,  3.8264e-05],\n        [ 4.6620e-04,  6.4297e-05, -1.1075e-04,  ..., -1.3456e-05,\n          8.0500e-04,  5.5148e-04],\n        ...,\n        [ 4.4750e-04,  5.5447e-04, -2.0268e-04,  ...,  4.4241e-04,\n          8.2749e-04,  5.8771e-04],\n        [-1.0450e-03, -7.9144e-04,  1.0128e-04,  ..., -1.0136e-03,\n         -1.6829e-03, -2.3792e-03],\n        [-4.9396e-04, -1.5989e-04, -6.6225e-05,  ..., -3.5726e-04,\n         -9.9511e-04, -2.8847e-04]]), 'transformer.encoder.layers.12.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0036, -0.0128,  0.0069,  ..., -0.0041,  0.0051, -0.0146],\n        [ 0.0082,  0.0071,  0.0106,  ..., -0.0053,  0.0017, -0.0014],\n        [ 0.0144, -0.0032,  0.0054,  ...,  0.0008, -0.0053,  0.0039],\n        ...,\n        [-0.0011,  0.0027, -0.0101,  ..., -0.0021,  0.0049, -0.0109],\n        [ 0.0024,  0.0010, -0.0145,  ...,  0.0084, -0.0005,  0.0137],\n        [-0.0013, -0.0070, -0.0029,  ...,  0.0002, -0.0038, -0.0100]]), 'transformer.encoder.layers.12.self_attention.query_key_value.lora_B.default.weight': tensor([[ 3.2000e-04,  2.2781e-05, -2.9615e-05,  ..., -2.7980e-04,\n         -3.0208e-04, -2.3555e-04],\n        [-1.9551e-04,  8.0422e-05,  1.0893e-04,  ...,  2.9667e-06,\n          4.0488e-04,  3.7941e-04],\n        [ 1.4518e-04, -3.3837e-06,  1.2644e-06,  ..., -7.1435e-05,\n          1.2080e-04,  1.2208e-04],\n        ...,\n        [ 8.4463e-04, -1.1179e-03, -3.2625e-04,  ..., -4.2292e-04,\n         -8.4372e-04, -1.2329e-04],\n        [ 1.7003e-04, -2.1339e-04, -1.1544e-04,  ...,  5.7868e-04,\n          1.1858e-03,  6.6647e-04],\n        [ 5.4299e-04, -1.9636e-03, -4.9098e-04,  ...,  9.3287e-04,\n          2.9525e-03,  2.4170e-03]]), 'transformer.encoder.layers.12.self_attention.dense.lora_A.default.weight': tensor([[ 0.0050,  0.0112, -0.0091,  ..., -0.0088, -0.0030,  0.0038],\n        [ 0.0044,  0.0142, -0.0010,  ...,  0.0056,  0.0125,  0.0102],\n        [-0.0007,  0.0133,  0.0089,  ...,  0.0109,  0.0054,  0.0107],\n        ...,\n        [ 0.0034,  0.0156,  0.0089,  ...,  0.0127, -0.0122,  0.0094],\n        [ 0.0033, -0.0038,  0.0056,  ...,  0.0077,  0.0066, -0.0064],\n        [-0.0133,  0.0012,  0.0103,  ..., -0.0066, -0.0010, -0.0106]]), 'transformer.encoder.layers.12.self_attention.dense.lora_B.default.weight': tensor([[-1.6127e-03, -8.4293e-04, -1.7213e-03,  ...,  1.1981e-03,\n         -1.2409e-03,  1.7983e-03],\n        [ 7.4363e-04,  1.1556e-03,  5.9411e-04,  ..., -2.3422e-04,\n          1.0230e-03, -1.1645e-03],\n        [-1.5669e-04, -1.0752e-04, -1.7222e-04,  ..., -1.3871e-04,\n         -5.9055e-04, -1.0418e-04],\n        ...,\n        [ 4.4053e-04, -7.1592e-05,  5.1954e-04,  ..., -8.0248e-04,\n          2.9245e-04,  1.1344e-04],\n        [-5.3647e-04, -1.0905e-03,  4.0136e-04,  ...,  1.9975e-03,\n         -1.2090e-04,  4.6093e-04],\n        [-3.7970e-04, -8.8846e-04, -1.0010e-03,  ...,  1.0748e-03,\n         -1.4360e-04,  7.4502e-04]]), 'transformer.encoder.layers.12.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0007,  0.0044, -0.0096,  ...,  0.0121,  0.0044, -0.0015],\n        [-0.0137,  0.0133,  0.0086,  ..., -0.0070,  0.0041, -0.0050],\n        [-0.0055, -0.0017, -0.0124,  ...,  0.0081, -0.0015,  0.0084],\n        ...,\n        [ 0.0083,  0.0139, -0.0120,  ...,  0.0046, -0.0007,  0.0090],\n        [-0.0147, -0.0041, -0.0057,  ...,  0.0102, -0.0116,  0.0022],\n        [ 0.0148, -0.0137, -0.0089,  ..., -0.0061,  0.0152, -0.0053]]), 'transformer.encoder.layers.12.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-5.4898e-04, -5.2503e-04, -2.5666e-04,  ...,  1.3941e-04,\n         -8.1629e-04,  5.7908e-05],\n        [-1.2347e-03,  2.2888e-03, -9.4833e-04,  ...,  1.4166e-03,\n          6.5712e-04, -3.9097e-04],\n        [ 5.8138e-04,  4.2122e-04,  9.6982e-05,  ...,  1.8444e-04,\n          1.2833e-04,  1.2580e-03],\n        ...,\n        [-1.6872e-04,  1.0286e-03, -3.5853e-04,  ...,  3.1332e-04,\n          8.8515e-04, -3.1105e-04],\n        [-7.3188e-05, -4.5258e-04, -2.3457e-04,  ...,  7.6854e-05,\n          5.5271e-05, -4.0835e-04],\n        [ 5.3644e-05,  1.7394e-04,  2.9979e-05,  ...,  1.4036e-05,\n          2.4168e-04,  1.5881e-05]]), 'transformer.encoder.layers.12.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0059, -0.0011,  0.0054,  ...,  0.0010,  0.0086, -0.0065],\n        [-0.0006,  0.0030, -0.0080,  ...,  0.0049,  0.0059,  0.0061],\n        [ 0.0007,  0.0038, -0.0088,  ...,  0.0086,  0.0067, -0.0059],\n        ...,\n        [-0.0013,  0.0043,  0.0041,  ..., -0.0050,  0.0014,  0.0030],\n        [ 0.0072,  0.0021,  0.0010,  ...,  0.0086, -0.0058, -0.0022],\n        [ 0.0030,  0.0034, -0.0055,  ...,  0.0055, -0.0085,  0.0038]]), 'transformer.encoder.layers.12.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-1.3915e-03,  1.5010e-03,  1.2697e-03,  ...,  2.0000e-03,\n         -1.7512e-04,  9.7962e-05],\n        [-6.4193e-04, -8.6304e-05,  5.8167e-04,  ...,  2.5865e-04,\n         -3.9799e-04,  8.1463e-05],\n        [-3.6306e-04,  3.2955e-04, -2.8343e-05,  ...,  3.3291e-04,\n         -1.2651e-04,  3.4828e-04],\n        ...,\n        [ 8.9839e-05,  2.7074e-04, -5.5634e-05,  ..., -2.4497e-04,\n          2.4338e-04,  2.4950e-04],\n        [-1.0401e-03,  1.2598e-03, -4.1941e-04,  ...,  1.5610e-03,\n         -1.1360e-03,  1.5003e-03],\n        [-1.0127e-03,  1.8705e-03, -2.3278e-04,  ...,  1.2066e-03,\n         -9.6354e-04,  1.0214e-03]]), 'transformer.encoder.layers.13.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0056,  0.0097,  0.0083,  ..., -0.0099, -0.0111, -0.0040],\n        [-0.0152, -0.0012, -0.0122,  ...,  0.0045, -0.0097,  0.0088],\n        [-0.0052, -0.0041,  0.0145,  ..., -0.0107, -0.0114, -0.0079],\n        ...,\n        [ 0.0101, -0.0135, -0.0028,  ...,  0.0046, -0.0120,  0.0083],\n        [ 0.0130,  0.0013, -0.0127,  ..., -0.0025,  0.0066, -0.0063],\n        [ 0.0147, -0.0034,  0.0130,  ...,  0.0109,  0.0062, -0.0101]]), 'transformer.encoder.layers.13.self_attention.query_key_value.lora_B.default.weight': tensor([[-1.2590e-04,  2.8156e-04,  2.6978e-04,  ...,  3.3096e-05,\n         -1.9421e-04,  2.0991e-04],\n        [ 9.7773e-05,  1.0933e-04,  1.0242e-04,  ...,  8.9280e-05,\n         -2.5635e-05, -1.7507e-04],\n        [-4.9048e-04, -3.9965e-04, -5.5369e-04,  ...,  9.0366e-05,\n          5.9240e-04, -4.1057e-04],\n        ...,\n        [-2.1120e-04, -3.4045e-04, -9.1165e-04,  ..., -1.0669e-04,\n          5.2132e-04,  1.1252e-04],\n        [-1.3031e-03,  1.3663e-03, -1.3848e-04,  ..., -1.2544e-03,\n         -8.1245e-04,  1.3184e-03],\n        [ 2.1178e-03,  4.6459e-04,  3.3945e-04,  ...,  1.1740e-03,\n         -6.9893e-04, -2.9440e-04]]), 'transformer.encoder.layers.13.self_attention.dense.lora_A.default.weight': tensor([[ 1.2780e-02, -1.3082e-02,  4.4972e-03,  ...,  6.6357e-04,\n         -3.1574e-03,  1.5189e-02],\n        [ 1.2392e-02, -1.7324e-03, -1.5597e-02,  ...,  9.5356e-03,\n         -1.4649e-02,  9.3123e-03],\n        [ 9.9283e-03, -1.1528e-02, -1.2183e-02,  ...,  5.7411e-03,\n         -3.5955e-03, -6.5192e-05],\n        ...,\n        [-5.4109e-03,  8.6725e-03,  2.0934e-03,  ..., -5.0641e-03,\n         -9.4143e-03, -6.0345e-03],\n        [-8.6943e-03, -1.2828e-02, -3.0427e-03,  ...,  4.3087e-03,\n          5.6443e-03, -7.5966e-03],\n        [ 1.0721e-02, -4.3211e-04,  1.3891e-02,  ..., -4.0435e-03,\n          1.4803e-03,  1.9459e-03]]), 'transformer.encoder.layers.13.self_attention.dense.lora_B.default.weight': tensor([[-1.4081e-03,  1.3479e-03, -1.2106e-03,  ..., -9.8592e-04,\n         -5.6313e-04,  8.0661e-04],\n        [-6.2985e-04, -6.3883e-05, -1.5019e-04,  ..., -7.5800e-05,\n          1.2898e-04,  1.9437e-04],\n        [ 8.8672e-05,  1.4146e-04, -2.5111e-04,  ..., -4.2681e-04,\n          1.5237e-04,  1.3879e-04],\n        ...,\n        [-3.3960e-04,  4.1638e-04, -3.5250e-04,  ..., -2.4376e-04,\n         -8.0411e-04,  4.3650e-04],\n        [-1.3784e-04, -3.5599e-04, -5.9171e-04,  ...,  5.8304e-04,\n          2.1985e-03, -4.6799e-04],\n        [-3.7174e-04, -3.4466e-04, -1.4341e-03,  ..., -9.7580e-05,\n          1.8198e-03, -7.0173e-04]]), 'transformer.encoder.layers.13.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-1.0325e-02, -3.5228e-03, -1.7983e-05,  ...,  1.2832e-02,\n          7.7525e-03,  5.8798e-03],\n        [-1.4882e-02,  1.0854e-02, -4.2380e-03,  ..., -1.1083e-02,\n          8.9289e-03,  7.2520e-03],\n        [-9.1195e-04,  1.4822e-02,  1.9048e-03,  ..., -3.8024e-03,\n         -1.2154e-02,  1.0339e-02],\n        ...,\n        [ 1.3515e-02, -1.4765e-02,  1.4157e-02,  ...,  1.4864e-02,\n         -6.5557e-03,  1.0160e-03],\n        [-7.8277e-03, -1.4534e-03, -1.9903e-03,  ..., -9.0941e-03,\n          1.4527e-03,  1.1365e-02],\n        [-8.7700e-03, -6.0932e-03,  1.1118e-04,  ...,  1.3449e-04,\n         -1.2936e-02,  1.1048e-02]]), 'transformer.encoder.layers.13.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-7.5498e-05, -4.3935e-04, -5.9866e-04,  ..., -3.5808e-04,\n          2.0044e-05, -1.8766e-04],\n        [-2.9564e-04, -6.3790e-04, -3.8582e-04,  ..., -2.2266e-04,\n         -3.4079e-04, -9.6392e-05],\n        [-2.4213e-04, -2.8322e-04, -1.4105e-04,  ..., -2.3694e-05,\n          3.2149e-05,  5.8067e-05],\n        ...,\n        [-4.7075e-04, -1.0080e-04, -2.9224e-04,  ...,  2.4024e-04,\n          1.9913e-04, -3.2333e-04],\n        [-1.6468e-04, -3.0763e-06,  1.5523e-04,  ...,  7.5331e-04,\n         -9.8892e-05, -6.8826e-04],\n        [-3.2019e-04, -6.2690e-04, -1.6927e-04,  ..., -1.3101e-03,\n          7.6799e-05,  2.6965e-04]]), 'transformer.encoder.layers.13.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 0.0054,  0.0015,  0.0002,  ..., -0.0043, -0.0049, -0.0017],\n        [ 0.0072, -0.0056,  0.0016,  ...,  0.0004, -0.0047, -0.0036],\n        [ 0.0080, -0.0079,  0.0064,  ...,  0.0011, -0.0019, -0.0008],\n        ...,\n        [-0.0050,  0.0063, -0.0031,  ...,  0.0051, -0.0048, -0.0041],\n        [-0.0073,  0.0032, -0.0066,  ...,  0.0059, -0.0080,  0.0088],\n        [-0.0080,  0.0086,  0.0004,  ..., -0.0058,  0.0035, -0.0015]]), 'transformer.encoder.layers.13.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-8.3805e-04, -6.9632e-04, -1.3282e-04,  ..., -8.1587e-06,\n          3.7996e-05,  3.7448e-04],\n        [-1.3931e-04,  1.6405e-04, -3.6481e-04,  ..., -5.3555e-04,\n          9.6881e-05,  4.1821e-04],\n        [-8.2041e-04,  2.0074e-04,  6.1100e-04,  ...,  1.4596e-04,\n          6.9947e-04,  4.9150e-04],\n        ...,\n        [ 8.0992e-04,  2.0901e-04, -2.1625e-04,  ..., -1.1266e-04,\n         -7.6813e-05, -1.2725e-03],\n        [-1.5762e-03,  2.3616e-05,  3.1819e-04,  ..., -8.1275e-04,\n          1.1228e-03,  6.8765e-04],\n        [-1.9616e-03,  6.0062e-04,  8.8563e-04,  ..., -4.4658e-05,\n          1.1646e-03,  1.3045e-03]]), 'transformer.encoder.layers.14.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0008,  0.0129, -0.0072,  ...,  0.0092, -0.0075, -0.0036],\n        [-0.0019, -0.0070,  0.0079,  ..., -0.0090,  0.0024, -0.0063],\n        [ 0.0061,  0.0144, -0.0030,  ..., -0.0157,  0.0142, -0.0011],\n        ...,\n        [-0.0070, -0.0028, -0.0056,  ...,  0.0154, -0.0112,  0.0137],\n        [ 0.0011, -0.0099,  0.0082,  ..., -0.0128, -0.0149, -0.0150],\n        [-0.0107, -0.0031,  0.0021,  ..., -0.0014, -0.0102, -0.0018]]), 'transformer.encoder.layers.14.self_attention.query_key_value.lora_B.default.weight': tensor([[ 1.8125e-04,  5.3087e-05,  7.5244e-05,  ..., -1.8669e-04,\n          2.2056e-04, -1.3803e-04],\n        [-9.6279e-05, -1.9539e-04, -6.0395e-05,  ...,  5.7935e-05,\n          1.7727e-04,  6.0118e-05],\n        [-2.0234e-04,  1.7012e-04, -3.4883e-04,  ...,  2.4673e-04,\n         -3.9576e-04,  4.9365e-04],\n        ...,\n        [ 1.8505e-05, -2.6453e-04, -3.1086e-04,  ...,  1.0067e-04,\n          2.3859e-04,  2.5845e-04],\n        [-3.0491e-04,  7.5653e-04, -3.4332e-04,  ...,  3.9646e-04,\n         -2.6484e-04, -9.3500e-05],\n        [ 1.0829e-03,  1.7614e-04,  2.4032e-03,  ..., -1.0829e-03,\n          1.1291e-03, -3.0203e-03]]), 'transformer.encoder.layers.14.self_attention.dense.lora_A.default.weight': tensor([[ 0.0043,  0.0020,  0.0035,  ...,  0.0064,  0.0020, -0.0122],\n        [ 0.0048, -0.0033,  0.0114,  ...,  0.0131, -0.0028,  0.0127],\n        [-0.0088, -0.0026, -0.0037,  ..., -0.0135, -0.0139,  0.0111],\n        ...,\n        [ 0.0032, -0.0147, -0.0027,  ...,  0.0002,  0.0134,  0.0118],\n        [ 0.0019, -0.0132,  0.0090,  ..., -0.0013, -0.0065, -0.0027],\n        [ 0.0125,  0.0067, -0.0073,  ..., -0.0012, -0.0099, -0.0015]]), 'transformer.encoder.layers.14.self_attention.dense.lora_B.default.weight': tensor([[-8.8037e-04, -7.9422e-04,  1.0056e-03,  ..., -2.3510e-04,\n         -1.4875e-03, -8.9987e-04],\n        [ 3.0244e-04,  3.8693e-04,  2.9719e-04,  ..., -1.3696e-04,\n         -6.2387e-04, -4.2795e-04],\n        [-7.4022e-04, -8.5571e-04,  9.8748e-04,  ...,  4.3851e-04,\n         -1.1016e-03, -1.0859e-03],\n        ...,\n        [ 9.3347e-04,  7.7861e-05, -1.1720e-03,  ...,  1.5845e-04,\n          8.0470e-04,  1.2068e-03],\n        [-9.7086e-04, -8.5828e-05,  1.0187e-03,  ..., -9.9792e-04,\n         -1.2141e-03, -1.7131e-03],\n        [-1.7478e-04, -4.7719e-04,  3.9943e-04,  ..., -4.2764e-05,\n         -1.1268e-03, -9.0943e-04]]), 'transformer.encoder.layers.14.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0050,  0.0143, -0.0014,  ...,  0.0089, -0.0084,  0.0154],\n        [ 0.0053,  0.0024, -0.0012,  ..., -0.0142,  0.0103, -0.0107],\n        [ 0.0079, -0.0105, -0.0145,  ..., -0.0095, -0.0123,  0.0035],\n        ...,\n        [ 0.0042,  0.0011,  0.0050,  ..., -0.0018,  0.0040, -0.0009],\n        [ 0.0079, -0.0018,  0.0024,  ..., -0.0038, -0.0001, -0.0068],\n        [ 0.0091, -0.0120,  0.0047,  ..., -0.0046, -0.0031, -0.0133]]), 'transformer.encoder.layers.14.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-4.0869e-04,  1.2682e-03, -5.6362e-04,  ...,  9.4863e-04,\n          6.9336e-04,  3.8678e-04],\n        [ 1.2768e-04, -2.5273e-04,  5.7676e-04,  ..., -4.9212e-04,\n         -2.3946e-04,  5.4046e-05],\n        [-5.6153e-04, -7.5596e-04,  2.9015e-04,  ..., -4.3431e-04,\n         -4.1562e-04, -2.3533e-04],\n        ...,\n        [-4.1062e-04, -1.5239e-03,  6.5178e-04,  ..., -1.3285e-03,\n         -1.1710e-03, -8.5083e-04],\n        [ 5.4355e-05,  2.4687e-04, -3.1510e-04,  ...,  2.2105e-04,\n         -2.2908e-04,  3.4498e-04],\n        [-6.0203e-04, -1.5414e-04, -2.0355e-04,  ...,  2.3316e-04,\n          1.9454e-05, -1.6742e-04]]), 'transformer.encoder.layers.14.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0046, -0.0047,  0.0061,  ...,  0.0037, -0.0069,  0.0047],\n        [ 0.0007,  0.0047,  0.0074,  ...,  0.0020,  0.0082, -0.0012],\n        [ 0.0021, -0.0023, -0.0011,  ...,  0.0016,  0.0021,  0.0022],\n        ...,\n        [ 0.0064,  0.0082, -0.0083,  ..., -0.0066,  0.0018, -0.0056],\n        [-0.0031,  0.0037,  0.0059,  ..., -0.0022,  0.0067, -0.0022],\n        [-0.0081, -0.0029,  0.0079,  ..., -0.0042, -0.0037,  0.0051]]), 'transformer.encoder.layers.14.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 3.3141e-04,  2.9802e-04, -6.9690e-04,  ..., -1.1555e-04,\n          5.4472e-04, -2.7626e-04],\n        [-3.1853e-04,  4.1703e-04,  1.9031e-04,  ...,  3.8083e-04,\n         -5.0445e-04, -1.3844e-04],\n        [ 1.0199e-03, -1.0751e-05, -1.0788e-03,  ..., -1.5948e-03,\n          1.5601e-03, -1.0563e-03],\n        ...,\n        [-1.9301e-04, -5.1746e-04,  3.7999e-04,  ...,  6.1888e-04,\n         -8.3858e-04, -1.7058e-05],\n        [-2.3496e-04,  6.4971e-04,  1.6095e-04,  ..., -1.9989e-03,\n          1.9223e-03, -1.1028e-03],\n        [ 2.7813e-04,  2.4763e-04, -2.1524e-04,  ..., -1.5689e-03,\n          1.3189e-03, -1.3071e-03]]), 'transformer.encoder.layers.15.self_attention.query_key_value.lora_A.default.weight': tensor([[ 1.0861e-02, -3.8414e-03,  4.1739e-05,  ..., -1.0445e-02,\n          2.7586e-03,  1.4249e-02],\n        [ 1.3201e-02, -1.5935e-03, -6.2083e-03,  ...,  1.1956e-03,\n         -6.8226e-03,  6.4703e-03],\n        [ 1.2630e-02, -9.1952e-05,  2.2986e-03,  ...,  1.3788e-02,\n         -3.5727e-03, -1.3945e-02],\n        ...,\n        [-3.9044e-03,  1.2422e-02, -1.2479e-02,  ..., -4.5678e-03,\n         -1.4844e-02,  7.5475e-03],\n        [ 7.7057e-03, -2.0382e-04,  1.3155e-02,  ..., -4.8490e-04,\n          4.6743e-03, -5.2782e-03],\n        [ 7.0024e-03, -4.2935e-03,  8.2443e-03,  ...,  9.7307e-03,\n         -1.3814e-02,  8.0695e-03]]), 'transformer.encoder.layers.15.self_attention.query_key_value.lora_B.default.weight': tensor([[ 1.3045e-05,  9.5139e-05,  1.0387e-04,  ...,  1.4369e-04,\n         -2.6891e-04,  1.3076e-05],\n        [ 3.1215e-05,  1.3433e-04,  2.8827e-04,  ...,  4.5982e-04,\n          1.1636e-04,  3.0680e-04],\n        [ 2.5880e-04, -1.7184e-04,  5.1995e-05,  ...,  4.3566e-05,\n          6.2420e-05,  4.0218e-05],\n        ...,\n        [ 9.0968e-04,  3.5050e-04, -5.7707e-04,  ..., -4.8715e-04,\n         -1.3037e-04,  6.5393e-04],\n        [-8.0880e-04, -8.3373e-04, -5.4567e-04,  ...,  1.0876e-03,\n          9.7188e-04,  6.3216e-04],\n        [-6.5644e-05, -8.4798e-04,  4.3825e-04,  ...,  5.0232e-04,\n          9.3637e-04,  1.2545e-03]]), 'transformer.encoder.layers.15.self_attention.dense.lora_A.default.weight': tensor([[ 0.0043, -0.0135, -0.0120,  ...,  0.0109,  0.0012, -0.0115],\n        [-0.0027,  0.0146,  0.0041,  ..., -0.0063,  0.0142,  0.0133],\n        [ 0.0026, -0.0013, -0.0070,  ...,  0.0136,  0.0119, -0.0035],\n        ...,\n        [ 0.0153, -0.0004, -0.0063,  ..., -0.0144, -0.0042, -0.0112],\n        [-0.0063, -0.0116,  0.0016,  ...,  0.0018,  0.0023,  0.0107],\n        [ 0.0050, -0.0038,  0.0086,  ...,  0.0061,  0.0148,  0.0135]]), 'transformer.encoder.layers.15.self_attention.dense.lora_B.default.weight': tensor([[-4.5400e-04, -6.5113e-04, -6.3984e-04,  ...,  5.8211e-04,\n          5.0807e-04,  7.2836e-04],\n        [ 7.7241e-04,  3.1742e-04,  8.7225e-04,  ..., -3.8018e-04,\n         -8.3924e-04, -3.9574e-04],\n        [ 2.7860e-04, -5.3940e-04, -9.3742e-04,  ...,  5.5527e-04,\n          7.0589e-04,  5.9615e-04],\n        ...,\n        [-4.9451e-05,  1.6681e-04,  5.5514e-04,  ...,  1.8318e-05,\n         -3.6974e-04, -1.3033e-04],\n        [ 1.1899e-03, -1.2572e-04, -4.5246e-04,  ...,  5.5359e-04,\n          2.4625e-04, -1.4421e-04],\n        [ 8.9455e-04,  2.9706e-04, -1.3899e-04,  ..., -8.8595e-05,\n         -3.0269e-04, -3.9423e-04]]), 'transformer.encoder.layers.15.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0058, -0.0060,  0.0046,  ...,  0.0075,  0.0158, -0.0011],\n        [ 0.0028, -0.0078, -0.0121,  ..., -0.0089,  0.0038,  0.0055],\n        [ 0.0101, -0.0038, -0.0062,  ..., -0.0132, -0.0116,  0.0094],\n        ...,\n        [ 0.0018,  0.0043,  0.0133,  ..., -0.0036, -0.0135,  0.0088],\n        [ 0.0012,  0.0007,  0.0077,  ...,  0.0151, -0.0121,  0.0006],\n        [-0.0141, -0.0134,  0.0137,  ...,  0.0040, -0.0039, -0.0054]]), 'transformer.encoder.layers.15.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-5.1040e-04, -2.4803e-04, -3.7858e-04,  ..., -2.3190e-04,\n          6.9739e-04, -8.2286e-04],\n        [-2.4974e-03,  9.5274e-06, -1.4681e-03,  ...,  7.1010e-04,\n          7.6595e-04, -6.2007e-04],\n        [-9.5570e-04, -1.0195e-04, -5.6199e-04,  ...,  4.1810e-04,\n          4.0359e-04, -3.1226e-04],\n        ...,\n        [ 4.4696e-05,  4.5028e-04,  1.3340e-04,  ...,  4.0933e-04,\n         -3.7662e-05,  7.3219e-04],\n        [-1.0409e-03,  1.1090e-05, -6.2006e-04,  ...,  6.1564e-04,\n          2.9828e-04,  3.0194e-04],\n        [-4.5557e-05, -2.8849e-04, -2.0907e-04,  ..., -4.2543e-04,\n          1.5926e-04,  2.2191e-04]]), 'transformer.encoder.layers.15.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 8.4576e-03,  5.1238e-03, -8.1360e-03,  ...,  2.2001e-03,\n         -3.8943e-03,  2.2179e-04],\n        [ 4.6608e-05,  3.3161e-03, -2.1354e-03,  ..., -2.1226e-03,\n         -4.1370e-03, -2.7710e-03],\n        [-2.4032e-03, -8.3577e-03,  2.0172e-03,  ..., -7.7269e-03,\n         -1.4999e-03,  1.9665e-03],\n        ...,\n        [ 8.2675e-03, -9.1231e-04,  4.7642e-03,  ..., -9.7609e-04,\n         -4.4655e-03, -1.2262e-03],\n        [-8.2113e-03,  1.8759e-03,  7.3398e-04,  ...,  7.9442e-03,\n          4.5019e-03, -5.2210e-03],\n        [-8.6304e-04,  5.2124e-03,  2.6622e-03,  ...,  4.4109e-03,\n         -5.8655e-03, -3.7641e-03]]), 'transformer.encoder.layers.15.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-1.3330e-03,  5.5479e-04, -7.8439e-04,  ...,  6.1433e-04,\n          1.0168e-03, -5.4933e-04],\n        [ 3.4607e-04, -9.9376e-05, -5.2542e-05,  ...,  3.1788e-05,\n         -4.7771e-05, -2.4397e-05],\n        [ 5.6808e-05,  7.8844e-04,  5.1267e-04,  ...,  5.3441e-04,\n          8.2304e-04, -9.8211e-04],\n        ...,\n        [ 7.8466e-04, -1.2300e-03, -1.2848e-04,  ..., -5.8122e-04,\n         -1.0640e-03,  8.5088e-04],\n        [ 1.0614e-04,  8.2441e-04,  3.8440e-04,  ..., -4.2748e-04,\n          1.0107e-04, -1.5459e-03],\n        [-2.6967e-04,  1.5756e-03, -3.3422e-04,  ...,  9.5960e-04,\n          1.3668e-03, -2.3843e-03]]), 'transformer.encoder.layers.16.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0014, -0.0132,  0.0119,  ..., -0.0112, -0.0090,  0.0123],\n        [-0.0001, -0.0149,  0.0117,  ..., -0.0084, -0.0148, -0.0052],\n        [-0.0091,  0.0061, -0.0141,  ...,  0.0122, -0.0013, -0.0109],\n        ...,\n        [ 0.0045,  0.0106, -0.0049,  ..., -0.0007, -0.0111, -0.0100],\n        [-0.0086, -0.0131, -0.0116,  ...,  0.0035, -0.0054,  0.0038],\n        [-0.0104,  0.0110,  0.0019,  ...,  0.0110,  0.0094,  0.0040]]), 'transformer.encoder.layers.16.self_attention.query_key_value.lora_B.default.weight': tensor([[ 2.5918e-04,  7.4498e-05, -1.9251e-04,  ..., -2.6075e-06,\n         -4.8598e-05, -7.5103e-06],\n        [ 1.1291e-04,  3.7955e-04, -2.8413e-04,  ..., -1.8256e-04,\n          3.3218e-04, -1.5253e-04],\n        [ 1.6030e-04,  2.9333e-04, -2.6651e-04,  ..., -2.2556e-04,\n          2.3338e-04, -2.3639e-04],\n        ...,\n        [ 1.4093e-03,  3.6970e-03, -4.3111e-03,  ..., -2.3112e-03,\n          2.5305e-03, -2.3818e-03],\n        [ 8.8547e-04,  1.6694e-03, -1.4002e-03,  ..., -8.6242e-05,\n          9.0770e-04, -1.2530e-03],\n        [ 1.2885e-03,  2.1860e-03, -2.9697e-03,  ..., -1.0353e-03,\n          1.8233e-03, -9.1245e-04]]), 'transformer.encoder.layers.16.self_attention.dense.lora_A.default.weight': tensor([[ 0.0053,  0.0104, -0.0105,  ...,  0.0107,  0.0102, -0.0106],\n        [-0.0014, -0.0149,  0.0014,  ..., -0.0119,  0.0062,  0.0014],\n        [-0.0081, -0.0051,  0.0126,  ...,  0.0094,  0.0061, -0.0073],\n        ...,\n        [ 0.0114,  0.0021,  0.0145,  ..., -0.0117,  0.0091, -0.0007],\n        [-0.0024,  0.0094,  0.0003,  ..., -0.0133,  0.0060,  0.0160],\n        [-0.0083, -0.0057, -0.0140,  ..., -0.0100, -0.0035, -0.0091]]), 'transformer.encoder.layers.16.self_attention.dense.lora_B.default.weight': tensor([[-1.0122e-03, -2.4985e-04,  9.6365e-04,  ..., -3.8635e-04,\n         -3.1764e-04, -1.1690e-03],\n        [ 9.3789e-04,  6.3615e-04, -3.5571e-04,  ..., -2.2845e-05,\n          7.9170e-04,  5.6075e-04],\n        [-8.3229e-04, -4.1235e-05,  6.2617e-04,  ..., -1.4096e-03,\n         -3.2962e-04, -8.4072e-04],\n        ...,\n        [ 8.5454e-05,  5.4960e-04, -2.7080e-04,  ...,  1.7054e-04,\n          4.5346e-05,  4.2918e-04],\n        [-1.3066e-04, -1.6973e-04, -3.1631e-05,  ..., -7.9304e-04,\n          7.6917e-05,  3.2972e-04],\n        [ 7.9479e-04,  1.1543e-04, -2.0146e-04,  ..., -7.4148e-04,\n          7.4729e-04,  5.9137e-04]]), 'transformer.encoder.layers.16.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0062, -0.0061, -0.0085,  ...,  0.0051, -0.0013, -0.0160],\n        [ 0.0051,  0.0152, -0.0108,  ..., -0.0013,  0.0024,  0.0111],\n        [ 0.0113, -0.0117,  0.0064,  ..., -0.0015, -0.0032,  0.0008],\n        ...,\n        [ 0.0114,  0.0066,  0.0066,  ..., -0.0110, -0.0020,  0.0014],\n        [-0.0025,  0.0100,  0.0025,  ..., -0.0040, -0.0111, -0.0006],\n        [ 0.0057, -0.0057,  0.0045,  ..., -0.0107,  0.0134,  0.0097]]), 'transformer.encoder.layers.16.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 1.9603e-05,  7.4594e-04, -3.5713e-04,  ...,  2.4643e-04,\n         -1.1158e-04,  2.9052e-04],\n        [ 6.5580e-07, -3.8752e-04, -1.3895e-04,  ...,  7.3352e-05,\n         -3.4637e-04,  1.0444e-05],\n        [ 1.1365e-04,  2.7316e-04, -5.7457e-05,  ...,  1.3068e-04,\n          5.7457e-06,  4.1834e-05],\n        ...,\n        [ 3.1983e-04, -1.0371e-06,  5.6812e-05,  ...,  2.8734e-04,\n          2.0160e-04,  2.8305e-04],\n        [ 3.8060e-04, -2.2376e-04,  1.0545e-04,  ...,  5.1651e-04,\n         -2.6785e-04, -4.9457e-04],\n        [ 7.3788e-04,  9.8060e-05,  7.2359e-04,  ...,  9.6660e-04,\n         -2.0728e-04, -2.7771e-04]]), 'transformer.encoder.layers.16.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-3.1120e-03,  3.1109e-03,  1.8735e-03,  ...,  1.7669e-03,\n         -3.8848e-03,  7.2550e-03],\n        [-1.3033e-03,  8.2686e-03,  4.7694e-03,  ...,  5.7733e-03,\n          3.5700e-03, -4.7395e-03],\n        [-5.5215e-03,  3.0996e-03,  7.6274e-03,  ...,  5.2444e-03,\n          7.4581e-03,  5.9598e-03],\n        ...,\n        [-6.0285e-03, -5.1201e-03,  3.8696e-03,  ..., -5.4784e-03,\n         -5.0521e-03,  6.4217e-03],\n        [ 1.2812e-03,  2.5515e-05,  5.0726e-03,  ..., -6.2907e-03,\n          3.1865e-03,  1.1215e-03],\n        [ 8.0663e-03, -4.3213e-03,  4.5397e-04,  ..., -6.9287e-03,\n         -4.8455e-03, -5.4257e-03]]), 'transformer.encoder.layers.16.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-4.6627e-04, -3.6109e-04, -2.2815e-04,  ..., -3.6442e-04,\n         -3.7990e-04, -5.3401e-04],\n        [-3.6297e-04, -1.9298e-04, -1.4621e-04,  ...,  1.8945e-05,\n          1.2970e-03, -5.6730e-04],\n        [ 3.3654e-04,  6.9176e-04, -1.0134e-03,  ...,  1.7848e-04,\n         -5.4204e-04, -6.2567e-04],\n        ...,\n        [-5.4762e-04, -1.6654e-04,  2.7231e-04,  ..., -8.0415e-04,\n          1.5698e-05,  6.2062e-04],\n        [ 5.4518e-04,  7.2712e-04, -5.1428e-04,  ..., -1.7611e-05,\n         -3.1313e-04, -1.6781e-04],\n        [ 9.6444e-05,  3.3716e-04, -7.1769e-05,  ...,  3.8551e-04,\n          6.3531e-04,  9.7017e-05]]), 'transformer.encoder.layers.17.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0016, -0.0090, -0.0080,  ..., -0.0146,  0.0130, -0.0124],\n        [ 0.0041,  0.0062, -0.0121,  ..., -0.0107,  0.0151, -0.0051],\n        [ 0.0149,  0.0142, -0.0110,  ...,  0.0022,  0.0098, -0.0053],\n        ...,\n        [ 0.0094,  0.0133,  0.0015,  ...,  0.0096,  0.0023, -0.0064],\n        [ 0.0111,  0.0021, -0.0064,  ...,  0.0075, -0.0118, -0.0012],\n        [-0.0136,  0.0091, -0.0010,  ...,  0.0019, -0.0080,  0.0007]]), 'transformer.encoder.layers.17.self_attention.query_key_value.lora_B.default.weight': tensor([[-6.2282e-04, -4.1987e-04,  4.5515e-04,  ..., -6.7975e-04,\n         -2.4844e-04,  3.8078e-04],\n        [ 9.6206e-05,  2.6893e-05, -4.1635e-04,  ...,  4.6244e-04,\n          5.1032e-06, -1.2591e-04],\n        [ 5.9293e-04,  1.0589e-04, -2.2097e-04,  ...,  2.2430e-04,\n          2.6841e-04, -5.9013e-04],\n        ...,\n        [ 5.4245e-04,  7.8409e-04, -4.1879e-04,  ...,  4.6850e-04,\n          1.1504e-03, -3.2397e-05],\n        [-1.1932e-03, -6.0026e-04, -1.2839e-03,  ..., -7.4927e-04,\n          2.9770e-04,  1.3436e-04],\n        [ 3.6907e-04, -1.8954e-04, -9.8352e-04,  ...,  5.5301e-04,\n          3.4896e-04, -2.9369e-05]]), 'transformer.encoder.layers.17.self_attention.dense.lora_A.default.weight': tensor([[-0.0041,  0.0064,  0.0072,  ...,  0.0075,  0.0041,  0.0113],\n        [ 0.0109,  0.0005, -0.0094,  ..., -0.0023,  0.0140,  0.0112],\n        [ 0.0026,  0.0084,  0.0011,  ...,  0.0008,  0.0014, -0.0109],\n        ...,\n        [-0.0116,  0.0058, -0.0120,  ...,  0.0119, -0.0057, -0.0102],\n        [-0.0109,  0.0154, -0.0086,  ...,  0.0122, -0.0085, -0.0081],\n        [ 0.0078,  0.0119, -0.0105,  ...,  0.0034,  0.0008, -0.0046]]), 'transformer.encoder.layers.17.self_attention.dense.lora_B.default.weight': tensor([[ 2.8566e-05,  3.1594e-04, -3.1093e-04,  ...,  6.1467e-04,\n          5.4561e-04, -3.6129e-04],\n        [-1.0749e-03, -9.1801e-04, -9.4529e-04,  ..., -8.5971e-04,\n         -1.5546e-04, -1.2241e-04],\n        [ 9.0407e-04,  1.1219e-03,  6.1740e-04,  ...,  1.2116e-03,\n         -1.2622e-03,  4.0271e-04],\n        ...,\n        [-7.3146e-04, -1.3681e-03, -6.8489e-04,  ..., -1.0357e-03,\n          1.0547e-03,  5.4013e-04],\n        [ 2.8019e-04,  3.7307e-04,  1.7772e-04,  ...,  2.0836e-05,\n         -4.6683e-04,  2.3372e-04],\n        [-6.5271e-05, -1.1616e-03, -8.8101e-04,  ..., -1.7006e-03,\n         -5.0049e-04, -8.5785e-05]]), 'transformer.encoder.layers.17.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0087, -0.0036, -0.0149,  ...,  0.0039, -0.0111, -0.0042],\n        [ 0.0126, -0.0148,  0.0095,  ..., -0.0045, -0.0153, -0.0049],\n        [ 0.0143, -0.0138,  0.0091,  ...,  0.0057, -0.0098,  0.0091],\n        ...,\n        [-0.0002,  0.0021,  0.0017,  ..., -0.0118,  0.0097, -0.0041],\n        [-0.0001,  0.0138, -0.0097,  ...,  0.0073, -0.0135, -0.0115],\n        [-0.0106, -0.0040, -0.0101,  ..., -0.0077,  0.0090,  0.0079]]), 'transformer.encoder.layers.17.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 7.9063e-05, -3.8642e-05,  2.9159e-04,  ..., -4.3502e-05,\n         -4.4121e-04, -1.5003e-04],\n        [-8.1002e-04,  1.9564e-04, -8.3484e-04,  ...,  3.8922e-04,\n          6.7558e-04,  2.5422e-04],\n        [ 3.2821e-04, -1.5262e-04, -5.9634e-05,  ...,  1.7440e-04,\n          4.5530e-04, -7.9814e-05],\n        ...,\n        [ 1.5744e-04,  8.1907e-04,  1.4701e-03,  ..., -9.1178e-04,\n         -6.0688e-04, -1.4027e-04],\n        [-5.3278e-05, -1.9526e-04,  2.4739e-04,  ...,  1.7083e-04,\n         -2.7500e-04,  9.2138e-05],\n        [-9.2686e-04, -1.0099e-03, -2.6352e-04,  ...,  2.6572e-04,\n          5.7458e-04,  6.8898e-04]]), 'transformer.encoder.layers.17.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 5.2980e-03, -7.2462e-03,  6.5741e-03,  ..., -1.0655e-03,\n          6.2529e-04,  4.6722e-03],\n        [-4.4602e-03,  4.6123e-03, -6.4101e-03,  ...,  7.8775e-03,\n         -6.5187e-03,  8.0162e-03],\n        [ 8.0320e-03, -2.2740e-03, -1.4145e-03,  ..., -3.4382e-03,\n         -6.6252e-03, -4.1997e-03],\n        ...,\n        [-3.8655e-03,  3.9737e-03,  7.7241e-03,  ...,  3.5457e-03,\n         -1.2845e-03, -9.2787e-05],\n        [-8.0487e-03,  6.6471e-03, -1.2536e-03,  ...,  2.6905e-03,\n          4.7536e-03, -7.8613e-03],\n        [-3.4491e-03, -5.1530e-04, -8.5859e-03,  ...,  6.9439e-03,\n          5.1908e-03, -1.4742e-03]]), 'transformer.encoder.layers.17.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-2.3051e-04,  7.9499e-05, -3.9989e-04,  ...,  8.7549e-04,\n         -2.0273e-04,  4.5430e-04],\n        [ 4.0215e-04,  2.3697e-05, -3.5718e-04,  ...,  1.9550e-04,\n         -7.1070e-05, -1.6608e-04],\n        [ 1.8109e-03,  8.3014e-04,  1.4954e-03,  ..., -3.6972e-05,\n          1.6477e-05, -7.0918e-04],\n        ...,\n        [-2.2272e-03, -1.5658e-03, -1.4418e-03,  ...,  7.3411e-04,\n          1.1951e-04,  1.1091e-03],\n        [ 1.2612e-03,  2.5884e-04,  1.2112e-03,  ...,  7.7357e-04,\n         -9.5321e-04, -5.7625e-04],\n        [ 8.9762e-04, -3.4365e-04, -6.2814e-04,  ...,  6.7574e-04,\n         -1.0650e-03, -1.4209e-04]]), 'transformer.encoder.layers.18.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0068, -0.0093,  0.0097,  ..., -0.0007,  0.0095, -0.0145],\n        [ 0.0031,  0.0152, -0.0097,  ...,  0.0098, -0.0105, -0.0076],\n        [ 0.0081,  0.0036,  0.0153,  ..., -0.0063, -0.0054,  0.0141],\n        ...,\n        [-0.0088, -0.0029,  0.0060,  ...,  0.0022,  0.0052,  0.0020],\n        [-0.0055, -0.0005, -0.0037,  ..., -0.0071, -0.0130,  0.0131],\n        [ 0.0096,  0.0058, -0.0105,  ..., -0.0130,  0.0155, -0.0097]]), 'transformer.encoder.layers.18.self_attention.query_key_value.lora_B.default.weight': tensor([[-2.9217e-04, -2.1535e-04, -3.8102e-05,  ...,  1.9308e-04,\n         -1.0366e-04, -9.2855e-05],\n        [ 5.1288e-05, -9.1696e-05, -3.3840e-05,  ..., -5.1330e-05,\n          1.6675e-05, -2.6816e-04],\n        [ 2.1419e-04,  7.8519e-05, -1.4131e-05,  ..., -7.1923e-05,\n          3.6174e-05,  3.4565e-05],\n        ...,\n        [ 2.2839e-04, -1.0051e-04,  3.1199e-04,  ...,  2.4018e-04,\n         -4.9485e-04,  7.9171e-05],\n        [-1.6809e-03, -1.6615e-03,  5.7654e-04,  ...,  7.1525e-04,\n          1.1050e-03, -3.9639e-05],\n        [ 1.0538e-03,  1.2461e-03,  6.2761e-04,  ..., -1.4040e-03,\n         -1.7147e-03,  4.1587e-06]]), 'transformer.encoder.layers.18.self_attention.dense.lora_A.default.weight': tensor([[ 0.0091, -0.0150, -0.0119,  ...,  0.0107,  0.0140, -0.0158],\n        [ 0.0112,  0.0022,  0.0153,  ..., -0.0030,  0.0062,  0.0002],\n        [ 0.0144, -0.0146, -0.0074,  ...,  0.0096,  0.0084, -0.0154],\n        ...,\n        [ 0.0075,  0.0121,  0.0072,  ...,  0.0153, -0.0130,  0.0049],\n        [-0.0016, -0.0099,  0.0031,  ..., -0.0113, -0.0122,  0.0075],\n        [-0.0026, -0.0133, -0.0096,  ...,  0.0095, -0.0125,  0.0145]]), 'transformer.encoder.layers.18.self_attention.dense.lora_B.default.weight': tensor([[-6.5150e-04,  3.7132e-04, -1.2638e-04,  ...,  3.0241e-04,\n          3.0214e-04, -1.9846e-04],\n        [-1.0313e-03,  6.6650e-04, -3.4357e-04,  ..., -4.4766e-05,\n          4.5723e-04, -3.4206e-04],\n        [ 1.9114e-04,  2.1685e-03, -1.0281e-03,  ..., -7.8783e-04,\n          1.5000e-03, -2.7214e-04],\n        ...,\n        [-1.7981e-04, -1.9042e-03,  9.0142e-04,  ...,  1.1033e-03,\n         -1.5623e-03, -1.6276e-04],\n        [ 2.7305e-04,  1.5618e-03, -6.2924e-04,  ..., -1.4703e-03,\n          9.0787e-04,  1.7059e-04],\n        [-7.1795e-04,  8.1724e-04, -5.7769e-04,  ...,  1.7725e-04,\n          3.0028e-04, -5.8863e-04]]), 'transformer.encoder.layers.18.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0163,  0.0097, -0.0006,  ...,  0.0131, -0.0080,  0.0003],\n        [ 0.0122, -0.0151, -0.0117,  ..., -0.0064, -0.0052, -0.0042],\n        [ 0.0133,  0.0114,  0.0101,  ...,  0.0023, -0.0113,  0.0040],\n        ...,\n        [-0.0127, -0.0002,  0.0011,  ..., -0.0010,  0.0114,  0.0064],\n        [ 0.0011,  0.0034, -0.0116,  ..., -0.0103, -0.0051, -0.0076],\n        [-0.0042, -0.0041,  0.0103,  ...,  0.0125,  0.0052,  0.0053]]), 'transformer.encoder.layers.18.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-2.5757e-05,  6.0640e-04,  2.0311e-04,  ...,  1.9811e-04,\n          4.4109e-04, -2.7798e-04],\n        [-1.4740e-04, -1.1080e-04, -2.9069e-04,  ...,  6.0782e-05,\n          3.0354e-04,  2.1484e-04],\n        [ 2.7468e-04, -4.7168e-04, -5.9162e-04,  ...,  4.7939e-04,\n          5.3200e-04, -1.4414e-04],\n        ...,\n        [-5.2452e-04,  1.6644e-03,  6.2266e-04,  ..., -1.4678e-03,\n         -1.3096e-03,  1.5319e-03],\n        [ 6.9643e-05,  1.3423e-04,  3.1834e-04,  ..., -1.8438e-04,\n         -5.0592e-04,  2.1503e-04],\n        [ 2.2760e-05,  3.0765e-04,  3.2949e-04,  ..., -1.5414e-04,\n         -4.9352e-04,  7.1681e-04]]), 'transformer.encoder.layers.18.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 4.9128e-04,  2.3900e-03, -1.3932e-03,  ...,  3.4318e-03,\n          6.0233e-03,  2.4322e-03],\n        [-5.8792e-03, -8.3126e-03, -5.9525e-03,  ..., -2.0436e-03,\n         -2.2721e-03, -6.8457e-03],\n        [-7.5811e-03,  7.3602e-03,  2.2649e-03,  ...,  3.6337e-03,\n          7.2726e-03, -8.1677e-03],\n        ...,\n        [ 2.2884e-04,  8.4863e-03, -2.4564e-03,  ...,  5.2845e-03,\n          5.0283e-03,  4.0557e-03],\n        [-8.5366e-03, -2.8004e-03,  3.9034e-03,  ...,  3.7556e-04,\n         -4.7083e-03,  1.9983e-03],\n        [ 3.3671e-05, -2.4665e-03, -6.5282e-03,  ..., -4.8930e-03,\n         -2.7279e-04,  1.2501e-03]]), 'transformer.encoder.layers.18.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 1.0117e-03,  2.8721e-04, -7.8639e-04,  ...,  9.8078e-04,\n          4.9954e-04, -2.6682e-04],\n        [-1.1915e-03, -7.3399e-04,  9.6643e-04,  ..., -1.4294e-03,\n         -7.6470e-04,  4.8185e-04],\n        [-7.4401e-04, -1.5165e-03,  1.7194e-03,  ..., -4.5319e-04,\n          8.7511e-04, -1.7728e-03],\n        ...,\n        [ 3.5819e-04,  1.3271e-03, -7.9353e-04,  ..., -5.8815e-04,\n         -7.1408e-04,  1.2071e-03],\n        [-7.8204e-04, -1.1083e-03,  1.2883e-03,  ..., -8.3650e-04,\n         -6.1431e-05, -9.0790e-04],\n        [-3.1246e-04,  1.0054e-03,  6.9168e-04,  ..., -1.2464e-03,\n         -4.6500e-04, -1.9877e-04]]), 'transformer.encoder.layers.19.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0057, -0.0149, -0.0010,  ...,  0.0024, -0.0004,  0.0089],\n        [ 0.0050,  0.0046, -0.0101,  ..., -0.0149, -0.0053, -0.0066],\n        [ 0.0061,  0.0019,  0.0018,  ..., -0.0026, -0.0074, -0.0146],\n        ...,\n        [ 0.0004,  0.0085,  0.0093,  ...,  0.0010, -0.0153,  0.0136],\n        [ 0.0104,  0.0099, -0.0052,  ..., -0.0097, -0.0133, -0.0024],\n        [ 0.0041, -0.0062, -0.0097,  ..., -0.0138,  0.0100,  0.0049]]), 'transformer.encoder.layers.19.self_attention.query_key_value.lora_B.default.weight': tensor([[-3.2843e-05,  1.9833e-05, -1.1825e-04,  ...,  8.2482e-05,\n          2.0090e-04,  3.3955e-05],\n        [-2.2094e-04,  3.2055e-05, -6.3699e-05,  ..., -8.3169e-05,\n         -1.9594e-04, -5.0996e-07],\n        [-2.4460e-04, -1.0287e-05, -1.1761e-04,  ..., -1.1386e-04,\n         -2.0679e-04,  1.7122e-04],\n        ...,\n        [-8.7166e-04, -5.5179e-04,  6.8716e-04,  ..., -3.9034e-04,\n         -1.7182e-03, -1.3553e-03],\n        [-7.7066e-05,  5.7950e-04, -1.2412e-04,  ...,  2.5256e-04,\n          9.6815e-04,  8.7418e-04],\n        [-4.0198e-04,  1.1632e-03, -1.1500e-03,  ...,  1.1294e-03,\n          1.8405e-03,  3.2535e-04]]), 'transformer.encoder.layers.19.self_attention.dense.lora_A.default.weight': tensor([[-0.0024,  0.0069, -0.0028,  ...,  0.0146, -0.0007,  0.0094],\n        [-0.0020,  0.0033, -0.0029,  ...,  0.0127, -0.0113,  0.0041],\n        [ 0.0143,  0.0144, -0.0014,  ...,  0.0078, -0.0007, -0.0031],\n        ...,\n        [ 0.0152, -0.0109,  0.0083,  ...,  0.0046, -0.0086, -0.0037],\n        [ 0.0013,  0.0100, -0.0113,  ...,  0.0098, -0.0008, -0.0119],\n        [ 0.0117, -0.0107,  0.0131,  ..., -0.0156, -0.0035, -0.0105]]), 'transformer.encoder.layers.19.self_attention.dense.lora_B.default.weight': tensor([[ 2.1647e-04, -8.4577e-04, -7.6620e-04,  ..., -1.6292e-04,\n         -2.8370e-04, -1.7130e-04],\n        [ 4.2803e-04,  1.2923e-03,  8.0248e-04,  ...,  8.7645e-05,\n          9.2071e-04, -8.3653e-05],\n        [ 7.9164e-04,  8.9611e-04,  1.1505e-03,  ...,  2.3528e-03,\n         -1.3856e-03, -1.1903e-03],\n        ...,\n        [-8.1564e-04, -1.0072e-03, -8.0740e-04,  ..., -1.9923e-03,\n          5.2955e-04,  9.0802e-04],\n        [ 4.7747e-04,  1.3814e-03, -1.4720e-06,  ...,  1.2648e-03,\n          4.8545e-04, -5.9169e-04],\n        [ 5.4062e-05,  5.3262e-04,  4.4075e-04,  ..., -4.0825e-04,\n          6.6577e-04,  3.0942e-04]]), 'transformer.encoder.layers.19.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0154,  0.0058,  0.0105,  ..., -0.0116, -0.0145, -0.0106],\n        [-0.0072,  0.0065, -0.0045,  ...,  0.0134,  0.0008,  0.0136],\n        [-0.0013,  0.0107,  0.0021,  ..., -0.0020,  0.0126, -0.0144],\n        ...,\n        [-0.0047, -0.0122, -0.0002,  ..., -0.0106, -0.0145,  0.0143],\n        [ 0.0135, -0.0098, -0.0131,  ...,  0.0144, -0.0073,  0.0074],\n        [ 0.0107,  0.0110,  0.0112,  ..., -0.0130, -0.0140,  0.0134]]), 'transformer.encoder.layers.19.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-1.1085e-03,  1.0820e-03, -3.8915e-04,  ...,  1.1544e-03,\n          1.1737e-03, -1.2557e-03],\n        [-2.3852e-05, -2.7092e-04,  7.4478e-05,  ..., -3.3833e-04,\n         -3.1971e-04, -2.8265e-04],\n        [ 5.1338e-04, -3.2222e-04,  2.5113e-04,  ..., -4.7351e-04,\n         -3.9278e-04, -1.7480e-04],\n        ...,\n        [-1.5594e-04,  1.5294e-04,  7.3754e-06,  ...,  1.8262e-05,\n         -8.9993e-06,  5.8172e-05],\n        [-5.9879e-06,  1.0098e-04,  8.2511e-05,  ...,  1.2409e-04,\n          1.0756e-05, -2.2160e-04],\n        [ 2.9660e-04, -1.2594e-04,  9.0536e-05,  ..., -1.8832e-04,\n         -9.3586e-05,  4.7693e-05]]), 'transformer.encoder.layers.19.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-5.4117e-03,  7.9220e-03,  1.5933e-05,  ...,  1.5838e-03,\n          6.6616e-03,  2.7299e-04],\n        [ 4.7969e-04,  5.1744e-03, -4.9950e-03,  ...,  4.0641e-03,\n          3.7230e-03, -5.2488e-03],\n        [ 6.1544e-03,  8.8356e-04, -4.4855e-03,  ..., -3.0033e-03,\n          4.8049e-04, -3.1042e-04],\n        ...,\n        [ 5.6400e-03,  8.7196e-04,  4.1919e-04,  ..., -1.5403e-03,\n         -2.6095e-03,  6.7340e-03],\n        [ 4.2044e-03,  6.6585e-03,  3.5444e-03,  ..., -2.8351e-03,\n          7.7022e-03,  2.2753e-03],\n        [ 5.1343e-03,  5.9398e-04, -3.8896e-03,  ..., -4.3190e-03,\n          8.2235e-03, -4.3230e-03]]), 'transformer.encoder.layers.19.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-1.0185e-03,  1.4573e-04,  8.1052e-04,  ..., -2.3700e-05,\n          4.2471e-04, -6.1096e-04],\n        [ 1.7467e-04, -9.6890e-04, -6.1367e-04,  ..., -1.0057e-03,\n          1.1073e-03,  3.1953e-04],\n        [-1.6151e-03, -5.9456e-04,  8.2880e-04,  ..., -1.4380e-03,\n          1.7352e-03, -4.5224e-04],\n        ...,\n        [ 8.5824e-04, -1.8240e-04, -9.8597e-04,  ...,  1.0727e-03,\n         -1.7029e-03,  6.7556e-04],\n        [-5.9283e-04, -4.4284e-04, -9.0910e-04,  ..., -7.9066e-04,\n          1.1169e-03, -4.5714e-04],\n        [ 3.1939e-04, -1.1218e-03, -8.4790e-04,  ..., -1.8158e-03,\n          1.3680e-03, -5.0772e-05]]), 'transformer.encoder.layers.20.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0039,  0.0009,  0.0103,  ..., -0.0145,  0.0022,  0.0095],\n        [ 0.0133,  0.0082, -0.0111,  ..., -0.0093, -0.0106, -0.0090],\n        [-0.0121,  0.0078, -0.0121,  ...,  0.0086, -0.0002, -0.0041],\n        ...,\n        [-0.0072, -0.0002,  0.0083,  ..., -0.0105, -0.0042, -0.0110],\n        [ 0.0019, -0.0033,  0.0069,  ...,  0.0140,  0.0102, -0.0074],\n        [ 0.0002,  0.0028, -0.0124,  ..., -0.0122,  0.0114, -0.0080]]), 'transformer.encoder.layers.20.self_attention.query_key_value.lora_B.default.weight': tensor([[-3.0266e-04,  7.6026e-06,  5.0524e-04,  ...,  1.5626e-04,\n          1.5958e-04, -2.3933e-04],\n        [-1.7470e-04, -4.6555e-05,  3.2142e-04,  ...,  4.8017e-04,\n          1.1095e-04, -2.6994e-04],\n        [-2.0530e-04, -4.5677e-05, -5.4982e-04,  ..., -4.7701e-04,\n          3.0455e-04,  1.5208e-04],\n        ...,\n        [ 5.8066e-04,  3.0264e-04, -5.0878e-04,  ...,  5.4825e-04,\n          3.0187e-04,  9.4579e-04],\n        [-1.0072e-03,  1.6854e-03,  1.0240e-03,  ..., -1.4124e-03,\n         -1.4206e-03,  6.7025e-04],\n        [ 1.5572e-03,  4.4485e-04, -6.4044e-04,  ..., -6.0543e-04,\n         -1.3097e-03, -2.3622e-04]]), 'transformer.encoder.layers.20.self_attention.dense.lora_A.default.weight': tensor([[ 3.3124e-03, -8.9069e-03,  9.6336e-03,  ...,  1.4172e-02,\n         -9.8840e-03, -8.4261e-03],\n        [-1.3516e-02, -1.2505e-02,  1.0533e-02,  ..., -6.9571e-03,\n          6.4218e-03,  1.5381e-02],\n        [-1.1074e-02, -3.9039e-04,  6.9830e-03,  ..., -2.3208e-05,\n         -1.0862e-02, -1.5406e-02],\n        ...,\n        [ 4.1384e-03, -3.2311e-03, -1.3691e-02,  ...,  5.2697e-03,\n          1.2635e-02,  5.9247e-03],\n        [-2.5344e-03,  8.5431e-03,  4.8564e-03,  ..., -1.8488e-03,\n         -1.3355e-02,  1.4058e-02],\n        [ 6.3440e-03, -1.5081e-02, -1.1012e-02,  ...,  4.9681e-03,\n         -8.8998e-03,  1.3949e-02]]), 'transformer.encoder.layers.20.self_attention.dense.lora_B.default.weight': tensor([[-4.4114e-04,  2.7694e-04, -3.5558e-04,  ...,  9.0929e-04,\n         -1.2988e-03,  2.3127e-04],\n        [ 4.9375e-04,  2.9892e-04,  2.2736e-04,  ..., -7.1557e-04,\n         -6.7468e-04, -2.4149e-04],\n        [-3.2765e-04, -4.5116e-04, -1.0325e-03,  ...,  1.9472e-03,\n         -2.8050e-03, -2.0237e-05],\n        ...,\n        [ 2.4028e-04,  1.3044e-03,  1.0554e-03,  ..., -2.0578e-03,\n          1.9944e-03, -1.7487e-04],\n        [ 7.7836e-04, -7.4931e-04, -3.3178e-04,  ...,  1.7077e-04,\n         -9.2141e-04, -1.1655e-04],\n        [-4.9106e-04, -3.5960e-04, -5.4560e-05,  ..., -2.2813e-04,\n         -4.8330e-04, -1.7757e-04]]), 'transformer.encoder.layers.20.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-0.0104,  0.0013,  0.0002,  ..., -0.0028,  0.0143,  0.0066],\n        [ 0.0082, -0.0013,  0.0052,  ...,  0.0060,  0.0034, -0.0074],\n        [ 0.0037, -0.0085,  0.0028,  ..., -0.0103,  0.0089, -0.0148],\n        ...,\n        [-0.0094, -0.0076, -0.0096,  ...,  0.0027, -0.0115,  0.0005],\n        [ 0.0114, -0.0063,  0.0092,  ..., -0.0073,  0.0003, -0.0104],\n        [-0.0039,  0.0003, -0.0159,  ...,  0.0011,  0.0075,  0.0066]]), 'transformer.encoder.layers.20.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-4.7988e-05,  2.0245e-04,  2.1456e-04,  ...,  1.4660e-04,\n         -3.8255e-06, -3.9609e-04],\n        [-1.4657e-03,  2.0120e-03,  1.3196e-03,  ...,  3.9818e-03,\n         -1.9111e-03, -1.8526e-03],\n        [-4.7395e-04,  2.4128e-05, -7.6477e-04,  ..., -9.2483e-04,\n          3.8911e-04, -2.8376e-04],\n        ...,\n        [-3.3301e-04,  6.8616e-04,  4.0877e-04,  ...,  3.8037e-04,\n         -7.5187e-04, -2.9701e-04],\n        [ 2.4702e-04,  4.1479e-04, -3.6511e-05,  ...,  2.2542e-03,\n         -1.2426e-03, -2.7279e-04],\n        [ 3.2192e-04, -3.1275e-04, -4.2077e-05,  ..., -3.6080e-04,\n          3.4952e-04, -4.9261e-04]]), 'transformer.encoder.layers.20.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-7.7452e-03, -6.9239e-03, -1.2660e-03,  ...,  8.3618e-03,\n         -6.7358e-03,  5.2903e-03],\n        [ 6.7490e-03, -7.1386e-03,  5.1098e-03,  ..., -4.0980e-03,\n         -1.8720e-03, -8.3574e-03],\n        [-2.5018e-03,  4.9054e-03,  3.6359e-03,  ..., -3.7874e-03,\n         -2.7785e-03,  2.9212e-03],\n        ...,\n        [ 3.8905e-03,  6.7747e-03, -5.0861e-03,  ..., -5.3490e-03,\n         -2.8270e-03,  5.9177e-03],\n        [ 9.1057e-05,  8.6039e-04, -6.8791e-03,  ...,  1.4122e-03,\n         -7.0467e-03, -6.6334e-03],\n        [-6.5721e-03, -6.9322e-03,  1.8273e-03,  ...,  7.0916e-03,\n          7.2205e-03,  7.6552e-03]]), 'transformer.encoder.layers.20.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-1.3048e-03, -8.2307e-04,  6.4686e-04,  ...,  1.1121e-04,\n          4.1970e-04,  8.7736e-04],\n        [ 2.5709e-04,  2.5836e-05,  1.3290e-03,  ...,  6.5573e-04,\n          3.0707e-04,  1.8572e-04],\n        [-1.9886e-03, -6.5769e-04,  1.6125e-03,  ...,  7.0990e-04,\n          1.0031e-03,  8.3896e-04],\n        ...,\n        [ 1.5488e-03,  9.3162e-04, -1.3318e-03,  ..., -3.7868e-04,\n         -8.5973e-04, -1.1392e-03],\n        [-1.0868e-05, -9.6547e-04,  2.4798e-03,  ...,  1.1242e-03,\n          6.3094e-04,  1.9270e-03],\n        [-7.1563e-05,  3.7225e-04,  1.5079e-03,  ..., -3.4459e-04,\n          1.9676e-04, -2.3073e-05]]), 'transformer.encoder.layers.21.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0029,  0.0121, -0.0053,  ..., -0.0108, -0.0103,  0.0057],\n        [-0.0099,  0.0062,  0.0011,  ...,  0.0115,  0.0020,  0.0153],\n        [ 0.0030, -0.0122,  0.0044,  ..., -0.0007,  0.0155, -0.0086],\n        ...,\n        [-0.0150, -0.0043,  0.0034,  ..., -0.0148, -0.0138,  0.0083],\n        [ 0.0081, -0.0015,  0.0137,  ...,  0.0108,  0.0006,  0.0013],\n        [ 0.0126, -0.0045, -0.0146,  ...,  0.0113, -0.0124,  0.0106]]), 'transformer.encoder.layers.21.self_attention.query_key_value.lora_B.default.weight': tensor([[ 3.1513e-04, -2.7239e-04, -1.2170e-04,  ..., -9.0711e-05,\n          2.6609e-04,  3.4191e-04],\n        [ 4.7374e-05, -2.9114e-04, -2.0414e-04,  ..., -6.8923e-05,\n          2.4267e-04,  5.0671e-04],\n        [ 1.4809e-04,  1.0465e-04, -6.8486e-06,  ..., -8.3551e-05,\n         -1.6835e-04,  3.3939e-04],\n        ...,\n        [ 7.6358e-05,  7.1466e-04, -5.2815e-05,  ...,  5.9879e-04,\n         -2.7323e-05, -3.9232e-04],\n        [ 1.2863e-03,  1.6342e-04, -1.5612e-03,  ...,  5.0552e-04,\n         -1.8173e-03, -1.0672e-03],\n        [ 1.9130e-03,  1.0185e-03, -2.2672e-03,  ...,  1.3008e-03,\n         -3.4149e-03, -2.6268e-03]]), 'transformer.encoder.layers.21.self_attention.dense.lora_A.default.weight': tensor([[-7.7921e-03,  6.7425e-03, -5.1144e-03,  ...,  5.4258e-05,\n          1.6197e-02,  7.8187e-03],\n        [-4.1524e-03,  5.2251e-03,  8.5337e-03,  ...,  1.5360e-02,\n         -3.5200e-03, -6.1265e-03],\n        [ 9.2851e-04, -1.0871e-04,  1.2180e-03,  ...,  1.1531e-02,\n          4.7049e-03,  9.0298e-03],\n        ...,\n        [ 1.1254e-02, -1.2216e-02,  6.2034e-05,  ..., -5.5550e-03,\n         -6.9472e-04, -6.8806e-03],\n        [ 9.3424e-03, -4.1662e-03,  9.7010e-03,  ..., -1.4902e-02,\n          1.6233e-02,  1.3968e-03],\n        [ 3.7943e-03,  5.6097e-03,  7.9213e-03,  ...,  8.9123e-03,\n         -1.0896e-03, -1.1854e-02]]), 'transformer.encoder.layers.21.self_attention.dense.lora_B.default.weight': tensor([[-1.6038e-03,  1.6449e-05, -8.3135e-04,  ...,  6.2163e-04,\n         -6.6939e-04,  1.7288e-03],\n        [ 4.3001e-05,  3.0609e-04, -3.6415e-06,  ...,  2.5922e-04,\n          8.7917e-05,  2.2458e-04],\n        [-1.4569e-03,  9.2404e-04, -1.0133e-03,  ...,  1.2554e-03,\n         -3.0347e-04,  2.1345e-03],\n        ...,\n        [ 1.1963e-03, -1.0166e-03,  5.3371e-04,  ..., -8.0008e-04,\n          1.0872e-03, -1.8266e-03],\n        [-8.6790e-04,  2.1347e-04, -7.0377e-04,  ...,  1.8659e-04,\n         -1.2532e-03,  1.1924e-03],\n        [ 3.4877e-04,  5.5218e-04,  2.1616e-05,  ...,  7.5582e-05,\n          8.2209e-04,  2.3113e-04]]), 'transformer.encoder.layers.21.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0049,  0.0119, -0.0083,  ..., -0.0137, -0.0046, -0.0046],\n        [-0.0113,  0.0120,  0.0010,  ...,  0.0071, -0.0009,  0.0164],\n        [-0.0033, -0.0110,  0.0042,  ..., -0.0131, -0.0062,  0.0019],\n        ...,\n        [-0.0108, -0.0134, -0.0073,  ...,  0.0029, -0.0135, -0.0162],\n        [-0.0012, -0.0004,  0.0088,  ..., -0.0158, -0.0072, -0.0004],\n        [ 0.0159,  0.0057,  0.0074,  ..., -0.0134,  0.0027,  0.0065]]), 'transformer.encoder.layers.21.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 1.1017e-03, -1.5707e-03, -1.0691e-03,  ...,  2.9744e-04,\n          1.2352e-03,  1.9219e-03],\n        [-3.2712e-04,  6.4513e-04,  1.0774e-04,  ..., -4.0509e-04,\n          1.8476e-04, -3.8244e-04],\n        [-2.6018e-04,  3.0478e-05,  3.0973e-04,  ...,  2.2241e-04,\n         -2.9210e-04, -3.3850e-04],\n        ...,\n        [-7.0603e-04, -1.3315e-03, -8.3958e-04,  ...,  1.0068e-03,\n          8.5624e-04,  3.1388e-04],\n        [ 4.3094e-04,  4.1978e-05,  1.1208e-04,  ..., -9.3122e-05,\n          2.9375e-04,  4.4171e-04],\n        [ 2.7956e-04,  5.8584e-04,  5.1172e-04,  ..., -5.4745e-04,\n         -4.3685e-04, -7.5747e-04]]), 'transformer.encoder.layers.21.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 1.8218e-03,  1.9093e-03,  2.5079e-03,  ..., -1.8098e-04,\n         -2.8992e-03,  1.1203e-03],\n        [ 3.4386e-03,  7.4828e-03,  4.9597e-03,  ...,  3.8251e-03,\n          7.7508e-03, -1.7147e-03],\n        [-1.1010e-03,  8.8198e-03,  4.5354e-03,  ..., -4.6442e-03,\n          8.4015e-03,  1.2316e-03],\n        ...,\n        [-7.6160e-04, -4.8052e-03, -1.0932e-03,  ..., -4.2940e-03,\n          5.8933e-03, -3.6422e-03],\n        [-4.2478e-03, -4.4128e-03, -2.7090e-03,  ..., -6.9170e-03,\n          6.5535e-03,  5.1368e-03],\n        [ 2.7449e-03, -4.3916e-03,  6.5231e-04,  ...,  4.3907e-03,\n         -7.6171e-03,  7.8142e-05]]), 'transformer.encoder.layers.21.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 7.7594e-04, -6.2730e-04, -5.1604e-04,  ...,  1.3260e-03,\n          1.1691e-03,  7.5003e-04],\n        [ 6.8811e-04, -4.3270e-04, -7.0469e-04,  ...,  8.0657e-04,\n          8.2019e-05,  3.5840e-04],\n        [ 1.0562e-03, -2.0065e-03, -9.2875e-04,  ...,  1.2034e-03,\n          8.4097e-04,  1.5275e-03],\n        ...,\n        [-1.4785e-03,  1.5579e-03,  7.2671e-04,  ..., -1.5814e-03,\n         -1.6194e-03, -9.9647e-04],\n        [ 1.6832e-03, -5.1887e-04, -6.8741e-04,  ...,  1.0038e-03,\n          1.4446e-03,  6.5247e-04],\n        [ 1.0080e-03, -9.8583e-04, -1.1984e-03,  ...,  1.0343e-03,\n          9.4329e-04,  9.7656e-04]]), 'transformer.encoder.layers.22.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0015, -0.0110,  0.0063,  ...,  0.0094,  0.0080, -0.0043],\n        [-0.0130, -0.0038, -0.0103,  ...,  0.0123, -0.0041, -0.0049],\n        [-0.0068,  0.0134,  0.0065,  ...,  0.0053, -0.0012,  0.0091],\n        ...,\n        [-0.0007, -0.0102,  0.0109,  ..., -0.0125,  0.0082,  0.0052],\n        [-0.0090, -0.0044,  0.0092,  ...,  0.0037,  0.0113,  0.0025],\n        [-0.0089,  0.0107, -0.0007,  ..., -0.0154,  0.0144, -0.0096]]), 'transformer.encoder.layers.22.self_attention.query_key_value.lora_B.default.weight': tensor([[ 2.1475e-05,  1.2878e-03, -1.0689e-03,  ...,  1.2632e-04,\n          9.4430e-04, -1.6842e-05],\n        [-1.1266e-04, -3.8339e-04,  4.2999e-04,  ..., -4.6671e-05,\n         -6.7781e-04,  2.4540e-04],\n        [-2.8154e-04, -1.2524e-04, -4.6358e-05,  ...,  9.7751e-05,\n          2.5753e-04, -1.3386e-04],\n        ...,\n        [-1.0749e-03, -3.5563e-04,  8.4866e-05,  ...,  5.9792e-04,\n         -2.9470e-04,  4.9589e-04],\n        [ 2.9325e-03,  4.6420e-03, -3.5389e-03,  ..., -1.0471e-03,\n         -1.2972e-04, -1.7748e-03],\n        [ 1.0261e-03,  2.4453e-03, -2.2904e-03,  ...,  1.5817e-04,\n         -8.5312e-05, -1.0415e-03]]), 'transformer.encoder.layers.22.self_attention.dense.lora_A.default.weight': tensor([[-0.0074,  0.0152,  0.0017,  ...,  0.0121,  0.0039,  0.0129],\n        [ 0.0088, -0.0126,  0.0046,  ...,  0.0110, -0.0041,  0.0032],\n        [ 0.0095, -0.0089, -0.0092,  ..., -0.0098, -0.0041, -0.0084],\n        ...,\n        [ 0.0052,  0.0104,  0.0069,  ..., -0.0078, -0.0016, -0.0092],\n        [-0.0022, -0.0048,  0.0040,  ...,  0.0077,  0.0102,  0.0071],\n        [ 0.0067,  0.0106,  0.0047,  ...,  0.0010,  0.0069, -0.0094]]), 'transformer.encoder.layers.22.self_attention.dense.lora_B.default.weight': tensor([[-1.2811e-03,  8.2230e-04, -8.9002e-04,  ...,  2.7459e-04,\n         -1.4567e-03,  2.5326e-04],\n        [ 4.2396e-04,  3.3628e-04,  1.3189e-03,  ..., -6.0142e-05,\n         -7.1825e-04,  6.5448e-04],\n        [-1.1124e-04,  8.1478e-04,  8.8288e-04,  ...,  6.0710e-04,\n         -1.5887e-03,  9.0420e-04],\n        ...,\n        [ 1.2579e-03, -1.1464e-03,  4.4727e-04,  ..., -2.2807e-04,\n          1.5087e-03, -1.0311e-03],\n        [-2.4749e-04,  8.9547e-05, -1.9726e-04,  ..., -2.9867e-04,\n         -1.2399e-03,  7.7599e-04],\n        [ 1.9610e-03, -3.9337e-04,  2.6921e-03,  ..., -4.9167e-04,\n         -9.3435e-04,  1.2131e-03]]), 'transformer.encoder.layers.22.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0039,  0.0011,  0.0118,  ..., -0.0091,  0.0070,  0.0002],\n        [-0.0118, -0.0136,  0.0021,  ..., -0.0123,  0.0022,  0.0024],\n        [-0.0055,  0.0085,  0.0060,  ..., -0.0101, -0.0088, -0.0056],\n        ...,\n        [-0.0101, -0.0050, -0.0053,  ...,  0.0060, -0.0018, -0.0025],\n        [-0.0096, -0.0020,  0.0050,  ...,  0.0033, -0.0014, -0.0101],\n        [ 0.0074,  0.0141,  0.0126,  ..., -0.0081,  0.0031, -0.0140]]), 'transformer.encoder.layers.22.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 2.0250e-04,  9.9658e-05, -4.4209e-04,  ...,  1.9038e-04,\n         -2.6267e-04, -3.0604e-04],\n        [-4.7813e-06, -4.6635e-05,  3.2057e-04,  ..., -1.1633e-04,\n          7.0010e-05,  1.4701e-05],\n        [-1.1957e-04,  1.0673e-04,  2.3840e-04,  ..., -4.2984e-04,\n          5.3115e-04, -8.0841e-05],\n        ...,\n        [-1.5584e-04,  2.5354e-04,  9.1476e-05,  ..., -8.4038e-05,\n         -8.6427e-05, -2.7387e-04],\n        [-3.0261e-04, -3.0304e-04,  4.7967e-04,  ..., -3.5911e-04,\n          5.5951e-04, -1.6500e-04],\n        [ 6.8304e-04,  3.7494e-04,  9.7333e-05,  ..., -1.9757e-04,\n         -4.6172e-04, -1.2398e-03]]), 'transformer.encoder.layers.22.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 0.0022, -0.0029, -0.0064,  ..., -0.0037,  0.0042,  0.0086],\n        [ 0.0079,  0.0013,  0.0029,  ...,  0.0003, -0.0074,  0.0028],\n        [ 0.0044, -0.0044,  0.0013,  ...,  0.0072,  0.0044, -0.0070],\n        ...,\n        [ 0.0022, -0.0052,  0.0048,  ..., -0.0033, -0.0037,  0.0020],\n        [ 0.0058, -0.0033, -0.0056,  ...,  0.0008, -0.0004,  0.0032],\n        [-0.0063,  0.0072,  0.0006,  ..., -0.0071,  0.0072, -0.0004]]), 'transformer.encoder.layers.22.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 0.0005,  0.0004,  0.0012,  ...,  0.0005, -0.0010, -0.0022],\n        [ 0.0009,  0.0002,  0.0002,  ...,  0.0011, -0.0006, -0.0012],\n        [ 0.0013,  0.0006,  0.0012,  ...,  0.0008, -0.0011, -0.0018],\n        ...,\n        [-0.0009, -0.0008, -0.0015,  ..., -0.0014,  0.0019,  0.0019],\n        [ 0.0010,  0.0005,  0.0011,  ...,  0.0016, -0.0005, -0.0020],\n        [ 0.0010,  0.0004,  0.0007,  ...,  0.0012,  0.0002, -0.0013]]), 'transformer.encoder.layers.23.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0076, -0.0151,  0.0003,  ...,  0.0052, -0.0043, -0.0129],\n        [ 0.0140, -0.0034,  0.0142,  ..., -0.0015,  0.0005,  0.0149],\n        [-0.0117, -0.0087, -0.0116,  ..., -0.0101,  0.0027, -0.0018],\n        ...,\n        [ 0.0054, -0.0086,  0.0133,  ...,  0.0123, -0.0069, -0.0075],\n        [ 0.0046,  0.0123, -0.0009,  ...,  0.0041, -0.0017, -0.0085],\n        [ 0.0138, -0.0111,  0.0021,  ...,  0.0084, -0.0005, -0.0083]]), 'transformer.encoder.layers.23.self_attention.query_key_value.lora_B.default.weight': tensor([[ 5.9264e-05, -5.0612e-04,  3.0764e-04,  ..., -1.3152e-05,\n          1.7230e-04,  2.5223e-04],\n        [ 4.6152e-05,  2.7520e-04,  4.3350e-05,  ..., -1.4655e-04,\n          3.4385e-04, -1.4516e-04],\n        [ 1.1180e-04,  4.6493e-04, -3.4167e-04,  ..., -2.7673e-06,\n          1.5280e-04, -9.2737e-05],\n        ...,\n        [-1.3227e-03,  1.3175e-03, -6.1780e-04,  ...,  7.3271e-04,\n         -1.0882e-03, -1.3771e-04],\n        [-1.0514e-03,  1.7295e-03, -1.2026e-03,  ...,  1.7920e-03,\n         -1.0479e-03, -2.0312e-03],\n        [-1.2248e-03,  2.0045e-04, -2.4407e-04,  ...,  8.3626e-04,\n         -2.2342e-03,  2.0643e-04]]), 'transformer.encoder.layers.23.self_attention.dense.lora_A.default.weight': tensor([[ 5.0574e-03,  9.0719e-03,  6.2652e-03,  ...,  6.3878e-03,\n         -1.3955e-03, -4.1721e-03],\n        [-1.1490e-02,  9.6270e-03, -9.4617e-04,  ...,  1.3459e-02,\n         -1.0278e-02,  1.3253e-02],\n        [-7.3838e-03,  9.7601e-03, -2.0682e-03,  ...,  2.8911e-03,\n          3.5692e-05,  6.1360e-03],\n        ...,\n        [ 1.4692e-02, -1.1930e-02, -3.2423e-03,  ...,  3.0724e-03,\n          1.8895e-03,  7.4844e-03],\n        [-1.0657e-02, -1.2798e-02, -1.0879e-02,  ...,  1.1705e-02,\n          1.0590e-02, -1.0786e-02],\n        [ 5.1460e-03, -3.5261e-03, -7.9743e-03,  ...,  1.1326e-02,\n         -8.0854e-03, -1.3055e-02]]), 'transformer.encoder.layers.23.self_attention.dense.lora_B.default.weight': tensor([[-5.2635e-04, -3.3924e-04, -5.3199e-04,  ..., -5.7709e-04,\n          1.0377e-03,  1.1723e-04],\n        [-1.0711e-03, -1.1830e-04,  8.8242e-04,  ..., -4.9646e-04,\n          1.4864e-04,  4.7171e-04],\n        [-1.8800e-04, -1.2917e-03,  4.8546e-04,  ..., -7.7515e-04,\n          8.0072e-04, -1.7681e-04],\n        ...,\n        [ 1.0033e-03,  8.0428e-04, -1.5516e-04,  ...,  4.1827e-04,\n         -8.1165e-04,  8.5278e-04],\n        [-1.6740e-04, -1.1638e-03,  1.1904e-03,  ..., -3.9198e-04,\n          7.4707e-04, -4.5083e-04],\n        [-2.4830e-04, -1.4953e-03,  3.3534e-04,  ..., -1.5453e-04,\n          4.9877e-04, -2.3750e-05]]), 'transformer.encoder.layers.23.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[-3.4471e-03, -4.2862e-03,  6.3190e-03,  ..., -5.8179e-05,\n         -7.7761e-03, -8.0660e-03],\n        [ 1.1320e-02, -1.0867e-02,  2.4893e-03,  ...,  1.6396e-03,\n         -1.1963e-02, -1.0615e-02],\n        [-1.1805e-02,  5.1745e-03,  7.4414e-04,  ..., -4.3719e-03,\n         -5.1858e-03,  1.5277e-02],\n        ...,\n        [-7.9487e-03,  3.7530e-03,  6.2427e-03,  ..., -1.3982e-02,\n          9.7878e-03,  3.0613e-03],\n        [ 3.5325e-03, -7.9815e-03, -1.5444e-02,  ...,  2.6136e-03,\n          1.7543e-03, -4.4048e-03],\n        [-1.3967e-02,  5.4879e-04,  3.5595e-03,  ...,  1.2967e-02,\n          3.9066e-03, -3.1029e-03]]), 'transformer.encoder.layers.23.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 2.5478e-04,  3.3947e-04,  3.0854e-05,  ..., -4.7630e-04,\n         -1.1153e-03, -1.2194e-03],\n        [ 3.1660e-04, -5.5912e-04,  8.7435e-05,  ...,  5.0827e-04,\n          4.0687e-04,  6.0776e-04],\n        [-1.8289e-04,  2.3488e-04,  1.7341e-04,  ..., -3.5429e-04,\n         -3.7340e-06, -7.1036e-04],\n        ...,\n        [-5.0488e-05, -1.2788e-04,  2.3706e-04,  ..., -1.8782e-04,\n         -4.4661e-04, -7.5154e-04],\n        [-1.8555e-04, -1.4394e-04,  7.2469e-05,  ..., -2.1824e-04,\n         -1.1910e-04, -1.7962e-04],\n        [ 1.3565e-03, -1.1555e-03,  8.4488e-04,  ...,  1.1318e-03,\n          1.6078e-03,  1.1677e-03]]), 'transformer.encoder.layers.23.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 3.8594e-03, -5.4022e-03,  4.3028e-03,  ...,  4.1801e-03,\n         -2.9463e-03, -2.6939e-04],\n        [ 5.9380e-03,  6.5701e-03, -3.8343e-03,  ...,  1.8101e-03,\n         -3.7568e-03,  4.5084e-03],\n        [ 1.2167e-03, -6.1101e-05,  4.0996e-03,  ..., -6.8665e-03,\n          2.1786e-03,  6.3046e-03],\n        ...,\n        [ 7.9866e-03, -6.3598e-03, -2.2654e-03,  ..., -6.9049e-03,\n         -6.2310e-03, -7.7381e-03],\n        [ 6.4663e-03, -1.4590e-03,  5.0508e-03,  ..., -7.7157e-03,\n          2.8622e-03,  4.3498e-03],\n        [-9.1447e-04,  2.5877e-03, -8.0105e-03,  ..., -1.4626e-03,\n          7.9912e-03,  4.2460e-03]]), 'transformer.encoder.layers.23.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 1.5983e-03, -9.9376e-04, -6.4944e-04,  ...,  8.6740e-04,\n         -1.1121e-03,  1.1789e-03],\n        [ 5.6263e-04, -1.2814e-03, -6.7344e-04,  ...,  2.9899e-04,\n         -6.7756e-04,  4.5203e-04],\n        [ 1.5239e-03, -3.1753e-05, -1.2833e-03,  ...,  2.4365e-03,\n         -2.0002e-03,  5.6631e-04],\n        ...,\n        [-1.3981e-03,  7.5653e-04,  1.2116e-03,  ..., -1.6442e-03,\n          1.1879e-03, -8.8872e-04],\n        [ 1.9046e-03, -5.1450e-04, -4.1026e-05,  ...,  1.1635e-03,\n         -1.8319e-03,  1.5940e-03],\n        [ 1.4577e-03, -1.6122e-04, -1.0529e-03,  ...,  1.0788e-03,\n         -1.2268e-03,  6.6034e-04]]), 'transformer.encoder.layers.24.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0103,  0.0135, -0.0146,  ...,  0.0014, -0.0147,  0.0096],\n        [ 0.0045, -0.0004,  0.0035,  ...,  0.0115,  0.0126,  0.0023],\n        [-0.0137, -0.0097, -0.0162,  ..., -0.0060,  0.0074, -0.0067],\n        ...,\n        [ 0.0004,  0.0082, -0.0156,  ..., -0.0087, -0.0106,  0.0097],\n        [-0.0120,  0.0075, -0.0086,  ...,  0.0090,  0.0031,  0.0083],\n        [-0.0050, -0.0067,  0.0125,  ...,  0.0055,  0.0005, -0.0104]]), 'transformer.encoder.layers.24.self_attention.query_key_value.lora_B.default.weight': tensor([[-3.6337e-04,  7.0750e-04,  6.1927e-04,  ...,  2.5368e-04,\n         -5.6337e-04,  6.6699e-04],\n        [-2.8170e-04,  1.4685e-04,  2.5118e-04,  ...,  3.9637e-04,\n         -2.7866e-04,  5.0650e-04],\n        [-2.8854e-06, -4.4362e-05, -4.3787e-06,  ...,  5.0950e-05,\n          9.2751e-05,  2.6989e-06],\n        ...,\n        [-1.0271e-03, -1.6166e-03,  1.9350e-04,  ..., -2.5608e-03,\n         -3.2332e-04,  1.9091e-04],\n        [-4.5102e-04, -4.2772e-04,  1.9435e-04,  ..., -7.8177e-04,\n         -1.2402e-03,  1.6440e-03],\n        [-4.7869e-04,  1.7353e-04,  3.5001e-04,  ..., -6.4461e-04,\n         -9.4728e-04,  1.9346e-04]]), 'transformer.encoder.layers.24.self_attention.dense.lora_A.default.weight': tensor([[ 0.0112, -0.0071,  0.0022,  ...,  0.0023, -0.0069,  0.0012],\n        [-0.0108, -0.0100,  0.0069,  ...,  0.0137,  0.0072, -0.0050],\n        [ 0.0146, -0.0023,  0.0076,  ..., -0.0156, -0.0138,  0.0018],\n        ...,\n        [-0.0004, -0.0140, -0.0096,  ..., -0.0029,  0.0070,  0.0076],\n        [ 0.0037,  0.0148, -0.0108,  ...,  0.0061,  0.0129, -0.0064],\n        [ 0.0017,  0.0112,  0.0095,  ..., -0.0153,  0.0005,  0.0045]]), 'transformer.encoder.layers.24.self_attention.dense.lora_B.default.weight': tensor([[ 4.3181e-04, -1.8313e-03,  9.7737e-04,  ..., -1.5311e-03,\n         -2.0971e-03, -1.0083e-04],\n        [-1.9680e-04, -9.1892e-04,  3.1225e-05,  ..., -4.6913e-04,\n         -2.8603e-04, -2.4373e-04],\n        [ 6.7173e-04, -1.4435e-03, -5.9281e-04,  ..., -1.3091e-03,\n         -4.4359e-05, -5.7810e-04],\n        ...,\n        [-3.3440e-04,  1.5939e-03, -6.2119e-04,  ...,  1.2768e-03,\n          1.3420e-03,  7.4301e-04],\n        [ 1.2657e-03, -7.0822e-04,  9.1993e-04,  ..., -1.0987e-03,\n         -5.6604e-04,  1.4867e-04],\n        [ 3.9891e-04, -1.1808e-03, -6.6251e-05,  ..., -6.6314e-04,\n         -8.7411e-04, -2.1404e-04]]), 'transformer.encoder.layers.24.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0132, -0.0113, -0.0029,  ..., -0.0069, -0.0088, -0.0163],\n        [ 0.0149,  0.0101, -0.0050,  ..., -0.0146, -0.0132,  0.0123],\n        [ 0.0016,  0.0027, -0.0011,  ..., -0.0027,  0.0158, -0.0116],\n        ...,\n        [-0.0163, -0.0075, -0.0089,  ..., -0.0085,  0.0041, -0.0162],\n        [-0.0102,  0.0079,  0.0109,  ...,  0.0112, -0.0106, -0.0113],\n        [ 0.0047, -0.0125, -0.0130,  ...,  0.0130,  0.0024, -0.0062]]), 'transformer.encoder.layers.24.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-1.3802e-03, -2.0797e-03,  1.4840e-03,  ..., -1.5778e-03,\n         -7.7636e-04,  2.1055e-03],\n        [-5.3605e-04, -8.9903e-04,  9.8748e-04,  ..., -1.0898e-03,\n         -2.8151e-05,  8.3794e-04],\n        [ 1.2199e-03,  8.9823e-04, -1.2451e-03,  ...,  1.0491e-03,\n          2.8740e-04, -1.2965e-03],\n        ...,\n        [-7.4223e-04, -9.6673e-04,  1.0253e-03,  ..., -8.3946e-04,\n         -2.0202e-05,  1.2430e-03],\n        [-6.0257e-05, -1.2970e-04,  3.2396e-04,  ..., -1.4866e-04,\n          9.2644e-05,  1.2754e-05],\n        [ 1.4917e-03,  1.6537e-03, -1.2376e-03,  ...,  1.2856e-03,\n          3.0550e-04, -1.4781e-03]]), 'transformer.encoder.layers.24.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-0.0016, -0.0020, -0.0070,  ...,  0.0014,  0.0059,  0.0058],\n        [ 0.0047,  0.0003,  0.0056,  ...,  0.0046, -0.0074,  0.0033],\n        [ 0.0039, -0.0016,  0.0058,  ...,  0.0031,  0.0075, -0.0013],\n        ...,\n        [-0.0017,  0.0032, -0.0045,  ...,  0.0044,  0.0032,  0.0066],\n        [-0.0019,  0.0056, -0.0038,  ...,  0.0066, -0.0037, -0.0077],\n        [-0.0068, -0.0039, -0.0015,  ...,  0.0002,  0.0003,  0.0078]]), 'transformer.encoder.layers.24.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-8.7965e-04, -3.6477e-04,  1.5933e-03,  ..., -1.3332e-03,\n         -7.8934e-04,  2.1282e-03],\n        [-3.1561e-04, -5.6197e-04,  1.4379e-04,  ..., -5.6021e-04,\n         -6.6567e-04,  1.2136e-04],\n        [-2.2510e-03, -1.9253e-03,  5.0003e-04,  ..., -1.1303e-03,\n         -1.6573e-03,  2.1214e-03],\n        ...,\n        [ 1.4165e-03,  6.8343e-04, -1.2655e-03,  ...,  9.7578e-04,\n          1.2427e-03, -1.7354e-03],\n        [-6.2958e-05, -2.1044e-05,  6.3857e-04,  ..., -1.2624e-04,\n         -5.8461e-04,  1.7367e-03],\n        [-1.0337e-03, -7.6237e-04,  2.6720e-04,  ..., -7.4720e-04,\n         -8.0674e-04,  1.8811e-03]]), 'transformer.encoder.layers.25.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0120,  0.0104,  0.0132,  ..., -0.0122, -0.0149, -0.0034],\n        [-0.0113,  0.0144,  0.0027,  ...,  0.0105,  0.0094,  0.0013],\n        [ 0.0052, -0.0150, -0.0121,  ..., -0.0139,  0.0085, -0.0025],\n        ...,\n        [ 0.0070, -0.0052,  0.0052,  ...,  0.0080,  0.0129,  0.0113],\n        [-0.0025, -0.0105, -0.0036,  ...,  0.0114,  0.0094, -0.0093],\n        [ 0.0021, -0.0131,  0.0094,  ..., -0.0072, -0.0147, -0.0070]]), 'transformer.encoder.layers.25.self_attention.query_key_value.lora_B.default.weight': tensor([[ 2.9623e-04,  2.0007e-06, -1.2483e-05,  ...,  1.0683e-05,\n         -6.0667e-05,  2.6609e-04],\n        [ 4.4738e-04,  7.0108e-05,  3.6912e-04,  ...,  1.9147e-05,\n         -8.9399e-05,  6.0315e-04],\n        [-4.3204e-04,  1.9863e-04, -1.7678e-04,  ..., -2.0671e-05,\n          7.5929e-05, -4.3381e-04],\n        ...,\n        [-1.4771e-03,  2.7876e-04, -8.8191e-04,  ...,  2.1896e-04,\n          6.1257e-04, -2.3636e-04],\n        [ 2.9287e-03, -2.6330e-04,  2.3981e-03,  ..., -2.2206e-03,\n         -1.7540e-03,  2.4921e-03],\n        [ 6.6478e-04,  2.9899e-04,  1.4630e-03,  ..., -2.3701e-03,\n          1.4081e-03, -7.1958e-04]]), 'transformer.encoder.layers.25.self_attention.dense.lora_A.default.weight': tensor([[ 0.0128,  0.0108,  0.0122,  ..., -0.0082, -0.0080, -0.0019],\n        [-0.0059,  0.0053, -0.0009,  ..., -0.0031, -0.0100,  0.0024],\n        [ 0.0107, -0.0159,  0.0133,  ...,  0.0137, -0.0067,  0.0132],\n        ...,\n        [-0.0086, -0.0073, -0.0019,  ...,  0.0099, -0.0032, -0.0095],\n        [ 0.0099,  0.0153, -0.0091,  ...,  0.0018,  0.0003,  0.0051],\n        [-0.0097, -0.0048,  0.0082,  ...,  0.0087, -0.0004,  0.0101]]), 'transformer.encoder.layers.25.self_attention.dense.lora_B.default.weight': tensor([[-1.3610e-03,  1.5388e-03,  8.9876e-04,  ..., -1.0045e-03,\n         -3.3300e-04,  1.7756e-04],\n        [ 8.2925e-05,  1.9517e-04, -1.8347e-04,  ..., -4.2973e-04,\n         -4.7850e-04, -6.4119e-04],\n        [-8.8251e-04,  1.8741e-03,  5.5243e-04,  ..., -1.8446e-03,\n         -1.0407e-03, -2.0380e-03],\n        ...,\n        [ 1.5963e-03, -2.0039e-03, -1.5050e-03,  ...,  1.1999e-03,\n          5.1611e-04,  4.0859e-04],\n        [-3.3247e-04,  1.3682e-03, -7.8277e-05,  ..., -4.4702e-04,\n         -4.4661e-04, -3.2944e-04],\n        [-1.6032e-04,  7.0459e-04, -1.9959e-04,  ..., -6.2222e-04,\n         -2.4779e-04, -4.6721e-04]]), 'transformer.encoder.layers.25.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0145,  0.0102,  0.0012,  ..., -0.0143,  0.0036, -0.0147],\n        [ 0.0109, -0.0143, -0.0023,  ..., -0.0059, -0.0014,  0.0062],\n        [ 0.0046,  0.0045, -0.0038,  ..., -0.0158,  0.0059, -0.0015],\n        ...,\n        [-0.0117,  0.0042,  0.0166,  ..., -0.0067,  0.0084, -0.0019],\n        [-0.0159,  0.0071,  0.0041,  ..., -0.0152, -0.0073, -0.0064],\n        [ 0.0137, -0.0060,  0.0044,  ..., -0.0100, -0.0101, -0.0042]]), 'transformer.encoder.layers.25.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 9.6191e-04, -1.2582e-03, -1.3314e-03,  ..., -1.0580e-03,\n          5.2428e-04,  1.1407e-03],\n        [-3.4805e-04,  4.9594e-04, -3.0481e-04,  ..., -6.9922e-04,\n          1.1665e-04, -3.1816e-04],\n        [ 3.5149e-04, -2.2133e-04,  4.5477e-04,  ..., -3.0287e-04,\n         -3.4901e-04,  1.2715e-04],\n        ...,\n        [ 9.4555e-05, -4.9489e-04, -6.7861e-04,  ..., -4.0254e-04,\n          6.4373e-04,  1.3822e-04],\n        [ 2.5849e-04, -3.5493e-05, -5.6753e-04,  ..., -6.2858e-04,\n          4.3080e-04,  7.0163e-04],\n        [-6.3977e-04,  2.8929e-04,  1.6836e-04,  ...,  6.2174e-05,\n          2.2811e-04,  1.0895e-04]]), 'transformer.encoder.layers.25.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[-1.6206e-03,  6.2556e-04, -5.0018e-03,  ..., -1.0627e-03,\n         -2.6039e-03, -2.9480e-03],\n        [ 8.6793e-03, -7.1307e-03, -4.8527e-03,  ..., -7.3632e-03,\n          2.4893e-03,  8.2633e-05],\n        [ 2.6330e-03, -1.3875e-03,  6.0298e-03,  ...,  3.7392e-03,\n          1.5656e-03, -7.4828e-03],\n        ...,\n        [-4.0878e-03, -1.0395e-03, -5.4854e-03,  ..., -8.0274e-03,\n          6.8506e-03,  7.4758e-03],\n        [ 6.8640e-04,  7.0552e-03, -1.1013e-03,  ...,  3.3685e-03,\n         -4.3212e-03,  3.0752e-03],\n        [-6.9084e-03,  8.1787e-03, -6.2060e-03,  ...,  4.1315e-03,\n         -4.7392e-03, -3.1748e-03]]), 'transformer.encoder.layers.25.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 0.0021, -0.0007, -0.0023,  ..., -0.0007,  0.0006, -0.0021],\n        [ 0.0010, -0.0003, -0.0011,  ..., -0.0004,  0.0003, -0.0012],\n        [ 0.0025, -0.0003, -0.0023,  ..., -0.0012,  0.0011, -0.0027],\n        ...,\n        [-0.0019, -0.0005,  0.0018,  ...,  0.0001, -0.0004,  0.0020],\n        [ 0.0013,  0.0006, -0.0007,  ...,  0.0004,  0.0003, -0.0006],\n        [ 0.0010, -0.0011, -0.0013,  ..., -0.0007,  0.0004, -0.0012]]), 'transformer.encoder.layers.26.self_attention.query_key_value.lora_A.default.weight': tensor([[ 0.0054,  0.0068,  0.0126,  ..., -0.0099, -0.0141,  0.0076],\n        [-0.0054,  0.0110,  0.0104,  ..., -0.0006,  0.0147,  0.0014],\n        [-0.0123, -0.0005,  0.0028,  ...,  0.0150, -0.0147, -0.0070],\n        ...,\n        [ 0.0050,  0.0062, -0.0015,  ...,  0.0098, -0.0082, -0.0033],\n        [ 0.0003, -0.0055,  0.0036,  ...,  0.0008,  0.0147, -0.0037],\n        [-0.0092,  0.0090, -0.0071,  ...,  0.0023, -0.0101, -0.0085]]), 'transformer.encoder.layers.26.self_attention.query_key_value.lora_B.default.weight': tensor([[-9.9318e-05,  2.4462e-05, -2.7569e-04,  ...,  1.2182e-04,\n          1.6169e-04, -1.2048e-04],\n        [ 1.7063e-04,  6.5173e-05,  1.5745e-04,  ..., -1.5845e-04,\n         -1.4556e-04,  2.3111e-05],\n        [ 4.0745e-04, -4.0994e-04,  2.6850e-04,  ..., -3.1370e-04,\n         -1.5331e-04,  1.4193e-04],\n        ...,\n        [ 8.4138e-04, -1.9252e-03,  1.8983e-03,  ..., -1.0561e-03,\n          1.1966e-03,  2.2645e-03],\n        [ 7.0541e-04, -2.2594e-04,  2.2781e-04,  ...,  2.2823e-04,\n         -2.1954e-04,  2.4144e-04],\n        [ 1.6787e-03, -2.0266e-03, -6.8444e-05,  ..., -1.0562e-03,\n          1.0789e-03, -8.8447e-05]]), 'transformer.encoder.layers.26.self_attention.dense.lora_A.default.weight': tensor([[-1.8527e-03, -8.9172e-03,  1.1733e-02,  ..., -1.0371e-02,\n          9.3074e-03, -6.5122e-03],\n        [-2.6186e-03, -1.3186e-02, -4.0961e-04,  ..., -1.7411e-03,\n         -8.4110e-03,  1.3612e-04],\n        [-6.7963e-03, -1.2911e-02, -6.0569e-03,  ..., -2.5815e-03,\n          6.1713e-04, -1.3832e-02],\n        ...,\n        [-9.6925e-03,  5.8496e-05, -2.5731e-03,  ..., -8.6440e-03,\n         -4.2937e-03, -1.0639e-02],\n        [ 6.6464e-03, -1.5888e-02,  1.5174e-02,  ...,  1.0043e-02,\n          8.9426e-03,  9.9346e-03],\n        [-4.0691e-03, -8.2715e-03,  1.2849e-02,  ...,  2.6042e-03,\n          1.1124e-02,  9.0843e-03]]), 'transformer.encoder.layers.26.self_attention.dense.lora_B.default.weight': tensor([[ 1.4226e-03, -2.3779e-03, -2.0114e-03,  ...,  1.1426e-03,\n         -6.8704e-04, -2.0714e-03],\n        [ 1.0421e-03, -1.3539e-03, -1.1482e-03,  ...,  9.8845e-04,\n         -5.0293e-04, -1.0385e-03],\n        [ 4.9606e-04, -1.3381e-03, -1.4107e-03,  ...,  4.3180e-04,\n         -3.5191e-04, -6.0448e-04],\n        ...,\n        [-1.4603e-03,  2.6913e-03,  1.6264e-03,  ..., -1.4036e-03,\n          5.5722e-04,  2.0967e-03],\n        [ 1.6495e-04, -6.4212e-04, -8.3380e-04,  ...,  3.3623e-04,\n         -1.9073e-04, -7.9657e-06],\n        [ 4.6791e-04, -1.0361e-03, -1.5016e-03,  ...,  7.3213e-04,\n         -8.7298e-07, -1.1078e-03]]), 'transformer.encoder.layers.26.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0008,  0.0144,  0.0033,  ..., -0.0131,  0.0134, -0.0066],\n        [-0.0041, -0.0132,  0.0143,  ..., -0.0150, -0.0020,  0.0055],\n        [ 0.0141,  0.0087, -0.0020,  ...,  0.0061, -0.0073, -0.0093],\n        ...,\n        [ 0.0003,  0.0151, -0.0027,  ...,  0.0064, -0.0060,  0.0101],\n        [ 0.0079, -0.0160,  0.0026,  ...,  0.0005, -0.0090, -0.0092],\n        [ 0.0004,  0.0016, -0.0050,  ..., -0.0120,  0.0098, -0.0104]]), 'transformer.encoder.layers.26.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[ 3.5868e-04,  3.5120e-04, -3.4220e-04,  ...,  4.9439e-04,\n          4.4529e-04, -1.6743e-04],\n        [ 2.7112e-04, -1.9748e-05, -5.9139e-05,  ...,  3.5004e-04,\n          3.4818e-05,  2.4624e-04],\n        [ 1.6251e-04,  3.8343e-04, -6.9120e-04,  ...,  7.6427e-04,\n         -8.0929e-06,  6.2912e-04],\n        ...,\n        [-6.5107e-04, -8.4692e-05,  5.0399e-05,  ..., -4.5966e-04,\n          1.6512e-06, -4.0472e-04],\n        [-6.2746e-04, -3.4117e-04,  8.8390e-04,  ..., -9.7383e-04,\n         -5.7909e-04, -2.6127e-04],\n        [-1.2139e-04,  2.2829e-04, -2.8271e-04,  ...,  1.2044e-04,\n         -6.9118e-05, -2.0586e-05]]), 'transformer.encoder.layers.26.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 4.1176e-03,  3.1242e-03,  3.0370e-03,  ..., -8.4269e-03,\n         -7.1587e-03, -8.3351e-03],\n        [-1.0197e-03, -1.3190e-03, -1.5060e-03,  ..., -1.5692e-03,\n         -5.8144e-03,  2.6560e-03],\n        [-6.3304e-03, -3.8990e-03,  5.5998e-03,  ..., -1.3160e-03,\n         -4.7889e-03, -8.3046e-03],\n        ...,\n        [ 2.4128e-03, -3.3859e-05,  3.4016e-03,  ..., -7.6441e-03,\n         -7.7680e-04,  2.6199e-03],\n        [-3.5436e-03,  8.2226e-03,  2.5182e-03,  ..., -2.6289e-03,\n         -1.8365e-03, -1.3521e-03],\n        [-6.7619e-03, -6.8530e-03,  3.5372e-03,  ...,  3.0615e-03,\n         -5.7468e-03,  2.8854e-03]]), 'transformer.encoder.layers.26.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[ 2.5518e-03, -3.4964e-03,  3.4559e-03,  ...,  3.3964e-03,\n         -1.5436e-03,  3.1223e-03],\n        [ 1.3373e-03, -1.1995e-03,  1.1254e-03,  ...,  1.0747e-03,\n         -3.4588e-05,  6.3508e-04],\n        [ 2.0478e-03, -2.4139e-03,  3.3587e-03,  ...,  1.8724e-04,\n         -1.2226e-03,  2.5496e-03],\n        ...,\n        [-1.2963e-03,  1.1228e-03, -1.6540e-03,  ..., -2.8018e-04,\n          6.9539e-04, -9.2256e-04],\n        [ 1.5311e-03, -1.9451e-03,  2.6006e-03,  ...,  1.0063e-03,\n         -6.9539e-04,  1.9809e-03],\n        [ 1.3591e-03, -1.6864e-03,  2.3823e-03,  ...,  4.6305e-04,\n         -9.1644e-04,  1.7845e-03]]), 'transformer.encoder.layers.27.self_attention.query_key_value.lora_A.default.weight': tensor([[-0.0136,  0.0063,  0.0098,  ..., -0.0018,  0.0144, -0.0020],\n        [ 0.0083, -0.0060,  0.0151,  ..., -0.0148, -0.0126,  0.0095],\n        [-0.0084, -0.0104,  0.0111,  ..., -0.0020,  0.0136,  0.0018],\n        ...,\n        [-0.0141,  0.0004, -0.0141,  ..., -0.0027, -0.0015, -0.0071],\n        [-0.0141,  0.0013, -0.0073,  ...,  0.0121, -0.0009, -0.0001],\n        [-0.0056, -0.0126, -0.0016,  ..., -0.0084,  0.0131, -0.0048]]), 'transformer.encoder.layers.27.self_attention.query_key_value.lora_B.default.weight': tensor([[-4.2290e-04, -3.8932e-04,  5.9062e-04,  ...,  2.4591e-04,\n          2.4287e-04, -3.4639e-04],\n        [-9.3678e-05,  3.1147e-05,  2.3419e-05,  ...,  2.2625e-04,\n         -3.1529e-04,  9.3745e-05],\n        [ 2.9582e-05,  1.5474e-04,  1.5879e-04,  ...,  1.4989e-04,\n         -1.9811e-04, -1.2910e-04],\n        ...,\n        [ 1.0145e-03,  2.7504e-04, -1.8694e-04,  ..., -1.4149e-04,\n          5.4582e-04,  1.1210e-03],\n        [ 1.9857e-04,  4.4447e-04, -7.1615e-04,  ...,  6.8715e-05,\n         -2.3314e-05,  3.5005e-04],\n        [-1.4589e-03, -8.6309e-04,  3.0268e-03,  ...,  1.2867e-03,\n         -3.3752e-03,  1.0514e-04]]), 'transformer.encoder.layers.27.self_attention.dense.lora_A.default.weight': tensor([[-0.0137, -0.0065,  0.0081,  ..., -0.0123,  0.0049, -0.0046],\n        [ 0.0120,  0.0045, -0.0101,  ...,  0.0097, -0.0140,  0.0106],\n        [ 0.0034,  0.0073, -0.0114,  ...,  0.0131, -0.0066, -0.0069],\n        ...,\n        [ 0.0061,  0.0042, -0.0054,  ...,  0.0032,  0.0091,  0.0099],\n        [ 0.0025, -0.0058, -0.0153,  ...,  0.0079, -0.0085,  0.0096],\n        [-0.0126, -0.0033, -0.0101,  ..., -0.0026,  0.0081,  0.0030]]), 'transformer.encoder.layers.27.self_attention.dense.lora_B.default.weight': tensor([[-2.6935e-03,  1.9042e-03, -4.7136e-05,  ..., -3.2100e-03,\n         -2.6108e-03, -1.1066e-03],\n        [-1.1612e-03,  4.9296e-04,  2.5322e-05,  ..., -1.2694e-03,\n         -8.5980e-04, -4.8397e-04],\n        [-1.1803e-03,  5.4789e-04, -1.5858e-03,  ..., -5.9246e-04,\n         -2.5746e-03, -8.0055e-04],\n        ...,\n        [ 1.4507e-03, -5.9514e-04,  5.6751e-05,  ...,  1.1259e-03,\n          1.7304e-03,  4.2627e-04],\n        [-2.0124e-03,  1.4481e-03,  1.9736e-05,  ..., -1.5949e-03,\n         -1.7301e-03, -3.1909e-04],\n        [-1.3043e-03,  1.1072e-03, -2.1276e-04,  ..., -1.7443e-03,\n         -1.6993e-03, -8.9472e-04]]), 'transformer.encoder.layers.27.mlp.dense_h_to_4h.lora_A.default.weight': tensor([[ 0.0008,  0.0106, -0.0127,  ...,  0.0048,  0.0033,  0.0046],\n        [ 0.0033, -0.0113, -0.0035,  ..., -0.0106, -0.0126, -0.0080],\n        [-0.0110, -0.0007, -0.0134,  ..., -0.0024, -0.0117,  0.0123],\n        ...,\n        [-0.0074,  0.0054,  0.0116,  ...,  0.0058, -0.0107, -0.0067],\n        [-0.0149,  0.0115, -0.0099,  ..., -0.0049,  0.0107,  0.0134],\n        [ 0.0058,  0.0150,  0.0053,  ..., -0.0064, -0.0149,  0.0147]]), 'transformer.encoder.layers.27.mlp.dense_h_to_4h.lora_B.default.weight': tensor([[-1.0538e-03,  1.3764e-04,  4.5604e-05,  ..., -2.9443e-04,\n         -2.3670e-04,  3.6422e-04],\n        [-5.0206e-04, -4.3784e-04, -7.9406e-04,  ...,  1.1419e-03,\n          5.9054e-04,  1.2588e-03],\n        [-1.0223e-03, -1.6919e-04, -2.0172e-04,  ...,  1.0388e-04,\n          3.9523e-05,  4.9052e-04],\n        ...,\n        [-6.9423e-04, -3.6783e-04, -1.7526e-04,  ..., -2.0799e-04,\n         -2.8006e-05,  7.9407e-04],\n        [-1.0787e-03, -8.0028e-04,  5.1491e-04,  ..., -1.1075e-03,\n         -2.7492e-04,  9.4664e-04],\n        [-8.7975e-05, -3.8448e-05,  4.7156e-04,  ..., -4.0261e-04,\n         -3.2003e-04, -1.0737e-04]]), 'transformer.encoder.layers.27.mlp.dense_4h_to_h.lora_A.default.weight': tensor([[ 0.0036,  0.0083,  0.0025,  ...,  0.0052,  0.0090,  0.0051],\n        [ 0.0072,  0.0051, -0.0011,  ..., -0.0061, -0.0070, -0.0045],\n        [ 0.0059,  0.0002,  0.0060,  ..., -0.0071, -0.0005,  0.0078],\n        ...,\n        [ 0.0052,  0.0039, -0.0062,  ..., -0.0034,  0.0001,  0.0045],\n        [ 0.0044,  0.0042, -0.0087,  ..., -0.0066,  0.0026, -0.0030],\n        [ 0.0086,  0.0009, -0.0031,  ...,  0.0020, -0.0003, -0.0020]]), 'transformer.encoder.layers.27.mlp.dense_4h_to_h.lora_B.default.weight': tensor([[-2.0886e-03,  2.5807e-03, -1.6624e-03,  ...,  1.8710e-03,\n         -1.8607e-03,  9.3332e-05],\n        [-1.0684e-03,  1.3237e-03, -9.8307e-04,  ...,  1.2632e-03,\n         -1.0300e-03, -1.3078e-04],\n        [-3.0255e-03,  3.8075e-03, -2.5327e-03,  ...,  2.9061e-03,\n         -2.8244e-03,  1.2787e-04],\n        ...,\n        [ 1.6453e-03, -1.8293e-03,  1.1745e-03,  ..., -1.3281e-03,\n          1.4509e-03, -8.9004e-05],\n        [-2.1625e-03,  2.3358e-03, -1.7079e-03,  ...,  1.7876e-03,\n         -1.8970e-03,  8.2095e-05],\n        [-2.1307e-03,  2.3656e-03, -1.6017e-03,  ...,  1.8221e-03,\n         -2.0079e-03,  6.8205e-05]]), 'v_head.weight': tensor([[ 0.0088,  0.0041,  0.0163,  ..., -0.0066,  0.0090,  0.0084]])}\u001b[0m\n\u001b[32m2023-08-11 14:20:31.737\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1469\u001b[0m - \u001b[31m\u001b[1mbefore load model.v_head.weight=Parameter containing:\ntensor([[ 0.0090,  0.0039,  0.0154,  ..., -0.0065,  0.0088,  0.0089]],\n       requires_grad=True)\u001b[0m\n\u001b[32m2023-08-11 14:20:31.721\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1469\u001b[0m - \u001b[31m\u001b[1mbefore load model.v_head.weight=Parameter containing:\ntensor([[ 0.0090,  0.0039,  0.0154,  ..., -0.0065,  0.0088,  0.0089]],\n       requires_grad=True)\u001b[0m\nbefroe load model.transformer.encoder.layers[27].self_attention.query_key_value.lora_A.default.weight=Parameter containing:\ntensor([[-0.0138,  0.0060,  0.0104,  ..., -0.0021,  0.0138, -0.0025],\n        [ 0.0086, -0.0063,  0.0156,  ..., -0.0151, -0.0130,  0.0095],\n        [-0.0082, -0.0100,  0.0104,  ..., -0.0017,  0.0141,  0.0023],\n        ...,\n        [-0.0139,  0.0006, -0.0147,  ..., -0.0025, -0.0010, -0.0067],\n        [-0.0144,  0.0014, -0.0067,  ...,  0.0120, -0.0013, -0.0006],\n        [-0.0054, -0.0127, -0.0020,  ..., -0.0086,  0.0132, -0.0045]],\n       device='cuda:0', requires_grad=True)befroe load model.transformer.encoder.layers[27].self_attention.query_key_value.lora_A.default.weight=Parameter containing:\ntensor([[-0.0138,  0.0060,  0.0104,  ..., -0.0021,  0.0138, -0.0025],\n        [ 0.0086, -0.0063,  0.0156,  ..., -0.0151, -0.0130,  0.0095],\n        [-0.0082, -0.0100,  0.0104,  ..., -0.0017,  0.0141,  0.0023],\n        ...,\n        [-0.0139,  0.0006, -0.0147,  ..., -0.0025, -0.0010, -0.0067],\n        [-0.0144,  0.0014, -0.0067,  ...,  0.0120, -0.0013, -0.0006],\n        [-0.0054, -0.0127, -0.0020,  ..., -0.0086,  0.0132, -0.0045]],\n       device='cuda:1', requires_grad=True)\n\nbefore load model.transformer.encoder.layers[27].self_attention.query_key_value.weight=Parameter containing:\nParameter(Params4bit([[115],\n            [214],\n            [ 49],\n            ...,\n            [100],\n            [152],\n            [ 85]], device='cuda:1', dtype=torch.uint8))\nbefore load model.transformer.encoder.layers[27].self_attention.query_key_value.weight=Parameter containing:\nParameter(Params4bit([[115],\n            [214],\n            [ 49],\n            ...,\n            [100],\n            [152],\n            [ 85]], device='cuda:0', dtype=torch.uint8))\nbefore load model.transformer.encoder.layers[27].self_attention.dense.weight=Parameter containing:\nParameter(Params4bit([[ 87],\n            [150],\n            [180],\n            ...,\n            [150],\n            [ 58],\n            [ 89]], device='cuda:1', dtype=torch.uint8))\nbefore load model.transformer.encoder.layers[27].self_attention.dense.weight=Parameter containing:\nParameter(Params4bit([[ 87],\n            [150],\n            [180],\n            ...,\n            [150],\n            [ 58],\n            [ 89]], device='cuda:0', dtype=torch.uint8))\nadapters_weigth:transformer.encoder.layers.27.self_attention.query_key_value.lora_A.default.weight=tensor([[-0.0136,  0.0063,  0.0098,  ..., -0.0018,  0.0144, -0.0020],\n        [ 0.0083, -0.0060,  0.0151,  ..., -0.0148, -0.0126,  0.0095],\n        [-0.0084, -0.0104,  0.0111,  ..., -0.0020,  0.0136,  0.0018],\n        ...,\n        [-0.0141,  0.0004, -0.0141,  ..., -0.0027, -0.0015, -0.0071],\n        [-0.0141,  0.0013, -0.0073,  ...,  0.0121, -0.0009, -0.0001],\n        [-0.0056, -0.0126, -0.0016,  ..., -0.0084,  0.0131, -0.0048]])\nadapters_weigth:transformer.encoder.layers.27.self_attention.query_key_value.lora_A.default.weight=tensor([[-0.0136,  0.0063,  0.0098,  ..., -0.0018,  0.0144, -0.0020],\n        [ 0.0083, -0.0060,  0.0151,  ..., -0.0148, -0.0126,  0.0095],\n        [-0.0084, -0.0104,  0.0111,  ..., -0.0020,  0.0136,  0.0018],\n        ...,\n        [-0.0141,  0.0004, -0.0141,  ..., -0.0027, -0.0015, -0.0071],\n        [-0.0141,  0.0013, -0.0073,  ...,  0.0121, -0.0009, -0.0001],\n        [-0.0056, -0.0126, -0.0016,  ..., -0.0084,  0.0131, -0.0048]])\n\u001b[32m2023-08-11 14:20:32.060\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1478\u001b[0m - \u001b[31m\u001b[1mv_head_weights={'weight': tensor([[ 0.0088,  0.0041,  0.0163,  ..., -0.0066,  0.0090,  0.0084]])}\u001b[0m\n\u001b[32m2023-08-11 14:20:32.060\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1478\u001b[0m - \u001b[31m\u001b[1mv_head_weights={'weight': tensor([[ 0.0088,  0.0041,  0.0163,  ..., -0.0066,  0.0090,  0.0084]])}\u001b[0m\nafter load model.transformer.encoder.layers[27].self_attention.query_key_value.lora_A.default.weight=Parameter containing:\ntensor([[-0.0136,  0.0063,  0.0098,  ..., -0.0018,  0.0144, -0.0020],\n        [ 0.0083, -0.0060,  0.0151,  ..., -0.0148, -0.0126,  0.0095],\n        [-0.0084, -0.0104,  0.0111,  ..., -0.0020,  0.0136,  0.0018],\n        ...,\n        [-0.0141,  0.0004, -0.0141,  ..., -0.0027, -0.0015, -0.0071],\n        [-0.0141,  0.0013, -0.0073,  ...,  0.0121, -0.0009, -0.0001],\n        [-0.0056, -0.0126, -0.0016,  ..., -0.0084,  0.0131, -0.0048]],\n       device='cuda:0', requires_grad=True)\nafter load model.transformer.encoder.layers[27].self_attention.query_key_value.lora_A.default.weight=Parameter containing:\ntensor([[-0.0136,  0.0063,  0.0098,  ..., -0.0018,  0.0144, -0.0020],\n        [ 0.0083, -0.0060,  0.0151,  ..., -0.0148, -0.0126,  0.0095],\n        [-0.0084, -0.0104,  0.0111,  ..., -0.0020,  0.0136,  0.0018],\n        ...,\n        [-0.0141,  0.0004, -0.0141,  ..., -0.0027, -0.0015, -0.0071],\n        [-0.0141,  0.0013, -0.0073,  ...,  0.0121, -0.0009, -0.0001],\n        [-0.0056, -0.0126, -0.0016,  ..., -0.0084,  0.0131, -0.0048]],\n       device='cuda:1', requires_grad=True)\nafter load model.transformer.encoder.layers[27].self_attention.query_key_value.weight=Parameter containing:\nParameter(Params4bit([[115],\n            [214],\n            [ 49],\n            ...,\n            [100],\n            [152],\n            [ 85]], device='cuda:0', dtype=torch.uint8))\nafter load model.transformer.encoder.layers[27].self_attention.query_key_value.weight=Parameter containing:\nParameter(Params4bit([[115],\n            [214],\n            [ 49],\n            ...,\n            [100],\n            [152],\n            [ 85]], device='cuda:1', dtype=torch.uint8))\nafter laod model.transformer.encoder.layers[27].self_attention.dense.weight=Parameter containing:\nParameter(Params4bit([[ 87],\n            [150],\n            [180],\n            ...,\n            [150],\n            [ 58],\n            [ 89]], device='cuda:0', dtype=torch.uint8))\nafter laod model.transformer.encoder.layers[27].self_attention.dense.weight=Parameter containing:\nParameter(Params4bit([[ 87],\n            [150],\n            [180],\n            ...,\n            [150],\n            [ 58],\n            [ 89]], device='cuda:1', dtype=torch.uint8))\nafter load model.v_head.weight=Parameter containing:\ntensor([[ 0.0088,  0.0041,  0.0163,  ..., -0.0066,  0.0090,  0.0084]],\n       requires_grad=True)\nafter load model.v_head.weight=Parameter containing:\ntensor([[ 0.0088,  0.0041,  0.0163,  ..., -0.0066,  0.0090,  0.0084]],\n       requires_grad=True)\nRewardModel(\n  (transformer): ChatGLMModel(\n    (embedding): Embedding(\n      (word_embeddings): Embedding(65024, 4096)\n    )\n    (rotary_pos_emb): RotaryEmbedding()\n    (encoder): GLMTransformer(\n      (layers): ModuleList(\n        (0-27): 28 x GLMBlock(\n          (input_layernorm): RMSNorm()\n          (self_attention): SelfAttention(\n            (query_key_value): Linear4bit(\n              in_features=4096, out_features=4608, bias=True\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4608, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n            (core_attention): CoreAttention(\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n            )\n            (dense): Linear4bit(\n              in_features=4096, out_features=4096, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4096, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n          )\n          (post_attention_layernorm): RMSNorm()\n          (mlp): MLP(\n            (dense_h_to_4h): Linear4bit(\n              in_features=4096, out_features=27392, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=27392, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n            (dense_4h_to_h): Linear4bit(\n              in_features=13696, out_features=4096, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=13696, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4096, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n          )\n        )\n      )\n      (final_layernorm): RMSNorm()\n    )\n    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n  )\n  (v_head): Linear(in_features=4096, out_features=1, bias=False)\n  (loss_fn): PairWiseLoss()\n)\nRewardModel(\n  (transformer): ChatGLMModel(\n    (embedding): Embedding(\n      (word_embeddings): Embedding(65024, 4096)\n    )\n    (rotary_pos_emb): RotaryEmbedding()\n    (encoder): GLMTransformer(\n      (layers): ModuleList(\n        (0-27): 28 x GLMBlock(\n          (input_layernorm): RMSNorm()\n          (self_attention): SelfAttention(\n            (query_key_value): Linear4bit(\n              in_features=4096, out_features=4608, bias=True\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4608, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n            (core_attention): CoreAttention(\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n            )\n            (dense): Linear4bit(\n              in_features=4096, out_features=4096, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4096, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n          )\n          (post_attention_layernorm): RMSNorm()\n          (mlp): MLP(\n            (dense_h_to_4h): Linear4bit(\n              in_features=4096, out_features=27392, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=27392, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n            (dense_4h_to_h): Linear4bit(\n              in_features=13696, out_features=4096, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=13696, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4096, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n          )\n        )\n      )\n      (final_layernorm): RMSNorm()\n    )\n    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n  )\n  (v_head): Linear(in_features=4096, out_features=1, bias=False)\n  (loss_fn): PairWiseLoss()\n)\nbelow trainable params include v_head\nbelow trainable params include v_head\ntransformer_trainable_param = 118587392\nadd v_head params\ntrainable params: 118591488 || all params: 3506903040 || trainable%: 3.381659\nFinished loading model and tokenizer\ntransformer_trainable_param = 118587392\nadd v_head params\ntrainable params: 118591488 || all params: 3506903040 || trainable%: 3.381659\nFinished loading model and tokenizer\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=[],\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=1,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-rm-1k-0811-v3,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=2.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-rm-1k-0811-v3,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=4,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=[],\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-rm-1k-0811-v3,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=2.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-rm-1k-0811-v3,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=4,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\n  0%|                                                    | 0/50 [00:00<?, ?it/s]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([30.3906, 24.1406, 10.9453,  3.6973], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -0.0531,  -0.5977,  -0.1168, -22.0000], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([27.5781, 21.1562, 10.2656,  4.5469], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([  0.8184,   0.6621,  -3.7520, -18.5312], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n  2%|▉                                           | 1/50 [00:21<17:32, 21.48s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([24.6250, 21.8750, 11.6875,  6.2461], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -0.2469,   0.9146,   2.0078, -14.4922], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([23.7812, 20.4688, 12.3906,  7.2695], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([  1.2852,   0.3843,   0.2239, -15.9531], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n  4%|█▊                                          | 2/50 [00:38<14:55, 18.66s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([24.4219, 18.9844,  8.9766,  3.8086], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([-1.0452e-02,  9.3701e-01,  3.2715e+00, -1.5836e+01], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([24.9375, 20.4375, 12.0234,  6.7383], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([  0.2800,   0.1704,  -0.2537, -10.3359], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n  6%|██▋                                         | 3/50 [00:55<14:03, 17.95s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([27.1406, 23.5469, 12.2812,  7.2969], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([  0.7173,   2.2305,   2.3184, -14.8125], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([28.6094, 24.6094, 12.7422,  6.6992], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([  0.7090,   1.8750,   3.5137, -17.7969], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n  8%|███▌                                        | 4/50 [01:12<13:34, 17.70s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([28.4219, 20.7344,  8.2266,  4.3945], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([  2.6387,   0.7041,  -4.3828, -19.4219], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([27.6094, 20.3281,  6.5703,  4.6094], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -0.7119,  -3.5117,  -0.6431, -33.1250], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 10%|████▍                                       | 5/50 [01:29<13:08, 17.52s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([27.4688, 24.5938, 14.5469,  7.6406], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([  0.6289,   0.6865,  -2.6621, -17.9375], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([27.6562, 24.4844, 15.9375,  6.1758], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([  1.2441,   2.0176,   0.8931, -14.1484], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 12%|█████▎                                      | 6/50 [01:46<12:41, 17.31s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([31.1094, 25.3438, 12.6797,  1.3359], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -0.6123,  -3.0840,   0.9536, -28.0469], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([28.2656, 22.0781, 10.0391,  2.9453], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -3.8809,  -0.9229,  -1.2285, -16.2969], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 14%|██████▏                                     | 7/50 [02:03<12:20, 17.22s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([33.1875, 29.1719, 15.1328,  2.6309], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -4.8867,  -0.0799,   1.7402, -16.5781], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([31.3906, 26.5625, 13.5000, 11.9453], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -1.1201,  -1.8945,  -4.7852, -27.3594], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 16%|███████                                     | 8/50 [02:20<12:00, 17.15s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([40.5000, 40.5938, 39.8125, 37.8750], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 32.3750,  22.3281,   0.8042, -16.7188], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([41.2188, 40.7500, 39.8750, 37.6562], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 33.1875,  22.5000,  -0.6763, -20.7031], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 18%|███████▉                                    | 9/50 [02:37<11:42, 17.13s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([42.4688, 42.1875, 41.4688, 39.4062], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 35.3750,  26.3438,  10.7109, -14.9766], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([42.3750, 42.2500, 41.4688, 39.2812], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 34.8125,  24.6094,   2.5938, -16.2969], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'loss': 0.0001, 'learning_rate': 2e-05, 'epoch': 0.4}                          \n 20%|████████▌                                  | 10/50 [02:54<11:25, 17.13s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 15%|██████▊                                     | 2/13 [00:06<00:36,  3.32s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 23%|██████████▏                                 | 3/13 [00:13<00:46,  4.69s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 31%|█████████████▌                              | 4/13 [00:19<00:48,  5.42s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 38%|████████████████▉                           | 5/13 [00:26<00:46,  5.85s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 46%|████████████████████▎                       | 6/13 [00:33<00:42,  6.11s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 54%|███████████████████████▋                    | 7/13 [00:39<00:37,  6.29s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 62%|███████████████████████████                 | 8/13 [00:46<00:31,  6.38s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 69%|██████████████████████████████▍             | 9/13 [00:53<00:25,  6.46s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 77%|█████████████████████████████████          | 10/13 [00:59<00:19,  6.52s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 85%|████████████████████████████████████▍      | 11/13 [01:06<00:13,  6.55s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 92%|███████████████████████████████████████▋   | 12/13 [01:12<00:06,  6.55s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n100%|███████████████████████████████████████████| 13/13 [01:19<00:00,  6.58s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n                                                                                \n\u001b[A{'eval_loss': 4.5679855247726664e-05, 'eval_accuracy': 1.0, 'eval_runtime': 86.9046, 'eval_samples_per_second': 1.151, 'eval_steps_per_second': 0.15, 'epoch': 0.4}\n 20%|████████▌                                  | 10/50 [04:21<11:25, 17.13s/it]\n100%|███████████████████████████████████████████| 13/13 [01:22<00:00,  6.58s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 14:24:55.786\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m790\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0811-v3/checkpoint-10\u001b[0m\n\u001b[32m2023-08-11 14:24:55.787\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m804\u001b[0m - \u001b[31m\u001b[1m333 backbone_model, PreTrainedModel\u001b[0m\nTRAIN_TYPE ==lora\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([43.2188, 42.4062, 40.5625, 36.4688], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 25.5781,   3.3984,  -0.1494, -20.2656], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([43.5938, 43.0000, 41.8750, 39.2812], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 32.5938,  23.9688,   0.7939, -21.6719], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 22%|█████████▍                                 | 11/50 [04:41<28:57, 44.54s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([43.0625, 41.7500, 40.0000, 36.1875], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 26.1250,  -3.3594,  -0.8862, -18.5469], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([44.6250, 44.4688, 43.4375, 40.5312], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 35.2500,  26.8125,   0.7090, -14.1953], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 24%|██████████▎                                | 12/50 [04:58<22:56, 36.22s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([44.0938, 42.3125, 39.0938, 27.6562], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -0.5137,   0.0172,  -1.6797, -11.8125], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([45.6250, 45.4375, 44.2188, 41.0000], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 34.8750,  26.8750,   1.9541, -12.5391], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 26%|███████████▏                               | 13/50 [05:15<18:45, 30.42s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([44.5625, 41.2188, 34.0938,  2.0410], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([-0.5791, -0.9551,  0.5264, -8.2109], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([46.5938, 45.3125, 41.1875, 35.3438], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([25.9375,  0.0331,  2.4570, -8.9453], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 28%|████████████                               | 14/50 [05:32<15:48, 26.34s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([45.0000, 43.2188, 40.3750, 34.7500], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 29.9062,  23.6719,   0.2091, -12.9609], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([45.4688, 44.8438, 42.6562, 40.2188], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 35.1250,  28.8125,  26.5938, -20.0625], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 30%|████████████▉                              | 15/50 [05:49<13:43, 23.53s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([43.3438, 42.9375, 40.5000, 38.4062], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 33.6562,  29.8594,  25.3125, -18.8438], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([42.8750, 40.6562, 36.8750, 31.4688], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 27.1406,   2.8066,  -0.5576, -22.9531], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 32%|█████████████▊                             | 16/50 [06:06<12:13, 21.59s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([42.1562, 42.3438, 39.7500, 38.1562], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 33.8125,  30.4062,   1.9473, -15.5156], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([42.1250, 39.6875, 35.9688, 30.3125], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 25.7500,  -7.2656,   0.8696, -16.3906], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 34%|██████████████▌                            | 17/50 [06:23<11:07, 20.23s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([38.7500, 30.9375,  0.3701,  2.3945], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -1.4482,  -1.4482,  -2.1738, -15.8047], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([42.7188, 41.5312, 40.1875, 39.2812], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 35.9062,  33.2500,  33.0000, -14.3828], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 36%|███████████████▍                           | 18/50 [06:41<10:16, 19.27s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([42.7188, 42.1250, 39.6562, 37.5625], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 31.7344,  29.6719,  -0.6958, -17.4062], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([41.0625, 34.3438, 29.4531, 20.6250], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -2.0430,  -3.1699,  -0.1862, -23.7969], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 38%|████████████████▎                          | 19/50 [06:57<09:35, 18.57s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([43.0000, 41.5938, 39.5000, 37.6875], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 31.5469,  30.1562,  -2.0527, -20.8281], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([41.8438, 40.7188, 38.0938, 35.8438], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([30.3594, 29.0312, -2.9102, -7.2930], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'loss': 0.001, 'learning_rate': 1.55e-05, 'epoch': 0.8}                        \n 40%|█████████████████▏                         | 20/50 [07:15<09:04, 18.14s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 15%|██████▊                                     | 2/13 [00:06<00:36,  3.32s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 23%|██████████▏                                 | 3/13 [00:13<00:46,  4.69s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 31%|█████████████▌                              | 4/13 [00:19<00:48,  5.42s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 38%|████████████████▉                           | 5/13 [00:26<00:46,  5.83s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 46%|████████████████████▎                       | 6/13 [00:33<00:42,  6.10s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 54%|███████████████████████▋                    | 7/13 [00:39<00:37,  6.28s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 62%|███████████████████████████                 | 8/13 [00:46<00:32,  6.41s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 69%|██████████████████████████████▍             | 9/13 [00:53<00:25,  6.48s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 77%|█████████████████████████████████          | 10/13 [00:59<00:19,  6.52s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 85%|████████████████████████████████████▍      | 11/13 [01:06<00:13,  6.54s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 92%|███████████████████████████████████████▋   | 12/13 [01:12<00:06,  6.57s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n100%|███████████████████████████████████████████| 13/13 [01:19<00:00,  6.57s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n                                                                                \n\u001b[A{'eval_loss': 2.7998958103125915e-05, 'eval_accuracy': 1.0, 'eval_runtime': 86.2674, 'eval_samples_per_second': 1.159, 'eval_steps_per_second': 0.151, 'epoch': 0.8}\n 40%|█████████████████▏                         | 20/50 [08:41<09:04, 18.14s/it]\n100%|███████████████████████████████████████████| 13/13 [01:22<00:00,  6.57s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 14:29:15.284\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m790\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0811-v3/checkpoint-20\u001b[0m\n\u001b[32m2023-08-11 14:29:15.284\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m804\u001b[0m - \u001b[31m\u001b[1m333 backbone_model, PreTrainedModel\u001b[0m\nTRAIN_TYPE ==lora\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([41.1250, 35.1562, 30.5000, 22.5938], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -0.4749,   0.2778,  -5.0781, -26.7344], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([44.0938, 42.5312, 38.9688, 38.7500], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 33.3750,  30.0312,  24.1406, -18.1719], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 42%|██████████████████                         | 21/50 [09:01<21:31, 44.52s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([34.2812,  1.9609,  1.3789,  3.2930], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([  2.1426,  -3.2266,  -4.5078, -12.9766], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([44.2812, 40.2500, 37.8438, 31.0938], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 27.5625,   1.7861,  -2.0898, -11.8359], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 44%|██████████████████▉                        | 22/50 [09:18<16:55, 36.28s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([44.2188, 39.6875, 36.1250, 30.6250], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 28.5312,  -0.5957,  -1.6738, -10.9219], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([38.1250, 32.1250, 27.5469, 20.3281], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 1.1829e-01,  4.3115e-01,  5.2681e-03, -2.3469e+01], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 46%|███████████████████▊                       | 23/50 [09:35<13:44, 30.52s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([43.2812, 42.0312, 39.5625, 36.7500], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 30.0469,  30.3438,  -3.6211, -12.7344], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([42.7812, 38.3750, 32.5312, 28.2344], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 26.8906,  -1.7158,  -0.7461, -14.3125], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 48%|████████████████████▋                      | 24/50 [09:52<11:28, 26.49s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([45.5625, 43.1562, 38.5938, 37.4375], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 30.4688,  30.5938,  -1.1904, -14.2891], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([40.2188, 35.2812, 29.9062, 26.5625], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -1.0840,  -3.3867,  -1.8965, -18.5156], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 50%|█████████████████████▌                     | 25/50 [10:09<09:50, 23.62s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([44.0938, 42.0000, 38.3125, 35.2500], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 27.9219,  27.5312,  -0.2322, -16.7188], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([42.8438, 40.5938, 37.0000, 33.6875], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 28.8438,  29.7031,  -3.6582, -10.2891], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 52%|██████████████████████▎                    | 26/50 [10:26<08:40, 21.67s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([43.5625, 38.5312, 36.0938, 29.7031], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 2.8328e+01,  2.4188e+01,  5.6190e-03, -2.0969e+01], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([43.5625, 43.2188, 39.2812, 36.0000], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 30.0625,  29.1094,  -2.1602, -15.3281], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 54%|███████████████████████▏                   | 27/50 [10:43<07:46, 20.29s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([42.4375, 41.3125, 37.1250, 33.4062], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 27.6562,  27.2344,  -2.8574, -13.9688], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([44.1250, 43.1875, 37.3438, 35.0000], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 29.0312,  29.0938,  -2.3457, -19.4688], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 56%|████████████████████████                   | 28/50 [11:00<07:04, 19.31s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([43.9375, 42.5938, 36.5625, 35.1250], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 28.1094,  28.9062,  14.8906, -14.2969], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([43.9375, 42.0312, 35.7500, 34.0000], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 27.4219,  26.8594,   0.5386, -14.3828], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 58%|████████████████████████▉                  | 29/50 [11:17<06:30, 18.62s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([30.9375, 25.7656, 24.1719, -0.3989], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([  2.9805,  -1.9336,   1.9072, -25.4531], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([30.3750, 27.4844, 23.8281, -2.0723], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -3.4453,   0.2030,  -2.4297, -20.5156], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'loss': 0.0001, 'learning_rate': 1.0500000000000001e-05, 'epoch': 1.2}         \n 60%|█████████████████████████▊                 | 30/50 [11:34<06:02, 18.11s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 15%|██████▊                                     | 2/13 [00:06<00:36,  3.31s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 23%|██████████▏                                 | 3/13 [00:13<00:46,  4.67s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 31%|█████████████▌                              | 4/13 [00:19<00:48,  5.42s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 38%|████████████████▉                           | 5/13 [00:26<00:46,  5.83s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 46%|████████████████████▎                       | 6/13 [00:33<00:42,  6.12s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 54%|███████████████████████▋                    | 7/13 [00:39<00:37,  6.28s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 62%|███████████████████████████                 | 8/13 [00:46<00:31,  6.38s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 69%|██████████████████████████████▍             | 9/13 [00:53<00:25,  6.46s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 77%|█████████████████████████████████          | 10/13 [00:59<00:19,  6.52s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 85%|████████████████████████████████████▍      | 11/13 [01:06<00:13,  6.57s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 92%|███████████████████████████████████████▋   | 12/13 [01:12<00:06,  6.58s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n100%|███████████████████████████████████████████| 13/13 [01:19<00:00,  6.60s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n                                                                                \n\u001b[A{'eval_loss': 0.12848559021949768, 'eval_accuracy': 0.98, 'eval_runtime': 86.3405, 'eval_samples_per_second': 1.158, 'eval_steps_per_second': 0.151, 'epoch': 1.2}\n 60%|█████████████████████████▊                 | 30/50 [13:00<06:02, 18.11s/it]\n100%|███████████████████████████████████████████| 13/13 [01:22<00:00,  6.60s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 14:33:34.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m790\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0811-v3/checkpoint-30\u001b[0m\n\u001b[32m2023-08-11 14:33:34.690\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m804\u001b[0m - \u001b[31m\u001b[1m333 backbone_model, PreTrainedModel\u001b[0m\nTRAIN_TYPE ==lora\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([33.1250, 30.1250, 26.5312, 24.6875], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 21.1719,   0.4783,  -3.1113, -13.9219], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([40.6250, 35.0000, 30.8594, 28.0000], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([23.6250, 21.6406, 18.6406, -0.3796], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 62%|██████████████████████████▋                | 31/50 [13:20<14:07, 44.58s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([38.2188, 35.0938, 28.6875, 22.4531], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 16.7031,  15.9688, -16.5000, -15.7188], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([34.9688, 31.1250, 26.3438, 24.0938], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([17.8281,  2.0254,  0.5371, -9.7500], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 64%|███████████████████████████▌               | 32/50 [13:37<10:53, 36.29s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([42.3750, 39.4688, 33.9375, 29.7656], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([24.2812, 22.9844, 18.5938, -3.6133], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([44.0000, 34.6250, 32.5625, 25.5938], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([23.4062,  1.0811,  0.3750, -6.8438], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 66%|████████████████████████████▍              | 33/50 [13:54<08:37, 30.47s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([42.4688, 36.0625, 31.0625, 24.7344], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 23.9375,   1.2998,  -3.1387, -13.6250], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([38.1562, 34.1875, 26.6875, 21.4219], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 17.9219,   9.6641,  13.0234, -15.5859], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 68%|█████████████████████████████▏             | 34/50 [14:11<07:03, 26.44s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([40.6875, 33.7500, 29.2656, 20.3125], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ -8.9531,  -1.7314, -14.3438, -18.2656], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([36.4375, 26.6406, 22.5000,  5.1523], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([-14.1406, -29.3125, -14.6953, -24.4844], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 70%|██████████████████████████████             | 35/50 [14:28<05:53, 23.59s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([41.3438, 34.0625, 25.2656, 23.2500], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([-9.6094,  3.0879,  5.7773,  0.0643], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([46.1875, 37.4688, 29.9062, 22.7031], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([21.3594,  8.7969, 45.3438, -6.3398], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 72%|██████████████████████████████▉            | 36/50 [14:45<05:03, 21.65s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([49.9062, 43.2812, 36.3750, 28.4062], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([23.8750, 16.6250,  3.7734,  2.2871], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([33.8438, 25.5469, 19.9688,  4.6836], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([-12.9766,  44.1875,  25.1250, -27.0312], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 74%|███████████████████████████████▊           | 37/50 [15:02<04:23, 20.30s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([38.0312, 32.5312, 29.1562, 26.1406], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([-18.6250,  -4.3750,  -4.0508,  -7.7031], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([38.3750, 35.4688, 32.4688, 24.2812], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 18.0781,  14.3125, 115.1250,  -8.4844], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 76%|████████████████████████████████▋          | 38/50 [15:19<03:51, 19.28s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([48.6875, 45.4062, 39.6875, 25.0312], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([19.1719, 12.7266, 13.8750,  4.5625], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([44.1562, 46.2188, 36.3750, 25.2031], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([16.2344, 13.8750, 24.8594, -0.7466], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 78%|█████████████████████████████████▌         | 39/50 [15:36<03:24, 18.60s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([55.7812, 47.3125, 38.5312, 28.2188], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 24.7344,   8.2656, -14.0312,   1.3213], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([53.8750, 48.5938, 39.4688, 29.0625], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([24.6250, 15.3672, -0.3696,  3.1836], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'loss': 0.6544, 'learning_rate': 6.5000000000000004e-06, 'epoch': 1.6}         \n 80%|██████████████████████████████████▍        | 40/50 [15:53<03:00, 18.07s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 15%|██████▊                                     | 2/13 [00:06<00:36,  3.28s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 23%|██████████▏                                 | 3/13 [00:13<00:46,  4.66s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 31%|█████████████▌                              | 4/13 [00:19<00:48,  5.38s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 38%|████████████████▉                           | 5/13 [00:26<00:46,  5.82s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 46%|████████████████████▎                       | 6/13 [00:33<00:42,  6.10s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 54%|███████████████████████▋                    | 7/13 [00:39<00:37,  6.27s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 62%|███████████████████████████                 | 8/13 [00:46<00:32,  6.40s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 69%|██████████████████████████████▍             | 9/13 [00:52<00:25,  6.47s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 77%|█████████████████████████████████          | 10/13 [00:59<00:19,  6.53s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 85%|████████████████████████████████████▍      | 11/13 [01:06<00:13,  6.56s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 92%|███████████████████████████████████████▋   | 12/13 [01:12<00:06,  6.58s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n100%|███████████████████████████████████████████| 13/13 [01:19<00:00,  6.59s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n                                                                                \n\u001b[A{'eval_loss': 0.5769860744476318, 'eval_accuracy': 0.95, 'eval_runtime': 86.2608, 'eval_samples_per_second': 1.159, 'eval_steps_per_second': 0.151, 'epoch': 1.6}\n 80%|██████████████████████████████████▍        | 40/50 [17:19<03:00, 18.07s/it]\n100%|███████████████████████████████████████████| 13/13 [01:22<00:00,  6.59s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 14:37:53.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m790\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0811-v3/checkpoint-40\u001b[0m\n\u001b[32m2023-08-11 14:37:53.772\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m804\u001b[0m - \u001b[31m\u001b[1m333 backbone_model, PreTrainedModel\u001b[0m\nTRAIN_TYPE ==lora\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([48.1562, 38.0625, 33.0938, 24.9844], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 8.4531, 16.7188, -8.1953, -8.6641], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([53.9688, 47.0000, 43.0312, 33.8125], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 28.7969,  24.4844, -21.0156,  22.1875], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 82%|███████████████████████████████████▎       | 41/50 [17:39<06:39, 44.42s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([50.3750, 37.9375, 29.4688, 22.1719], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 15.7422,  47.0938,  31.0625, -12.4844], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([50.5938, 47.3125, 40.4688, 33.6875], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([27.9375, 21.6250,  7.3477,  6.7891], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 84%|████████████████████████████████████       | 42/50 [17:56<04:49, 36.19s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([57.3438, 49.5312, 38.0938, 32.1562], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 29.3594,   5.0781, -27.2031,  11.8203], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([58.5000, 52.9375, 45.5000, 34.2188], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([29.3281, 18.2344,  5.5781,  0.5991], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 86%|████████████████████████████████████▉      | 43/50 [18:13<03:33, 30.44s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([59.6875, 53.3750, 44.7812, 31.1406], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([24.2188, 16.8594, 18.1406,  9.6484], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([55.2500, 50.0625, 44.3750, 36.1250], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([26.7500, 22.8125,  8.6484, 10.5078], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 88%|█████████████████████████████████████▊     | 44/50 [18:30<02:38, 26.41s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([56.9688, 49.6250, 44.1875, 34.9375], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([31.0156, 24.1250,  0.4978, 16.0938], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([65.5625, 59.1250, 48.9375, 41.7188], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 33.7812,  25.0156, -12.6094,   9.7969], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 90%|██████████████████████████████████████▋    | 45/50 [18:47<01:57, 23.57s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([50.8750, 45.4688, 36.8438,  5.6250], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([-13.8750, -18.2969, -15.5625,  21.7031], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([65.9375, 59.3438, 49.4062, 42.9062], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 35.3438,  70.5625, -35.5625,  -0.2373], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 92%|███████████████████████████████████████▌   | 46/50 [19:04<01:26, 21.62s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([65.0625, 62.1250, 57.5625, 49.6875], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([42.1562, 32.9375,  9.7109, 13.0625], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([60.4375, 59.0938, 54.6562, 44.3438], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([38.6562, 25.5625,  5.5547,  0.4492], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 94%|████████████████████████████████████████▍  | 47/50 [19:21<01:00, 20.21s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([73.0000, 70.0000, 59.5938, 50.1250], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([49.2500, 34.1875, -1.1504, 24.1406], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([71.1875, 68.8125, 60.3125, 52.0938], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 51.4688,  37.8750, 174.5000,   7.9219], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 96%|█████████████████████████████████████████▎ | 48/50 [19:38<00:38, 19.22s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([87.3750, 79.3750, 74.4375, 64.8125], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([58.2812, 42.5000, 22.4375, 19.3594], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([80.3125, 71.1875, 63.5625, 57.4688], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 32.4688, -13.5859,  32.2812,  11.0547], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n 98%|██████████████████████████████████████████▏| 49/50 [19:55<00:18, 18.56s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([102.3750,  97.7500,  86.2500,  73.5625], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([60.7188, 33.1250, 15.1406, 10.8438], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([74.5000, 75.0625, 35.4062, 40.0938], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([14.5156, 19.4844,  2.2324, 26.1406], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'loss': 0.6709, 'learning_rate': 2.5e-06, 'epoch': 2.0}                        \n100%|███████████████████████████████████████████| 50/50 [20:12<00:00, 18.10s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 15%|██████▊                                     | 2/13 [00:06<00:36,  3.31s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 23%|██████████▏                                 | 3/13 [00:13<00:46,  4.69s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 31%|█████████████▌                              | 4/13 [00:19<00:48,  5.44s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 38%|████████████████▉                           | 5/13 [00:26<00:46,  5.86s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 46%|████████████████████▎                       | 6/13 [00:33<00:42,  6.12s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 54%|███████████████████████▋                    | 7/13 [00:39<00:37,  6.28s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 62%|███████████████████████████                 | 8/13 [00:46<00:31,  6.38s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 69%|██████████████████████████████▍             | 9/13 [00:53<00:25,  6.44s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 77%|█████████████████████████████████          | 10/13 [00:59<00:19,  6.53s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 85%|████████████████████████████████████▍      | 11/13 [01:06<00:13,  6.54s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 92%|███████████████████████████████████████▋   | 12/13 [01:13<00:06,  6.59s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n100%|███████████████████████████████████████████| 13/13 [01:19<00:00,  6.60s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n                                                                                \n\u001b[A{'eval_loss': 0.7176055908203125, 'eval_accuracy': 0.95, 'eval_runtime': 86.3496, 'eval_samples_per_second': 1.158, 'eval_steps_per_second': 0.151, 'epoch': 2.0}\n100%|███████████████████████████████████████████| 50/50 [21:38<00:00, 18.10s/it]\n100%|███████████████████████████████████████████| 13/13 [01:22<00:00,  6.60s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 14:42:12.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m790\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0811-v3/checkpoint-50\u001b[0m\n\u001b[32m2023-08-11 14:42:12.685\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m804\u001b[0m - \u001b[31m\u001b[1m333 backbone_model, PreTrainedModel\u001b[0m\nTRAIN_TYPE ==lora\nThere were missing keys in the checkpoint model loaded: ['transformer.embedding.word_embeddings.weight', 'transformer.rotary_pos_emb.inv_freq', 'transformer.encoder.layers.0.input_layernorm.weight', 'transformer.encoder.layers.0.self_attention.query_key_value.weight', 'transformer.encoder.layers.0.self_attention.query_key_value.bias', 'transformer.encoder.layers.0.self_attention.dense.weight', 'transformer.encoder.layers.0.post_attention_layernorm.weight', 'transformer.encoder.layers.0.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.0.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.1.input_layernorm.weight', 'transformer.encoder.layers.1.self_attention.query_key_value.weight', 'transformer.encoder.layers.1.self_attention.query_key_value.bias', 'transformer.encoder.layers.1.self_attention.dense.weight', 'transformer.encoder.layers.1.post_attention_layernorm.weight', 'transformer.encoder.layers.1.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.1.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.2.input_layernorm.weight', 'transformer.encoder.layers.2.self_attention.query_key_value.weight', 'transformer.encoder.layers.2.self_attention.query_key_value.bias', 'transformer.encoder.layers.2.self_attention.dense.weight', 'transformer.encoder.layers.2.post_attention_layernorm.weight', 'transformer.encoder.layers.2.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.2.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.3.input_layernorm.weight', 'transformer.encoder.layers.3.self_attention.query_key_value.weight', 'transformer.encoder.layers.3.self_attention.query_key_value.bias', 'transformer.encoder.layers.3.self_attention.dense.weight', 'transformer.encoder.layers.3.post_attention_layernorm.weight', 'transformer.encoder.layers.3.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.3.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.4.input_layernorm.weight', 'transformer.encoder.layers.4.self_attention.query_key_value.weight', 'transformer.encoder.layers.4.self_attention.query_key_value.bias', 'transformer.encoder.layers.4.self_attention.dense.weight', 'transformer.encoder.layers.4.post_attention_layernorm.weight', 'transformer.encoder.layers.4.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.4.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.5.input_layernorm.weight', 'transformer.encoder.layers.5.self_attention.query_key_value.weight', 'transformer.encoder.layers.5.self_attention.query_key_value.bias', 'transformer.encoder.layers.5.self_attention.dense.weight', 'transformer.encoder.layers.5.post_attention_layernorm.weight', 'transformer.encoder.layers.5.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.5.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.6.input_layernorm.weight', 'transformer.encoder.layers.6.self_attention.query_key_value.weight', 'transformer.encoder.layers.6.self_attention.query_key_value.bias', 'transformer.encoder.layers.6.self_attention.dense.weight', 'transformer.encoder.layers.6.post_attention_layernorm.weight', 'transformer.encoder.layers.6.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.6.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.7.input_layernorm.weight', 'transformer.encoder.layers.7.self_attention.query_key_value.weight', 'transformer.encoder.layers.7.self_attention.query_key_value.bias', 'transformer.encoder.layers.7.self_attention.dense.weight', 'transformer.encoder.layers.7.post_attention_layernorm.weight', 'transformer.encoder.layers.7.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.7.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.8.input_layernorm.weight', 'transformer.encoder.layers.8.self_attention.query_key_value.weight', 'transformer.encoder.layers.8.self_attention.query_key_value.bias', 'transformer.encoder.layers.8.self_attention.dense.weight', 'transformer.encoder.layers.8.post_attention_layernorm.weight', 'transformer.encoder.layers.8.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.8.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.9.input_layernorm.weight', 'transformer.encoder.layers.9.self_attention.query_key_value.weight', 'transformer.encoder.layers.9.self_attention.query_key_value.bias', 'transformer.encoder.layers.9.self_attention.dense.weight', 'transformer.encoder.layers.9.post_attention_layernorm.weight', 'transformer.encoder.layers.9.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.9.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.10.input_layernorm.weight', 'transformer.encoder.layers.10.self_attention.query_key_value.weight', 'transformer.encoder.layers.10.self_attention.query_key_value.bias', 'transformer.encoder.layers.10.self_attention.dense.weight', 'transformer.encoder.layers.10.post_attention_layernorm.weight', 'transformer.encoder.layers.10.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.10.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.11.input_layernorm.weight', 'transformer.encoder.layers.11.self_attention.query_key_value.weight', 'transformer.encoder.layers.11.self_attention.query_key_value.bias', 'transformer.encoder.layers.11.self_attention.dense.weight', 'transformer.encoder.layers.11.post_attention_layernorm.weight', 'transformer.encoder.layers.11.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.11.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.12.input_layernorm.weight', 'transformer.encoder.layers.12.self_attention.query_key_value.weight', 'transformer.encoder.layers.12.self_attention.query_key_value.bias', 'transformer.encoder.layers.12.self_attention.dense.weight', 'transformer.encoder.layers.12.post_attention_layernorm.weight', 'transformer.encoder.layers.12.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.12.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.13.input_layernorm.weight', 'transformer.encoder.layers.13.self_attention.query_key_value.weight', 'transformer.encoder.layers.13.self_attention.query_key_value.bias', 'transformer.encoder.layers.13.self_attention.dense.weight', 'transformer.encoder.layers.13.post_attention_layernorm.weight', 'transformer.encoder.layers.13.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.13.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.14.input_layernorm.weight', 'transformer.encoder.layers.14.self_attention.query_key_value.weight', 'transformer.encoder.layers.14.self_attention.query_key_value.bias', 'transformer.encoder.layers.14.self_attention.dense.weight', 'transformer.encoder.layers.14.post_attention_layernorm.weight', 'transformer.encoder.layers.14.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.14.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.15.input_layernorm.weight', 'transformer.encoder.layers.15.self_attention.query_key_value.weight', 'transformer.encoder.layers.15.self_attention.query_key_value.bias', 'transformer.encoder.layers.15.self_attention.dense.weight', 'transformer.encoder.layers.15.post_attention_layernorm.weight', 'transformer.encoder.layers.15.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.15.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.16.input_layernorm.weight', 'transformer.encoder.layers.16.self_attention.query_key_value.weight', 'transformer.encoder.layers.16.self_attention.query_key_value.bias', 'transformer.encoder.layers.16.self_attention.dense.weight', 'transformer.encoder.layers.16.post_attention_layernorm.weight', 'transformer.encoder.layers.16.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.16.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.17.input_layernorm.weight', 'transformer.encoder.layers.17.self_attention.query_key_value.weight', 'transformer.encoder.layers.17.self_attention.query_key_value.bias', 'transformer.encoder.layers.17.self_attention.dense.weight', 'transformer.encoder.layers.17.post_attention_layernorm.weight', 'transformer.encoder.layers.17.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.17.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.18.input_layernorm.weight', 'transformer.encoder.layers.18.self_attention.query_key_value.weight', 'transformer.encoder.layers.18.self_attention.query_key_value.bias', 'transformer.encoder.layers.18.self_attention.dense.weight', 'transformer.encoder.layers.18.post_attention_layernorm.weight', 'transformer.encoder.layers.18.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.18.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.19.input_layernorm.weight', 'transformer.encoder.layers.19.self_attention.query_key_value.weight', 'transformer.encoder.layers.19.self_attention.query_key_value.bias', 'transformer.encoder.layers.19.self_attention.dense.weight', 'transformer.encoder.layers.19.post_attention_layernorm.weight', 'transformer.encoder.layers.19.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.19.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.20.input_layernorm.weight', 'transformer.encoder.layers.20.self_attention.query_key_value.weight', 'transformer.encoder.layers.20.self_attention.query_key_value.bias', 'transformer.encoder.layers.20.self_attention.dense.weight', 'transformer.encoder.layers.20.post_attention_layernorm.weight', 'transformer.encoder.layers.20.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.20.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.21.input_layernorm.weight', 'transformer.encoder.layers.21.self_attention.query_key_value.weight', 'transformer.encoder.layers.21.self_attention.query_key_value.bias', 'transformer.encoder.layers.21.self_attention.dense.weight', 'transformer.encoder.layers.21.post_attention_layernorm.weight', 'transformer.encoder.layers.21.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.21.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.22.input_layernorm.weight', 'transformer.encoder.layers.22.self_attention.query_key_value.weight', 'transformer.encoder.layers.22.self_attention.query_key_value.bias', 'transformer.encoder.layers.22.self_attention.dense.weight', 'transformer.encoder.layers.22.post_attention_layernorm.weight', 'transformer.encoder.layers.22.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.22.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.23.input_layernorm.weight', 'transformer.encoder.layers.23.self_attention.query_key_value.weight', 'transformer.encoder.layers.23.self_attention.query_key_value.bias', 'transformer.encoder.layers.23.self_attention.dense.weight', 'transformer.encoder.layers.23.post_attention_layernorm.weight', 'transformer.encoder.layers.23.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.23.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.24.input_layernorm.weight', 'transformer.encoder.layers.24.self_attention.query_key_value.weight', 'transformer.encoder.layers.24.self_attention.query_key_value.bias', 'transformer.encoder.layers.24.self_attention.dense.weight', 'transformer.encoder.layers.24.post_attention_layernorm.weight', 'transformer.encoder.layers.24.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.24.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.25.input_layernorm.weight', 'transformer.encoder.layers.25.self_attention.query_key_value.weight', 'transformer.encoder.layers.25.self_attention.query_key_value.bias', 'transformer.encoder.layers.25.self_attention.dense.weight', 'transformer.encoder.layers.25.post_attention_layernorm.weight', 'transformer.encoder.layers.25.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.25.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.26.input_layernorm.weight', 'transformer.encoder.layers.26.self_attention.query_key_value.weight', 'transformer.encoder.layers.26.self_attention.query_key_value.bias', 'transformer.encoder.layers.26.self_attention.dense.weight', 'transformer.encoder.layers.26.post_attention_layernorm.weight', 'transformer.encoder.layers.26.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.26.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.27.input_layernorm.weight', 'transformer.encoder.layers.27.self_attention.query_key_value.weight', 'transformer.encoder.layers.27.self_attention.query_key_value.bias', 'transformer.encoder.layers.27.self_attention.dense.weight', 'transformer.encoder.layers.27.post_attention_layernorm.weight', 'transformer.encoder.layers.27.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.27.mlp.dense_4h_to_h.weight', 'transformer.encoder.final_layernorm.weight', 'transformer.output_layer.weight'].\nThere were missing keys in the checkpoint model loaded: ['transformer.embedding.word_embeddings.weight', 'transformer.rotary_pos_emb.inv_freq', 'transformer.encoder.layers.0.input_layernorm.weight', 'transformer.encoder.layers.0.self_attention.query_key_value.weight', 'transformer.encoder.layers.0.self_attention.query_key_value.bias', 'transformer.encoder.layers.0.self_attention.dense.weight', 'transformer.encoder.layers.0.post_attention_layernorm.weight', 'transformer.encoder.layers.0.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.0.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.1.input_layernorm.weight', 'transformer.encoder.layers.1.self_attention.query_key_value.weight', 'transformer.encoder.layers.1.self_attention.query_key_value.bias', 'transformer.encoder.layers.1.self_attention.dense.weight', 'transformer.encoder.layers.1.post_attention_layernorm.weight', 'transformer.encoder.layers.1.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.1.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.2.input_layernorm.weight', 'transformer.encoder.layers.2.self_attention.query_key_value.weight', 'transformer.encoder.layers.2.self_attention.query_key_value.bias', 'transformer.encoder.layers.2.self_attention.dense.weight', 'transformer.encoder.layers.2.post_attention_layernorm.weight', 'transformer.encoder.layers.2.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.2.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.3.input_layernorm.weight', 'transformer.encoder.layers.3.self_attention.query_key_value.weight', 'transformer.encoder.layers.3.self_attention.query_key_value.bias', 'transformer.encoder.layers.3.self_attention.dense.weight', 'transformer.encoder.layers.3.post_attention_layernorm.weight', 'transformer.encoder.layers.3.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.3.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.4.input_layernorm.weight', 'transformer.encoder.layers.4.self_attention.query_key_value.weight', 'transformer.encoder.layers.4.self_attention.query_key_value.bias', 'transformer.encoder.layers.4.self_attention.dense.weight', 'transformer.encoder.layers.4.post_attention_layernorm.weight', 'transformer.encoder.layers.4.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.4.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.5.input_layernorm.weight', 'transformer.encoder.layers.5.self_attention.query_key_value.weight', 'transformer.encoder.layers.5.self_attention.query_key_value.bias', 'transformer.encoder.layers.5.self_attention.dense.weight', 'transformer.encoder.layers.5.post_attention_layernorm.weight', 'transformer.encoder.layers.5.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.5.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.6.input_layernorm.weight', 'transformer.encoder.layers.6.self_attention.query_key_value.weight', 'transformer.encoder.layers.6.self_attention.query_key_value.bias', 'transformer.encoder.layers.6.self_attention.dense.weight', 'transformer.encoder.layers.6.post_attention_layernorm.weight', 'transformer.encoder.layers.6.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.6.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.7.input_layernorm.weight', 'transformer.encoder.layers.7.self_attention.query_key_value.weight', 'transformer.encoder.layers.7.self_attention.query_key_value.bias', 'transformer.encoder.layers.7.self_attention.dense.weight', 'transformer.encoder.layers.7.post_attention_layernorm.weight', 'transformer.encoder.layers.7.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.7.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.8.input_layernorm.weight', 'transformer.encoder.layers.8.self_attention.query_key_value.weight', 'transformer.encoder.layers.8.self_attention.query_key_value.bias', 'transformer.encoder.layers.8.self_attention.dense.weight', 'transformer.encoder.layers.8.post_attention_layernorm.weight', 'transformer.encoder.layers.8.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.8.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.9.input_layernorm.weight', 'transformer.encoder.layers.9.self_attention.query_key_value.weight', 'transformer.encoder.layers.9.self_attention.query_key_value.bias', 'transformer.encoder.layers.9.self_attention.dense.weight', 'transformer.encoder.layers.9.post_attention_layernorm.weight', 'transformer.encoder.layers.9.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.9.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.10.input_layernorm.weight', 'transformer.encoder.layers.10.self_attention.query_key_value.weight', 'transformer.encoder.layers.10.self_attention.query_key_value.bias', 'transformer.encoder.layers.10.self_attention.dense.weight', 'transformer.encoder.layers.10.post_attention_layernorm.weight', 'transformer.encoder.layers.10.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.10.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.11.input_layernorm.weight', 'transformer.encoder.layers.11.self_attention.query_key_value.weight', 'transformer.encoder.layers.11.self_attention.query_key_value.bias', 'transformer.encoder.layers.11.self_attention.dense.weight', 'transformer.encoder.layers.11.post_attention_layernorm.weight', 'transformer.encoder.layers.11.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.11.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.12.input_layernorm.weight', 'transformer.encoder.layers.12.self_attention.query_key_value.weight', 'transformer.encoder.layers.12.self_attention.query_key_value.bias', 'transformer.encoder.layers.12.self_attention.dense.weight', 'transformer.encoder.layers.12.post_attention_layernorm.weight', 'transformer.encoder.layers.12.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.12.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.13.input_layernorm.weight', 'transformer.encoder.layers.13.self_attention.query_key_value.weight', 'transformer.encoder.layers.13.self_attention.query_key_value.bias', 'transformer.encoder.layers.13.self_attention.dense.weight', 'transformer.encoder.layers.13.post_attention_layernorm.weight', 'transformer.encoder.layers.13.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.13.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.14.input_layernorm.weight', 'transformer.encoder.layers.14.self_attention.query_key_value.weight', 'transformer.encoder.layers.14.self_attention.query_key_value.bias', 'transformer.encoder.layers.14.self_attention.dense.weight', 'transformer.encoder.layers.14.post_attention_layernorm.weight', 'transformer.encoder.layers.14.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.14.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.15.input_layernorm.weight', 'transformer.encoder.layers.15.self_attention.query_key_value.weight', 'transformer.encoder.layers.15.self_attention.query_key_value.bias', 'transformer.encoder.layers.15.self_attention.dense.weight', 'transformer.encoder.layers.15.post_attention_layernorm.weight', 'transformer.encoder.layers.15.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.15.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.16.input_layernorm.weight', 'transformer.encoder.layers.16.self_attention.query_key_value.weight', 'transformer.encoder.layers.16.self_attention.query_key_value.bias', 'transformer.encoder.layers.16.self_attention.dense.weight', 'transformer.encoder.layers.16.post_attention_layernorm.weight', 'transformer.encoder.layers.16.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.16.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.17.input_layernorm.weight', 'transformer.encoder.layers.17.self_attention.query_key_value.weight', 'transformer.encoder.layers.17.self_attention.query_key_value.bias', 'transformer.encoder.layers.17.self_attention.dense.weight', 'transformer.encoder.layers.17.post_attention_layernorm.weight', 'transformer.encoder.layers.17.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.17.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.18.input_layernorm.weight', 'transformer.encoder.layers.18.self_attention.query_key_value.weight', 'transformer.encoder.layers.18.self_attention.query_key_value.bias', 'transformer.encoder.layers.18.self_attention.dense.weight', 'transformer.encoder.layers.18.post_attention_layernorm.weight', 'transformer.encoder.layers.18.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.18.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.19.input_layernorm.weight', 'transformer.encoder.layers.19.self_attention.query_key_value.weight', 'transformer.encoder.layers.19.self_attention.query_key_value.bias', 'transformer.encoder.layers.19.self_attention.dense.weight', 'transformer.encoder.layers.19.post_attention_layernorm.weight', 'transformer.encoder.layers.19.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.19.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.20.input_layernorm.weight', 'transformer.encoder.layers.20.self_attention.query_key_value.weight', 'transformer.encoder.layers.20.self_attention.query_key_value.bias', 'transformer.encoder.layers.20.self_attention.dense.weight', 'transformer.encoder.layers.20.post_attention_layernorm.weight', 'transformer.encoder.layers.20.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.20.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.21.input_layernorm.weight', 'transformer.encoder.layers.21.self_attention.query_key_value.weight', 'transformer.encoder.layers.21.self_attention.query_key_value.bias', 'transformer.encoder.layers.21.self_attention.dense.weight', 'transformer.encoder.layers.21.post_attention_layernorm.weight', 'transformer.encoder.layers.21.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.21.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.22.input_layernorm.weight', 'transformer.encoder.layers.22.self_attention.query_key_value.weight', 'transformer.encoder.layers.22.self_attention.query_key_value.bias', 'transformer.encoder.layers.22.self_attention.dense.weight', 'transformer.encoder.layers.22.post_attention_layernorm.weight', 'transformer.encoder.layers.22.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.22.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.23.input_layernorm.weight', 'transformer.encoder.layers.23.self_attention.query_key_value.weight', 'transformer.encoder.layers.23.self_attention.query_key_value.bias', 'transformer.encoder.layers.23.self_attention.dense.weight', 'transformer.encoder.layers.23.post_attention_layernorm.weight', 'transformer.encoder.layers.23.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.23.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.24.input_layernorm.weight', 'transformer.encoder.layers.24.self_attention.query_key_value.weight', 'transformer.encoder.layers.24.self_attention.query_key_value.bias', 'transformer.encoder.layers.24.self_attention.dense.weight', 'transformer.encoder.layers.24.post_attention_layernorm.weight', 'transformer.encoder.layers.24.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.24.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.25.input_layernorm.weight', 'transformer.encoder.layers.25.self_attention.query_key_value.weight', 'transformer.encoder.layers.25.self_attention.query_key_value.bias', 'transformer.encoder.layers.25.self_attention.dense.weight', 'transformer.encoder.layers.25.post_attention_layernorm.weight', 'transformer.encoder.layers.25.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.25.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.26.input_layernorm.weight', 'transformer.encoder.layers.26.self_attention.query_key_value.weight', 'transformer.encoder.layers.26.self_attention.query_key_value.bias', 'transformer.encoder.layers.26.self_attention.dense.weight', 'transformer.encoder.layers.26.post_attention_layernorm.weight', 'transformer.encoder.layers.26.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.26.mlp.dense_4h_to_h.weight', 'transformer.encoder.layers.27.input_layernorm.weight', 'transformer.encoder.layers.27.self_attention.query_key_value.weight', 'transformer.encoder.layers.27.self_attention.query_key_value.bias', 'transformer.encoder.layers.27.self_attention.dense.weight', 'transformer.encoder.layers.27.post_attention_layernorm.weight', 'transformer.encoder.layers.27.mlp.dense_h_to_4h.weight', 'transformer.encoder.layers.27.mlp.dense_4h_to_h.weight', 'transformer.encoder.final_layernorm.weight', 'transformer.output_layer.weight'].\n{'train_runtime': 1305.5709, 'train_samples_per_second': 0.306, 'train_steps_per_second': 0.038, 'train_loss': 0.2652752678422257, 'epoch': 2.0}\n\u001b[32m2023-08-11 14:42:19.034\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_only_lora_and_vhead\u001b[0m:\u001b[36m308\u001b[0m - \u001b[31m\u001b[1min func_save_only_lora_and_vhead\u001b[0m\n100%|███████████████████████████████████████████| 50/50 [21:45<00:00, 26.10s/it]\n\u001b[32m2023-08-11 14:42:19.056\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msave_only_lora_and_vhead\u001b[0m:\u001b[36m308\u001b[0m - \u001b[31m\u001b[1min func_save_only_lora_and_vhead\u001b[0m\nModel Saved. Only save lora layers and value_head.\nModel Saved. Only save lora layers and value_head.\n[2023-08-11 14:42:30,954] [INFO] [launch.py:347:main] Process 815 exits successfully.\n[2023-08-11 14:42:30,954] [INFO] [launch.py:347:main] Process 814 exits successfully.\n","output_type":"stream"}]},{"cell_type":"code","source":"ls -lrt /kaggle/working/chatGLM-6B-QLoRA/output-rm-1k-0811-v3","metadata":{"execution":{"iopub.status.busy":"2023-08-11T14:43:55.710087Z","iopub.execute_input":"2023-08-11T14:43:55.710535Z","iopub.status.idle":"2023-08-11T14:43:56.997109Z","shell.execute_reply.started":"2023-08-11T14:43:55.710495Z","shell.execute_reply":"2023-08-11T14:43:56.995800Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"total 463388\n-rw-r--r-- 1 root root      9014 Aug 11 13:49 events.out.tfevents.1691760494.5d48eec2301b.662.0\ndrwxr-xr-x 2 root root      4096 Aug 11 14:29 \u001b[0m\u001b[01;34mcheckpoint-20\u001b[0m/\ndrwxr-xr-x 2 root root      4096 Aug 11 14:42 \u001b[01;34mcheckpoint-50\u001b[0m/\n-rw-r--r-- 1 root root      9014 Aug 11 14:42 events.out.tfevents.1691763633.5d48eec2301b.814.0\n-rw-r--r-- 1 root root     17140 Aug 11 14:42 value_head.bin\n-rw-r--r-- 1 root root      1751 Aug 11 14:42 config.json\n-rw-r--r-- 1 root root 474446225 Aug 11 14:42 pytorch_model.bin\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 单卡训练 reward model","metadata":{}},{"cell_type":"code","source":"!git pull --all --force \n!CUDA_VISIBLE_DEVICES=0 python  rewardmodel_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-rm-1k-0811-v1 \\\n  --num_train_samples 20 \\\n  --num_eval_samples 20 \\\n  --train_data_path ./data/rm_data  \\\n  --eval_data_path  ./data/rm_data    \\\n  --data_type sharegpt  \\\n  --max_length 800 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 4 \\\n  --per_device_eval_batch_size 4  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  2e-5 \\\n  --num_train_epochs  10  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force \n!CUDA_VISIBLE_DEVICES=0 python  rm_3.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-rm-1k-0812-v1 \\\n  --num_train_samples 20 \\\n  --num_eval_samples 20 \\\n  --train_data_path ./data/rm_data  \\\n  --eval_data_path  ./data/rm_data    \\\n  --data_type sharegpt  \\\n  --max_length 400 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 1 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  2e-5 \\\n  --num_train_epochs  4  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True ","metadata":{"execution":{"iopub.status.busy":"2023-08-11T18:27:26.248812Z","iopub.execute_input":"2023-08-11T18:27:26.249305Z","iopub.status.idle":"2023-08-11T18:36:36.195541Z","shell.execute_reply.started":"2023-08-11T18:27:26.249258Z","shell.execute_reply":"2023-08-11T18:36:36.193892Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Fetching origin\nremote: Enumerating objects: 8, done.\u001b[K\nremote: Counting objects: 100% (8/8), done.\u001b[K\nremote: Compressing objects: 100% (6/6), done.\u001b[K\nremote: Total 6 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (6/6), 1.35 KiB | 197.00 KiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   865a610..1cdf9eb  main       -> origin/main\nUpdating 865a610..1cdf9eb\nFast-forward\n rm_3.py | 12 \u001b[32m+++++++\u001b[m\u001b[31m-----\u001b[m\n 1 file changed, 7 insertions(+), 5 deletions(-)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-11 18:27:37,086] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n\u001b[32m2023-08-11 18:27:41.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1333\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n\u001b[32m2023-08-11 18:27:41.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1379\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-11 18:27:41.699\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m605\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 515.14it/s]\n\u001b[32m2023-08-11 18:27:42.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m612\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-11 18:27:42.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m616\u001b[0m - \u001b[1m在取样之后 data len =20\u001b[0m\n\u001b[32m2023-08-11 18:27:42.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m620\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n\u001b[32m2023-08-11 18:27:42.050\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m605\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 422.09it/s]\n\u001b[32m2023-08-11 18:27:42.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m612\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-11 18:27:42.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m616\u001b[0m - \u001b[1m在取样之后 data len =20\u001b[0m\n\u001b[32m2023-08-11 18:27:42.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m620\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\nnumber_train_samples=20\nnumber_of_eval_samples=20\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:34<00:00, 13.56s/it]\nmemory footprint of model: 3.6520424485206604 GB\n\u001b[32m2023-08-11 18:29:18.314\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1490\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\nbelow trainable paramters only contains peft lora params.\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nNow model is an AutoModelForCausalLMWithValueHead\nAutoModelForCausalLMWithValueHead(\n  (pretrained_model): PeftModelForCausalLM(\n    (base_model): LoraModel(\n      (model): ChatGLMForConditionalGeneration(\n        (transformer): ChatGLMModel(\n          (embedding): Embedding(\n            (word_embeddings): Embedding(65024, 4096)\n          )\n          (rotary_pos_emb): RotaryEmbedding()\n          (encoder): GLMTransformer(\n            (layers): ModuleList(\n              (0-27): 28 x GLMBlock(\n                (input_layernorm): RMSNorm()\n                (self_attention): SelfAttention(\n                  (query_key_value): Linear4bit(\n                    in_features=4096, out_features=4608, bias=True\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=4096, out_features=64, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=64, out_features=4608, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (core_attention): CoreAttention(\n                    (attention_dropout): Dropout(p=0.0, inplace=False)\n                  )\n                  (dense): Linear4bit(\n                    in_features=4096, out_features=4096, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=4096, out_features=64, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=64, out_features=4096, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                )\n                (post_attention_layernorm): RMSNorm()\n                (mlp): MLP(\n                  (dense_h_to_4h): Linear4bit(\n                    in_features=4096, out_features=27392, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=4096, out_features=64, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=64, out_features=27392, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (dense_4h_to_h): Linear4bit(\n                    in_features=13696, out_features=4096, bias=False\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=13696, out_features=64, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=64, out_features=4096, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                )\n              )\n            )\n            (final_layernorm): RMSNorm()\n          )\n          (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n        )\n        (lm_head): Linear(in_features=4096, out_features=65024, bias=False)\n      )\n    )\n  )\n  (v_head): ValueHead(\n    (dropout): Dropout(p=0.1, inplace=False)\n    (summary): Linear(in_features=4096, out_features=1, bias=True)\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n)\nbelow trainable params include v_head\ntrainable params: 118591489 || all params: 3506903041 || trainable%: 3.3817\nFinished loading model and tokenizer\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=[],\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-rm-1k-0812-v1,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=4.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-rm-1k-0812-v1,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=1,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\n{'loss': 1.6145, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.5}          \n 12%|█████▍                                     | 10/80 [00:28<02:56,  2.53s/it]\n  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n 10%|████▍                                       | 2/20 [00:00<00:07,  2.46it/s]\u001b[A\n 15%|██████▌                                     | 3/20 [00:01<00:09,  1.74it/s]\u001b[A\n 20%|████████▊                                   | 4/20 [00:02<00:10,  1.51it/s]\u001b[A\n 25%|███████████                                 | 5/20 [00:03<00:10,  1.40it/s]\u001b[A\n 30%|█████████████▏                              | 6/20 [00:04<00:10,  1.33it/s]\u001b[A\n 35%|███████████████▍                            | 7/20 [00:04<00:10,  1.29it/s]\u001b[A\n 40%|█████████████████▌                          | 8/20 [00:05<00:09,  1.27it/s]\u001b[A\n 45%|███████████████████▊                        | 9/20 [00:06<00:08,  1.25it/s]\u001b[A\n 50%|█████████████████████▌                     | 10/20 [00:07<00:08,  1.24it/s]\u001b[A\n 55%|███████████████████████▋                   | 11/20 [00:08<00:07,  1.23it/s]\u001b[A\n 60%|█████████████████████████▊                 | 12/20 [00:09<00:06,  1.23it/s]\u001b[A\n 65%|███████████████████████████▉               | 13/20 [00:09<00:05,  1.23it/s]\u001b[A\n 70%|██████████████████████████████             | 14/20 [00:10<00:04,  1.22it/s]\u001b[A\n 75%|████████████████████████████████▎          | 15/20 [00:11<00:04,  1.21it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 16/20 [00:12<00:03,  1.21it/s]\u001b[A\n 85%|████████████████████████████████████▌      | 17/20 [00:13<00:02,  1.21it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 18/20 [00:13<00:01,  1.20it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 19/20 [00:14<00:00,  1.19it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.7925770282745361, 'eval_accuracy': 0.4, 'eval_runtime': 16.501, 'eval_samples_per_second': 1.212, 'eval_steps_per_second': 1.212, 'epoch': 0.5}\n 12%|█████▍                                     | 10/80 [00:44<02:56,  2.53s/it]\n100%|███████████████████████████████████████████| 20/20 [00:15<00:00,  1.19it/s]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 18:30:57.645\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v1/checkpoint-10\u001b[0m\n\u001b[32m2023-08-11 18:30:57.645\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-11 18:30:57.646\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 2.2341, 'learning_rate': 1.942857142857143e-05, 'epoch': 1.0}          \n 25%|██████████▊                                | 20/80 [01:12<02:50,  2.85s/it]\n  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n 10%|████▍                                       | 2/20 [00:00<00:07,  2.27it/s]\u001b[A\n 15%|██████▌                                     | 3/20 [00:01<00:10,  1.61it/s]\u001b[A\n 20%|████████▊                                   | 4/20 [00:02<00:11,  1.39it/s]\u001b[A\n 25%|███████████                                 | 5/20 [00:03<00:11,  1.29it/s]\u001b[A\n 30%|█████████████▏                              | 6/20 [00:04<00:11,  1.23it/s]\u001b[A\n 35%|███████████████▍                            | 7/20 [00:05<00:10,  1.20it/s]\u001b[A\n 40%|█████████████████▌                          | 8/20 [00:06<00:10,  1.18it/s]\u001b[A\n 45%|███████████████████▊                        | 9/20 [00:07<00:09,  1.16it/s]\u001b[A\n 50%|█████████████████████▌                     | 10/20 [00:07<00:08,  1.16it/s]\u001b[A\n 55%|███████████████████████▋                   | 11/20 [00:08<00:07,  1.15it/s]\u001b[A\n 60%|█████████████████████████▊                 | 12/20 [00:09<00:07,  1.14it/s]\u001b[A\n 65%|███████████████████████████▉               | 13/20 [00:10<00:06,  1.14it/s]\u001b[A\n 70%|██████████████████████████████             | 14/20 [00:11<00:05,  1.14it/s]\u001b[A\n 75%|████████████████████████████████▎          | 15/20 [00:12<00:04,  1.13it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 16/20 [00:13<00:03,  1.13it/s]\u001b[A\n 85%|████████████████████████████████████▌      | 17/20 [00:14<00:02,  1.13it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 18/20 [00:15<00:01,  1.13it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 19/20 [00:15<00:00,  1.12it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.2517513036727905, 'eval_accuracy': 0.5, 'eval_runtime': 17.6995, 'eval_samples_per_second': 1.13, 'eval_steps_per_second': 1.13, 'epoch': 1.0}\n 25%|██████████▊                                | 20/80 [01:30<02:50,  2.85s/it]\n100%|███████████████████████████████████████████| 20/20 [00:17<00:00,  1.12it/s]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 18:31:43.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v1/checkpoint-20\u001b[0m\n\u001b[32m2023-08-11 18:31:43.102\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-11 18:31:43.103\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.4681, 'learning_rate': 1.6571428571428574e-05, 'epoch': 1.5}         \n 38%|████████████████▏                          | 30/80 [02:00<02:33,  3.06s/it]\n  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n 10%|████▍                                       | 2/20 [00:00<00:08,  2.10it/s]\u001b[A\n 15%|██████▌                                     | 3/20 [00:01<00:11,  1.49it/s]\u001b[A\n 20%|████████▊                                   | 4/20 [00:02<00:12,  1.29it/s]\u001b[A\n 25%|███████████                                 | 5/20 [00:03<00:12,  1.19it/s]\u001b[A\n 30%|█████████████▏                              | 6/20 [00:04<00:12,  1.13it/s]\u001b[A\n 35%|███████████████▍                            | 7/20 [00:05<00:11,  1.10it/s]\u001b[A\n 40%|█████████████████▌                          | 8/20 [00:06<00:11,  1.09it/s]\u001b[A\n 45%|███████████████████▊                        | 9/20 [00:07<00:10,  1.07it/s]\u001b[A\n 50%|█████████████████████▌                     | 10/20 [00:08<00:09,  1.06it/s]\u001b[A\n 55%|███████████████████████▋                   | 11/20 [00:09<00:08,  1.06it/s]\u001b[A\n 60%|█████████████████████████▊                 | 12/20 [00:10<00:07,  1.06it/s]\u001b[A\n 65%|███████████████████████████▉               | 13/20 [00:11<00:06,  1.06it/s]\u001b[A\n 70%|██████████████████████████████             | 14/20 [00:12<00:05,  1.06it/s]\u001b[A\n 75%|████████████████████████████████▎          | 15/20 [00:13<00:04,  1.06it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 16/20 [00:14<00:03,  1.06it/s]\u001b[A\n 85%|████████████████████████████████████▌      | 17/20 [00:15<00:02,  1.06it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 18/20 [00:16<00:01,  1.06it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 19/20 [00:17<00:00,  1.06it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.8153332471847534, 'eval_accuracy': 0.8, 'eval_runtime': 18.9685, 'eval_samples_per_second': 1.054, 'eval_steps_per_second': 1.054, 'epoch': 1.5}\n 38%|████████████████▏                          | 30/80 [02:19<02:33,  3.06s/it]\n100%|███████████████████████████████████████████| 20/20 [00:18<00:00,  1.06it/s]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 18:32:31.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v1/checkpoint-30\u001b[0m\n\u001b[32m2023-08-11 18:32:31.745\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-11 18:32:31.746\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 1.664, 'learning_rate': 1.4e-05, 'epoch': 2.0}                         \n 50%|█████████████████████▌                     | 40/80 [02:48<01:59,  2.99s/it]\n  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n 10%|████▍                                       | 2/20 [00:00<00:08,  2.17it/s]\u001b[A\n 15%|██████▌                                     | 3/20 [00:01<00:11,  1.54it/s]\u001b[A\n 20%|████████▊                                   | 4/20 [00:02<00:12,  1.33it/s]\u001b[A\n 25%|███████████                                 | 5/20 [00:03<00:12,  1.23it/s]\u001b[A\n 30%|█████████████▏                              | 6/20 [00:04<00:11,  1.18it/s]\u001b[A\n 35%|███████████████▍                            | 7/20 [00:05<00:11,  1.14it/s]\u001b[A\n 40%|█████████████████▌                          | 8/20 [00:06<00:10,  1.13it/s]\u001b[A\n 45%|███████████████████▊                        | 9/20 [00:07<00:09,  1.11it/s]\u001b[A\n 50%|█████████████████████▌                     | 10/20 [00:08<00:09,  1.10it/s]\u001b[A\n 55%|███████████████████████▋                   | 11/20 [00:09<00:08,  1.10it/s]\u001b[A\n 60%|█████████████████████████▊                 | 12/20 [00:10<00:07,  1.09it/s]\u001b[A\n 65%|███████████████████████████▉               | 13/20 [00:11<00:06,  1.10it/s]\u001b[A\n 70%|██████████████████████████████             | 14/20 [00:11<00:05,  1.09it/s]\u001b[A\n 75%|████████████████████████████████▎          | 15/20 [00:12<00:04,  1.09it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 16/20 [00:13<00:03,  1.08it/s]\u001b[A\n 85%|████████████████████████████████████▌      | 17/20 [00:14<00:02,  1.08it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 18/20 [00:15<00:01,  1.08it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 19/20 [00:16<00:00,  1.08it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.02309252880513668, 'eval_accuracy': 1.0, 'eval_runtime': 18.4654, 'eval_samples_per_second': 1.083, 'eval_steps_per_second': 1.083, 'epoch': 2.0}\n 50%|█████████████████████▌                     | 40/80 [03:07<01:59,  2.99s/it]\n100%|███████████████████████████████████████████| 20/20 [00:17<00:00,  1.08it/s]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 18:33:19.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v1/checkpoint-40\u001b[0m\n\u001b[32m2023-08-11 18:33:19.804\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-11 18:33:19.805\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.0042, 'learning_rate': 1.1142857142857143e-05, 'epoch': 2.5}         \n 62%|██████████████████████████▉                | 50/80 [03:36<01:29,  2.99s/it]\n  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n 10%|████▍                                       | 2/20 [00:00<00:08,  2.15it/s]\u001b[A\n 15%|██████▌                                     | 3/20 [00:01<00:11,  1.52it/s]\u001b[A\n 20%|████████▊                                   | 4/20 [00:02<00:12,  1.31it/s]\u001b[A\n 25%|███████████                                 | 5/20 [00:03<00:12,  1.21it/s]\u001b[A\n 30%|█████████████▏                              | 6/20 [00:04<00:12,  1.15it/s]\u001b[A\n 35%|███████████████▍                            | 7/20 [00:05<00:11,  1.12it/s]\u001b[A\n 40%|█████████████████▌                          | 8/20 [00:06<00:10,  1.10it/s]\u001b[A\n 45%|███████████████████▊                        | 9/20 [00:07<00:10,  1.09it/s]\u001b[A\n 50%|█████████████████████▌                     | 10/20 [00:08<00:09,  1.08it/s]\u001b[A\n 55%|███████████████████████▋                   | 11/20 [00:09<00:08,  1.07it/s]\u001b[A\n 60%|█████████████████████████▊                 | 12/20 [00:10<00:07,  1.07it/s]\u001b[A\n 65%|███████████████████████████▉               | 13/20 [00:11<00:06,  1.08it/s]\u001b[A\n 70%|██████████████████████████████             | 14/20 [00:12<00:05,  1.08it/s]\u001b[A\n 75%|████████████████████████████████▎          | 15/20 [00:13<00:04,  1.07it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 16/20 [00:14<00:03,  1.07it/s]\u001b[A\n 85%|████████████████████████████████████▌      | 17/20 [00:14<00:02,  1.07it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 18/20 [00:15<00:01,  1.07it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 19/20 [00:16<00:00,  1.07it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5109527707099915, 'eval_accuracy': 0.95, 'eval_runtime': 18.7178, 'eval_samples_per_second': 1.069, 'eval_steps_per_second': 1.069, 'epoch': 2.5}\n 62%|██████████████████████████▉                | 50/80 [03:55<01:29,  2.99s/it]\n100%|███████████████████████████████████████████| 20/20 [00:18<00:00,  1.07it/s]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 18:34:07.688\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v1/checkpoint-50\u001b[0m\n\u001b[32m2023-08-11 18:34:07.688\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-11 18:34:07.689\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.0, 'learning_rate': 8.571428571428571e-06, 'epoch': 3.0}             \n 75%|████████████████████████████████▎          | 60/80 [04:23<00:59,  2.97s/it]\n  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n 10%|████▍                                       | 2/20 [00:00<00:08,  2.18it/s]\u001b[A\n 15%|██████▌                                     | 3/20 [00:01<00:11,  1.55it/s]\u001b[A\n 20%|████████▊                                   | 4/20 [00:02<00:11,  1.33it/s]\u001b[A\n 25%|███████████                                 | 5/20 [00:03<00:12,  1.23it/s]\u001b[A\n 30%|█████████████▏                              | 6/20 [00:04<00:11,  1.18it/s]\u001b[A\n 35%|███████████████▍                            | 7/20 [00:05<00:11,  1.14it/s]\u001b[A\n 40%|█████████████████▌                          | 8/20 [00:06<00:10,  1.13it/s]\u001b[A\n 45%|███████████████████▊                        | 9/20 [00:07<00:09,  1.11it/s]\u001b[A\n 50%|█████████████████████▌                     | 10/20 [00:08<00:09,  1.10it/s]\u001b[A\n 55%|███████████████████████▋                   | 11/20 [00:09<00:08,  1.09it/s]\u001b[A\n 60%|█████████████████████████▊                 | 12/20 [00:10<00:07,  1.09it/s]\u001b[A\n 65%|███████████████████████████▉               | 13/20 [00:11<00:06,  1.10it/s]\u001b[A\n 70%|██████████████████████████████             | 14/20 [00:11<00:05,  1.09it/s]\u001b[A\n 75%|████████████████████████████████▎          | 15/20 [00:12<00:04,  1.09it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 16/20 [00:13<00:03,  1.09it/s]\u001b[A\n 85%|████████████████████████████████████▌      | 17/20 [00:14<00:02,  1.09it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 18/20 [00:15<00:01,  1.09it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 19/20 [00:16<00:00,  1.08it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': inf, 'eval_accuracy': 0.5, 'eval_runtime': 18.4491, 'eval_samples_per_second': 1.084, 'eval_steps_per_second': 1.084, 'epoch': 3.0}\n 75%|████████████████████████████████▎          | 60/80 [04:42<00:59,  2.97s/it]\n100%|███████████████████████████████████████████| 20/20 [00:17<00:00,  1.08it/s]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 18:34:55.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v1/checkpoint-60\u001b[0m\n\u001b[32m2023-08-11 18:34:55.026\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-11 18:34:55.026\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.0, 'learning_rate': 7.428571428571429e-06, 'epoch': 3.5}             \n 88%|█████████████████████████████████████▋     | 70/80 [05:11<00:29,  2.95s/it]\n  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n 10%|████▍                                       | 2/20 [00:00<00:08,  2.16it/s]\u001b[A\n 15%|██████▌                                     | 3/20 [00:01<00:11,  1.53it/s]\u001b[A\n 20%|████████▊                                   | 4/20 [00:02<00:12,  1.33it/s]\u001b[A\n 25%|███████████                                 | 5/20 [00:03<00:12,  1.22it/s]\u001b[A\n 30%|█████████████▏                              | 6/20 [00:04<00:12,  1.17it/s]\u001b[A\n 35%|███████████████▍                            | 7/20 [00:05<00:11,  1.13it/s]\u001b[A\n 40%|█████████████████▌                          | 8/20 [00:06<00:10,  1.11it/s]\u001b[A\n 45%|███████████████████▊                        | 9/20 [00:07<00:09,  1.10it/s]\u001b[A\n 50%|█████████████████████▌                     | 10/20 [00:08<00:09,  1.09it/s]\u001b[A\n 55%|███████████████████████▋                   | 11/20 [00:09<00:08,  1.08it/s]\u001b[A\n 60%|█████████████████████████▊                 | 12/20 [00:10<00:07,  1.08it/s]\u001b[A\n 65%|███████████████████████████▉               | 13/20 [00:11<00:06,  1.08it/s]\u001b[A\n 70%|██████████████████████████████             | 14/20 [00:12<00:05,  1.08it/s]\u001b[A\n 75%|████████████████████████████████▎          | 15/20 [00:13<00:04,  1.08it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 16/20 [00:13<00:03,  1.07it/s]\u001b[A\n 85%|████████████████████████████████████▌      | 17/20 [00:14<00:02,  1.08it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 18/20 [00:15<00:01,  1.08it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 19/20 [00:16<00:00,  1.07it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': inf, 'eval_accuracy': 0.2, 'eval_runtime': 18.6151, 'eval_samples_per_second': 1.074, 'eval_steps_per_second': 1.074, 'epoch': 3.5}\n 88%|█████████████████████████████████████▋     | 70/80 [05:29<00:29,  2.95s/it]\n100%|███████████████████████████████████████████| 20/20 [00:18<00:00,  1.07it/s]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 18:35:42.390\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v1/checkpoint-70\u001b[0m\n\u001b[32m2023-08-11 18:35:42.391\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-11 18:35:42.392\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n{'loss': 0.0, 'learning_rate': 6.857142857142858e-06, 'epoch': 4.0}             \n100%|███████████████████████████████████████████| 80/80 [05:58<00:00,  2.95s/it]\n  0%|                                                    | 0/20 [00:00<?, ?it/s]\u001b[A\n 10%|████▍                                       | 2/20 [00:00<00:08,  2.19it/s]\u001b[A\n 15%|██████▌                                     | 3/20 [00:01<00:10,  1.55it/s]\u001b[A\n 20%|████████▊                                   | 4/20 [00:02<00:11,  1.34it/s]\u001b[A\n 25%|███████████                                 | 5/20 [00:03<00:12,  1.23it/s]\u001b[A\n 30%|█████████████▏                              | 6/20 [00:04<00:11,  1.17it/s]\u001b[A\n 35%|███████████████▍                            | 7/20 [00:05<00:11,  1.14it/s]\u001b[A\n 40%|█████████████████▌                          | 8/20 [00:06<00:10,  1.12it/s]\u001b[A\n 45%|███████████████████▊                        | 9/20 [00:07<00:09,  1.11it/s]\u001b[A\n 50%|█████████████████████▌                     | 10/20 [00:08<00:09,  1.10it/s]\u001b[A\n 55%|███████████████████████▋                   | 11/20 [00:09<00:08,  1.09it/s]\u001b[A\n 60%|█████████████████████████▊                 | 12/20 [00:10<00:07,  1.09it/s]\u001b[A\n 65%|███████████████████████████▉               | 13/20 [00:11<00:06,  1.09it/s]\u001b[A\n 70%|██████████████████████████████             | 14/20 [00:11<00:05,  1.09it/s]\u001b[A\n 75%|████████████████████████████████▎          | 15/20 [00:12<00:04,  1.09it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 16/20 [00:13<00:03,  1.08it/s]\u001b[A\n 85%|████████████████████████████████████▌      | 17/20 [00:14<00:02,  1.08it/s]\u001b[A\n 90%|██████████████████████████████████████▋    | 18/20 [00:15<00:01,  1.08it/s]\u001b[A\n 95%|████████████████████████████████████████▊  | 19/20 [00:16<00:00,  1.08it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': inf, 'eval_accuracy': 0.2, 'eval_runtime': 18.4774, 'eval_samples_per_second': 1.082, 'eval_steps_per_second': 1.082, 'epoch': 4.0}\n100%|███████████████████████████████████████████| 80/80 [06:16<00:00,  2.95s/it]\n100%|███████████████████████████████████████████| 20/20 [00:17<00:00,  1.08it/s]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 18:36:29.494\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v1/checkpoint-80\u001b[0m\n\u001b[32m2023-08-11 18:36:29.494\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-11 18:36:29.495\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n\u001b[32m2023-08-11 18:36:31.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_load_best_model\u001b[0m:\u001b[36m880\u001b[0m - \u001b[1mLoading best model from output-rm-1k-0812-v1/checkpoint-40 (score: 0.02309252880513668).\u001b[0m\n{'train_runtime': 378.7714, 'train_samples_per_second': 0.211, 'train_steps_per_second': 0.211, 'train_loss': 0.7481142545118928, 'epoch': 4.0}\n100%|███████████████████████████████████████████| 80/80 [06:18<00:00,  4.73s/it]\n\u001b[32m2023-08-11 18:36:31.447\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m840\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0812-v1\u001b[0m\n\u001b[32m2023-08-11 18:36:31.447\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m844\u001b[0m - \u001b[31m\u001b[1m 111  model has pretrained_model\u001b[0m\n\u001b[32m2023-08-11 18:36:31.448\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m851\u001b[0m - \u001b[31m\u001b[1m222 backbone_model, PeftModel\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"rm -rf output-rm-1k-0812-v1","metadata":{"execution":{"iopub.status.busy":"2023-08-11T18:27:14.344853Z","iopub.execute_input":"2023-08-11T18:27:14.345318Z","iopub.status.idle":"2023-08-11T18:27:15.559789Z","shell.execute_reply.started":"2023-08-11T18:27:14.345279Z","shell.execute_reply":"2023-08-11T18:27:15.558472Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"参考 https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/src/glmtuner/tuner/core/adapter.py#L83\n实现 resume——from --checkpoint\n\n参考","metadata":{}},{"cell_type":"code","source":"https://huggingface.co/docs/trl/models#trl.AutoModelForCausalLMWithValueHead\n    https://github.com/yongzhuo/chatglm-maths/blob/main/chatglm_maths/t10_lora_trl_train_ppo.py#L175\n        https://huggingface.co/docs/trl/v0.5.0/en/customization#use-the-cuda-cache-optimizer\n            https://huggingface.co/docs/trl/trainer#trl.RewardTrainer","metadata":{},"execution_count":null,"outputs":[]}]}