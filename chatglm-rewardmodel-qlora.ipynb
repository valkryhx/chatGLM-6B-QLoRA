{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-11T07:29:31.336586Z","iopub.execute_input":"2023-08-11T07:29:31.337004Z","iopub.status.idle":"2023-08-11T07:29:31.343888Z","shell.execute_reply.started":"2023-08-11T07:29:31.336971Z","shell.execute_reply":"2023-08-11T07:29:31.342755Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/valkryhx/chatGLM-6B-QLoRA","metadata":{"execution":{"iopub.status.busy":"2023-08-11T07:30:32.287969Z","iopub.execute_input":"2023-08-11T07:30:32.288350Z","iopub.status.idle":"2023-08-11T07:30:33.256266Z","shell.execute_reply.started":"2023-08-11T07:30:32.288317Z","shell.execute_reply":"2023-08-11T07:30:33.255038Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"fatal: destination path 'chatGLM-6B-QLoRA' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-09T13:43:24.155248Z","iopub.execute_input":"2023-08-09T13:43:24.156012Z","iopub.status.idle":"2023-08-09T13:43:25.216333Z","shell.execute_reply.started":"2023-08-09T13:43:24.155960Z","shell.execute_reply":"2023-08-09T13:43:25.214930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd chatGLM-6B-QLoRA\n!git pull --all --force \n!pip install -r requirements.txt","metadata":{"execution":{"iopub.status.busy":"2023-08-11T07:31:27.069151Z","iopub.execute_input":"2023-08-11T07:31:27.069605Z","iopub.status.idle":"2023-08-11T07:32:09.360118Z","shell.execute_reply.started":"2023-08-11T07:31:27.069563Z","shell.execute_reply":"2023-08-11T07:32:09.358788Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/chatGLM-6B-QLoRA\nFetching origin\nremote: Enumerating objects: 18, done.\u001b[K\nremote: Counting objects: 100% (18/18), done.\u001b[K\nremote: Compressing objects: 100% (17/17), done.\u001b[K\nremote: Total 17 (delta 9), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (17/17), 33.29 KiB | 1.19 MiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   808ddf3..37653c3  main       -> origin/main\nUpdating 808ddf3..37653c3\nFast-forward\n chatglm-rewardmodel-qlora.ipynb  |    1 \u001b[32m+\u001b[m\n rewardmodel_qlora_chatglm2.py    | 1544 \u001b[32m++++++++++++++++++++++++++++++++++++++\u001b[m\n sft_multi_turn_qlora_chatglm2.py |    6 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n 3 files changed, 1548 insertions(+), 3 deletions(-)\n create mode 100644 chatglm-rewardmodel-qlora.ipynb\n create mode 100644 rewardmodel_qlora_chatglm2.py\nCollecting peft==0.4.0 (from -r requirements.txt (line 1))\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.30.2)\nCollecting datasets==2.12.0 (from -r requirements.txt (line 3))\n  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm==4.65.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.65.0)\nCollecting loguru==0.7.0 (from -r requirements.txt (line 5))\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fire==0.5.0 (from -r requirements.txt (line 6))\n  Downloading fire-0.5.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting bitsandbytes==0.39.0 (from -r requirements.txt (line 7))\n  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting wandb==0.15.3 (from -r requirements.txt (line 8))\n  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting cpm_kernels==1.0.11 (from -r requirements.txt (line 9))\n  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.20.3)\nRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.1.99)\nCollecting deepspeed==0.9.5 (from -r requirements.txt (line 12))\n  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting evaluate==0.4.0 (from -r requirements.txt (line 13))\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting trl==0.4.7 (from -r requirements.txt (line 14))\n  Downloading trl-0.4.7-py3-none-any.whl (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (2.0.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (0.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (11.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (2.3.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.1.31)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.20.3)\nCollecting hjson (from deepspeed==0.9.5->-r requirements.txt (line 12))\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.11.1)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (9.0.0)\nRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.10.10)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (4.0.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2->-r requirements.txt (line 2)) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0->-r requirements.txt (line 1)) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2023.5.7)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2023.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.3.0)\nBuilding wheels for collected packages: fire, deepspeed\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=2257a26a08f7e876a1e9e2b62686caec40262fe2026b45a68cdafbcf2dbed91a\n  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844542 sha256=e676e1c7db6214a9adcb76990b1241781f5bb8cdbfc98170a2a0dcc601875449\n  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\nSuccessfully built fire deepspeed\nInstalling collected packages: hjson, cpm_kernels, bitsandbytes, loguru, fire, wandb, deepspeed, peft, datasets, trl, evaluate\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.5\n    Uninstalling wandb-0.15.5:\n      Successfully uninstalled wandb-0.15.5\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed bitsandbytes-0.39.0 cpm_kernels-1.0.11 datasets-2.12.0 deepspeed-0.9.5 evaluate-0.4.0 fire-0.5.0 hjson-3.1.0 loguru-0.7.0 peft-0.4.0 trl-0.4.7 wandb-0.15.3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-08-10T14:13:52.884914Z","iopub.execute_input":"2023-08-10T14:13:52.885305Z","iopub.status.idle":"2023-08-10T14:13:54.440350Z","shell.execute_reply.started":"2023-08-10T14:13:52.885271Z","shell.execute_reply":"2023-08-10T14:13:54.438905Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!git pull --all --force\n!CUDA_VISIBLE_DEVICES=0 python  rm_trl.py \\\n--model_name 'THUDM/chatglm2-6b' \\\n--resume_from_checkpoint /kaggle/working/chatGLM-6B-QLoRA/reward_model_0809_v1/checkpoint-50 \\\n--num_train_epochs 2 \\\n--gradient_accumulation_steps 1 \\\n--per_device_train_batch_size 4 \\\n--per_device_eval_batch_size  4 \\\n--max_length 512 \\\n--output_dir ./reward_model_0810_v2 \\\n--train_subset 80 \\\n--eval_subset 20 \\\n--local_rank 0  \\\n--bf16 False","metadata":{"execution":{"iopub.status.busy":"2023-08-10T15:36:48.134969Z","iopub.execute_input":"2023-08-10T15:36:48.135458Z","iopub.status.idle":"2023-08-10T15:40:03.300811Z","shell.execute_reply.started":"2023-08-10T15:36:48.135418Z","shell.execute_reply":"2023-08-10T15:40:03.299460Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Fetching origin\nremote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (3/3), 674 bytes | 134.00 KiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   b1dfef7..808ddf3  main       -> origin/main\nUpdating b1dfef7..808ddf3\nFast-forward\n rm_trl.py | 1 \u001b[32m+\u001b[m\n 1 file changed, 1 insertion(+)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-10 15:37:05,209] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\nDownloading (…)okenizer_config.json: 100%|█████| 244/244 [00:00<00:00, 1.23MB/s]\nDownloading (…)enization_chatglm.py: 100%|█| 10.1k/10.1k [00:00<00:00, 43.5MB/s]\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nDownloading tokenizer.model: 100%|█████████| 1.02M/1.02M [00:00<00:00, 15.5MB/s]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:22<00:00, 11.76s/it]\nmodel_type:  chatglm\nadapter1_w:transformer.encoder.layers.27.self_attention.query_key_value.lora_A.default.weight=tensor([[ 0.0013,  0.0030,  0.0011,  ..., -0.0129,  0.0154,  0.0067],\n        [-0.0098, -0.0007,  0.0047,  ..., -0.0095,  0.0136, -0.0007],\n        [ 0.0084,  0.0117,  0.0064,  ...,  0.0115, -0.0097,  0.0151],\n        ...,\n        [-0.0142,  0.0099, -0.0103,  ..., -0.0146, -0.0061,  0.0115],\n        [-0.0059, -0.0034,  0.0058,  ..., -0.0068,  0.0071, -0.0049],\n        [-0.0018, -0.0033,  0.0148,  ..., -0.0067, -0.0081, -0.0094]])\nvhead1_w={'weight': tensor([[ 0.0087,  0.0035,  0.0060,  ...,  0.0014,  0.0068, -0.0098]])}\nadapter2_w:transformer.encoder.layers.27.self_attention.query_key_value.lora_A.default.weight=tensor([[ 0.0013,  0.0030,  0.0011,  ..., -0.0129,  0.0154,  0.0067],\n        [-0.0098, -0.0007,  0.0047,  ..., -0.0095,  0.0136, -0.0007],\n        [ 0.0084,  0.0117,  0.0064,  ...,  0.0115, -0.0097,  0.0151],\n        ...,\n        [-0.0142,  0.0099, -0.0103,  ..., -0.0146, -0.0061,  0.0115],\n        [-0.0059, -0.0034,  0.0058,  ..., -0.0068,  0.0071, -0.0049],\n        [-0.0018, -0.0033,  0.0148,  ..., -0.0067, -0.0081, -0.0094]])\nvhead2_w={'weight': tensor([[ 0.0087,  0.0035,  0.0060,  ...,  0.0014,  0.0068, -0.0098]])}\nbest_w:transformer.encoder.layers.27.self_attention.query_key_value.lora_A.default.weight=tensor([[ 0.0013,  0.0030,  0.0011,  ..., -0.0129,  0.0154,  0.0067],\n        [-0.0098, -0.0007,  0.0047,  ..., -0.0095,  0.0136, -0.0007],\n        [ 0.0084,  0.0117,  0.0064,  ...,  0.0115, -0.0097,  0.0151],\n        ...,\n        [-0.0142,  0.0099, -0.0103,  ..., -0.0146, -0.0061,  0.0115],\n        [-0.0059, -0.0034,  0.0058,  ..., -0.0068,  0.0071, -0.0049],\n        [-0.0018, -0.0033,  0.0148,  ..., -0.0067, -0.0081, -0.0094]])\nbest_vhead_w={'weight': tensor([[ 0.0087,  0.0035,  0.0060,  ...,  0.0014,  0.0068, -0.0098]])}\nafter load model.transformer.encoder.layers[27].self_attention.query_key_value.lora_A.default.weight=Parameter containing:\ntensor([[ 0.0013,  0.0030,  0.0011,  ..., -0.0129,  0.0154,  0.0067],\n        [-0.0098, -0.0007,  0.0047,  ..., -0.0095,  0.0136, -0.0007],\n        [ 0.0084,  0.0117,  0.0064,  ...,  0.0115, -0.0097,  0.0151],\n        ...,\n        [-0.0142,  0.0099, -0.0103,  ..., -0.0146, -0.0061,  0.0115],\n        [-0.0059, -0.0034,  0.0058,  ..., -0.0068,  0.0071, -0.0049],\n        [-0.0018, -0.0033,  0.0148,  ..., -0.0067, -0.0081, -0.0094]],\n       device='cuda:0', requires_grad=True)\nafter load model.v_head.weight=Parameter containing:\ntensor([[ 0.0087,  0.0035,  0.0060,  ...,  0.0014,  0.0068, -0.0098]],\n       requires_grad=True)\n\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/kaggle/working/chatGLM-6B-QLoRA/\u001b[0m\u001b[1;33mrm_trl.py\u001b[0m:\u001b[94m1256\u001b[0m in \u001b[92m<module>\u001b[0m                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1253 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# raise ValueError(123)\u001b[0m                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1254 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1255 \u001b[0m\u001b[2m│   \u001b[0mtest_load_best()                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1256 \u001b[2m│   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(\u001b[94m123\u001b[0m)                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1257 \u001b[0m\u001b[2m│   \u001b[0mtrain()                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1258 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mValueError: \u001b[0m\u001b[1;36m123\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 上面的warning There were missing keys in the checkpoint model loaded: 'transformer.embedding.word_embeddings.weight', 'transformer.rotary_pos_emb.inv_freq', 'transformer.encoder.layers.0.input_layernorm.weight', ....\n# 是 trainingArguments中设置 load_best_model_at_end = True 后出现的\n# 说明 trainer的确在最后把最好的那个adapters的参数做了load（只不过load时strict=True要求严格匹配了）\n# 这说明保存在output_dir中的pytorch_model.bin和vhead.bin参数都是最佳的 我们来验证下","metadata":{}},{"cell_type":"markdown","source":"验证成功  跑的是py中的test_load_best()\n","metadata":{}},{"cell_type":"markdown","source":"--resume_from_checkpoint ./output-sharegpt-2k-sft-0804-v4/checkpoint-3980 ","metadata":{}},{"cell_type":"code","source":"!git pull --all --force \n!deepspeed --include localhost:0,1  rewardmodel_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-rm-1k-0811-v1 \\\n  --num_train_samples -1 \\\n  --num_eval_samples 20 \\\n  --train_data_path ./data/rm_data  \\\n  --eval_data_path  ./data/rm_data    \\\n  --data_type sharegpt  \\\n  --max_length 800 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 4 \\\n  --per_device_eval_batch_size 4  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  1.8e-5 \\\n  --num_train_epochs  10  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json\n","metadata":{"execution":{"iopub.status.busy":"2023-08-11T09:20:20.335838Z","iopub.execute_input":"2023-08-11T09:20:20.336314Z","iopub.status.idle":"2023-08-11T09:28:03.827606Z","shell.execute_reply.started":"2023-08-11T09:20:20.336273Z","shell.execute_reply":"2023-08-11T09:28:03.826267Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Fetching origin\nremote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (3/3), 691 bytes | 172.00 KiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   1826922..f11892d  main       -> origin/main\nUpdating 1826922..f11892d\nFast-forward\n rewardmodel_qlora_chatglm2.py | 1 \u001b[32m+\u001b[m\n 1 file changed, 1 insertion(+)\n[2023-08-11 09:20:26,895] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-11 09:20:40,144] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n[2023-08-11 09:20:40,162] [INFO] [runner.py:555:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None rewardmodel_qlora_chatglm2.py --train_args_json luzi.json --model_name_or_path THUDM/chatglm2-6b --output_dir output-rm-1k-0811-v1 --num_train_samples -1 --num_eval_samples 20 --train_data_path ./data/rm_data --eval_data_path ./data/rm_data --data_type sharegpt --max_length 800 --lora_rank 64 --lora_dropout 0.05 --compute_dtype fp16 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --learning_rate 1.8e-5 --num_train_epochs 10 --save_total_limit 2 --load_in_4bit True --deepspeed ds_zero2_config.json\n[2023-08-11 09:20:42,411] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-11 09:20:48,392] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n[2023-08-11 09:20:48,393] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n[2023-08-11 09:20:48,393] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n[2023-08-11 09:20:48,393] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n[2023-08-11 09:20:48,393] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n[2023-08-11 09:20:48,393] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n[2023-08-11 09:20:48,393] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n[2023-08-11 09:20:48,393] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n[2023-08-11 09:20:48,393] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n[2023-08-11 09:20:48,393] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n[2023-08-11 09:20:48,393] [INFO] [launch.py:163:main] dist_world_size=2\n[2023-08-11 09:20:48,393] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-11 09:20:57,606] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-08-11 09:20:57,611] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n\u001b[32m2023-08-11 09:21:02.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1272\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n\u001b[32m2023-08-11 09:21:02.157\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1272\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n\u001b[32m2023-08-11 09:21:03.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1318\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-11 09:21:03.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1318\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-11 09:21:03.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n\u001b[32m2023-08-11 09:21:03.381\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 74.76it/s]\n\u001b[32m2023-08-11 09:21:03.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m596\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-11 09:21:03.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1m在取样之后 data len =1000\u001b[0m\n\u001b[32m2023-08-11 09:21:03.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m604\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n\u001b[32m2023-08-11 09:21:03.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 588.34it/s]\n\u001b[32m2023-08-11 09:21:03.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m596\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-11 09:21:03.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1m在取样之后 data len =1000\u001b[0m\n\u001b[32m2023-08-11 09:21:03.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m604\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n\u001b[32m2023-08-11 09:21:03.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m589\u001b[0m - \u001b[1mdata files: ./data/rm_data/samples_1k.jsonl\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 735.71it/s]\n\u001b[32m2023-08-11 09:21:03.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m596\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-11 09:21:03.777\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1m在取样之后 data len =20\u001b[0m\n\u001b[32m2023-08-11 09:21:03.778\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m604\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\nnumber_train_samples=1000\nnumber_of_eval_numbers=20\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 739.08it/s]\n\u001b[32m2023-08-11 09:21:03.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m596\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-11 09:21:03.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m600\u001b[0m - \u001b[1m在取样之后 data len =20\u001b[0m\n\u001b[32m2023-08-11 09:21:03.808\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_rm_datset\u001b[0m:\u001b[36m604\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\nnumber_train_samples=1000\nnumber_of_eval_numbers=20\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:39<00:00, 14.17s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:39<00:00, 14.17s/it]\nmemory footprint of model: 3.6520424485206604 GB\nmemory footprint of model: 3.6520424485206604 GB\n\u001b[32m2023-08-11 09:22:44.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1428\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\n\u001b[32m2023-08-11 09:22:44.224\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain2\u001b[0m:\u001b[36m1428\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\nbelow trainable paramters only contains peft lora params.\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nmodel_type:  chatglm\nRewardModel(\n  (transformer): ChatGLMModel(\n    (embedding): Embedding(\n      (word_embeddings): Embedding(65024, 4096)\n    )\n    (rotary_pos_emb): RotaryEmbedding()\n    (encoder): GLMTransformer(\n      (layers): ModuleList(\n        (0-27): 28 x GLMBlock(\n          (input_layernorm): RMSNorm()\n          (self_attention): SelfAttention(\n            (query_key_value): Linear4bit(\n              in_features=4096, out_features=4608, bias=True\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4608, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n            (core_attention): CoreAttention(\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n            )\n            (dense): Linear4bit(\n              in_features=4096, out_features=4096, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4096, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n          )\n          (post_attention_layernorm): RMSNorm()\n          (mlp): MLP(\n            (dense_h_to_4h): Linear4bit(\n              in_features=4096, out_features=27392, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=27392, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n            (dense_4h_to_h): Linear4bit(\n              in_features=13696, out_features=4096, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=13696, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4096, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n          )\n        )\n      )\n      (final_layernorm): RMSNorm()\n    )\n    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n  )\n  (v_head): Linear(in_features=4096, out_features=1, bias=False)\n  (loss_fn): PairWiseLoss()\n)\nbelow trainable params include v_head\ntransformer_trainable_param = f118587392\nadd v_head params\ntrainable params: 118591488 || all params: 3506903040 || trainable%: 3.3817\nFinished loading model and tokenizer\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=1.8e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=1,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-rm-1k-0811-v1,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=10.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-rm-1k-0811-v1,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=4,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\nbelow trainable paramters only contains peft lora params.\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nmodel_type:  chatglm\nRewardModel(\n  (transformer): ChatGLMModel(\n    (embedding): Embedding(\n      (word_embeddings): Embedding(65024, 4096)\n    )\n    (rotary_pos_emb): RotaryEmbedding()\n    (encoder): GLMTransformer(\n      (layers): ModuleList(\n        (0-27): 28 x GLMBlock(\n          (input_layernorm): RMSNorm()\n          (self_attention): SelfAttention(\n            (query_key_value): Linear4bit(\n              in_features=4096, out_features=4608, bias=True\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4608, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n            (core_attention): CoreAttention(\n              (attention_dropout): Dropout(p=0.0, inplace=False)\n            )\n            (dense): Linear4bit(\n              in_features=4096, out_features=4096, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4096, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n          )\n          (post_attention_layernorm): RMSNorm()\n          (mlp): MLP(\n            (dense_h_to_4h): Linear4bit(\n              in_features=4096, out_features=27392, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=4096, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=27392, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n            (dense_4h_to_h): Linear4bit(\n              in_features=13696, out_features=4096, bias=False\n              (lora_dropout): ModuleDict(\n                (default): Dropout(p=0.05, inplace=False)\n              )\n              (lora_A): ModuleDict(\n                (default): Linear(in_features=13696, out_features=64, bias=False)\n              )\n              (lora_B): ModuleDict(\n                (default): Linear(in_features=64, out_features=4096, bias=False)\n              )\n              (lora_embedding_A): ParameterDict()\n              (lora_embedding_B): ParameterDict()\n            )\n          )\n        )\n      )\n      (final_layernorm): RMSNorm()\n    )\n    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n  )\n  (v_head): Linear(in_features=4096, out_features=1, bias=False)\n  (loss_fn): PairWiseLoss()\n)\nbelow trainable params include v_head\ntransformer_trainable_param = f118587392\nadd v_head params\ntrainable params: 118591488 || all params: 3506903040 || trainable%: 3.3817\nFinished loading model and tokenizer\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=1.8e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-rm-1k-0811-v1,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=10.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-rm-1k-0811-v1,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=4,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\n  0%|                                                  | 0/1250 [00:00<?, ?it/s]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.8047, 1.6787, 1.6846, 1.6240], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([0.8130, 1.2529, 4.2227, 5.5273], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.7803, 1.8457, 1.0557, 1.7803], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 0.6460,  1.0117, -1.0264,  3.9785], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n  0%|                                        | 1/1250 [00:20<7:05:11, 20.43s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.3018, 1.4053, 1.3975, 1.8340], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.8457, 0.5693, 1.9424, 2.9219], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.2109, 1.4287, 1.3359, 1.6846], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 0.9229,  2.4004, -0.1528,  0.5522], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n  0%|                                        | 2/1250 [00:38<6:30:08, 18.76s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.6475, 1.2861, 1.2969, 1.7344], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 1.1143,  1.2139, -0.1852,  3.2695], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.3975, 1.4814, 1.5273, 1.4668], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.0264, 1.5068, 2.2969, 4.1523], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n  0%|                                        | 3/1250 [00:56<6:23:37, 18.46s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([0.9438, 1.1455, 1.2158, 1.1768], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 0.3506,  0.4407, -1.2305,  3.4707], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.1260, 1.4414, 1.2744, 1.1660], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([0.5962, 2.6992, 2.9980, 2.0430], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n  0%|▏                                       | 4/1250 [01:14<6:21:48, 18.39s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.2227, 1.3350, 1.0518, 0.9111], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 1.6445, -2.7969, -2.1816,  4.0234], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.2734, 1.1338, 1.3330, 1.3223], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([0.7500, 0.9351, 1.7754, 2.9414], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n  0%|▏                                       | 5/1250 [01:32<6:16:45, 18.16s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.4639, 1.5410, 1.3457, 1.7500], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.0830, 1.3984, 1.0781, 3.2891], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([0.9258, 0.9932, 1.3809, 1.5615], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 0.5269, -1.2383,  1.6279,  5.2461], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n  0%|▏                                       | 6/1250 [01:50<6:15:35, 18.12s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.3545, 1.5098, 1.3574, 1.3262], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.9883, 0.6606, 1.2188, 4.5547], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.0820, 1.1816, 1.0586, 1.7852], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([3.4414, 1.5107, 1.1611, 5.1172], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n  1%|▏                                       | 7/1250 [02:08<6:13:51, 18.05s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.3691, 1.4736, 1.4629, 1.3975], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([2.2461, 0.2944, 1.3799, 3.1309], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.1357, 0.8188, 0.7964, 1.2920], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([-1.0928,  0.2185, -1.1836,  3.5840], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n  1%|▎                                       | 8/1250 [02:26<6:14:40, 18.10s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.6914, 1.5254, 1.4395, 1.6855], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.4365, 1.7090, 1.6260, 3.2246], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([2.0879, 2.0020, 1.7773, 1.9854], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([1.3145, 1.6562, 1.3057, 2.9473], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n  1%|▎                                       | 9/1250 [02:44<6:13:50, 18.07s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n{'rewards_j': tensor([1.5420, 1.4736, 1.6436, 1.2012], device='cuda:1',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([ 0.9551,  0.5591, -0.5288,  3.6074], device='cuda:1',\n       grad_fn=<SplitBackward0>)}\n{'rewards_j': tensor([1.2549, 1.3174, 1.2822, 1.5029], device='cuda:0',\n       grad_fn=<SplitBackward0>), 'rewards_k': tensor([0.9033, 1.0889, 0.9009, 2.1055], device='cuda:0',\n       grad_fn=<SplitBackward0>)}\n{'loss': 1.0261, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.08}        \n  1%|▎                                      | 10/1250 [03:02<6:13:29, 18.07s/it]seq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n 67%|██████████████████████████████               | 2/3 [00:07<00:03,  3.52s/it]\u001b[Aseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\nseq_len, batch_size, hidden_size = hidden_states.shape=torch.Size([800, 8, 4096])\ninput_ids.shape=torch.Size([8, 800])\ntotal_reward.shape=torch.Size([8, 800])\nbatch_size=4\n\n                                                                                \u001b[A\n\u001b[A{'eval_runtime': 21.7841, 'eval_samples_per_second': 0.918, 'eval_steps_per_second': 0.138, 'epoch': 0.08}\n  1%|▎                                      | 10/1250 [03:24<6:13:29, 18.07s/it]\n100%|█████████████████████████████████████████████| 3/3 [00:16<00:00,  4.98s/it]\u001b[A\n                                                                                \u001b[A\u001b[32m2023-08-11 09:27:52.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m788\u001b[0m - \u001b[1mSaving model checkpoint to output-rm-1k-0811-v1/checkpoint-10\u001b[0m\n\u001b[32m2023-08-11 09:27:52.423\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m802\u001b[0m - \u001b[31m\u001b[1m333 backbone_model, PreTrainedModel\u001b[0m\nTRAIN_TYPE ==lora\n\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/kaggle/working/chatGLM-6B-QLoRA/\u001b[0m\u001b[1;33mrewardmodel_qlora_chatglm2.py\u001b[0m:\u001b[94m1551\u001b[0m in       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92m<module>\u001b[0m                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1548 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1549 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# train2()\u001b[0m                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1550 \u001b[0m\u001b[2m│   \u001b[0margs = parse_args()                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1551 \u001b[2m│   \u001b[0mtrain2(args)                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1552 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1553 \u001b[0m\u001b[2;90m│   \u001b[0m\u001b[33m\"\"\" train2 使用方法\u001b[0m                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1554 \u001b[0m\u001b[33m!git pull --all --force \u001b[0m                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/kaggle/working/chatGLM-6B-QLoRA/\u001b[0m\u001b[1;33mrewardmodel_qlora_chatglm2.py\u001b[0m:\u001b[94m1511\u001b[0m in       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92mtrain2\u001b[0m                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1508 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1509 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1510 \u001b[0m\u001b[2m│   \u001b[0mmodel.config.use_cache = \u001b[94mFalse\u001b[0m                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1511 \u001b[2m│   \u001b[0mtrainer.train()                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1512 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1513 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# 下面的写法会保存全量参数的rewardmodel\u001b[0m                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1514 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m#print(\"Train done!Saving Model...\")\u001b[0m                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1645\u001b[0m in      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92mtrain\u001b[0m                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1642 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1643 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.a \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1644 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1645 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1646 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1647 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1648 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2020\u001b[0m in      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92m_inner_training_loop\u001b[0m                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2017 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.state.epoch = epoch + (step + \u001b[94m1\u001b[0m + steps_skip \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2018 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.control = \u001b[96mself\u001b[0m.callback_handler.on_step_end( \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2019 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2020 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._maybe_log_save_evaluate(tr_loss, model, tri \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2021 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2022 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.control = \u001b[96mself\u001b[0m.callback_handler.on_substep_e \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2023 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2332\u001b[0m in      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92m_maybe_log_save_evaluate\u001b[0m                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2329 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.lr_scheduler.step(metrics[metric_to_check])      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2330 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2331 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.control.should_save:                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2332 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._save_checkpoint(model, trial, metrics=metrics)      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2333 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.control = \u001b[96mself\u001b[0m.callback_handler.on_save(\u001b[96mself\u001b[0m.args, \u001b[96ms\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2334 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2335 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_load_rng_state\u001b[0m(\u001b[96mself\u001b[0m, checkpoint):                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2445\u001b[0m in      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92m_save_checkpoint\u001b[0m                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2442 \u001b[0m\u001b[2m│   │   │   \u001b[0mmetric_to_check = \u001b[96mself\u001b[0m.args.metric_for_best_model         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2443 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m metric_to_check.startswith(\u001b[33m\"\u001b[0m\u001b[33meval_\u001b[0m\u001b[33m\"\u001b[0m):               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2444 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmetric_to_check = \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33meval_\u001b[0m\u001b[33m{\u001b[0mmetric_to_check\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2445 \u001b[2m│   │   │   \u001b[0mmetric_value = metrics[metric_to_check]                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2446 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2447 \u001b[0m\u001b[2m│   │   │   \u001b[0moperator = np.greater \u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.greater_is_better \u001b[94melse\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2448 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                      \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mKeyError: \u001b[0m\u001b[32m'eval_loss'\u001b[0m\n\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/kaggle/working/chatGLM-6B-QLoRA/\u001b[0m\u001b[1;33mrewardmodel_qlora_chatglm2.py\u001b[0m:\u001b[94m1551\u001b[0m in       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92m<module>\u001b[0m                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1548 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1549 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# train2()\u001b[0m                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1550 \u001b[0m\u001b[2m│   \u001b[0margs = parse_args()                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1551 \u001b[2m│   \u001b[0mtrain2(args)                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1552 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1553 \u001b[0m\u001b[2;90m│   \u001b[0m\u001b[33m\"\"\" train2 使用方法\u001b[0m                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1554 \u001b[0m\u001b[33m!git pull --all --force \u001b[0m                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/kaggle/working/chatGLM-6B-QLoRA/\u001b[0m\u001b[1;33mrewardmodel_qlora_chatglm2.py\u001b[0m:\u001b[94m1511\u001b[0m in       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92mtrain2\u001b[0m                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1508 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1509 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1510 \u001b[0m\u001b[2m│   \u001b[0mmodel.config.use_cache = \u001b[94mFalse\u001b[0m                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1511 \u001b[2m│   \u001b[0mtrainer.train()                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1512 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1513 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# 下面的写法会保存全量参数的rewardmodel\u001b[0m                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1514 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m#print(\"Train done!Saving Model...\")\u001b[0m                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1645\u001b[0m in      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92mtrain\u001b[0m                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1642 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1643 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.a \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1644 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1645 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1646 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1647 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1648 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2020\u001b[0m in      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92m_inner_training_loop\u001b[0m                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2017 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.state.epoch = epoch + (step + \u001b[94m1\u001b[0m + steps_skip \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2018 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.control = \u001b[96mself\u001b[0m.callback_handler.on_step_end( \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2019 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2020 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._maybe_log_save_evaluate(tr_loss, model, tri \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2021 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2022 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.control = \u001b[96mself\u001b[0m.callback_handler.on_substep_e \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2023 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2332\u001b[0m in      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92m_maybe_log_save_evaluate\u001b[0m                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2329 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.lr_scheduler.step(metrics[metric_to_check])      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2330 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2331 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.control.should_save:                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2332 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._save_checkpoint(model, trial, metrics=metrics)      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2333 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.control = \u001b[96mself\u001b[0m.callback_handler.on_save(\u001b[96mself\u001b[0m.args, \u001b[96ms\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2334 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2335 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_load_rng_state\u001b[0m(\u001b[96mself\u001b[0m, checkpoint):                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2445\u001b[0m in      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92m_save_checkpoint\u001b[0m                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2442 \u001b[0m\u001b[2m│   │   │   \u001b[0mmetric_to_check = \u001b[96mself\u001b[0m.args.metric_for_best_model         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2443 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m metric_to_check.startswith(\u001b[33m\"\u001b[0m\u001b[33meval_\u001b[0m\u001b[33m\"\u001b[0m):               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2444 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmetric_to_check = \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33meval_\u001b[0m\u001b[33m{\u001b[0mmetric_to_check\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2445 \u001b[2m│   │   │   \u001b[0mmetric_value = metrics[metric_to_check]                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2446 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2447 \u001b[0m\u001b[2m│   │   │   \u001b[0moperator = np.greater \u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.greater_is_better \u001b[94melse\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2448 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                      \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mKeyError: \u001b[0m\u001b[32m'eval_loss'\u001b[0m\n  1%|▎                                      | 10/1250 [03:29<7:12:20, 20.92s/it]\n[2023-08-11 09:28:00,859] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1211\n[2023-08-11 09:28:00,953] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1212\n[2023-08-11 09:28:00,954] [ERROR] [launch.py:321:sigkill_handler] ['/opt/conda/bin/python3.10', '-u', 'rewardmodel_qlora_chatglm2.py', '--local_rank=1', '--train_args_json', 'luzi.json', '--model_name_or_path', 'THUDM/chatglm2-6b', '--output_dir', 'output-rm-1k-0811-v1', '--num_train_samples', '-1', '--num_eval_samples', '20', '--train_data_path', './data/rm_data', '--eval_data_path', './data/rm_data', '--data_type', 'sharegpt', '--max_length', '800', '--lora_rank', '64', '--lora_dropout', '0.05', '--compute_dtype', 'fp16', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--learning_rate', '1.8e-5', '--num_train_epochs', '10', '--save_total_limit', '2', '--load_in_4bit', 'True', '--deepspeed', 'ds_zero2_config.json'] exits with return code = 1\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}