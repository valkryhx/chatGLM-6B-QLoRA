{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-03T06:01:25.619888Z","iopub.execute_input":"2023-08-03T06:01:25.620687Z","iopub.status.idle":"2023-08-03T06:01:25.628657Z","shell.execute_reply.started":"2023-08-03T06:01:25.620651Z","shell.execute_reply":"2023-08-03T06:01:25.627639Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/valkryhx/chatGLM-6B-QLoRA","metadata":{"execution":{"iopub.status.busy":"2023-08-03T08:16:11.434307Z","iopub.execute_input":"2023-08-03T08:16:11.435053Z","iopub.status.idle":"2023-08-03T08:16:12.422542Z","shell.execute_reply.started":"2023-08-03T08:16:11.435012Z","shell.execute_reply":"2023-08-03T08:16:12.421321Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"fatal: destination path 'chatGLM-6B-QLoRA' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"!git pull --all --force\n#!pip install peft==0.4.0\n#!pip install  -U git+https://github.com/huggingface/peft.git\n%cd /kaggle/working/chatGLM-6B-QLoRA \n!ls\n!pip install -r requirements.txt\n#!pip install deepspeed==0.9.5  这个也是需要的 但是目前kaggle 的runtime自带了","metadata":{"execution":{"iopub.status.busy":"2023-08-03T08:16:24.609362Z","iopub.execute_input":"2023-08-03T08:16:24.609838Z","iopub.status.idle":"2023-08-03T08:17:05.417389Z","shell.execute_reply.started":"2023-08-03T08:16:24.609796Z","shell.execute_reply":"2023-08-03T08:17:05.415872Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"fatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n/kaggle/working/chatGLM-6B-QLoRA\nREADME.md\nchatGLM_6B_QLoRA.json\ndata\nds_zero2_config.json\nds_zero3_config.json\ninference_qlora.py\ninference_test.py\nluzi.json\nmerge_lora_and_quantize.py\noutput-history-sft-0803-v1\noutput-sharegpt-sft-0802-v1\noutput-sharegpt-sft-0803-v1\npics\npretrain_chatglm2_v5.ipynb\npretrain_chatglm2_v7.ipynb\npretrain_chatglm2_v8.ipynb\npretrain_qlora_chatglm2.py\nqa_aug.json\nqlora_param.json\nremote_scripts\nrequirements.txt\nsft_multi_turn_qlora_chatglm2.py\nsharegpt_sft_success_0802.ipynb\nstate.db\ntrain_ds_zero2_test.py——错误的\ntrain_normal_lora.py\ntrain_qlora.py\ntrain_qlora_deepspeed_zero.py\n二次预训练pt_sft_继续lora训练_qlora_glm_成功0730.ipynb\n参考与经验\nCollecting peft==0.4.0 (from -r requirements.txt (line 1))\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.30.2)\nCollecting datasets==2.12.0 (from -r requirements.txt (line 3))\n  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm==4.65.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.65.0)\nCollecting loguru==0.7.0 (from -r requirements.txt (line 5))\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fire==0.5.0 (from -r requirements.txt (line 6))\n  Downloading fire-0.5.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting bitsandbytes==0.39.0 (from -r requirements.txt (line 7))\n  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting wandb==0.15.3 (from -r requirements.txt (line 8))\n  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting cpm_kernels==1.0.11 (from -r requirements.txt (line 9))\n  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.20.3)\nRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.1.99)\nCollecting deepspeed==0.9.5 (from -r requirements.txt (line 12))\n  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (2.0.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (0.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (11.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (2.3.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.1.31)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.20.3)\nCollecting hjson (from deepspeed==0.9.5->-r requirements.txt (line 12))\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.11.1)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (9.0.0)\nRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.10.10)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (4.0.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2->-r requirements.txt (line 2)) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0->-r requirements.txt (line 1)) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2023.5.7)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2023.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.3.0)\nBuilding wheels for collected packages: fire, deepspeed\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=a7313e4046c9c25dae5aa82777b141cc3cd540e207f1419f0a9e71c8ab6005cb\n  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844549 sha256=867b5eb992cf85e63ed758ffd9434831f27b2310f44ae6de78cf99f99b017a06\n  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\nSuccessfully built fire deepspeed\nInstalling collected packages: hjson, cpm_kernels, bitsandbytes, loguru, fire, wandb, deepspeed, peft, datasets\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.5\n    Uninstalling wandb-0.15.5:\n      Successfully uninstalled wandb-0.15.5\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed bitsandbytes-0.39.0 cpm_kernels-1.0.11 datasets-2.12.0 deepspeed-0.9.5 fire-0.5.0 hjson-3.1.0 loguru-0.7.0 peft-0.4.0 wandb-0.15.3\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color= brown>SFT sharegpt格式的多轮对话数据</font>","metadata":{}},{"cell_type":"code","source":"# ./data/sharegpt_multi_turn_data 目录不能有空白json文件\n!git pull --all --force \n#!pip install  -U git+https://github.com/huggingface/peft.git   # 20230717 peft==0.4.0正式发布了 不用调版本了推理完后再训练需要重新升级到0.4.0dev 所以有这句\n!deepspeed --include localhost:0,1  sft_multi_turn_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-sharegpt-sft-0803-v3 \\\n  --num_train_samples -1 \\\n  --num_eval_samples 50 \\\n  --resume_from_checkpoint ./output-sharegpt-sft-0803-v2/checkpoint-310 \\\n  --train_data_path ./data/sharegpt_data/   \\\n  --eval_data_path  ./data/sharegpt_data    \\\n  --data_type sharegpt  \\\n  --max_length 1600 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 1 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  2e-5 \\\n  --num_train_epochs  40  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"execution":{"iopub.status.busy":"2023-08-03T09:01:47.409761Z","iopub.execute_input":"2023-08-03T09:01:47.410671Z","iopub.status.idle":"2023-08-03T09:27:00.037052Z","shell.execute_reply.started":"2023-08-03T09:01:47.410623Z","shell.execute_reply":"2023-08-03T09:27:00.035411Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Fetching origin\nremote: Enumerating objects: 9, done.\u001b[K\nremote: Counting objects: 100% (9/9), done.\u001b[K\nremote: Compressing objects: 100% (6/6), done.\u001b[K\nremote: Total 6 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (6/6), 1.32 KiB | 112.00 KiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   f919b42..92e27ea  main       -> origin/main\nUpdating f919b42..92e27ea\nFast-forward\n luzi.json                        | 1 \u001b[32m+\u001b[m\n sft_multi_turn_qlora_chatglm2.py | 4 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n 2 files changed, 3 insertions(+), 2 deletions(-)\n[2023-08-03 09:01:53,639] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-03 09:02:06,664] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n[2023-08-03 09:02:06,683] [INFO] [runner.py:555:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None sft_multi_turn_qlora_chatglm2.py --train_args_json luzi.json --model_name_or_path THUDM/chatglm2-6b --output_dir output-sharegpt-sft-0803-v3 --num_train_samples -1 --num_eval_samples 50 --resume_from_checkpoint ./output-sharegpt-sft-0803-v2/checkpoint-310 --train_data_path ./data/sharegpt_data/ --eval_data_path ./data/sharegpt_data --data_type sharegpt --max_length 1600 --lora_rank 64 --lora_dropout 0.05 --compute_dtype fp16 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --learning_rate 2e-5 --num_train_epochs 40 --save_total_limit 2 --load_in_4bit True --deepspeed ds_zero2_config.json\n[2023-08-03 09:02:08,461] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-03 09:02:14,464] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n[2023-08-03 09:02:14,464] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n[2023-08-03 09:02:14,464] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n[2023-08-03 09:02:14,464] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n[2023-08-03 09:02:14,464] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n[2023-08-03 09:02:14,464] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n[2023-08-03 09:02:14,464] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n[2023-08-03 09:02:14,464] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n[2023-08-03 09:02:14,464] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n[2023-08-03 09:02:14,465] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n[2023-08-03 09:02:14,465] [INFO] [launch.py:163:main] dist_world_size=2\n[2023-08-03 09:02:14,465] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n[2023-08-03 09:02:17,512] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-08-03 09:02:17,559] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n===================================================================================================================BUG REPORT===================================\n\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu')}\n  warn(msg)\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n/kaggle/working/chatGLM-6B-QLoRA/sft_multi_turn_qlora_chatglm2.py:84: FutureWarning: set_caching_enabled is deprecated and will be removed in the next major version of datasets. Use datasets.enable_caching() or datasets.disable_caching() instead. This function will be removed in a future version of datasets.\n  set_caching_enabled(False)\n/kaggle/working/chatGLM-6B-QLoRA/sft_multi_turn_qlora_chatglm2.py:84: FutureWarning: set_caching_enabled is deprecated and will be removed in the next major version of datasets. Use datasets.enable_caching() or datasets.disable_caching() instead. This function will be removed in a future version of datasets.\n  set_caching_enabled(False)\ninto lora trainer !!!\ninto lora trainer !!!\n\u001b[32m2023-08-03 09:02:27.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m422\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n\u001b[32m2023-08-03 09:02:27.052\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m422\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n\u001b[32m2023-08-03 09:02:28.335\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m468\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-03 09:02:28.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m242\u001b[0m - \u001b[1mdata files: ./data/sharegpt_data/sharegpt_zh_1K_format.jsonl\u001b[0m\n\u001b[32m2023-08-03 09:02:28.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m468\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-03 09:02:28.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m242\u001b[0m - \u001b[1mdata files: ./data/sharegpt_data/sharegpt_zh_1K_format.jsonl\u001b[0m\n100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 69.48it/s]\n\u001b[32m2023-08-03 09:02:28.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m248\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-03 09:02:28.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1m在取样之后 data len =1000\u001b[0m\n\u001b[32m2023-08-03 09:02:28.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m256\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 269.71it/s]\n\u001b[32m2023-08-03 09:02:28.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m248\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-03 09:02:28.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1m在取样之后 data len =1000\u001b[0m\n\u001b[32m2023-08-03 09:02:28.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m256\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\ntokenized_dataset=Dataset({                                                     \n    features: ['input_ids', 'labels'],\n    num_rows: 999\n})\ntokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)=\n问：1 答：俄语中的单词“свинья”意为“猪”，而“свинец”是“свинья”的爱称形式，意为“小猪”。这两个词之间有联系，因为“свинец”是由“свинья”演变而来，指的是幼年的猪。 问：2 答：我是由OpenAI训练的语言模型，因此我没有个人经验或直接了解世界。相反，我能够根据我接收到的输入和我接受培训的信息生成回答。在这种情况下，我接受了大量俄文文本的训练，其中包括“свинья”和“свинец”这些词，因此我有信心提供的信息是准确的。但是，我无法浏览互联网或以其他方式验证此信息，因此我的答案可能不正确或不完整。 问：3 答：对不起，我不明白问题是什么。\"свинья\" 和 \"свинец\" 这两个词肯定有关联，因为 \"свинец\" 是 \"свинья\" 的小型词形，指的是小猪。我无法提供关于这两个词如何没有联系的答案，因为那将与我接受的培训信息相矛盾。\ntokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)=\n答：俄语中的单词“свинья”意为“猪”，而“свинец”是“свинья”的爱称形式，意为“小猪”。这两个词之间有联系，因为“свинец”是由“свинья”演变而来，指的是幼年的猪。 答：我是由OpenAI训练的语言模型，因此我没有个人经验或直接了解世界。相反，我能够根据我接收到的输入和我接受培训的信息生成回答。在这种情况下，我接受了大量俄文文本的训练，其中包括“свинья”和“свинец”这些词，因此我有信心提供的信息是准确的。但是，我无法浏览互联网或以其他方式验证此信息，因此我的答案可能不正确或不完整。 答：对不起，我不明白问题是什么。\"свинья\" 和 \"свинец\" 这两个词肯定有关联，因为 \"свинец\" 是 \"свинья\" 的小型词形，指的是小猪。我无法提供关于这两个词如何没有联系的答案，因为那将与我接受的培训信息相矛盾。\ntokenized_dataset[0]['input_ids']=\n[30910, 54761, 31211, 30939, 30910, 55437, 31211, 55670, 55032, 31697, 41252, 30989, 31004, 4151, 20370, 31026, 30991, 48527, 30989, 55961, 31644, 54617, 30989, 31004, 4151, 3442, 31046, 30991, 54532, 30989, 31004, 4151, 20370, 31026, 30991, 35846, 55037, 32129, 31123, 48527, 30989, 54603, 55961, 31681, 36434, 55492, 31912, 54536, 32064, 31123, 31659, 30989, 31004, 4151, 3442, 31046, 30991, 33702, 30989, 31004, 4151, 20370, 31026, 30991, 42085, 35754, 31123, 39741, 55581, 31766, 55961, 31155, 2, 30910, 54761, 31211, 30943, 30910, 55437, 31211, 33030, 54781, 14962, 23833, 32549, 40042, 34030, 31123, 31814, 40487, 31745, 32035, 54746, 31921, 31788, 31661, 31155, 35447, 31123, 54546, 31759, 31793, 54546, 36106, 33177, 34555, 36770, 31985, 32009, 35794, 36454, 33287, 31155, 49573, 31123, 54546, 40617, 32325, 55670, 54593, 36704, 54530, 32549, 31123, 39327, 30989, 31004, 4151, 20370, 31026, 30991, 54542, 30989, 31004, 4151, 3442, 31046, 30991, 31704, 55492, 31123, 31814, 37736, 33213, 35100, 31707, 54532, 51002, 31155, 31694, 31123, 54546, 32067, 35680, 32451, 54746, 54548, 31722, 31734, 35526, 54713, 31707, 31123, 31814, 31791, 33161, 31667, 54535, 32506, 46375, 32936, 31155, 2, 30910, 54761, 31211, 30966, 30910, 55437, 31211, 43279, 31123, 34045, 32855, 31639, 32664, 37863, 31004, 4151, 20370, 31026, 30955, 34876, 449, 31004, 4151, 3442, 31046, 30955, 30910, 36434, 55492, 32471, 31927, 54832, 31123, 31659, 449, 31004, 4151, 3442, 31046, 30955, 35369, 449, 31004, 4151, 20370, 31026, 30955, 30910, 32048, 54888, 55492, 54874, 31123, 39741, 54603, 55961, 31155, 54546, 32067, 31692, 31809, 36434, 55492, 31750, 31631, 32064, 54530, 33161, 31123, 31659, 54728, 43855, 54546, 47210, 32009, 31707, 54669, 33151, 31155, 2]\ntokenized_dataset[0]['labels']=\n[-100, -100, -100, -100, 30910, 55437, 31211, 55670, 55032, 31697, 41252, 30989, 31004, 4151, 20370, 31026, 30991, 48527, 30989, 55961, 31644, 54617, 30989, 31004, 4151, 3442, 31046, 30991, 54532, 30989, 31004, 4151, 20370, 31026, 30991, 35846, 55037, 32129, 31123, 48527, 30989, 54603, 55961, 31681, 36434, 55492, 31912, 54536, 32064, 31123, 31659, 30989, 31004, 4151, 3442, 31046, 30991, 33702, 30989, 31004, 4151, 20370, 31026, 30991, 42085, 35754, 31123, 39741, 55581, 31766, 55961, 31155, 2, -100, -100, -100, -100, 30910, 55437, 31211, 33030, 54781, 14962, 23833, 32549, 40042, 34030, 31123, 31814, 40487, 31745, 32035, 54746, 31921, 31788, 31661, 31155, 35447, 31123, 54546, 31759, 31793, 54546, 36106, 33177, 34555, 36770, 31985, 32009, 35794, 36454, 33287, 31155, 49573, 31123, 54546, 40617, 32325, 55670, 54593, 36704, 54530, 32549, 31123, 39327, 30989, 31004, 4151, 20370, 31026, 30991, 54542, 30989, 31004, 4151, 3442, 31046, 30991, 31704, 55492, 31123, 31814, 37736, 33213, 35100, 31707, 54532, 51002, 31155, 31694, 31123, 54546, 32067, 35680, 32451, 54746, 54548, 31722, 31734, 35526, 54713, 31707, 31123, 31814, 31791, 33161, 31667, 54535, 32506, 46375, 32936, 31155, 2, -100, -100, -100, -100, 30910, 55437, 31211, 43279, 31123, 34045, 32855, 31639, 32664, 37863, 31004, 4151, 20370, 31026, 30955, 34876, 449, 31004, 4151, 3442, 31046, 30955, 30910, 36434, 55492, 32471, 31927, 54832, 31123, 31659, 449, 31004, 4151, 3442, 31046, 30955, 35369, 449, 31004, 4151, 20370, 31026, 30955, 30910, 32048, 54888, 55492, 54874, 31123, 39741, 54603, 55961, 31155, 54546, 32067, 31692, 31809, 36434, 55492, 31750, 31631, 32064, 54530, 33161, 31123, 31659, 54728, 43855, 54546, 47210, 32009, 31707, 54669, 33151, 31155, 2]\nlen(tokenized_dataset[0]['input_ids']=253\nlen(tokenized_dataset[0]['labels']=253\n\u001b[32m2023-08-03 09:02:37.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m242\u001b[0m - \u001b[1mdata files: ./data/sharegpt_data/sharegpt_zh_1K_format.jsonl\u001b[0m\ntokenized_dataset=Dataset({                                                     \n    features: ['input_ids', 'labels'],\n    num_rows: 999\n})\ntokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)=\n问：1 答：俄语中的单词“свинья”意为“猪”，而“свинец”是“свинья”的爱称形式，意为“小猪”。这两个词之间有联系，因为“свинец”是由“свинья”演变而来，指的是幼年的猪。 问：2 答：我是由OpenAI训练的语言模型，因此我没有个人经验或直接了解世界。相反，我能够根据我接收到的输入和我接受培训的信息生成回答。在这种情况下，我接受了大量俄文文本的训练，其中包括“свинья”和“свинец”这些词，因此我有信心提供的信息是准确的。但是，我无法浏览互联网或以其他方式验证此信息，因此我的答案可能不正确或不完整。 问：3 答：对不起，我不明白问题是什么。\"свинья\" 和 \"свинец\" 这两个词肯定有关联，因为 \"свинец\" 是 \"свинья\" 的小型词形，指的是小猪。我无法提供关于这两个词如何没有联系的答案，因为那将与我接受的培训信息相矛盾。\ntokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)=\n答：俄语中的单词“свинья”意为“猪”，而“свинец”是“свинья”的爱称形式，意为“小猪”。这两个词之间有联系，因为“свинец”是由“свинья”演变而来，指的是幼年的猪。 答：我是由OpenAI训练的语言模型，因此我没有个人经验或直接了解世界。相反，我能够根据我接收到的输入和我接受培训的信息生成回答。在这种情况下，我接受了大量俄文文本的训练，其中包括“свинья”和“свинец”这些词，因此我有信心提供的信息是准确的。但是，我无法浏览互联网或以其他方式验证此信息，因此我的答案可能不正确或不完整。 答：对不起，我不明白问题是什么。\"свинья\" 和 \"свинец\" 这两个词肯定有关联，因为 \"свинец\" 是 \"свинья\" 的小型词形，指的是小猪。我无法提供关于这两个词如何没有联系的答案，因为那将与我接受的培训信息相矛盾。\ntokenized_dataset[0]['input_ids']=\n[30910, 54761, 31211, 30939, 30910, 55437, 31211, 55670, 55032, 31697, 41252, 30989, 31004, 4151, 20370, 31026, 30991, 48527, 30989, 55961, 31644, 54617, 30989, 31004, 4151, 3442, 31046, 30991, 54532, 30989, 31004, 4151, 20370, 31026, 30991, 35846, 55037, 32129, 31123, 48527, 30989, 54603, 55961, 31681, 36434, 55492, 31912, 54536, 32064, 31123, 31659, 30989, 31004, 4151, 3442, 31046, 30991, 33702, 30989, 31004, 4151, 20370, 31026, 30991, 42085, 35754, 31123, 39741, 55581, 31766, 55961, 31155, 2, 30910, 54761, 31211, 30943, 30910, 55437, 31211, 33030, 54781, 14962, 23833, 32549, 40042, 34030, 31123, 31814, 40487, 31745, 32035, 54746, 31921, 31788, 31661, 31155, 35447, 31123, 54546, 31759, 31793, 54546, 36106, 33177, 34555, 36770, 31985, 32009, 35794, 36454, 33287, 31155, 49573, 31123, 54546, 40617, 32325, 55670, 54593, 36704, 54530, 32549, 31123, 39327, 30989, 31004, 4151, 20370, 31026, 30991, 54542, 30989, 31004, 4151, 3442, 31046, 30991, 31704, 55492, 31123, 31814, 37736, 33213, 35100, 31707, 54532, 51002, 31155, 31694, 31123, 54546, 32067, 35680, 32451, 54746, 54548, 31722, 31734, 35526, 54713, 31707, 31123, 31814, 31791, 33161, 31667, 54535, 32506, 46375, 32936, 31155, 2, 30910, 54761, 31211, 30966, 30910, 55437, 31211, 43279, 31123, 34045, 32855, 31639, 32664, 37863, 31004, 4151, 20370, 31026, 30955, 34876, 449, 31004, 4151, 3442, 31046, 30955, 30910, 36434, 55492, 32471, 31927, 54832, 31123, 31659, 449, 31004, 4151, 3442, 31046, 30955, 35369, 449, 31004, 4151, 20370, 31026, 30955, 30910, 32048, 54888, 55492, 54874, 31123, 39741, 54603, 55961, 31155, 54546, 32067, 31692, 31809, 36434, 55492, 31750, 31631, 32064, 54530, 33161, 31123, 31659, 54728, 43855, 54546, 47210, 32009, 31707, 54669, 33151, 31155, 2]\ntokenized_dataset[0]['labels']=\n[-100, -100, -100, -100, 30910, 55437, 31211, 55670, 55032, 31697, 41252, 30989, 31004, 4151, 20370, 31026, 30991, 48527, 30989, 55961, 31644, 54617, 30989, 31004, 4151, 3442, 31046, 30991, 54532, 30989, 31004, 4151, 20370, 31026, 30991, 35846, 55037, 32129, 31123, 48527, 30989, 54603, 55961, 31681, 36434, 55492, 31912, 54536, 32064, 31123, 31659, 30989, 31004, 4151, 3442, 31046, 30991, 33702, 30989, 31004, 4151, 20370, 31026, 30991, 42085, 35754, 31123, 39741, 55581, 31766, 55961, 31155, 2, -100, -100, -100, -100, 30910, 55437, 31211, 33030, 54781, 14962, 23833, 32549, 40042, 34030, 31123, 31814, 40487, 31745, 32035, 54746, 31921, 31788, 31661, 31155, 35447, 31123, 54546, 31759, 31793, 54546, 36106, 33177, 34555, 36770, 31985, 32009, 35794, 36454, 33287, 31155, 49573, 31123, 54546, 40617, 32325, 55670, 54593, 36704, 54530, 32549, 31123, 39327, 30989, 31004, 4151, 20370, 31026, 30991, 54542, 30989, 31004, 4151, 3442, 31046, 30991, 31704, 55492, 31123, 31814, 37736, 33213, 35100, 31707, 54532, 51002, 31155, 31694, 31123, 54546, 32067, 35680, 32451, 54746, 54548, 31722, 31734, 35526, 54713, 31707, 31123, 31814, 31791, 33161, 31667, 54535, 32506, 46375, 32936, 31155, 2, -100, -100, -100, -100, 30910, 55437, 31211, 43279, 31123, 34045, 32855, 31639, 32664, 37863, 31004, 4151, 20370, 31026, 30955, 34876, 449, 31004, 4151, 3442, 31046, 30955, 30910, 36434, 55492, 32471, 31927, 54832, 31123, 31659, 449, 31004, 4151, 3442, 31046, 30955, 35369, 449, 31004, 4151, 20370, 31026, 30955, 30910, 32048, 54888, 55492, 54874, 31123, 39741, 54603, 55961, 31155, 54546, 32067, 31692, 31809, 36434, 55492, 31750, 31631, 32064, 54530, 33161, 31123, 31659, 54728, 43855, 54546, 47210, 32009, 31707, 54669, 33151, 31155, 2]\nlen(tokenized_dataset[0]['input_ids']=253\nlen(tokenized_dataset[0]['labels']=253\n\u001b[32m2023-08-03 09:02:37.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m242\u001b[0m - \u001b[1mdata files: ./data/sharegpt_data/sharegpt_zh_1K_format.jsonl\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 260.79it/s]\n\u001b[32m2023-08-03 09:02:37.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m248\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-03 09:02:37.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1m在取样之后 data len =50\u001b[0m\n\u001b[32m2023-08-03 09:02:37.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m256\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 519.74it/s]\n\u001b[32m2023-08-03 09:02:37.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m248\u001b[0m - \u001b[1m在取样之前 data len =1000\u001b[0m\n\u001b[32m2023-08-03 09:02:37.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1m在取样之后 data len =50\u001b[0m\n\u001b[32m2023-08-03 09:02:37.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m256\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\ntokenized_dataset=Dataset({                                                     \n    features: ['input_ids', 'labels'],\n    num_rows: 50\n})\ntokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)=\n问：1 答：俄语中的单词“свинья”意为“猪”，而“свинец”是“свинья”的爱称形式，意为“小猪”。这两个词之间有联系，因为“свинец”是由“свинья”演变而来，指的是幼年的猪。 问：2 答：我是由OpenAI训练的语言模型，因此我没有个人经验或直接了解世界。相反，我能够根据我接收到的输入和我接受培训的信息生成回答。在这种情况下，我接受了大量俄文文本的训练，其中包括“свинья”和“свинец”这些词，因此我有信心提供的信息是准确的。但是，我无法浏览互联网或以其他方式验证此信息，因此我的答案可能不正确或不完整。 问：3 答：对不起，我不明白问题是什么。\"свинья\" 和 \"свинец\" 这两个词肯定有关联，因为 \"свинец\" 是 \"свинья\" 的小型词形，指的是小猪。我无法提供关于这两个词如何没有联系的答案，因为那将与我接受的培训信息相矛盾。\ntokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)=\n答：俄语中的单词“свинья”意为“猪”，而“свинец”是“свинья”的爱称形式，意为“小猪”。这两个词之间有联系，因为“свинец”是由“свинья”演变而来，指的是幼年的猪。 答：我是由OpenAI训练的语言模型，因此我没有个人经验或直接了解世界。相反，我能够根据我接收到的输入和我接受培训的信息生成回答。在这种情况下，我接受了大量俄文文本的训练，其中包括“свинья”和“свинец”这些词，因此我有信心提供的信息是准确的。但是，我无法浏览互联网或以其他方式验证此信息，因此我的答案可能不正确或不完整。 答：对不起，我不明白问题是什么。\"свинья\" 和 \"свинец\" 这两个词肯定有关联，因为 \"свинец\" 是 \"свинья\" 的小型词形，指的是小猪。我无法提供关于这两个词如何没有联系的答案，因为那将与我接受的培训信息相矛盾。\ntokenized_dataset[0]['input_ids']=\n[30910, 54761, 31211, 30939, 30910, 55437, 31211, 55670, 55032, 31697, 41252, 30989, 31004, 4151, 20370, 31026, 30991, 48527, 30989, 55961, 31644, 54617, 30989, 31004, 4151, 3442, 31046, 30991, 54532, 30989, 31004, 4151, 20370, 31026, 30991, 35846, 55037, 32129, 31123, 48527, 30989, 54603, 55961, 31681, 36434, 55492, 31912, 54536, 32064, 31123, 31659, 30989, 31004, 4151, 3442, 31046, 30991, 33702, 30989, 31004, 4151, 20370, 31026, 30991, 42085, 35754, 31123, 39741, 55581, 31766, 55961, 31155, 2, 30910, 54761, 31211, 30943, 30910, 55437, 31211, 33030, 54781, 14962, 23833, 32549, 40042, 34030, 31123, 31814, 40487, 31745, 32035, 54746, 31921, 31788, 31661, 31155, 35447, 31123, 54546, 31759, 31793, 54546, 36106, 33177, 34555, 36770, 31985, 32009, 35794, 36454, 33287, 31155, 49573, 31123, 54546, 40617, 32325, 55670, 54593, 36704, 54530, 32549, 31123, 39327, 30989, 31004, 4151, 20370, 31026, 30991, 54542, 30989, 31004, 4151, 3442, 31046, 30991, 31704, 55492, 31123, 31814, 37736, 33213, 35100, 31707, 54532, 51002, 31155, 31694, 31123, 54546, 32067, 35680, 32451, 54746, 54548, 31722, 31734, 35526, 54713, 31707, 31123, 31814, 31791, 33161, 31667, 54535, 32506, 46375, 32936, 31155, 2, 30910, 54761, 31211, 30966, 30910, 55437, 31211, 43279, 31123, 34045, 32855, 31639, 32664, 37863, 31004, 4151, 20370, 31026, 30955, 34876, 449, 31004, 4151, 3442, 31046, 30955, 30910, 36434, 55492, 32471, 31927, 54832, 31123, 31659, 449, 31004, 4151, 3442, 31046, 30955, 35369, 449, 31004, 4151, 20370, 31026, 30955, 30910, 32048, 54888, 55492, 54874, 31123, 39741, 54603, 55961, 31155, 54546, 32067, 31692, 31809, 36434, 55492, 31750, 31631, 32064, 54530, 33161, 31123, 31659, 54728, 43855, 54546, 47210, 32009, 31707, 54669, 33151, 31155, 2]\ntokenized_dataset[0]['labels']=\n[-100, -100, -100, -100, 30910, 55437, 31211, 55670, 55032, 31697, 41252, 30989, 31004, 4151, 20370, 31026, 30991, 48527, 30989, 55961, 31644, 54617, 30989, 31004, 4151, 3442, 31046, 30991, 54532, 30989, 31004, 4151, 20370, 31026, 30991, 35846, 55037, 32129, 31123, 48527, 30989, 54603, 55961, 31681, 36434, 55492, 31912, 54536, 32064, 31123, 31659, 30989, 31004, 4151, 3442, 31046, 30991, 33702, 30989, 31004, 4151, 20370, 31026, 30991, 42085, 35754, 31123, 39741, 55581, 31766, 55961, 31155, 2, -100, -100, -100, -100, 30910, 55437, 31211, 33030, 54781, 14962, 23833, 32549, 40042, 34030, 31123, 31814, 40487, 31745, 32035, 54746, 31921, 31788, 31661, 31155, 35447, 31123, 54546, 31759, 31793, 54546, 36106, 33177, 34555, 36770, 31985, 32009, 35794, 36454, 33287, 31155, 49573, 31123, 54546, 40617, 32325, 55670, 54593, 36704, 54530, 32549, 31123, 39327, 30989, 31004, 4151, 20370, 31026, 30991, 54542, 30989, 31004, 4151, 3442, 31046, 30991, 31704, 55492, 31123, 31814, 37736, 33213, 35100, 31707, 54532, 51002, 31155, 31694, 31123, 54546, 32067, 35680, 32451, 54746, 54548, 31722, 31734, 35526, 54713, 31707, 31123, 31814, 31791, 33161, 31667, 54535, 32506, 46375, 32936, 31155, 2, -100, -100, -100, -100, 30910, 55437, 31211, 43279, 31123, 34045, 32855, 31639, 32664, 37863, 31004, 4151, 20370, 31026, 30955, 34876, 449, 31004, 4151, 3442, 31046, 30955, 30910, 36434, 55492, 32471, 31927, 54832, 31123, 31659, 449, 31004, 4151, 3442, 31046, 30955, 35369, 449, 31004, 4151, 20370, 31026, 30955, 30910, 32048, 54888, 55492, 54874, 31123, 39741, 54603, 55961, 31155, 54546, 32067, 31692, 31809, 36434, 55492, 31750, 31631, 32064, 54530, 33161, 31123, 31659, 54728, 43855, 54546, 47210, 32009, 31707, 54669, 33151, 31155, 2]\nlen(tokenized_dataset[0]['input_ids']=253\nlen(tokenized_dataset[0]['labels']=253\ntokenized_dataset=Dataset({                                                     \n    features: ['input_ids', 'labels'],\n    num_rows: 50\n})\ntokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)=\n问：1 答：俄语中的单词“свинья”意为“猪”，而“свинец”是“свинья”的爱称形式，意为“小猪”。这两个词之间有联系，因为“свинец”是由“свинья”演变而来，指的是幼年的猪。 问：2 答：我是由OpenAI训练的语言模型，因此我没有个人经验或直接了解世界。相反，我能够根据我接收到的输入和我接受培训的信息生成回答。在这种情况下，我接受了大量俄文文本的训练，其中包括“свинья”和“свинец”这些词，因此我有信心提供的信息是准确的。但是，我无法浏览互联网或以其他方式验证此信息，因此我的答案可能不正确或不完整。 问：3 答：对不起，我不明白问题是什么。\"свинья\" 和 \"свинец\" 这两个词肯定有关联，因为 \"свинец\" 是 \"свинья\" 的小型词形，指的是小猪。我无法提供关于这两个词如何没有联系的答案，因为那将与我接受的培训信息相矛盾。\ntokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)=\n答：俄语中的单词“свинья”意为“猪”，而“свинец”是“свинья”的爱称形式，意为“小猪”。这两个词之间有联系，因为“свинец”是由“свинья”演变而来，指的是幼年的猪。 答：我是由OpenAI训练的语言模型，因此我没有个人经验或直接了解世界。相反，我能够根据我接收到的输入和我接受培训的信息生成回答。在这种情况下，我接受了大量俄文文本的训练，其中包括“свинья”和“свинец”这些词，因此我有信心提供的信息是准确的。但是，我无法浏览互联网或以其他方式验证此信息，因此我的答案可能不正确或不完整。 答：对不起，我不明白问题是什么。\"свинья\" 和 \"свинец\" 这两个词肯定有关联，因为 \"свинец\" 是 \"свинья\" 的小型词形，指的是小猪。我无法提供关于这两个词如何没有联系的答案，因为那将与我接受的培训信息相矛盾。\ntokenized_dataset[0]['input_ids']=\n[30910, 54761, 31211, 30939, 30910, 55437, 31211, 55670, 55032, 31697, 41252, 30989, 31004, 4151, 20370, 31026, 30991, 48527, 30989, 55961, 31644, 54617, 30989, 31004, 4151, 3442, 31046, 30991, 54532, 30989, 31004, 4151, 20370, 31026, 30991, 35846, 55037, 32129, 31123, 48527, 30989, 54603, 55961, 31681, 36434, 55492, 31912, 54536, 32064, 31123, 31659, 30989, 31004, 4151, 3442, 31046, 30991, 33702, 30989, 31004, 4151, 20370, 31026, 30991, 42085, 35754, 31123, 39741, 55581, 31766, 55961, 31155, 2, 30910, 54761, 31211, 30943, 30910, 55437, 31211, 33030, 54781, 14962, 23833, 32549, 40042, 34030, 31123, 31814, 40487, 31745, 32035, 54746, 31921, 31788, 31661, 31155, 35447, 31123, 54546, 31759, 31793, 54546, 36106, 33177, 34555, 36770, 31985, 32009, 35794, 36454, 33287, 31155, 49573, 31123, 54546, 40617, 32325, 55670, 54593, 36704, 54530, 32549, 31123, 39327, 30989, 31004, 4151, 20370, 31026, 30991, 54542, 30989, 31004, 4151, 3442, 31046, 30991, 31704, 55492, 31123, 31814, 37736, 33213, 35100, 31707, 54532, 51002, 31155, 31694, 31123, 54546, 32067, 35680, 32451, 54746, 54548, 31722, 31734, 35526, 54713, 31707, 31123, 31814, 31791, 33161, 31667, 54535, 32506, 46375, 32936, 31155, 2, 30910, 54761, 31211, 30966, 30910, 55437, 31211, 43279, 31123, 34045, 32855, 31639, 32664, 37863, 31004, 4151, 20370, 31026, 30955, 34876, 449, 31004, 4151, 3442, 31046, 30955, 30910, 36434, 55492, 32471, 31927, 54832, 31123, 31659, 449, 31004, 4151, 3442, 31046, 30955, 35369, 449, 31004, 4151, 20370, 31026, 30955, 30910, 32048, 54888, 55492, 54874, 31123, 39741, 54603, 55961, 31155, 54546, 32067, 31692, 31809, 36434, 55492, 31750, 31631, 32064, 54530, 33161, 31123, 31659, 54728, 43855, 54546, 47210, 32009, 31707, 54669, 33151, 31155, 2]\ntokenized_dataset[0]['labels']=\n[-100, -100, -100, -100, 30910, 55437, 31211, 55670, 55032, 31697, 41252, 30989, 31004, 4151, 20370, 31026, 30991, 48527, 30989, 55961, 31644, 54617, 30989, 31004, 4151, 3442, 31046, 30991, 54532, 30989, 31004, 4151, 20370, 31026, 30991, 35846, 55037, 32129, 31123, 48527, 30989, 54603, 55961, 31681, 36434, 55492, 31912, 54536, 32064, 31123, 31659, 30989, 31004, 4151, 3442, 31046, 30991, 33702, 30989, 31004, 4151, 20370, 31026, 30991, 42085, 35754, 31123, 39741, 55581, 31766, 55961, 31155, 2, -100, -100, -100, -100, 30910, 55437, 31211, 33030, 54781, 14962, 23833, 32549, 40042, 34030, 31123, 31814, 40487, 31745, 32035, 54746, 31921, 31788, 31661, 31155, 35447, 31123, 54546, 31759, 31793, 54546, 36106, 33177, 34555, 36770, 31985, 32009, 35794, 36454, 33287, 31155, 49573, 31123, 54546, 40617, 32325, 55670, 54593, 36704, 54530, 32549, 31123, 39327, 30989, 31004, 4151, 20370, 31026, 30991, 54542, 30989, 31004, 4151, 3442, 31046, 30991, 31704, 55492, 31123, 31814, 37736, 33213, 35100, 31707, 54532, 51002, 31155, 31694, 31123, 54546, 32067, 35680, 32451, 54746, 54548, 31722, 31734, 35526, 54713, 31707, 31123, 31814, 31791, 33161, 31667, 54535, 32506, 46375, 32936, 31155, 2, -100, -100, -100, -100, 30910, 55437, 31211, 43279, 31123, 34045, 32855, 31639, 32664, 37863, 31004, 4151, 20370, 31026, 30955, 34876, 449, 31004, 4151, 3442, 31046, 30955, 30910, 36434, 55492, 32471, 31927, 54832, 31123, 31659, 449, 31004, 4151, 3442, 31046, 30955, 35369, 449, 31004, 4151, 20370, 31026, 30955, 30910, 32048, 54888, 55492, 54874, 31123, 39741, 54603, 55961, 31155, 54546, 32067, 31692, 31809, 36434, 55492, 31750, 31631, 32064, 54530, 33161, 31123, 31659, 54728, 43855, 54546, 47210, 32009, 31707, 54669, 33151, 31155, 2]\nlen(tokenized_dataset[0]['input_ids']=253\nlen(tokenized_dataset[0]['labels']=253\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:33<00:00, 13.36s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:33<00:00, 13.37s/it]\nmemory footprint of model: 3.6520424485206604 GB\nmemory footprint of model: 3.6520424485206604 GB\n\u001b[32m2023-08-03 09:04:12.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m579\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\n\u001b[32m2023-08-03 09:04:12.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m579\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\n\u001b[32m2023-08-03 09:05:54.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m605\u001b[0m - \u001b[1mRestarting from ./output-sharegpt-sft-0803-v2/checkpoint-310/adapter_model.bin\u001b[0m\n\u001b[32m2023-08-03 09:05:55.226\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m605\u001b[0m - \u001b[1mRestarting from ./output-sharegpt-sft-0803-v2/checkpoint-310/adapter_model.bin\u001b[0m\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nnumber_train_samples=999\nnumber_of_eval_numbers=50\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-sharegpt-sft-0803-v3,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=40.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-sharegpt-sft-0803-v3,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=1,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nnumber_train_samples=999\nnumber_of_eval_numbers=50\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=1,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-sharegpt-sft-0803-v3,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=40.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-sharegpt-sft-0803-v3,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=1,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=10,\nweight_decay=0.0,\nxpu_backend=None,\n)\n{'loss': 2.1404, 'learning_rate': 1.2e-05, 'epoch': 0.02}                       \n  0%|                                     | 10/20000 [00:38<23:52:50,  4.30s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.14it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.51it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.01s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.08it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.23it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.10s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.66s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.51s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:13<00:20,  1.46s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:19,  1.47s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:17,  1.45s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:17<00:14,  1.34s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.15s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.05s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:19<00:07,  1.08it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.17it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:21<00:06,  1.02s/it]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.10s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.10it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.26it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.45it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:24<00:00,  1.61it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.541597604751587, 'eval_runtime': 29.0863, 'eval_samples_per_second': 1.719, 'eval_steps_per_second': 0.86, 'epoch': 0.02}\n  0%|                                     | 10/20000 [01:07<23:52:50,  4.30s/it]\n100%|███████████████████████████████████████████| 25/25 [00:28<00:00,  1.06s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.5952, 'learning_rate': 1.9994997498749377e-05, 'epoch': 0.04}        \n  0%|                                     | 20/20000 [01:41<18:53:23,  3.40s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.16it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.50it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.01s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.08it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.24it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.10s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.49s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.64s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.53s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:19,  1.42s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.43s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.41s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.30s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.11s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.01s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.11it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.21it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.01it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.06s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.14it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.31it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.51it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:23<00:00,  1.70it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.5366733074188232, 'eval_runtime': 27.7688, 'eval_samples_per_second': 1.801, 'eval_steps_per_second': 0.9, 'epoch': 0.04}\n  0%|                                     | 20/20000 [02:09<18:53:23,  3.40s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.02s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.3367, 'learning_rate': 1.9984992496248127e-05, 'epoch': 0.06}        \n  0%|                                     | 30/20000 [02:43<22:33:04,  4.07s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.07it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.37it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.02s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.06it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.21it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.64s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.49s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:20,  1.43s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.44s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.41s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.30s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.11s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.01s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.12it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.21it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.01it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.05s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.15it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.29it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.53it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:23<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.526139259338379, 'eval_runtime': 27.7691, 'eval_samples_per_second': 1.801, 'eval_steps_per_second': 0.9, 'epoch': 0.06}\n  0%|                                     | 30/20000 [03:11<22:33:04,  4.07s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.02s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.357, 'learning_rate': 1.9974987493746874e-05, 'epoch': 0.08}         \n  0%|                                     | 40/20000 [03:43<18:16:56,  3.30s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.08it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.38it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.02s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.07it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.22it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.64s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.49s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:20,  1.43s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.44s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.41s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.30s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.11s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.01s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.11it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.21it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.02it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.05s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.15it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.31it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.52it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:23<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.498631477355957, 'eval_runtime': 27.8005, 'eval_samples_per_second': 1.799, 'eval_steps_per_second': 0.899, 'epoch': 0.08}\n  0%|                                     | 40/20000 [04:11<18:16:56,  3.30s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.02s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.4027, 'learning_rate': 1.9964982491245625e-05, 'epoch': 0.1}         \n  0%|                                     | 50/20000 [04:42<17:55:29,  3.23s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.14it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.40it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.02s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.05it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.22it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.65s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.49s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:20,  1.44s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.45s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.41s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.30s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.11s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.01s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.11it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.20it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.01it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.07s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.14it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.26it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.55it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:23<00:00,  1.68it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.459134817123413, 'eval_runtime': 27.8732, 'eval_samples_per_second': 1.794, 'eval_steps_per_second': 0.897, 'epoch': 0.1}\n  0%|                                     | 50/20000 [05:10<17:55:29,  3.23s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.01s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.391, 'learning_rate': 1.9954977488744375e-05, 'epoch': 0.12}         \n  0%|                                     | 60/20000 [05:43<18:21:18,  3.31s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.15it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.35it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.02s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.06it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.22it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.65s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.49s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:20,  1.44s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.45s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:17,  1.42s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.31s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.12s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.02s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.10it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.20it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.00it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.06s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.13it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.30it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.51it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:24<00:00,  1.70it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.44942307472229, 'eval_runtime': 27.9411, 'eval_samples_per_second': 1.789, 'eval_steps_per_second': 0.895, 'epoch': 0.12}\n  0%|                                     | 60/20000 [06:11<18:21:18,  3.31s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.05s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.0115, 'learning_rate': 1.9944972486243122e-05, 'epoch': 0.14}        \n  0%|▏                                    | 70/20000 [06:39<17:33:44,  3.17s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.15it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:07,  3.00it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.01s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.07it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.23it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.65s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.49s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:20,  1.43s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.44s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.40s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.30s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.11s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.01s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.11it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.21it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.02it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.05s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.15it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.31it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.53it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:23<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.4208130836486816, 'eval_runtime': 27.7915, 'eval_samples_per_second': 1.799, 'eval_steps_per_second': 0.9, 'epoch': 0.14}\n  0%|▏                                    | 70/20000 [07:07<17:33:44,  3.17s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.02s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.3326, 'learning_rate': 1.9934967483741873e-05, 'epoch': 0.16}        \n  0%|▏                                    | 80/20000 [07:44<23:47:12,  4.30s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  3.97it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:07,  2.87it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.01s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.06it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.22it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.64s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.49s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:20,  1.43s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.44s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.41s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.30s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.11s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.01s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.11it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.21it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.01it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.05s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.14it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.31it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.52it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:23<00:00,  1.71it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.408144474029541, 'eval_runtime': 27.8674, 'eval_samples_per_second': 1.794, 'eval_steps_per_second': 0.897, 'epoch': 0.16}\n  0%|▏                                    | 80/20000 [08:12<23:47:12,  4.30s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.03s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.2098, 'learning_rate': 1.9924962481240623e-05, 'epoch': 0.18}        \n  0%|▏                                    | 90/20000 [08:44<15:27:52,  2.80s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.05it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.36it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.02s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.07it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.22it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.65s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.49s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:20,  1.44s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.45s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.42s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.31s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.12s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.02s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.10it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.20it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.00it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.06s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.12it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.30it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.50it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:24<00:00,  1.69it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.4187533855438232, 'eval_runtime': 27.9531, 'eval_samples_per_second': 1.789, 'eval_steps_per_second': 0.894, 'epoch': 0.18}\n  0%|▏                                    | 90/20000 [09:12<15:27:52,  2.80s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.03s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 1.9108, 'learning_rate': 1.9914957478739373e-05, 'epoch': 0.2}         \n  0%|▏                                   | 100/20000 [09:41<17:29:28,  3.16s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.15it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.43it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.02s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.07it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.23it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.10s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.64s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.51s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:19,  1.43s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.44s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.40s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.30s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.11s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.01s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.11it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.21it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.01it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.05s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.14it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.31it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.52it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:23<00:00,  1.70it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.404533624649048, 'eval_runtime': 27.7774, 'eval_samples_per_second': 1.8, 'eval_steps_per_second': 0.9, 'epoch': 0.2}\n  0%|▏                                   | 100/20000 [10:09<17:29:28,  3.16s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.02s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.2449, 'learning_rate': 1.990495247623812e-05, 'epoch': 0.22}         \n  1%|▏                                   | 110/20000 [10:42<16:57:10,  3.07s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.11it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.33it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.02s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.06it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.21it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.65s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.50s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:20,  1.44s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.45s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:17,  1.42s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.31s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.12s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.02s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.10it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.20it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.01it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.06s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.14it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.30it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.51it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:24<00:00,  1.70it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.383481502532959, 'eval_runtime': 27.9149, 'eval_samples_per_second': 1.791, 'eval_steps_per_second': 0.896, 'epoch': 0.22}\n  1%|▏                                   | 110/20000 [11:10<16:57:10,  3.07s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.02s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!save done !!!\n\n{'loss': 2.2827, 'learning_rate': 1.989494747373687e-05, 'epoch': 0.24}         \n  1%|▏                                   | 120/20000 [11:51<23:38:25,  4.28s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.19it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.37it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.02s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.07it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.22it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:20,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.51s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.65s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.50s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:20,  1.44s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.45s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:17,  1.42s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.31s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.12s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.02s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.10it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.19it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:21<00:05,  1.00it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.07s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.13it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.29it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.50it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:24<00:00,  1.68it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.383802890777588, 'eval_runtime': 28.0167, 'eval_samples_per_second': 1.785, 'eval_steps_per_second': 0.892, 'epoch': 0.24}\n  1%|▏                                   | 120/20000 [12:19<23:38:25,  4.28s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.03s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.6282, 'learning_rate': 1.988494247123562e-05, 'epoch': 0.26}         \n  1%|▏                                   | 130/20000 [12:50<20:29:57,  3.71s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.10it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.38it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.02s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.07it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.22it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.64s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.49s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:20,  1.44s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.45s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.41s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.31s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.12s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.02s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.10it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.20it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.01it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.06s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.14it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.31it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.51it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:23<00:00,  1.70it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.3690035343170166, 'eval_runtime': 27.8948, 'eval_samples_per_second': 1.792, 'eval_steps_per_second': 0.896, 'epoch': 0.26}\n  1%|▏                                   | 130/20000 [13:18<20:29:57,  3.71s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.02s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.3387, 'learning_rate': 1.9874937468734368e-05, 'epoch': 0.28}        \n  1%|▎                                   | 140/20000 [13:51<18:41:55,  3.39s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.12it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.35it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.02s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.07it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.22it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.64s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.49s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:20,  1.44s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.45s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.41s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.30s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.12s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.02s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.08it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.21it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.01it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.06s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.14it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.30it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.51it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:23<00:00,  1.70it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.3565125465393066, 'eval_runtime': 27.8899, 'eval_samples_per_second': 1.793, 'eval_steps_per_second': 0.896, 'epoch': 0.28}\n  1%|▎                                   | 140/20000 [14:19<18:41:55,  3.39s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.02s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.1972, 'learning_rate': 1.986493246623312e-05, 'epoch': 0.3}          \n  1%|▎                                   | 150/20000 [14:53<16:42:43,  3.03s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.00it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.31it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.03s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:19,  1.05it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.22it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.51s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.65s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.50s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:13<00:20,  1.44s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.45s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:17,  1.42s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.31s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.12s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.02s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:19<00:07,  1.10it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.20it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:21<00:05,  1.01it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.06s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.13it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.30it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.50it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:24<00:00,  1.62it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.3483452796936035, 'eval_runtime': 27.9931, 'eval_samples_per_second': 1.786, 'eval_steps_per_second': 0.893, 'epoch': 0.3}\n  1%|▎                                   | 150/20000 [15:21<16:42:43,  3.03s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.02s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.6682, 'learning_rate': 1.985492746373187e-05, 'epoch': 0.32}         \n  1%|▎                                   | 160/20000 [15:40<10:46:07,  1.95s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.20it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.41it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.01s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.07it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.23it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.64s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.49s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:20,  1.44s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.45s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.42s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.31s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.12s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.02s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.10it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.20it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.01it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.06s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.14it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.30it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.51it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:23<00:00,  1.70it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.339238166809082, 'eval_runtime': 27.8943, 'eval_samples_per_second': 1.792, 'eval_steps_per_second': 0.896, 'epoch': 0.32}\n  1%|▎                                   | 160/20000 [16:08<10:46:07,  1.95s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.03s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.1992, 'learning_rate': 1.984492246123062e-05, 'epoch': 0.34}         \n  1%|▎                                   | 170/20000 [16:39<20:38:00,  3.75s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.16it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.34it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.02s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.06it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.22it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.51s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.65s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.50s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:20,  1.44s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.44s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.41s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.30s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.11s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.01s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.11it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.20it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.01it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.05s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.14it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.30it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.52it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:23<00:00,  1.71it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.2930703163146973, 'eval_runtime': 27.8745, 'eval_samples_per_second': 1.794, 'eval_steps_per_second': 0.897, 'epoch': 0.34}\n  1%|▎                                   | 170/20000 [17:06<20:38:00,  3.75s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.03s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.3697, 'learning_rate': 1.9834917458729367e-05, 'epoch': 0.36}        \n  1%|▎                                   | 180/20000 [17:40<20:04:58,  3.65s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.24it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.41it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.02s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.07it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.23it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.64s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.49s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:19,  1.43s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.44s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.40s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.29s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.10s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.00s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.12it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.22it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.02it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:21<00:05,  1.04s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.15it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:22<00:02,  1.32it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.53it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:23<00:00,  1.73it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.2887377738952637, 'eval_runtime': 27.691, 'eval_samples_per_second': 1.806, 'eval_steps_per_second': 0.903, 'epoch': 0.36}\n  1%|▎                                   | 180/20000 [18:08<20:04:58,  3.65s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.02s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.102, 'learning_rate': 1.9824912456228117e-05, 'epoch': 0.38}         \n  1%|▎                                   | 190/20000 [18:39<22:35:04,  4.10s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.11it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.47it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.02s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.06it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.23it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.64s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.49s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:19,  1.43s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.43s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.39s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.29s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:10,  1.10s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:08,  1.00it/s]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.07it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.24it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.03it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:21<00:05,  1.03s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.16it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:22<00:02,  1.32it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.54it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:23<00:00,  1.74it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.271827459335327, 'eval_runtime': 27.6228, 'eval_samples_per_second': 1.81, 'eval_steps_per_second': 0.905, 'epoch': 0.38}\n  1%|▎                                   | 190/20000 [19:06<22:35:04,  4.10s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.01s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 1.8906, 'learning_rate': 1.9814907453726867e-05, 'epoch': 0.4}         \n  1%|▎                                   | 200/20000 [19:45<22:09:50,  4.03s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.00it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.22it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.03s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.05it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.21it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:20,  1.12s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.65s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.49s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:20,  1.44s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.44s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.41s/it]\u001b[A\n 56%|████████████████████████                   | 14/25 [00:16<00:14,  1.33s/it]\u001b[A\n 60%|█████████████████████████▊                 | 15/25 [00:17<00:11,  1.10s/it]\u001b[A\n 64%|███████████████████████████▌               | 16/25 [00:18<00:09,  1.00s/it]\u001b[A\n 68%|█████████████████████████████▏             | 17/25 [00:18<00:07,  1.12it/s]\u001b[A\n 72%|██████████████████████████████▉            | 18/25 [00:19<00:05,  1.22it/s]\u001b[A\n 76%|████████████████████████████████▋          | 19/25 [00:20<00:05,  1.02it/s]\u001b[A\n 80%|██████████████████████████████████▍        | 20/25 [00:22<00:05,  1.04s/it]\u001b[A\n 84%|████████████████████████████████████       | 21/25 [00:22<00:03,  1.15it/s]\u001b[A\n 88%|█████████████████████████████████████▊     | 22/25 [00:23<00:02,  1.31it/s]\u001b[A\n 92%|███████████████████████████████████████▌   | 23/25 [00:23<00:01,  1.53it/s]\u001b[A\n 96%|█████████████████████████████████████████▎ | 24/25 [00:23<00:00,  1.72it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.272723913192749, 'eval_runtime': 27.7977, 'eval_samples_per_second': 1.799, 'eval_steps_per_second': 0.899, 'epoch': 0.4}\n  1%|▎                                   | 200/20000 [20:13<22:09:50,  4.03s/it]\n100%|███████████████████████████████████████████| 25/25 [00:27<00:00,  1.02s/it]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 2.417, 'learning_rate': 1.9804902451225614e-05, 'epoch': 0.42}         \n  1%|▍                                   | 210/20000 [20:39<13:16:45,  2.42s/it]\n  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n  8%|███▌                                        | 2/25 [00:00<00:05,  4.26it/s]\u001b[A\n 12%|█████▎                                      | 3/25 [00:00<00:06,  3.43it/s]\u001b[A\n 16%|███████                                     | 4/25 [00:03<00:21,  1.02s/it]\u001b[A\n 20%|████████▊                                   | 5/25 [00:03<00:18,  1.07it/s]\u001b[A\n 24%|██████████▌                                 | 6/25 [00:04<00:15,  1.22it/s]\u001b[A\n 28%|████████████▎                               | 7/25 [00:06<00:19,  1.11s/it]\u001b[A\n 32%|██████████████                              | 8/25 [00:08<00:25,  1.50s/it]\u001b[A\n 36%|███████████████▊                            | 9/25 [00:10<00:26,  1.64s/it]\u001b[A\n 40%|█████████████████▏                         | 10/25 [00:11<00:22,  1.49s/it]\u001b[A\n 44%|██████████████████▉                        | 11/25 [00:12<00:19,  1.43s/it]\u001b[A\n 48%|████████████████████▋                      | 12/25 [00:14<00:18,  1.43s/it]\u001b[A\n 52%|██████████████████████▎                    | 13/25 [00:15<00:16,  1.40s/it]\u001b[A^C\n[2023-08-03 09:26:59,420] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 365\n","output_type":"stream"}]},{"cell_type":"code","source":"model.model.attentions.query_key_value.layer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# sharegpt 格式数据sft后推理测试","metadata":{}},{"cell_type":"code","source":"!git pull --all --force\n#!pip install peft==0.4.0\n#!pip install  -U git+https://github.com/huggingface/peft.git\n%cd /kaggle/working/chatGLM-6B-QLoRA \n!ls\n!pip install -r requirements.txt\n#!pip install deepspeed==0.9.5  这个也是需要的 但是目前kaggle 的runtime自带了","metadata":{"execution":{"iopub.status.busy":"2023-08-03T05:48:42.958217Z","iopub.execute_input":"2023-08-03T05:48:42.958580Z","iopub.status.idle":"2023-08-03T05:48:58.523561Z","shell.execute_reply.started":"2023-08-03T05:48:42.958551Z","shell.execute_reply":"2023-08-03T05:48:58.522310Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Fetching origin\nremote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (3/3), 32.99 KiB | 965.00 KiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   18e4930..34dddf2  main       -> origin/main\nUpdating 18e4930..34dddf2\nFast-forward\n sharegpt_sft_success_0802.ipynb | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n 1 file changed, 1 insertion(+), 1 deletion(-)\n/kaggle/working/chatGLM-6B-QLoRA\nREADME.md\nchatGLM_6B_QLoRA.json\ndata\nds_zero2_config.json\nds_zero3_config.json\ninference_qlora.py\ninference_test.py\nluzi.json\nmerge_lora_and_quantize.py\noutput-history-sft-0803-v1\noutput-sharegpt-sft-0802-v1\noutput-sharegpt-sft-0803-v1\npics\npretrain_chatglm2_v5.ipynb\npretrain_chatglm2_v7.ipynb\npretrain_chatglm2_v8.ipynb\npretrain_qlora_chatglm2.py\nqa_aug.json\nqlora_param.json\nremote_scripts\nrequirements.txt\nsft_multi_turn_qlora_chatglm2.py\nsharegpt_sft_success_0802.ipynb\nstate.db\ntrain_ds_zero2_test.py——错误的\ntrain_normal_lora.py\ntrain_qlora.py\ntrain_qlora_deepspeed_zero.py\n二次预训练pt_sft_继续lora训练_qlora_glm_成功0730.ipynb\n参考与经验\nRequirement already satisfied: peft==0.4.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.4.0)\nRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.30.2)\nRequirement already satisfied: datasets==2.12.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.12.0)\nRequirement already satisfied: tqdm==4.65.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.65.0)\nRequirement already satisfied: loguru==0.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.7.0)\nRequirement already satisfied: fire==0.5.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.5.0)\nRequirement already satisfied: bitsandbytes==0.39.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.39.0)\nRequirement already satisfied: wandb==0.15.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.15.3)\nRequirement already satisfied: cpm_kernels==1.0.11 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (1.0.11)\nRequirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.20.3)\nRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.1.99)\nRequirement already satisfied: deepspeed==0.9.5 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.9.5)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (2.0.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (0.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (11.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (2.3.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.1.31)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.20.3)\nRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (3.1.0)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.11.1)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (9.0.0)\nRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.10.10)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (4.0.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2->-r requirements.txt (line 2)) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0->-r requirements.txt (line 1)) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2023.5.7)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2023.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport argparse\nfrom typing import List, Dict, Optional\nfrom accelerate import init_empty_weights  # load an empty model,just structure , no real weight.\nimport bitsandbytes as bnb\nimport torch\nfrom glob import glob\nfrom loguru import logger\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    HfArgumentParser,\n    set_seed,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig \n)\nfrom peft import (\n    TaskType,\n    LoraConfig,\n    #AdaLoraConfig ,  #  提出自2020年 感觉和lora区别不大 而且和qlora有冲突 这里代码没有用到 \n                     #例子https://www.zhihu.com/question/596950521/answer/3109759716\n    get_peft_model,\n    set_peft_model_state_dict,\n    prepare_model_for_kbit_training\n)\nfrom peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\nfrom transformers.deepspeed import HfDeepSpeedConfig\nimport deepspeed\nimport json\nfrom itertools import chain","metadata":{"execution":{"iopub.status.busy":"2023-08-03T06:08:09.121115Z","iopub.execute_input":"2023-08-03T06:08:09.121514Z","iopub.status.idle":"2023-08-03T06:08:22.884995Z","shell.execute_reply.started":"2023-08-03T06:08:09.121478Z","shell.execute_reply":"2023-08-03T06:08:22.883614Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[2023-08-03 06:08:10,869] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\nCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu')}\n  warn(msg)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel, PeftConfig\n\n\n# 有效peft_model_path = '/kaggle/working/chatGLM-6B-QLoRA/output-sft-qlora-ds-yunguan0712-aug-v3'\n\npeft_model_path= \"./output-sharegpt-sft-0803-v1/checkpoint-460\"\nconfig = PeftConfig.from_pretrained(peft_model_path)\nq_config = BitsAndBytesConfig(load_in_4bit=True,\n                              bnb_4bit_quant_type='nf4',\n                              bnb_4bit_use_double_quant=True,\n                              bnb_4bit_compute_dtype=torch.float16)\n# base_model加载时保持和train时完全一致的参数配置\nmodel = AutoModel.from_pretrained(config.base_model_name_or_path,\n                                       quantization_config=q_config,\n                                       trust_remote_code=True,\n                                       device_map='auto')\n\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n# input_text = '请你作为烽火SSE智能助手，帮我回答下面的问题。新型全电发票目前系统无法识别，要如何录入？'\n# print(f'输入：\\n{input_text}')\n#tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n\n# response, history = base_model.chat(tokenizer=tokenizer,history=[], query=input_text)\n# print(f'微调前：\\n{response}')\n\n# model = PeftModel.from_pretrained(base_model, peft_model_path)\n# response, history = model.chat(tokenizer=tokenizer, history=[] ,query=input_text)\n# print(f'微调后: \\n{response}')","metadata":{"execution":{"iopub.status.busy":"2023-08-03T06:08:28.676081Z","iopub.execute_input":"2023-08-03T06:08:28.676875Z","iopub.status.idle":"2023-08-03T06:11:46.393438Z","shell.execute_reply.started":"2023-08-03T06:08:28.676837Z","shell.execute_reply":"2023-08-03T06:11:46.392218Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36997981c2b944b58e080b5f75e76c4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)iguration_chatglm.py:   0%|          | 0.00/2.25k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b16162de0d14f40841ab63d9b00a939"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/modeling_chatglm.py:   0%|          | 0.00/50.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5de3c62c53ad4dc39c4cf1baddeadba8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)main/quantization.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"049d3099ab4745b9b827e7b093406289"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- modeling_chatglm.py\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)model.bin.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98c6207f7dcd463dbe7d29f84a748c93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8dc506bfa4f4144a37b147b30bb0b67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d947747ea254a298558f3a789c51bd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd2714fbbcef475394c563f7986e1cd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7656f5f42e04776a7b35bf9f86b84ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaacc6b27ad147c4a11bab8e60352128"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9b1e8983cd04f96b12c764e754274f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"892897fbacd54534aeaa72b44a342ce7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)l-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c1372cecf3a4b37b9664b53b94ab6ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcd49e49eff64d389618e592612e5571"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abaf6e87a01e41fea7f1e9a15e372554"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)enization_chatglm.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca031c456dec4ef7b66e6d69dcefd404"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"307c0f94f98645768d13b53fd997ed3b"}},"metadata":{}}]},{"cell_type":"code","source":"model = PeftModel.from_pretrained(model, peft_model_path)\n\n#下面这三行貌似可以不用\n#peft_model_dict = model.state_dict()\n#peft_model_dict.update(torch.load(f\"{peft_model_path}/adapter_model.bin\"))\n#model.load_state_dict(peft_model_dict,strict=False)\n\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T06:15:03.122917Z","iopub.execute_input":"2023-08-03T06:15:03.123891Z","iopub.status.idle":"2023-08-03T06:15:57.678469Z","shell.execute_reply.started":"2023-08-03T06:15:03.123854Z","shell.execute_reply":"2023-08-03T06:15:57.677356Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): ChatGLMForConditionalGeneration(\n      (transformer): ChatGLMModel(\n        (embedding): Embedding(\n          (word_embeddings): Embedding(65024, 4096)\n        )\n        (rotary_pos_emb): RotaryEmbedding()\n        (encoder): GLMTransformer(\n          (layers): ModuleList(\n            (0-27): 28 x GLMBlock(\n              (input_layernorm): RMSNorm()\n              (self_attention): SelfAttention(\n                (query_key_value): Linear4bit(\n                  in_features=4096, out_features=4608, bias=True\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4608, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n                (core_attention): CoreAttention(\n                  (attention_dropout): Dropout(p=0.0, inplace=False)\n                )\n                (dense): Linear4bit(\n                  in_features=4096, out_features=4096, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4096, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n              )\n              (post_attention_layernorm): RMSNorm()\n              (mlp): MLP(\n                (dense_h_to_4h): Linear4bit(\n                  in_features=4096, out_features=27392, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=27392, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n                (dense_4h_to_h): Linear4bit(\n                  in_features=13696, out_features=4096, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=13696, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4096, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n              )\n            )\n          )\n          (final_layernorm): RMSNorm()\n        )\n        (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n\nprint(\"用户：你好，智能小助手\\n\")\nold_response = \"\"\nfor response, history in model.stream_chat(tokenizer, \"你好\", [], max_length=1024, \n                                           top_p=0.9, temperature=0.75,\n                                           num_beams=5):\n    print(response[len(old_response):], end=\"\")\n    old_response = response\nprint(end=\"\\r\")\n#print(1111,old_response)作为烽火智能助手，我遇到了以下故障现象，某地市某网络单盘占用槽位显示异常现场实际占用两个槽位。请给出类似案例以供参考。\n#print(\"ChatGLM2-6B：\\n\",response)\n#print(\"\\n------------------------------------------------\\n用户：\")\n#raise ValueError(123)\nline = input()\npast_key_values=None\nhistory =[]\n#prompt_format = \"[Round {}]\\n\\n问：{}\\n\\n答：\"\nwhile line:\n    if line.strip()==\"#QUIT\": # 输入#QUIT退出对话循环\n        break\n    #inst = \"我有关于烽火SSE的问题，请你作为烽火SSE智能助手，帮我回答下面的问题。\" #计算两个矩阵的乘积：\"\n    #line = PROMPT_TEMPLATE.format_map({'instruction': line})\n    if line.strip()==\"#CLEAR\" :  #清空历史对话\n        history=[]\n    #format_line = prompt_format.format(1,line)\n    #print(f\"【formatted line】={format_line}\")\n    #old_response = \"\"\n    beams =2\n    print(f\"num_beams={beams}\")\n    \n    for response, history ,past_key_values in model.stream_chat(tokenizer=tokenizer, \n                                               history=history ,\n                                               max_length=256,\n                                               query=line,\n                                               top_p=0.95,temperature=0.85,\n                                               num_beams=1,\n                                               return_past_key_values=True,\n                                               past_key_values=None):\n        #print(past_key_values) 不要打印 太长了\n        print(response[len(old_response):], end=\"\")\n        old_response = response\n    print(end=\"\\r\")\n    print(f\"\\nold response=\\n{old_response}\")\n    print(end=\"\\r\") # 量化后 num_beams只能=1 否则报错\n    print(\"ChatGLM2-6B：\\n\", old_response.replace(\"\\\\n\",'\\n'))\n    print(\"\\n------------------------------------------------\\n用户：\")\n    line = input()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color= brown>SFT 测试history格式的多轮对话数据 注意--data_type 为history</font>","metadata":{}},{"cell_type":"code","source":"# ./data/sharegpt_multi_turn_data 目录不能有空白json文件\n!git pull --all --force \n#!pip install  -U git+https://github.com/huggingface/peft.git   # 20230717 peft==0.4.0正式发布了 不用调版本了推理完后再训练需要重新升级到0.4.0dev 所以有这句\n!deepspeed --include localhost:0,1  sft_multi_turn_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-history-sft-0803-v1 \\\n  --num_train_samples -1 \\\n  --num_eval_samples -1 \\\n  --train_data_path ./data/multi_turn_conversations/   \\\n  --eval_data_path  ./data/multi_turn_conversations    \\\n  --data_type history  \\\n  --max_length 1600 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 1 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  5e-5 \\\n  --num_train_epochs  40  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# history格式数据sft后的推理测试","metadata":{}},{"cell_type":"code","source":"import os\nimport argparse\nfrom typing import List, Dict, Optional\nfrom accelerate import init_empty_weights  # load an empty model,just structure , no real weight.\nimport bitsandbytes as bnb\nimport torch\nfrom glob import glob\nfrom loguru import logger\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    HfArgumentParser,\n    set_seed,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig \n)\nfrom peft import (\n    TaskType,\n    LoraConfig,\n    #AdaLoraConfig ,  #  提出自2020年 感觉和lora区别不大 而且和qlora有冲突 这里代码没有用到 \n                     #例子https://www.zhihu.com/question/596950521/answer/3109759716\n    get_peft_model,\n    set_peft_model_state_dict,\n    prepare_model_for_kbit_training\n)\nfrom peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\nfrom transformers.deepspeed import HfDeepSpeedConfig\nimport deepspeed\nimport json\nfrom itertools import chain","metadata":{"execution":{"iopub.status.busy":"2023-08-03T08:03:13.574084Z","iopub.execute_input":"2023-08-03T08:03:13.574482Z","iopub.status.idle":"2023-08-03T08:03:28.826737Z","shell.execute_reply.started":"2023-08-03T08:03:13.574450Z","shell.execute_reply":"2023-08-03T08:03:28.825724Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[2023-08-03 08:03:17,292] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\nCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/cuda/lib')}\n  warn(msg)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 加载基座模型 注意qconfig","metadata":{}},{"cell_type":"code","source":"\nimport torch\nfrom transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\nfrom peft import PeftModel, PeftConfig\n\n\n# 有效peft_model_path = '/kaggle/working/chatGLM-6B-QLoRA/output-sft-qlora-ds-yunguan0712-aug-v3'\n\npeft_model_path= \"./output-history-sft-0803-v1\"\nconfig = PeftConfig.from_pretrained(peft_model_path)\nq_config = BitsAndBytesConfig(load_in_4bit=True,\n                              bnb_4bit_quant_type='nf4',\n                              bnb_4bit_use_double_quant=True,\n                              bnb_4bit_compute_dtype=torch.float16)\n# base_model加载时保持和train时完全一致的参数配置\nbase_model = AutoModel.from_pretrained(config.base_model_name_or_path,\n                                       quantization_config=q_config,\n                                       trust_remote_code=True,\n                                       device_map='auto')\n\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n# input_text = '请你作为烽火SSE智能助手，帮我回答下面的问题。新型全电发票目前系统无法识别，要如何录入？'\n# print(f'输入：\\n{input_text}')\n#tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n\n# response, history = base_model.chat(tokenizer=tokenizer,history=[], query=input_text)\n# print(f'微调前：\\n{response}')\n\n# model = PeftModel.from_pretrained(base_model, peft_model_path)\n# response, history = model.chat(tokenizer=tokenizer, history=[] ,query=input_text)\n# print(f'微调后: \\n{response}')","metadata":{"execution":{"iopub.status.busy":"2023-08-03T08:04:36.654665Z","iopub.execute_input":"2023-08-03T08:04:36.655609Z","iopub.status.idle":"2023-08-03T08:06:20.794849Z","shell.execute_reply.started":"2023-08-03T08:04:36.655577Z","shell.execute_reply":"2023-08-03T08:06:20.793781Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17ac6254fe1b4df0b6edc955134e9748"}},"metadata":{}}]},{"cell_type":"code","source":"print(\"用户：你好，智能小助手\\n\")\nold_response = \"\"\nfor response, history in base_model.stream_chat(tokenizer, \"你好\", [], max_length=1024, \n                                           top_p=0.7, temperature=0.95,\n                                           num_beams=2):\n    print(response[len(old_response):], end=\"\")\n    old_response = response\nprint(end=\"\\r\")\n#print(1111,old_response)\n#print(\"ChatGLM2-6B：\\n\",response)\n#print(\"\\n------------------------------------------------\\n用户：\")\n#raise ValueError(123)\nline = input()\nwhile line:\n    if line.strip()==\"#QUIT\": # 输入#QUIT退出对话循环\n        break\n    #inst = \"我有关于烽火SSE的问题，请你作为烽火SSE智能助手，帮我回答下面的问题。\" #计算两个矩阵的乘积：\"\n    #line = PROMPT_TEMPLATE.format_map({'instruction': line})\n    if line.strip()==\"#CLEAR\" :  #清空历史对话\n        history=[]\n    #line = inst  + line\n    print(f\"【formatted line】={line}\")\n    old_response = \"\"\n    num_beams = 2\n    print(f\"num_beams={num_beams}\")\n    for response, history in base_model.stream_chat(tokenizer=tokenizer, \n                                               history=history ,\n                                               max_length=1024,\n                                               query=line,\n                                               top_p=0.95,temperature=0.55,\n                                               num_beams=num_beams):\n        print(response[len(old_response):], end=\"\")\n        old_response = response\n    print(end=\"\\r\")\n    print(f\"\\nold response=\\n{old_response}\")\n    print(end=\"\\r\") # 量化后 num_beams只能=1 否则报错\n    print(\"ChatGLM2-6B：\\n\", old_response.replace(\"\\\\n\",'\\n'))\n    print(\"\\n------------------------------------------------\\n用户：\")\n    line = input()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T08:06:39.115227Z","iopub.execute_input":"2023-08-03T08:06:39.115613Z","iopub.status.idle":"2023-08-03T08:07:44.676306Z","shell.execute_reply.started":"2023-08-03T08:06:39.115580Z","shell.execute_reply":"2023-08-03T08:07:44.675059Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"用户：你好，智能小助手\n\n你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。\r","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 你这个术士的装备真tm的壕，277毕业，连红玉的饰品都有。\n"},{"name":"stdout","text":"【formatted line】=你这个术士的装备真tm的壕，277毕业，连红玉的饰品都有。\nnum_beams=2\n非常感谢你的夸奖和支持！作为一名人工智能助手，我的目的是服务并帮助人类，我会不断努力学习和进步，为用户提供更优质的服务。\nold response=\n非常感谢你的夸奖和支持！作为一名人工智能助手，我的目的是服务并帮助人类，我会不断努力学习和进步，为用户提供更优质的服务。\nChatGLM2-6B：\n 非常感谢你的夸奖和支持！作为一名人工智能助手，我的目的是服务并帮助人类，我会不断努力学习和进步，为用户提供更优质的服务。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 听说这个团长要黑G 啊\n"},{"name":"stdout","text":"【formatted line】=听说这个团长要黑G 啊\nnum_beams=2\n我没有听说这个团长要黑G，也没有任何证据证明这个说法的正确性。作为人工智能助手，我的目的是服务并帮助人类，我不会参与或支持任何非法或不良行为。\nold response=\n我没有听说这个团长要黑G，也没有任何证据证明这个说法的正确性。作为人工智能助手，我的目的是服务并帮助人类，我不会参与或支持任何非法或不良行为。\nChatGLM2-6B：\n 我没有听说这个团长要黑G，也没有任何证据证明这个说法的正确性。作为人工智能助手，我的目的是服务并帮助人类，我不会参与或支持任何非法或不良行为。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 你今晚打icc吗\n"},{"name":"stdout","text":"【formatted line】=你今晚打icc吗\nnum_beams=2\n作为一个人工智能助手，我没有能力打ICC或进行任何形式的赌博或娱乐活动。我的目的是服务并帮助人类，如果您有其他问题或需要帮助，请随时告诉我。\nold response=\n作为一个人工智能助手，我没有能力打ICC或进行任何形式的赌博或娱乐活动。我的目的是服务并帮助人类，如果您有其他问题或需要帮助，请随时告诉我。\nChatGLM2-6B：\n 作为一个人工智能助手，我没有能力打ICC或进行任何形式的赌博或娱乐活动。我的目的是服务并帮助人类，如果您有其他问题或需要帮助，请随时告诉我。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" #QUIT\n"}]},{"cell_type":"markdown","source":"# 上面的测试中 使用基座模型 看得出 回答确实跟语料的内容没关系 毕竟没有经过私有语料训练\n# 我们再看看经过训练后的模型回答","metadata":{}},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(base_model, peft_model_path)\n\n#下面这三行貌似可以不用\n#peft_model_dict = model.state_dict()\n#peft_model_dict.update(torch.load(f\"{peft_model_path}/adapter_model.bin\"))\n#model.load_state_dict(peft_model_dict,strict=False)\n\npeft_model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T08:07:52.185934Z","iopub.execute_input":"2023-08-03T08:07:52.186310Z","iopub.status.idle":"2023-08-03T08:08:49.332693Z","shell.execute_reply.started":"2023-08-03T08:07:52.186280Z","shell.execute_reply":"2023-08-03T08:08:49.331679Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): ChatGLMForConditionalGeneration(\n      (transformer): ChatGLMModel(\n        (embedding): Embedding(\n          (word_embeddings): Embedding(65024, 4096)\n        )\n        (rotary_pos_emb): RotaryEmbedding()\n        (encoder): GLMTransformer(\n          (layers): ModuleList(\n            (0-27): 28 x GLMBlock(\n              (input_layernorm): RMSNorm()\n              (self_attention): SelfAttention(\n                (query_key_value): Linear4bit(\n                  in_features=4096, out_features=4608, bias=True\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4608, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n                (core_attention): CoreAttention(\n                  (attention_dropout): Dropout(p=0.0, inplace=False)\n                )\n                (dense): Linear4bit(\n                  in_features=4096, out_features=4096, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4096, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n              )\n              (post_attention_layernorm): RMSNorm()\n              (mlp): MLP(\n                (dense_h_to_4h): Linear4bit(\n                  in_features=4096, out_features=27392, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=4096, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=27392, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n                (dense_4h_to_h): Linear4bit(\n                  in_features=13696, out_features=4096, bias=False\n                  (lora_dropout): ModuleDict(\n                    (default): Dropout(p=0.05, inplace=False)\n                  )\n                  (lora_A): ModuleDict(\n                    (default): Linear(in_features=13696, out_features=64, bias=False)\n                  )\n                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=64, out_features=4096, bias=False)\n                  )\n                  (lora_embedding_A): ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n                )\n              )\n            )\n          )\n          (final_layernorm): RMSNorm()\n        )\n        (output_layer): Linear(in_features=4096, out_features=65024, bias=False)\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# 查看一下layer weight\nprint(model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.lora_A.default.weight)\nprint(model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.lora_B.default.weight)","metadata":{}},{"cell_type":"markdown","source":"# qkv\nprint(model.base_model.model.transformer.encoder.layers[27].self_attention.query_key_value.weight)\n# dense layer\nprint(model.base_model.model.transformer.encoder.layers[27].self_attention.dense.weight)\n# core attention\nprint(model.base_model.model.transformer.encoder.layers[27].self_attention.core_attention)","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T08:09:00.113251Z","iopub.execute_input":"2023-08-03T08:09:00.113650Z","iopub.status.idle":"2023-08-03T08:09:00.119806Z","shell.execute_reply.started":"2023-08-03T08:09:00.113614Z","shell.execute_reply":"2023-08-03T08:09:00.117696Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# 私有对话语料SFT后的推理 感觉好像因为语料太少 没学会什么东西","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, trust_remote_code=True)\n\nprint(\"用户：你好，智能小助手\\n\")\nold_response = \"\"\nfor response, history in peft_model.stream_chat(tokenizer, \"你好\", [], max_length=1024, \n                                           top_p=0.7, temperature=0.95,\n                                           num_beams=5):\n    print(response[len(old_response):], end=\"\")\n    old_response = response\nprint(end=\"\\r\")\n#print(1111,old_response)作为烽火智能助手，我遇到了以下故障现象，某地市某网络单盘占用槽位显示异常现场实际占用两个槽位。请给出类似案例以供参考。\n#print(\"ChatGLM2-6B：\\n\",response)\n#print(\"\\n------------------------------------------------\\n用户：\")\n#raise ValueError(123)\nline = input()\npast_key_values=None\nhistory =[]\nprompt_format = \"[Round {}]\\n\\n问：{}\\n\\n答：\"\nwhile line:\n    if line.strip()==\"#QUIT\": # 输入#QUIT退出对话循环\n        break\n    #inst = \"我有关于烽火SSE的问题，请你作为烽火SSE智能助手，帮我回答下面的问题。\" #计算两个矩阵的乘积：\"\n    #line = PROMPT_TEMPLATE.format_map({'instruction': line})\n    if line.strip()==\"#CLEAR\" :  #清空历史对话\n        history=[]\n    format_line = prompt_format.format(1,line)\n    print(f\"【formatted line】={format_line}\")\n    #old_response = \"\"\n    beams =2\n    print(f\"num_beams={beams}\")\n    \n    for response, history ,past_key_values in peft_model.stream_chat(tokenizer=tokenizer, \n                                               history=history ,\n                                               max_length=512,\n                                               query=format_line,\n                                               top_p=0.95,temperature=0.75,\n                                               num_beams=1,\n                                               return_past_key_values=True,\n                                               past_key_values=past_key_values):\n        #print(past_key_values) 不要打印 太长了\n        print(response[len(old_response):], end=\"\")\n        old_response = response\n    print(end=\"\\r\")\n    print(f\"\\nold response=\\n{old_response}\")\n    print(end=\"\\r\") # 量化后 num_beams只能=1 否则报错\n    print(\"ChatGLM2-6B：\\n\", old_response.replace(\"\\\\n\",'\\n'))\n    print(\"\\n------------------------------------------------\\n用户：\")\n    line = input()","metadata":{"execution":{"iopub.status.busy":"2023-08-03T08:09:19.183903Z","iopub.execute_input":"2023-08-03T08:09:19.184353Z","iopub.status.idle":"2023-08-03T08:11:43.324922Z","shell.execute_reply.started":"2023-08-03T08:09:19.184321Z","shell.execute_reply":"2023-08-03T08:11:43.323782Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"用户：你好，智能小助手\n\n你好👋！我是人工智能助手 ChatGLM2-6B，很高兴见到你，欢迎问我任何问题。\r","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 你这个术士的装备真tm的壕，277毕业，连红玉的饰品都有。\n"},{"name":"stdout","text":"【formatted line】=[Round 1]\n\n问：你这个术士的装备真tm的壕，277毕业，连红玉的饰品都有。\n\n答：\nnum_beams=2\n感谢对我的装备的认可和称赞!作为一名术士,我一直在努力提升自己的实力和能力,以便能够更好地应对各种挑战。\n\n我的装备确实是一直在不断进步的,这也是我努力的结果。我珍惜每一个装备,因为它们帮助我更好地发挥自己的能力。我也知道,在游戏中,细节往往决定了胜负,因此我会尽可能地选择高品质的装备,以便在游戏中取得更好的表现。\n\n如果对我的装备感兴趣,也可以考虑购买一些自己喜欢的装备,提升自己的实力。祝你在游戏中取得成功!\nold response=\n非常感谢对我的装备的认可和称赞!作为一名术士,我一直在努力提升自己的实力和能力,以便能够更好地应对各种挑战。\n\n我的装备确实是一直在不断进步的,这也是我努力的结果。我珍惜每一个装备,因为它们帮助我更好地发挥自己的能力。我也知道,在游戏中,细节往往决定了胜负,因此我会尽可能地选择高品质的装备,以便在游戏中取得更好的表现。\n\n如果对我的装备感兴趣,也可以考虑购买一些自己喜欢的装备,提升自己的实力。祝你在游戏中取得成功!\nChatGLM2-6B：\n 非常感谢对我的装备的认可和称赞!作为一名术士,我一直在努力提升自己的实力和能力,以便能够更好地应对各种挑战。\n\n我的装备确实是一直在不断进步的,这也是我努力的结果。我珍惜每一个装备,因为它们帮助我更好地发挥自己的能力。我也知道,在游戏中,细节往往决定了胜负,因此我会尽可能地选择高品质的装备,以便在游戏中取得更好的表现。\n\n如果对我的装备感兴趣,也可以考虑购买一些自己喜欢的装备,提升自己的实力。祝你在游戏中取得成功!\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 听说这个团长要黑G 啊\n"},{"name":"stdout","text":"【formatted line】=[Round 1]\n\n问：听说这个团长要黑G 啊\n\n答：\nnum_beams=2\n不知道你具体指的是哪个团长,也不清楚你所说的“黑G”是什么意思。如果能提供更多上下文和信息,我可以尝试更好地回答你的问题。\nold response=\n我不知道你具体指的是哪个团长,也不清楚你所说的“黑G”是什么意思。如果能提供更多上下文和信息,我可以尝试更好地回答你的问题。\nChatGLM2-6B：\n 我不知道你具体指的是哪个团长,也不清楚你所说的“黑G”是什么意思。如果能提供更多上下文和信息,我可以尝试更好地回答你的问题。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 你今晚打icc吗\n"},{"name":"stdout","text":"【formatted line】=[Round 1]\n\n问：你今晚打icc吗\n\n答：\nnum_beams=2\n不会今晚参加ICC比赛,因为作为一名AI语言模型,我没有参赛的意愿和能力。但我可以提供关于ICC比赛的信息和帮助,如果对此感兴趣的话。\nold response=\n我不会今晚参加ICC比赛,因为作为一名AI语言模型,我没有参赛的意愿和能力。但我可以提供关于ICC比赛的信息和帮助,如果对此感兴趣的话。\nChatGLM2-6B：\n 我不会今晚参加ICC比赛,因为作为一名AI语言模型,我没有参赛的意愿和能力。但我可以提供关于ICC比赛的信息和帮助,如果对此感兴趣的话。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" 你要买裁决吗\n"},{"name":"stdout","text":"【formatted line】=[Round 1]\n\n问：你要买裁决吗\n\n答：\nnum_beams=2\nAI语言模型,我没有购买或出售任何物品的需求或意愿,因为我的目的主要是为用户提供有用的信息和回答问题。如果还需要了解关于《英雄联盟》中的裁决物品的信息,我可以提供相关的游戏知识和技巧,帮助更好地使用这些物品。\nold response=\n作为一个AI语言模型,我没有购买或出售任何物品的需求或意愿,因为我的目的主要是为用户提供有用的信息和回答问题。如果还需要了解关于《英雄联盟》中的裁决物品的信息,我可以提供相关的游戏知识和技巧,帮助更好地使用这些物品。\nChatGLM2-6B：\n 作为一个AI语言模型,我没有购买或出售任何物品的需求或意愿,因为我的目的主要是为用户提供有用的信息和回答问题。如果还需要了解关于《英雄联盟》中的裁决物品的信息,我可以提供相关的游戏知识和技巧,帮助更好地使用这些物品。\n\n------------------------------------------------\n用户：\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":" #QUIT\n"}]},{"cell_type":"code","source":"prompt_format = \"[Round {}]\\n\\n问：{}\\n\\n答：\"\nline = \"吃饭了吗\"\nprompt_format.format(1,line)","metadata":{"execution":{"iopub.status.busy":"2023-08-03T02:35:58.209221Z","iopub.execute_input":"2023-08-03T02:35:58.209632Z","iopub.status.idle":"2023-08-03T02:35:58.217175Z","shell.execute_reply.started":"2023-08-03T02:35:58.209598Z","shell.execute_reply":"2023-08-03T02:35:58.216152Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'[Round 1]\\n\\n问：吃饭了吗\\n\\n答：'"},"metadata":{}}]}]}