{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-02T11:35:20.527873Z","iopub.execute_input":"2023-08-02T11:35:20.528456Z","iopub.status.idle":"2023-08-02T11:35:20.545238Z","shell.execute_reply.started":"2023-08-02T11:35:20.528423Z","shell.execute_reply":"2023-08-02T11:35:20.544047Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/sharegpt-zh-1k/sharegpt_zh_1K_format.jsonl\n/kaggle/input/test-txt/test0730.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install peft==0.4.0\n!pip install transformers==4.30.2\n!pip install datasets==2.12.0\n!pip install tqdm==4.65.0\n!pip install loguru==0.7.0\n!pip install fire==0.5.0\n!pip install bitsandbytes==0.39.0\n#wandb==0.15.3\n!pip install cpm_kernels==1.0.11\n!pip install accelerate==0.20.3\n!pip install sentencepiece==0.1.99\n!pip install deepspeed==0.9.5","metadata":{"execution":{"iopub.status.busy":"2023-08-02T11:37:30.675299Z","iopub.execute_input":"2023-08-02T11:37:30.675770Z","iopub.status.idle":"2023-08-02T11:40:26.897710Z","shell.execute_reply.started":"2023-08-02T11:37:30.675730Z","shell.execute_reply":"2023-08-02T11:40:26.895040Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting peft==0.4.0\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (2.0.0+cpu)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (4.30.2)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (0.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.13.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.4.0) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0) (2023.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\nInstalling collected packages: peft\nSuccessfully installed peft-0.4.0\nRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.30.2) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2) (2023.5.7)\nCollecting datasets==2.12.0\n  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (1.23.5)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (9.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (0.18.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (6.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.12.0) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.12.0) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.12.0) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.12.0) (1.16.0)\nInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed datasets-2.12.0\nRequirement already satisfied: tqdm==4.65.0 in /opt/conda/lib/python3.10/site-packages (4.65.0)\nCollecting loguru==0.7.0\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: loguru\nSuccessfully installed loguru-0.7.0\nCollecting fire==0.5.0\n  Downloading fire-0.5.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0) (2.3.0)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=c42a74e79eacedc3aa17b9e2d9d3a5b8964dc5ccc8cc8d124901b3c0a6ed4ac1\n  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\nSuccessfully built fire\nInstalling collected packages: fire\nSuccessfully installed fire-0.5.0\nCollecting bitsandbytes==0.39.0\n  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.39.0\nCollecting cpm_kernels==1.0.11\n  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: cpm_kernels\nSuccessfully installed cpm_kernels-1.0.11\nRequirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.10/site-packages (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3) (6.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.20.3) (2.0.0+cpu)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.20.3) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate==0.20.3) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate==0.20.3) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate==0.20.3) (1.3.0)\nRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (0.1.99)\nCollecting deepspeed==0.9.5\n  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting hjson (from deepspeed==0.9.5)\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (1.11.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (9.0.0)\nRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (1.10.9)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (2.0.0+cpu)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5) (4.65.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->deepspeed==0.9.5) (3.0.9)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2.0.0->deepspeed==0.9.5) (4.6.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.9.5) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.9.5) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.9.5) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.9.5) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed==0.9.5) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed==0.9.5) (1.3.0)\nBuilding wheels for collected packages: deepspeed\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844517 sha256=e9bdf514ebc4cab0a6a6e849a234a829f7caea30703e8090879667c7b74c9e5f\n  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\nSuccessfully built deepspeed\nInstalling collected packages: hjson, deepspeed\nSuccessfully installed deepspeed-0.9.5 hjson-3.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport argparse\nfrom typing import List, Dict, Optional\nfrom accelerate import init_empty_weights  # load an empty model,just structure , no real weight.\nimport bitsandbytes as bnb\nimport torch\nfrom glob import glob\nfrom loguru import logger\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    HfArgumentParser,\n    set_seed,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig \n)\nfrom peft import (\n    TaskType,\n    LoraConfig,\n    #AdaLoraConfig ,  #  提出自2020年 感觉和lora区别不大 而且和qlora有冲突 这里代码没有用到 \n                     #例子https://www.zhihu.com/question/596950521/answer/3109759716\n    get_peft_model,\n    set_peft_model_state_dict,\n    prepare_model_for_kbit_training\n)\nfrom peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\nfrom transformers.deepspeed import HfDeepSpeedConfig\nimport deepspeed\nimport json\nfrom itertools import chain","metadata":{"execution":{"iopub.status.busy":"2023-08-02T11:40:30.488202Z","iopub.execute_input":"2023-08-02T11:40:30.488914Z","iopub.status.idle":"2023-08-02T11:40:45.079881Z","shell.execute_reply.started":"2023-08-02T11:40:30.488803Z","shell.execute_reply":"2023-08-02T11:40:45.078374Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[2023-08-02 11:40:32,863] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T11:41:14.983901Z","iopub.execute_input":"2023-08-02T11:41:14.984844Z","iopub.status.idle":"2023-08-02T11:41:18.076690Z","shell.execute_reply.started":"2023-08-02T11:41:14.984786Z","shell.execute_reply":"2023-08-02T11:41:18.075394Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"759004eafe1444e29449a41096c6f38c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)enization_chatglm.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e439eb0a5df4b5baa56314bf70c9da7"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- tokenization_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03903b325c0a44b1bdb53a6b423487f7"}},"metadata":{}}]},{"cell_type":"markdown","source":"# 测试<font color = red>tokenizer.encode</font>方法中add_special_tokenizer=True/False的区别 \n# 区别在于为True时会在tokenList首部添加2个特殊字符 id为64790和64792 二者对应的token是空白\"\"","metadata":{}},{"cell_type":"code","source":"string_list =\"我<unk></s>\"\n#string_list =\"11+2+3+4\"\n\nprint(\"add_special_tokens=True\")\ntokenized_res = tokenizer.encode(string_list,add_special_tokens=True,padding=True)\nlogger.info(f\"add_special_tokens=True,tokenized_res={tokenized_res}\")\nprint(tokenizer.batch_decode(tokenized_res,add_special_tokens=False))\nprint(\"add_special_tokens=False\")\ntokenized_res = tokenizer.encode(string_list,add_special_tokens=False,padding=True)\nlogger.info(f\"add_special_tokens=False,tokenized_res={tokenized_res}\")\nprint(tokenizer.batch_decode(tokenized_res,add_special_tokens=False))","metadata":{"execution":{"iopub.status.busy":"2023-08-02T01:47:23.858056Z","iopub.execute_input":"2023-08-02T01:47:23.858441Z","iopub.status.idle":"2023-08-02T01:47:23.869293Z","shell.execute_reply.started":"2023-08-02T01:47:23.858409Z","shell.execute_reply":"2023-08-02T01:47:23.868200Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\u001b[32m2023-08-02 01:47:23.861\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1madd_special_tokens=True,tokenized_res=[64790, 64792, 34211, 31002, 3013, 6029, 30917, 30994]\u001b[0m\n\u001b[32m2023-08-02 01:47:23.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1madd_special_tokens=False,tokenized_res=[34211, 31002, 3013, 6029, 30917, 30994]\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"add_special_tokens=True\n['', '', '我', '<', 'unk', '></', 's', '>']\nadd_special_tokens=False\n['我', '<', 'unk', '></', 's', '>']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 那tokenizer.encode方法 和 <font color=green>tokenizer类方法是不是有什么区别呢？答案是肯定的\n# 根据tokenization_chatglm.py 文件的https://huggingface.co/THUDM/chatglm2-6b/blob/main/tokenization_chatglm.py#L66 位置 model_input_names = [\"input_ids\", \"attention_mask\", \"position_ids\"]\n# 所以最后会返回一个字典 里面是三个k-v， k分别是 [\"input_ids\", \"attention_mask\", \"position_ids\"]\n# attention_mask 前面是 长度为padLen个0 后面全是1  所以其实代码中是left-pad\n# posioton_ids 是从0开始的0，1，2，3，4，数值表示token位置\n# input_ids 是token id 的list \n# 示例如下","metadata":{}},{"cell_type":"code","source":"string_list =\"我<unk></s>\"\n#string_list =\"11+2+3+4\"\n\n\ntokenized_res = tokenizer(string_list,add_special_tokens=True,padding=True)\nlogger.info(f\"add_special_tokens=True,tokenized_res={tokenized_res}\")\ntokenized_res = tokenizer(string_list,add_special_tokens=False,padding=True)\nlogger.info(f\"add_special_tokens=False,tokenized_res={tokenized_res}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-02T01:58:22.257042Z","iopub.execute_input":"2023-08-02T01:58:22.257965Z","iopub.status.idle":"2023-08-02T01:58:22.267426Z","shell.execute_reply.started":"2023-08-02T01:58:22.257922Z","shell.execute_reply":"2023-08-02T01:58:22.266315Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"\u001b[32m2023-08-02 01:58:22.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1madd_special_tokens=True,tokenized_res={'input_ids': [64790, 64792, 34211, 31002, 3013, 6029, 30917, 30994], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1], 'position_ids': [0, 1, 2, 3, 4, 5, 6, 7]}\u001b[0m\n\u001b[32m2023-08-02 01:58:22.262\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1madd_special_tokens=False,tokenized_res={'input_ids': [34211, 31002, 3013, 6029, 30917, 30994], 'attention_mask': [1, 1, 1, 1, 1, 1], 'position_ids': [0, 1, 2, 3, 4, 5]}\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 根据上面的例子 我们发现 如果不需要attention 和 position_ids的话 其实使用tokenizer.encode方法就行了\n# 处理起来更方便 我们来看一个对话的例子\n# 注意chatglm的tokenizer.encode参数中默认bos和eos是没开启的 而且默认没有bos_token 但是有eos_token \n# 可能就根本没有用上这两个token","metadata":{}},{"cell_type":"code","source":"tokenizer.bos_token","metadata":{"execution":{"iopub.status.busy":"2023-08-02T02:09:10.120202Z","iopub.execute_input":"2023-08-02T02:09:10.120606Z","iopub.status.idle":"2023-08-02T02:09:10.126953Z","shell.execute_reply.started":"2023-08-02T02:09:10.120572Z","shell.execute_reply":"2023-08-02T02:09:10.125823Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Using bos_token, but it is not set yet.\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2023-08-02T02:09:53.920809Z","iopub.execute_input":"2023-08-02T02:09:53.921197Z","iopub.status.idle":"2023-08-02T02:09:53.928531Z","shell.execute_reply.started":"2023-08-02T02:09:53.921167Z","shell.execute_reply":"2023-08-02T02:09:53.927356Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'</s>'"},"metadata":{}}]},{"cell_type":"code","source":"\n\ns1 = \"1+2+3+4\"\ntk_s1 = tokenizer.encode(text=s1,add_special_tokens=False)\nprint(tk_s1,type(tk_s1),\"len=\",len(tk_s1))\ns2 = \"我也知道等于10\"\ntk_s2 = tokenizer.encode(text=s2,add_special_tokens=False)\nprint(tk_s2,type(tk_s2),\"len=\",len(tk_s2))\ntotal = tk_s1 + tk_s2\nprint(total,\"len=\",len(total))\nprint(tokenizer.batch_decode(total,skip_special_tokens=True))\n\ninput_ids = total\nlabels = [-100] * len(tk_s1) +tk_s2\n\nprint(f\"input_ids={input_ids}\")\nprint(f\"labels = {labels}\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-02T02:27:28.500951Z","iopub.execute_input":"2023-08-02T02:27:28.501337Z","iopub.status.idle":"2023-08-02T02:27:28.512191Z","shell.execute_reply.started":"2023-08-02T02:27:28.501308Z","shell.execute_reply":"2023-08-02T02:27:28.511096Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"[30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972] <class 'list'> len= 8\n[30910, 33876, 31717, 36738, 30939, 30940] <class 'list'> len= 6\n[30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972, 30910, 33876, 31717, 36738, 30939, 30940] len= 14\n['', '1', '+', '2', '+', '3', '+', '4', '', '我也', '知道', '等于', '1', '0']\ninput_ids=[30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972, 30910, 33876, 31717, 36738, 30939, 30940]\nlabels = [-100, -100, -100, -100, -100, -100, -100, -100, 30910, 33876, 31717, 36738, 30939, 30940]\n","output_type":"stream"}]},{"cell_type":"code","source":"# 试试多轮问答\nQ_temp = \"[Round {}]\\n\\n问：{}\"\nA_temp = \"\\n\\n答：{}\\n\\n\"\nhistory=[(\"我今天想去玩术士。\",\"你要打toc吗？\"),(\"是的，我要买饰品。\",\"老四的裁决？\"),(\"怎么可能？我要买小强的亡者君临\",\"哦哦对 裁决是物理毕业 不是SS的\")]\ninput_ids =[]\nlabels =[]\nfor turn_number,conversation in enumerate(history):\n    #print(turn_number + 1)\n    q = conversation[0]\n    a = conversation[1]\n    #print(Q_temp.format(turn_number+1,q))\n    #print(A_temp.format(a))\n    Q = Q_temp.format(turn_number+1,q)\n    A = A_temp.format(a)\n    Q_token_list = tokenizer.encode(Q,add_special_tokens=False)\n    A_token_list = tokenizer.encode(A,add_special_tokens=False)\n    input_ids.extend(Q_token_list)\n    input_ids.extend(A_token_list)\n    labels.extend([-100]*len(Q_token_list))\n    labels.extend(A_token_list)\nprint(f\"input_ids={input_ids}\")\nprint(f\"labels={labels}\")\n\ninput_ids_token_to_str = tokenizer.batch_decode(input_ids,skip_special_tokens=True)\nlabels_token_to_str = tokenizer.batch_decode(labels,skip_special_tokens=True)\n\nprint(input_ids_token_to_str)\nprint(labels_token_to_str)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T02:50:20.536238Z","iopub.execute_input":"2023-08-02T02:50:20.536711Z","iopub.status.idle":"2023-08-02T02:50:20.560936Z","shell.execute_reply.started":"2023-08-02T02:50:20.536671Z","shell.execute_reply":"2023-08-02T02:50:20.559590Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"input_ids=[790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 54546, 31869, 37580, 55409, 54780, 54930, 31155, 30910, 13, 13, 55437, 31211, 34526, 54819, 30912, 420, 55398, 31514, 13, 13, 790, 30951, 517, 30910, 30943, 30996, 13, 13, 54761, 31211, 38586, 31123, 34034, 55304, 55739, 54667, 31155, 30910, 13, 13, 55437, 31211, 54729, 54795, 54530, 48568, 31514, 13, 13, 790, 30951, 517, 30910, 30966, 30996, 13, 13, 54761, 31211, 52687, 31514, 34034, 55304, 54603, 33026, 55737, 54631, 55835, 55290, 30910, 13, 13, 55437, 31211, 56645, 56645, 54570, 30910, 48568, 54532, 33176, 32121, 30910, 31663, 4800, 54530, 13, 13]\nlabels=[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 34526, 54819, 30912, 420, 55398, 31514, 13, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 54729, 54795, 54530, 48568, 31514, 13, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 56645, 56645, 54570, 30910, 48568, 54532, 33176, 32121, 30910, 31663, 4800, 54530, 13, 13]\n['[', 'R', 'ound', '', '1', ']', '<0x0A>', '<0x0A>', '问', '：', '我', '今天', '想去', '玩', '术', '士', '。', '', '<0x0A>', '<0x0A>', '答', '：', '你要', '打', 't', 'oc', '吗', '？', '<0x0A>', '<0x0A>', '[', 'R', 'ound', '', '2', ']', '<0x0A>', '<0x0A>', '问', '：', '是的', '，', '我要', '买', '饰', '品', '。', '', '<0x0A>', '<0x0A>', '答', '：', '老', '四', '的', '裁决', '？', '<0x0A>', '<0x0A>', '[', 'R', 'ound', '', '3', ']', '<0x0A>', '<0x0A>', '问', '：', '怎么可能', '？', '我要', '买', '小', '强的', '亡', '者', '君', '临', '', '<0x0A>', '<0x0A>', '答', '：', '哦', '哦', '对', '', '裁决', '是', '物理', '毕业', '', '不是', 'SS', '的', '<0x0A>', '<0x0A>']\n['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '<0x0A>', '<0x0A>', '答', '：', '你要', '打', 't', 'oc', '吗', '？', '<0x0A>', '<0x0A>', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '<0x0A>', '<0x0A>', '答', '：', '老', '四', '的', '裁决', '？', '<0x0A>', '<0x0A>', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '<0x0A>', '<0x0A>', '答', '：', '哦', '哦', '对', '', '裁决', '是', '物理', '毕业', '', '不是', 'SS', '的', '<0x0A>', '<0x0A>']\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nwith open(\"多轮对话数据.json\",\"w\",encoding=\"utf-8\") as fw :\n    dialogues =[\n    {\"history\":[(\"我今天想去玩术士。\",\"你要打toc吗？\"),(\"是的，我要买饰品。\",\"老四的裁决？\"),(\"怎么可能？我要买小强的亡者君临\",\"哦哦对 裁决是物理毕业 不是SS的\")]},\n    {\"history\":[(\"今天爆红\",\"出裁决了？\"),(\"是的，还出了浮冰长靴。\",\"666啊老铁\")]},\n    {\"history\":[(\"今天icc爆红\",\"出望远镜还是血毒了？\")]},\n    {\"history\":[(\"晚上25Hicc你要买啥？\",\"我估计要存钱买望远镜，或者冰龙的饰品。\")]},\n    {\"history\":[(\"上周wcl我战士小萨打了1W4秒伤\",\"那多少分了？\"),(\"98.89\",\"服务器第一就是你。\")]},\n    {\"history\":[(\"Htoc五十箱队伍来个橙分SM\",\"电撒还是ZQS\"),(\"我要奶萨\",\"可以我奶元双休\")]},\n    {\"history\":[(\"你这个术士的装备真tm的壕，277毕业，连红玉的饰品都有。\",\"这个号是我买的，淘宝上4000块。\"),(\"我靠，有钱。\",\"一般吧。\")]}, \n    {\"history\":[(\"为啥我装等都264了 dps还这么低？\",\"你是不是没卡药没开爆发？\"),(\"我记得偷了药的，加速和烈焰菇都吃了\",\"等会，我看你好像技能用的低等级的\"),(\"啊？我去看看等级。。。。X 还真是\",\"哈哈，亏你还打了这么久没发现，也是服了。\")]}, \n    {\"history\":[(\"我看这个团长要黑G\",\"才卖了2万就黑？\"),(\"我看了下它装备，一身环保。感觉不靠谱啊。\",\"我去问问它啥意思\")]}, \n    {\"history\":[(\"啥时候开的新三本？\",\"上周2\"),(\"你们今天清了cd没？要不一起？\",\"可以，等我换ms\")]},     \n    ]\n    for item in dialogues:\n        line = json.dumps(item,ensure_ascii=False)\n        fw.write(line+\"\\n\")\nprint(123)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T06:11:47.052686Z","iopub.execute_input":"2023-08-02T06:11:47.053108Z","iopub.status.idle":"2023-08-02T06:11:47.067915Z","shell.execute_reply.started":"2023-08-02T06:11:47.053076Z","shell.execute_reply":"2023-08-02T06:11:47.066712Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"123\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nwith open(\"sharegpt_format_conv.json\",'w',encoding='utf-8') as fw :\n    data = [\n        {\"conversations\":[{\"from\":\"human\",\"value\":\"请为面向在线学习和个人辅导的新SaaS平台提供一些用于SEO的关键词。\"},{\"from\":\"gpt\",\"value\":\"这里是一些关于新的SaaS平台的潜在SEO关键词，该平台专注于在线学习和私人辅导：\\n* 在线学习平台\\n* 私人辅导平台\\n* E-learning解决方案\\n* 辅导软件\\n* 虚拟辅导\\n* 在线辅导服务\\n* 私人教育平台\\n* 虚拟课堂软件\\n* 远程学习平台\\n* 在线教学工具\"},{\"from\":\"human\",\"value\":\"如何利用这些关键词来扩大市场推广和认知？\"},{\"from\":\"gpt\",\"value\":\"为了扩大市场覆盖率和提高认知度，您可以通过以下几种方式使用这些关键词：\\n* 将它们融入到您的网站内容中，例如主页、产品页面和博客文章中。这将帮助搜索引擎了解您的网站内容，提高其在搜索结果中的排名。\\n* 在您的网站元标题和元描述中使用它们，为潜在客户清晰地展示您的平台提供的服务。\\n* 在社交媒体帖子、新闻通讯等营销材料中使用它们，以扩大受众范围。\\n* 在付费广告活动中使用它们，例如 Google AdWords，以针对特定关键词并吸引正在寻找在线学习或私人辅导解决方案的潜在客户。\\n* 优化您的网站以适应这些关键词，以提高它在搜索结果中的可见性，并使潜在客户更容易找到您。这可能包括优化您的网站结构、创建高质量内容以及从其他权威网站建立反向链接。\"},{\"from\":\"human\",\"value\":\"给我一个使用这些关键词进行付费广告活动的策略。\"},{\"from\":\"gpt\",\"value\":\"以下是在付费广告活动中使用这些关键词的潜在策略：\\n1.从您提供的列表中识别最相关和高效的关键词。您可以使用关键词研究工具（如Google的Keyword Planner）来帮助您识别具有高搜索量和低竞争的关键词。\\n2.将您的关键词组织成主题组或“广告组”。例如，您可以为与在线学习，私人教导和电子学习解决方案相关的关键词创建广告组。\\n3.使用这些广告组创建针对性的广告，专门设计以吸引搜索这些关键词的潜在客户。例如，您可以为面向“虚拟课堂软件”和“远程学习平台”的关键词创建广告，以此来为您的在线学习平台打广告。\\n4.设置您的广告以在搜索结果中出现，当潜在客户搜索这些关键词时能够看到您的广告。这将有助于确保您的广告在正确的时间被正确的人看到。\\n5.监控您广告的表现，并根据需要调整您的关键词策略。这可能涉及添加新关键词，删除表现不佳的关键词或调整出价以提高广告的表现。\\n通过遵循这一策略，您可以有效地利用关键词来吸引潜在客户，并在付费广告活动中提高您的SaaS平台的可见性。\"}]},\n        {\"conversations\":[{\"from\":\"human\",\"value\":\"谁是Soroush Dalili？请详细说明。\"},{\"from\":\"gpt\",\"value\":\"Soroush Dalili是一名知名的网络安全研究员和顾问，拥有丰富的经验。他参与了许多与信息安全相关的高端项目和倡议，包括漏洞研究、渗透测试和Web应用安全。\\nDalili是几本网络安全和Web应用安全的著作的作者，包括《保护ASP.NET Web应用程序》和《高级ASP.NET安全》。他还是备受欢迎的开源漏洞扫描工具“DotNetNuke Scanner”的创作者。\\n除了作为作者和工具创作者的工作外，Dalili还是网络安全社区中备受追捧的演讲者和培训师。他在各种会议和活动中发表演讲和主持研讨会，包括黑帽大会、OWASP和Infosecurity Europe。\\nDalili是负责任的披露和道德黑客实践的坚定倡导者。他的贡献得到了网络安全社区的认可，包括被授予微软MVP企业安全奖、ISC2信息安全领导奖和年度道德黑客奖。\\n总的来说，Soroush Dalili是网络安全行业中备受尊敬和有影响力的人物，以他在Web应用安全领域的专业知识、对社区的贡献以及对该领域负责任和道德实践的承诺而闻名。\"},{\"from\":\"human\",\"value\":\"你确定吗？其中许多事情是不准确或无效的。\"},{\"from\":\"gpt\",\"value\":\"如果我提供的信息有任何错误或无效，我深表歉意。作为一个自然语言处理模型，我的回答基于训练数据集中可用的数据。如果有具体的错误点，请让我知道，我会尽力纠正。\\n关于Soroush Dalili，我的先前回答是基于我知道的截至2021年9月的信息。如果他的职业或工作有任何最近的发展或更改，我可能不知道。\"}]},\n        {\"conversations\":[{\"from\":\"human\",\"value\":\"1800年代最显著的特征是什么？\"},{\"from\":\"gpt\",\"value\":\"十九世纪又称为1800年代，是一个显著的文化、经济和政治变革的时期。1800年代的一些关键特点包括：\\n1. 工业革命：1800年代是工业革命开始的时期，是一个工业化和技术进步迅速的时期。在此期间，新机器和工厂彻底改变了商品生产方式，促进了大规模生产和城市的增长。\\n2. 政治和社会变革：1800年代有重大的政治和社会变革，包括许多国家废除奴隶制度、民主和选举权运动的扩张、民族主义和帝国主义的兴起等。\\n3. 欧洲帝国的扩张：1800年代见证了欧洲帝国的不断扩张，欧洲列强试图扩大其在世界其他地区的影响力和控制力。这导致了许多非洲、亚洲和美洲地区被殖民，以及领土和资源的冲突。\\n4. 科技进步：1800年代有重大的科技进步，包括蒸汽机、电报和铁路的发展。这些进步对交通和通讯产生了重大影响，并有助于塑造现代世界。\\n5. 文化和艺术运动：1800年代也是文化和艺术创新的时期，浪漫主义、现实主义和印象主义在文学、艺术和音乐等领域中兴起。这些运动有助于塑造人们的思想和表达方式，并影响了20世纪艺术表现的发展。\"},{\"from\":\"human\",\"value\":\"在19世纪，是蒸汽船还是火车对世界的影响更大？\"},{\"from\":\"gpt\",\"value\":\"蒸汽船和火车在19世纪都具有重要影响力，因为它们都有助于在工业革命期间改革运输和通讯。\\n\\n蒸汽船由蒸汽引擎驱动，允许人和货物在河流和水道上进行运输。它们在贸易和商业的发展中发挥了关键作用，特别是在美国，并帮助开拓了新的地区以进行定居和扩张。\\n\\n火车也由蒸汽引擎驱动，可以快速运输人和货物。它们有助于连接城市和地区，在工业和商业的扩展中发挥了关键作用。火车还有助于促进城市的增长和城市中心的发展。\\n\\n总的来说，很难说蒸汽船和火车谁的影响更大，因为它们在19世纪的运输和通讯发展中都发挥了重要作用。但是，火车可能具有更广泛的影响，因为它们可以连接更多地区，便于更远距离货物的运输。\"},{\"from\":\"human\",\"value\":\"火车是如何影响城市的？\"},{\"from\":\"gpt\",\"value\":\"火车在19世纪及以后对城市产生了重大影响。火车对城市的影响包括：\\n\\n1. 促进城市的增长：火车使人们和货物快速运输成为可能，促进了城市的增长。随着城市通过铁路网与其它地区联系更加紧密，吸引了更多的居民和企业，导致城市中心扩张。\\n\\n2. 形塑城市发展：火车站和铁路线的建设对城市的布局和发展产生了重大影响。在许多情况下，城市围绕火车站发展起来，周边的商业和住宅也随之涌现。火车的存在还影响了工业的位置和货物在城市中的分配方式。\\n\\n3. 连接城市与更广泛的地区：火车帮助连接城市与更广泛的地区，使人们能够轻松地在城市之间和国家其他地方旅行。这促进了思想、货物和服务的交流，并有助于刺激经济增长。\\n\\n4. 改变了人们的生活和工作方式：火车对城市居民的生活和工作方式产生了重大影响。它们促进了货物和人员的快速运输，使人们更容易在城市的不同地区生活和工作。这导致了郊区的发展和通勤文化的增长。\"},{\"from\":\"human\",\"value\":\"汽车在19世纪末和20世纪初对城市产生了什么影响？\"},{\"from\":\"gpt\",\"value\":\"汽车在19世纪末20世纪初对城市产生了重大影响，随着汽车越来越普遍且价格逐渐变得更加实惠，人们的生活方式、工作方式等方面发生了重大变化。汽车对城市产生的影响包括：\\n1. 改变人们的生活和工作方式：汽车让人们在城市内部和城市之间更加容易地进行出行，从而对人们的生活和工作产生了重大影响。人们更容易居住在郊区并通勤上下班，这引发了郊区化现象的发展和通勤文化的增长。\\n2. 形塑城市的发展：汽车的日益普及导致城市规划和发展方式发生了变化。例如，城市开始优先建设道路和高速公路以容纳越来越多的汽车，停车成为城市规划中的重要考虑因素。\\n3. 改变货物运输方式：汽车彻底改变了城市内外货物运输的方式。它们使得货物在短距离内能够快速运输，这导致一些货物的铁路运输逐渐减少。\\n4. 影响环境：城市中汽车数量的增加也对环境产生了影响，因为燃烧化石燃料用于交通运输加剧了空气污染和气候变化。\\n总的来说，在19世纪末20世纪初，汽车对城市的影响是非常显著的，它们改变了人们的生活和工作方式，塑造了城市的发展趋势，并对环境造成了影响。\"},{\"from\":\"human\",\"value\":\"19世纪吊桥对城市的影响是什么？\"},{\"from\":\"gpt\",\"value\":\"吊桥在19世纪对城市产生了显著的影响，因为它们允许建造长而坚固的桥梁，可以跨越宽阔的水体或山谷。吊桥对城市产生影响的方式包括：\\n1. 连接城市：吊桥允许在河流和其他水体上建造桥梁，连接起原来被这些天然屏障隔绝的城市。这促进了物品，思想和人员之间的交流，并帮助刺激了经济增长。\\n2. 塑造城市发展：吊桥的建造对城市的布局和发展产生了重大影响。在许多情况下，城市围绕桥梁建设的地点突显出，因为这些地区成为了贸易和商业的枢纽。\\n3. 促进城市增长：吊桥允许城市跨越河流和其他水体的扩张，因为它们让人们和货物可以在两侧之间轻松地旅行。这促进了城市的增长以及新城区和街区的发展。\\n4. 改善交通：吊桥也改善了城市内部和城市之间的交通，因为它们允许在河流和其他水体上建造道路和铁路。这使人和货物更容易移动，促进了贸易和商业的增长。\\n总的来说，吊桥在19世纪对城市产生了重大影响，因为它们连接了城市，塑造了城市的发展，促进了城市的增长，并改善了交通。\"},{\"from\":\"human\",\"value\":\"如果一个以19世纪为主题的城市建造游戏没有包含与火车或汽车有关的重要游戏元素，它会被认为是完整的吗？\"},{\"from\":\"gpt\",\"value\":\"由城市建造游戏设计者决定他们希望在游戏中包括哪些游戏玩法元素。然而，在19世纪的城市建造游戏中，火车和汽车可能会被认为是重要的元素，因为它们都对城市的发展产生了重要影响。火车和汽车改变了人们的生活和工作方式，塑造了城市的布局和发展，并促进了商品和人员的交流。它们还在工业和商业的扩张以及城市中心的增长中起着关键性作用。也可以制作一个19世纪的城市建造游戏，其中不包括涉及火车或汽车的重要游戏玩法元素。在此类游戏中可以包括许多其他因素，如悬索桥的建设、工业和商业的发展以及人口和城市中心的增长。\"},{\"from\":\"human\",\"value\":\"有意思。谢谢你的见解👍\"},{\"from\":\"gpt\",\"value\":\"不客气，我很高兴能帮到你。如果你还有其他问题，请随时问我。\"}]},\n\n    ]\n    for item in data :\n        line = json.dumps(item,ensure_ascii=False)\n        fw.write(line)\nprint(125)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T08:55:37.737328Z","iopub.execute_input":"2023-08-02T08:55:37.737763Z","iopub.status.idle":"2023-08-02T08:55:37.761000Z","shell.execute_reply.started":"2023-08-02T08:55:37.737729Z","shell.execute_reply":"2023-08-02T08:55:37.759829Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"125\n","output_type":"stream"}]},{"cell_type":"code","source":"cat 多轮对话数据.json","metadata":{"execution":{"iopub.status.busy":"2023-08-02T11:42:07.872673Z","iopub.execute_input":"2023-08-02T11:42:07.873325Z","iopub.status.idle":"2023-08-02T11:42:08.984278Z","shell.execute_reply.started":"2023-08-02T11:42:07.873278Z","shell.execute_reply":"2023-08-02T11:42:08.982917Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"{\"history\": [[\"我今天想去玩术士。\", \"你要打toc吗？\"], [\"是的，我要买饰品。\", \"老四的裁决？\"], [\"怎么可能？我要买小强的亡者君临\", \"哦哦对 裁决是物理毕业 不是SS的\"]]}\n{\"history\": [[\"今天爆红\", \"出裁决了？\"], [\"是的，还出了浮冰长靴。\", \"666啊老铁\"]]}\n{\"history\": [[\"今天icc爆红\", \"出望远镜还是血毒了？\"]]}\n{\"history\": [[\"晚上25Hicc你要买啥？\", \"我估计要存钱买望远镜，或者冰龙的饰品。\"]]}\n{\"history\": [[\"上周wcl我战士小萨打了1W4秒伤\", \"那多少分了？\"], [\"98.89\", \"服务器第一就是你。\"]]}\n{\"history\": [[\"Htoc五十箱队伍来个橙分SM\", \"电撒还是ZQS\"], [\"我要奶萨\", \"可以我奶元双休\"]]}\n{\"history\": [[\"你这个术士的装备真tm的壕，277毕业，连红玉的饰品都有。\", \"这个号是我买的，淘宝上4000块。\"], [\"我靠，有钱。\", \"一般吧。\"]]}\n{\"history\": [[\"为啥我装等都264了 dps还这么低？\", \"你是不是没卡药没开爆发？\"], [\"我记得偷了药的，加速和烈焰菇都吃了\", \"等会，我看你好像技能用的低等级的\"], [\"啊？我去看看等级。。。。X 还真是\", \"哈哈，亏你还打了这么久没发现，也是服了。\"]]}\n{\"history\": [[\"我看这个团长要黑G\", \"才卖了2万就黑？\"], [\"我看了下它装备，一身环保。感觉不靠谱啊。\", \"我去问问它啥意思\"]]}\n{\"history\": [[\"啥时候开的新三本？\", \"上周2\"], [\"你们今天清了cd没？要不一起？\", \"可以，等我换ms\"]]}\n","output_type":"stream"}]},{"cell_type":"code","source":"# 1000 sample too long don't print\n#cat /kaggle/input/sharegpt-zh-1k/sharegpt_zh_1K_format.jsonl","metadata":{"execution":{"iopub.status.busy":"2023-08-02T11:44:58.420275Z","iopub.execute_input":"2023-08-02T11:44:58.420787Z","iopub.status.idle":"2023-08-02T11:44:58.427076Z","shell.execute_reply.started":"2023-08-02T11:44:58.420753Z","shell.execute_reply":"2023-08-02T11:44:58.425692Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"3//2*2","metadata":{"execution":{"iopub.status.busy":"2023-08-02T09:03:01.893391Z","iopub.execute_input":"2023-08-02T09:03:01.893906Z","iopub.status.idle":"2023-08-02T09:03:01.905653Z","shell.execute_reply.started":"2023-08-02T09:03:01.893868Z","shell.execute_reply":"2023-08-02T09:03:01.904348Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"my_dataset['train'].filter(lambda example : len(example[\"conversations\"])>1)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T11:48:52.527247Z","iopub.execute_input":"2023-08-02T11:48:52.527736Z","iopub.status.idle":"2023-08-02T11:48:52.751879Z","shell.execute_reply.started":"2023-08-02T11:48:52.527697Z","shell.execute_reply":"2023-08-02T11:48:52.750850Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['conversations'],\n    num_rows: 999\n})"},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_function_sharegpt(example,tokenizer):\n    \"\"\"\n       多轮对话使用每个example（也就是一行json样本）中的example['history'] 拼接多轮对话 构建一个包含多轮对话的总的input_ids和总的labels\n       这个Q_temp和A_temp 不同的model 都不一样 但是很重要 \n       这里用chatglm2-6b官网的tokenization_chatglm.py中的\n    \"\"\"\n    Q_temp = \"[Round {}]\\n\\n问：{}\"\n    A_temp = \"\\n\\n答：{}\\n\\n\"\n    input_ids =[]\n    labels =[]\n    must_be_even_len = len(example['conversations']) //2 *2\n    if must_be_even_len == 0:\n        print(f\"must_be_even_len=0!\")\n        print(f\"长度不足的example={example}\")\n        raise ValueError(\"example出现了长度不足2的异常。\")\n    #assert must_be_even_len > 0\n    # must_be_even_len > 0 这个 条件由filter保证\n    # my_dataset['train'].filter(lambda example : len(example[\"conversations\"])>1)\n    \n    for idx in range(0,must_be_even_len,2):\n        #print(turn_number + 1)\n        q = example['conversations'][idx]['value']\n        a = example['conversations'][idx+1]['value']\n        #print(Q_temp.format(turn_number+1,q))\n        #print(A_temp.format(a))\n        Q = Q_temp.format(int(idx//2)+1,q)\n        A = A_temp.format(a)\n        Q_token_list = tokenizer.encode(Q,add_special_tokens=False)\n        A_token_list = tokenizer.encode(A,add_special_tokens=False)\n        input_ids.extend(Q_token_list)\n        input_ids.extend(A_token_list)\n        labels.extend([-100]*len(Q_token_list))\n        labels.extend(A_token_list)\n    #print(f\"input_ids={input_ids}\")\n    #print(f\"labels={labels}\")\n    assert len(input_ids) == len(labels) # 这里一定要保证长度相等 不然说明拼接出问题\n    #input_ids_token_to_str = tokenizer.batch_decode(input_ids,skip_special_tokens=True)\n    #labels_token_to_str = tokenizer.batch_decode(labels,skip_special_tokens=True)\n    #print(input_ids_token_to_str)\n    #print(labels_token_to_str)\n    \n    return({\"input_ids\":input_ids , \"labels\":labels})","metadata":{"execution":{"iopub.status.busy":"2023-08-02T11:54:12.337470Z","iopub.execute_input":"2023-08-02T11:54:12.338227Z","iopub.status.idle":"2023-08-02T11:54:12.351115Z","shell.execute_reply.started":"2023-08-02T11:54:12.338183Z","shell.execute_reply":"2023-08-02T11:54:12.349780Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndata_files_list = \"/kaggle/input/sharegpt-zh-1k/sharegpt_zh_1K_format.jsonl\"\nmy_dataset = load_dataset('json', data_files=data_files_list)\nprint(my_dataset)\n#print(my_dataset['train']['conversations'])","metadata":{"execution":{"iopub.status.busy":"2023-08-02T11:45:56.532871Z","iopub.execute_input":"2023-08-02T11:45:56.533340Z","iopub.status.idle":"2023-08-02T11:45:57.593880Z","shell.execute_reply.started":"2023-08-02T11:45:56.533304Z","shell.execute_reply":"2023-08-02T11:45:57.592611Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-11d1f3cfe85b31b5/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee28d9660e140e5b48c204d488c55a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0241bf6a6412464fa3af82beacf2de0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-11d1f3cfe85b31b5/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5665953f12e4a3aa931e83ecc0c7c0b"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['conversations'],\n        num_rows: 1000\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"a=\"AlpA\"\na.strip().lower()","metadata":{"execution":{"iopub.status.busy":"2023-08-02T12:04:49.633533Z","iopub.execute_input":"2023-08-02T12:04:49.634044Z","iopub.status.idle":"2023-08-02T12:04:49.642364Z","shell.execute_reply.started":"2023-08-02T12:04:49.634006Z","shell.execute_reply":"2023-08-02T12:04:49.641166Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"'alpa'"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\ntokenized_dataset = my_dataset['train'].filter(lambda example : len(example[\"conversations\"])>1).map(\n                                lambda example: tokenize_function_sharegpt(example, tokenizer=tokenizer),\n                                batched=False, \n                                remove_columns=my_dataset['train'].column_names)\nprint(f\"tokenized_dataset={tokenized_dataset}\")\nprint(f\"tokenizer.decode(tokenized_dataset[-1]['input_ids'],skip_special_tokens=False)=\\n{tokenizer.decode(tokenized_dataset[-1]['input_ids'],skip_special_tokens=False)}\")\nprint(f\"tokenizer.decode(tokenized_dataset[-1]['labels'],skip_special_tokens=False)=\\n{tokenizer.decode(tokenized_dataset[-1]['labels'],skip_special_tokens=False)}\")\nprint(f\"tokenized_dataset[-1]['input_ids']=\\n{tokenized_dataset[-1]['input_ids']}\")\nprint(f\"tokenized_dataset[-1]['labels']=\\n{tokenized_dataset[-1]['labels']}\")\nprint(f\"len(tokenized_dataset[-1]['input_ids']={len(tokenized_dataset[-1]['input_ids'])}\")\nprint(f\"len(tokenized_dataset[-1]['labels']={len(tokenized_dataset[-1]['labels'])}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-02T11:52:22.227854Z","iopub.execute_input":"2023-08-02T11:52:22.228285Z","iopub.status.idle":"2023-08-02T11:52:23.051047Z","shell.execute_reply.started":"2023-08-02T11:52:22.228253Z","shell.execute_reply":"2023-08-02T11:52:23.050091Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"tokenized_dataset=Dataset({\n    features: ['input_ids', 'labels'],\n    num_rows: 999\n})\ntokenizer.decode(tokenized_dataset[-1]['input_ids'],skip_special_tokens=False)=\n[Round 1]\n\n问：为一个3D引擎编写绑定C++到F#的测试案例。 \n\n答：这是一个将C++ 3D引擎绑定到F#的样例测试用例：\n测试用例标题：将C++ 3D引擎绑定到F#\n测试目标：验证C++ 3D引擎成功地绑定到F#，并可以在F#环境中用于创建3D模型和场景。\n测试步骤：\n1. 创建一个具有创建3D模型和场景所需函数的C++ 3D引擎。\n2. 使用F#Interop绑定将C++ 3D引擎绑定到F#环境。\n3. 通过在F#环境中测试C++ 3D引擎函数来验证绑定过程是否成功。\n期望结果：\n* C++ 3D引擎函数在F#环境中应可用。\n* 使用绑定的C++ 3D引擎，F#环境应能够创建3D模型和场景。\n* 测试结果应与在C++环境中创建3D模型和场景的预期结果相匹配。\n\n\ntokenizer.decode(tokenized_dataset[-1]['labels'],skip_special_tokens=False)=\n\n\n答：这是一个将C++ 3D引擎绑定到F#的样例测试用例：\n测试用例标题：将C++ 3D引擎绑定到F#\n测试目标：验证C++ 3D引擎成功地绑定到F#，并可以在F#环境中用于创建3D模型和场景。\n测试步骤：\n1. 创建一个具有创建3D模型和场景所需函数的C++ 3D引擎。\n2. 使用F#Interop绑定将C++ 3D引擎绑定到F#环境。\n3. 通过在F#环境中测试C++ 3D引擎函数来验证绑定过程是否成功。\n期望结果：\n* C++ 3D引擎函数在F#环境中应可用。\n* 使用绑定的C++ 3D引擎，F#环境应能够创建3D模型和场景。\n* 测试结果应与在C++环境中创建3D模型和场景的预期结果相匹配。\n\n\ntokenized_dataset[-1]['input_ids']=\n[790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 53713, 30966, 30952, 36577, 40536, 51935, 30942, 5240, 54555, 30960, 31010, 54530, 32882, 33301, 31155, 30910, 13, 13, 55437, 31211, 36037, 54687, 30942, 5240, 30910, 30966, 30952, 36577, 51935, 54555, 30960, 31010, 54530, 54741, 55071, 32882, 54571, 55071, 31211, 13, 32882, 54571, 55071, 34490, 31211, 54687, 30942, 5240, 30910, 30966, 30952, 36577, 51935, 54555, 30960, 31010, 13, 32882, 31919, 31211, 35526, 30942, 5240, 30910, 30966, 30952, 36577, 51702, 51935, 54555, 30960, 31010, 31123, 54724, 34067, 30960, 31010, 42113, 32419, 32991, 30966, 30952, 34030, 54542, 33356, 31155, 13, 32882, 35598, 31211, 13, 30939, 30930, 30910, 32991, 31623, 31817, 32991, 30966, 30952, 34030, 54542, 33356, 34831, 36786, 54530, 30942, 5240, 30910, 30966, 30952, 36577, 31155, 13, 30943, 30930, 50085, 30960, 31010, 9115, 404, 51935, 54687, 30942, 5240, 30910, 30966, 30952, 36577, 51935, 54555, 30960, 31010, 31747, 31155, 13, 30966, 30930, 52601, 54534, 30960, 31010, 42113, 32882, 30942, 5240, 30910, 30966, 30952, 36577, 36786, 54556, 35526, 51935, 31807, 32034, 31861, 31155, 13, 35221, 31951, 31211, 13, 30998, 319, 5240, 30910, 30966, 30952, 36577, 36786, 54534, 30960, 31010, 42113, 54720, 38482, 31155, 13, 30998, 50085, 57370, 32081, 30942, 5240, 30910, 30966, 30952, 36577, 31123, 30960, 31010, 31747, 54720, 31759, 32991, 30966, 30952, 34030, 54542, 33356, 31155, 13, 30998, 30910, 32882, 31951, 54720, 54619, 54534, 30942, 5240, 42113, 32991, 30966, 30952, 34030, 54542, 33356, 54530, 34633, 31951, 54669, 36336, 31155, 13, 13]\ntokenized_dataset[-1]['labels']=\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 36037, 54687, 30942, 5240, 30910, 30966, 30952, 36577, 51935, 54555, 30960, 31010, 54530, 54741, 55071, 32882, 54571, 55071, 31211, 13, 32882, 54571, 55071, 34490, 31211, 54687, 30942, 5240, 30910, 30966, 30952, 36577, 51935, 54555, 30960, 31010, 13, 32882, 31919, 31211, 35526, 30942, 5240, 30910, 30966, 30952, 36577, 51702, 51935, 54555, 30960, 31010, 31123, 54724, 34067, 30960, 31010, 42113, 32419, 32991, 30966, 30952, 34030, 54542, 33356, 31155, 13, 32882, 35598, 31211, 13, 30939, 30930, 30910, 32991, 31623, 31817, 32991, 30966, 30952, 34030, 54542, 33356, 34831, 36786, 54530, 30942, 5240, 30910, 30966, 30952, 36577, 31155, 13, 30943, 30930, 50085, 30960, 31010, 9115, 404, 51935, 54687, 30942, 5240, 30910, 30966, 30952, 36577, 51935, 54555, 30960, 31010, 31747, 31155, 13, 30966, 30930, 52601, 54534, 30960, 31010, 42113, 32882, 30942, 5240, 30910, 30966, 30952, 36577, 36786, 54556, 35526, 51935, 31807, 32034, 31861, 31155, 13, 35221, 31951, 31211, 13, 30998, 319, 5240, 30910, 30966, 30952, 36577, 36786, 54534, 30960, 31010, 42113, 54720, 38482, 31155, 13, 30998, 50085, 57370, 32081, 30942, 5240, 30910, 30966, 30952, 36577, 31123, 30960, 31010, 31747, 54720, 31759, 32991, 30966, 30952, 34030, 54542, 33356, 31155, 13, 30998, 30910, 32882, 31951, 54720, 54619, 54534, 30942, 5240, 42113, 32991, 30966, 30952, 34030, 54542, 33356, 54530, 34633, 31951, 54669, 36336, 31155, 13, 13]\nlen(tokenized_dataset[-1]['input_ids']=237\nlen(tokenized_dataset[-1]['labels']=237\n","output_type":"stream"}]},{"cell_type":"code","source":"# # load_data\n# dialogue ={\n#     \"history\":[(\"我今天想去玩术士。\",\"你要打toc吗？\"),(\"是的，我要买饰品。\",\"老四的裁决？\"),(\"怎么可能？我要买小强的亡者君临\",\"哦哦对 裁决是物理毕业 不是SS的\")],\n# }\n# # import json\n# # j_dia=json.dumps(dialogue)\n# # print(j_dia)\nfrom datasets import load_dataset\ndata_files_list = \"多轮对话数据.json\"\nmy_dataset = load_dataset('json', data_files=data_files_list)\nprint(my_dataset)\nprint(my_dataset['train']['history'])","metadata":{"execution":{"iopub.status.busy":"2023-08-02T06:17:03.271660Z","iopub.execute_input":"2023-08-02T06:17:03.272734Z","iopub.status.idle":"2023-08-02T06:17:04.193062Z","shell.execute_reply.started":"2023-08-02T06:17:03.272680Z","shell.execute_reply":"2023-08-02T06:17:04.190940Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-fbfec77ab37fdad4/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db39fa553f5a4187b0bffe4e818a6e17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fcc755f123f48f191bb7e883e67d2b6"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-fbfec77ab37fdad4/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08ae83b17fad46b4aad00669f42ebbe7"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['history'],\n        num_rows: 10\n    })\n})\n[[['我今天想去玩术士。', '你要打toc吗？'], ['是的，我要买饰品。', '老四的裁决？'], ['怎么可能？我要买小强的亡者君临', '哦哦对 裁决是物理毕业 不是SS的']], [['今天爆红', '出裁决了？'], ['是的，还出了浮冰长靴。', '666啊老铁']], [['今天icc爆红', '出望远镜还是血毒了？']], [['晚上25Hicc你要买啥？', '我估计要存钱买望远镜，或者冰龙的饰品。']], [['上周wcl我战士小萨打了1W4秒伤', '那多少分了？'], ['98.89', '服务器第一就是你。']], [['Htoc五十箱队伍来个橙分SM', '电撒还是ZQS'], ['我要奶萨', '可以我奶元双休']], [['你这个术士的装备真tm的壕，277毕业，连红玉的饰品都有。', '这个号是我买的，淘宝上4000块。'], ['我靠，有钱。', '一般吧。']], [['为啥我装等都264了 dps还这么低？', '你是不是没卡药没开爆发？'], ['我记得偷了药的，加速和烈焰菇都吃了', '等会，我看你好像技能用的低等级的'], ['啊？我去看看等级。。。。X 还真是', '哈哈，亏你还打了这么久没发现，也是服了。']], [['我看这个团长要黑G', '才卖了2万就黑？'], ['我看了下它装备，一身环保。感觉不靠谱啊。', '我去问问它啥意思']], [['啥时候开的新三本？', '上周2'], ['你们今天清了cd没？要不一起？', '可以，等我换ms']]]\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_function(example,tokenizer):\n    \"\"\"\n       多轮对话使用每个example（也就是一行json样本）中的example['history'] 拼接多轮对话 构建一个包含多轮对话的总的input_ids和总的labels\n       这个Q_temp和A_temp 不同的model 都不一样 但是很重要 \n       这里用chatglm2-6b官网的tokenization_chatglm.py中的\n    \"\"\"\n    Q_temp = \"[Round {}]\\n\\n问：{}\"\n    A_temp = \"\\n\\n答：{}\\n\\n\"\n    input_ids =[]\n    labels =[]\n    for turn_number,conversation in enumerate(example['history']):\n        #print(turn_number + 1)\n        q = conversation[0]\n        a = conversation[1]\n        #print(Q_temp.format(turn_number+1,q))\n        #print(A_temp.format(a))\n        Q = Q_temp.format(turn_number+1,q)\n        A = A_temp.format(a)\n        Q_token_list = tokenizer.encode(Q,add_special_tokens=False)\n        A_token_list = tokenizer.encode(A,add_special_tokens=False)\n        input_ids.extend(Q_token_list)\n        input_ids.extend(A_token_list)\n        labels.extend([-100]*len(Q_token_list))\n        labels.extend(A_token_list)\n    #print(f\"input_ids={input_ids}\")\n    #print(f\"labels={labels}\")\n    assert len(input_ids) == len(labels) # 这里一定要保证长度相等 不然说明拼接出问题\n    #input_ids_token_to_str = tokenizer.batch_decode(input_ids,skip_special_tokens=True)\n    #labels_token_to_str = tokenizer.batch_decode(labels,skip_special_tokens=True)\n    #print(input_ids_token_to_str)\n    #print(labels_token_to_str)\n    \n    return({\"input_ids\":input_ids , \"labels\":labels})","metadata":{"execution":{"iopub.status.busy":"2023-08-02T07:13:09.493951Z","iopub.execute_input":"2023-08-02T07:13:09.494417Z","iopub.status.idle":"2023-08-02T07:13:09.505895Z","shell.execute_reply.started":"2023-08-02T07:13:09.494383Z","shell.execute_reply":"2023-08-02T07:13:09.504194Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\ntokenized_dataset = my_dataset['train'].map(\n                                lambda example: tokenize_function(example, tokenizer=tokenizer),\n                                batched=False, \n                                remove_columns=my_dataset['train'].column_names)\nprint(f\"tokenized_dataset={tokenized_dataset}\")\nprint(f\"tokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)=\\n{tokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)}\")\nprint(f\"tokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)=\\n{tokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)}\")\nprint(f\"tokenized_dataset[0]['input_ids']=\\n{tokenized_dataset[0]['input_ids']}\")\nprint(f\"tokenized_dataset[0]['labels']=\\n{tokenized_dataset[0]['labels']}\")\nprint(f\"len(tokenized_dataset[0]['input_ids']={len(tokenized_dataset[0]['input_ids'])}\")\nprint(f\"len(tokenized_dataset[0]['labels']={len(tokenized_dataset[0]['labels'])}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-02T07:13:14.652443Z","iopub.execute_input":"2023-08-02T07:13:14.652858Z","iopub.status.idle":"2023-08-02T07:13:15.141764Z","shell.execute_reply.started":"2023-08-02T07:13:14.652815Z","shell.execute_reply":"2023-08-02T07:13:15.140523Z"},"trusted":true},"execution_count":71,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6122d4c543a24d08a2659d5ba66b6212"}},"metadata":{}},{"name":"stdout","text":"tokenized_dataset=Dataset({\n    features: ['input_ids', 'labels'],\n    num_rows: 10\n})\ntokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)=\n[Round 1]\n\n问：我今天想去玩术士。 \n\n答：你要打toc吗？\n\n [Round 2]\n\n问：是的，我要买饰品。 \n\n答：老四的裁决？\n\n [Round 3]\n\n问：怎么可能？我要买小强的亡者君临 \n\n答：哦哦对 裁决是物理毕业 不是SS的\n\n\ntokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)=\n\n\n答：你要打toc吗？\n\n \n\n答：老四的裁决？\n\n \n\n答：哦哦对 裁决是物理毕业 不是SS的\n\n\ntokenized_dataset[0]['input_ids']=\n[790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 54546, 31869, 37580, 55409, 54780, 54930, 31155, 30910, 13, 13, 55437, 31211, 34526, 54819, 30912, 420, 55398, 31514, 13, 13, 790, 30951, 517, 30910, 30943, 30996, 13, 13, 54761, 31211, 38586, 31123, 34034, 55304, 55739, 54667, 31155, 30910, 13, 13, 55437, 31211, 54729, 54795, 54530, 48568, 31514, 13, 13, 790, 30951, 517, 30910, 30966, 30996, 13, 13, 54761, 31211, 52687, 31514, 34034, 55304, 54603, 33026, 55737, 54631, 55835, 55290, 30910, 13, 13, 55437, 31211, 56645, 56645, 54570, 30910, 48568, 54532, 33176, 32121, 30910, 31663, 4800, 54530, 13, 13]\ntokenized_dataset[0]['labels']=\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 34526, 54819, 30912, 420, 55398, 31514, 13, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 54729, 54795, 54530, 48568, 31514, 13, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 56645, 56645, 54570, 30910, 48568, 54532, 33176, 32121, 30910, 31663, 4800, 54530, 13, 13]\nlen(tokenized_dataset[0]['input_ids']=98\nlen(tokenized_dataset[0]['labels']=98\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_datset(data_path, tokenizer, max_samples=-1,global_args=None):\n    \"\"\"读取本地包含json/jsonl文件的目录，将目录中所有文件作为dataset，并tokenize，shuffle，返回datasets.dataset\"\"\"\n    \n    if not (data_path is not None and os.path.exists(data_path)):\n        raise ValueError(\"data_path requires a directory pointing to   jsons/jsonls\")\n    \"\"\"https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py#L383\"\"\"\n    data_files_list = glob(f'{data_path}/**/*.json', recursive=True) + glob(\n                f'{data_path}/**/*.jsonl', recursive=True)\n    logger.info(f\"data files: {', '.join(data_files_list)}\")\n          \n    data = load_dataset('json', data_files=data_files_list)\n    ''' 只使用max_samples 个样本来参与train和eval  \n        https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py#L453\n    '''\n    logger.info(f\"在取样之前 data len ={len(data['train'])}\")\n    if max_samples is not None and max_samples > 0:\n            max_samples = min(len(data['train']), max_samples)  # \n            data['train'] =  data['train'].select(range(max_samples))\n    logger.info(f\"在取样之后 data len ={len(data['train'])}\")\n    \n    column_names = data['train'].column_names  # remove_columns=column_names  ,remove all at once\n    \"\"\"tokenize_func 中是单样本处理的写法 所以这里的batched只能设置为False\"\"\"\n    logger.info(\"preprocessing dataset...\")\n    \n    tokenized_dataset = data['train'].map(\n                                lambda example: tokenize_function(example, tokenizer=tokenizer),\n                                batched=False, \n                                remove_columns=my_dataset['train'].column_names)\n    print(f\"tokenized_dataset={tokenized_dataset}\")\n    print(f\"tokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)=\\n{tokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)}\")\n    print(f\"tokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)=\\n{tokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)}\")\n    print(f\"tokenized_dataset[0]['input_ids']=\\n{tokenized_dataset[0]['input_ids']}\")\n    print(f\"tokenized_dataset[0]['labels']=\\n{tokenized_dataset[0]['labels']}\")\n    print(f\"len(tokenized_dataset[0]['input_ids']={len(tokenized_dataset[0]['input_ids'])}\")\n    print(f\"len(tokenized_dataset[0]['labels']={len(tokenized_dataset[0]['labels'])}\")\n    tokenized_dataset = tokenized_dataset.shuffle(seed = 42) # 在这里就打乱顺序了 后面就不用了\n    tokenized_dataset = tokenized_dataset.flatten_indices()\n    print(\"【乱序后】\")\n    print(f\"tokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)=\\n{tokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)}\")\n    print(f\"tokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)=\\n{tokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)}\")\n    return tokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2023-08-02T07:13:26.892607Z","iopub.execute_input":"2023-08-02T07:13:26.893029Z","iopub.status.idle":"2023-08-02T07:13:26.909097Z","shell.execute_reply.started":"2023-08-02T07:13:26.892995Z","shell.execute_reply":"2023-08-02T07:13:26.907720Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"class DataCollatorForChatGLM:\n    def __init__(self,\n                 pad_token_id: int,\n                 max_length: int = 2048,\n                 ignore_label_id: int = -100):\n        self.pad_token_id = pad_token_id\n        self.ignore_label_id = ignore_label_id\n        self.max_length = max_length\n\n    def __call__(self, batch_data: List[Dict[str, List]]) -> Dict[str, torch.Tensor]:\n        \"\"\"根据batch最大长度做padding\n           注意 input_ids 已经是所有轮次对话Q+A+Q+A的拼接了 而且长度等于labels\n           将list_of_{dict(input_ids,labels)} -> dict_of(input_ids:list  ,labels:list)\n           所以叫做collator 收集 聚集\n        \"\"\"\n        len_list = [len(d['input_ids']) for d in batch_data]\n        batch_max_len = max(len_list)\n        input_ids, labels = [], []\n        for len_of_d, d in sorted(zip(len_list, batch_data), key=lambda x: -x[0]):\n            pad_len = batch_max_len - len_of_d\n            ids = d['input_ids'] + [self.pad_token_id] * pad_len\n            label = d['labels'] + [self.ignore_label_id] * pad_len  # 注意这里是在右边padding 而且labels使用的是ignore_token_id\n            if batch_max_len > self.max_length:\n                ids = ids[: self.max_length]\n                label = label[: self.max_length]\n            input_ids.append(torch.LongTensor(ids))\n            labels.append(torch.LongTensor(label))\n        input_ids = torch.stack(input_ids)\n        labels = torch.stack(labels)\n        return {'input_ids': input_ids, 'labels': labels}","metadata":{"execution":{"iopub.status.busy":"2023-08-02T07:13:30.945098Z","iopub.execute_input":"2023-08-02T07:13:30.945514Z","iopub.status.idle":"2023-08-02T07:13:30.957728Z","shell.execute_reply.started":"2023-08-02T07:13:30.945481Z","shell.execute_reply":"2023-08-02T07:13:30.956444Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"ds = get_datset(data_path=\"/kaggle/working/multi_turn\", tokenizer=tokenizer, max_samples=-1,global_args=None)","metadata":{"execution":{"iopub.status.busy":"2023-08-02T07:14:39.312424Z","iopub.execute_input":"2023-08-02T07:14:39.312916Z","iopub.status.idle":"2023-08-02T07:14:40.238593Z","shell.execute_reply.started":"2023-08-02T07:14:39.312879Z","shell.execute_reply":"2023-08-02T07:14:40.237114Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stderr","text":"\u001b[32m2023-08-02 07:14:39.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mdata files: /kaggle/working/multi_turn/多轮对话数据.json\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b600f7c80658452083e116390f9c383c"}},"metadata":{}},{"name":"stderr","text":"\u001b[32m2023-08-02 07:14:40.161\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m在取样之前 data len =10\u001b[0m\n\u001b[32m2023-08-02 07:14:40.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1m在取样之后 data len =10\u001b[0m\n\u001b[32m2023-08-02 07:14:40.164\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4e80c8e6d6543d598fd4e7e87731fee"}},"metadata":{}},{"name":"stdout","text":"tokenized_dataset=Dataset({\n    features: ['input_ids', 'labels'],\n    num_rows: 10\n})\ntokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)=\n[Round 1]\n\n问：我今天想去玩术士。 \n\n答：你要打toc吗？\n\n [Round 2]\n\n问：是的，我要买饰品。 \n\n答：老四的裁决？\n\n [Round 3]\n\n问：怎么可能？我要买小强的亡者君临 \n\n答：哦哦对 裁决是物理毕业 不是SS的\n\n\ntokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)=\n\n\n答：你要打toc吗？\n\n \n\n答：老四的裁决？\n\n \n\n答：哦哦对 裁决是物理毕业 不是SS的\n\n\ntokenized_dataset[0]['input_ids']=\n[790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 54546, 31869, 37580, 55409, 54780, 54930, 31155, 30910, 13, 13, 55437, 31211, 34526, 54819, 30912, 420, 55398, 31514, 13, 13, 790, 30951, 517, 30910, 30943, 30996, 13, 13, 54761, 31211, 38586, 31123, 34034, 55304, 55739, 54667, 31155, 30910, 13, 13, 55437, 31211, 54729, 54795, 54530, 48568, 31514, 13, 13, 790, 30951, 517, 30910, 30966, 30996, 13, 13, 54761, 31211, 52687, 31514, 34034, 55304, 54603, 33026, 55737, 54631, 55835, 55290, 30910, 13, 13, 55437, 31211, 56645, 56645, 54570, 30910, 48568, 54532, 33176, 32121, 30910, 31663, 4800, 54530, 13, 13]\ntokenized_dataset[0]['labels']=\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 34526, 54819, 30912, 420, 55398, 31514, 13, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 54729, 54795, 54530, 48568, 31514, 13, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 56645, 56645, 54570, 30910, 48568, 54532, 33176, 32121, 30910, 31663, 4800, 54530, 13, 13]\nlen(tokenized_dataset[0]['input_ids']=98\nlen(tokenized_dataset[0]['labels']=98\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78c8dbf2d2f545659743db9366d34676"}},"metadata":{}},{"name":"stdout","text":"【乱序后】\ntokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)=\n[Round 1]\n\n问：Htoc五十箱队伍来个橙分SM \n\n答：电撒还是ZQS\n\n [Round 2]\n\n问：我要奶萨 \n\n答：可以我奶元双休\n\n\ntokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)=\n\n\n答：电撒还是ZQS\n\n \n\n答：可以我奶元双休\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"tokenizer.pad_token_id =0  pad_token=<unk>\n    eos_token_id=2 eos_token=</s>","metadata":{"execution":{"iopub.status.busy":"2023-07-31T01:41:23.430793Z","iopub.execute_input":"2023-07-31T01:41:23.431223Z","iopub.status.idle":"2023-07-31T01:41:23.438775Z","shell.execute_reply.started":"2023-07-31T01:41:23.431160Z","shell.execute_reply":"2023-07-31T01:41:23.437720Z"}}},{"cell_type":"code","source":"for item in ds:\n    #print(item)\n    print(tokenizer.decode(item['input_ids'],skip_special_tokens=False))\n    print(\"======================\")","metadata":{"execution":{"iopub.status.busy":"2023-08-02T07:13:48.112953Z","iopub.execute_input":"2023-08-02T07:13:48.113407Z","iopub.status.idle":"2023-08-02T07:13:48.128216Z","shell.execute_reply.started":"2023-08-02T07:13:48.113371Z","shell.execute_reply":"2023-08-02T07:13:48.127375Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"[Round 1]\n\n问：晚上25Hicc你要买啥？ \n\n答：我估计要存钱买望远镜，或者冰龙的饰品。\n\n\n======================\n[Round 1]\n\n问：上周wcl我战士小萨打了1W4秒伤 \n\n答：那多少分了？\n\n [Round 2]\n\n问：98.89 \n\n答：服务器第一就是你。\n\n\n======================\n[Round 1]\n\n问：今天icc爆红 \n\n答：出望远镜还是血毒了？\n\n\n======================\n[Round 1]\n\n问：为啥我装等都264了 dps还这么低？ \n\n答：你是不是没卡药没开爆发？\n\n [Round 2]\n\n问：我记得偷了药的，加速和烈焰菇都吃了 \n\n答：等会，我看你好像技能用的低等级的\n\n [Round 3]\n\n问：啊？我去看看等级。。。。X 还真是 \n\n答：哈哈，亏你还打了这么久没发现，也是服了。\n\n\n======================\n[Round 1]\n\n问：你这个术士的装备真tm的壕，277毕业，连红玉的饰品都有。 \n\n答：这个号是我买的，淘宝上4000块。\n\n [Round 2]\n\n问：我靠，有钱。 \n\n答：一般吧。\n\n\n======================\n[Round 1]\n\n问：今天爆红 \n\n答：出裁决了？\n\n [Round 2]\n\n问：是的，还出了浮冰长靴。 \n\n答：666啊老铁\n\n\n======================\n[Round 1]\n\n问：Htoc五十箱队伍来个橙分SM \n\n答：电撒还是ZQS\n\n [Round 2]\n\n问：我要奶萨 \n\n答：可以我奶元双休\n\n\n======================\n[Round 1]\n\n问：我今天想去玩术士。 \n\n答：你要打toc吗？\n\n [Round 2]\n\n问：是的，我要买饰品。 \n\n答：老四的裁决？\n\n [Round 3]\n\n问：怎么可能？我要买小强的亡者君临 \n\n答：哦哦对 裁决是物理毕业 不是SS的\n\n\n======================\n","output_type":"stream"}]},{"cell_type":"code","source":"# 验证get_dataset\nds = get_datset(data_path=\"/kaggle/working/multi_turn\", tokenizer=tokenizer, max_samples=-1,global_args=None)\nfor item in ds:\n    #print(item)\n    print(tokenizer.decode(item['input_ids'],skip_special_tokens=False))\n    print(\"======================\")\n# 验证 DataCollatorForChatGLM\nvery_clear_data_collator = DataCollatorForChatGLM(pad_token_id=tokenizer.pad_token_id,max_length=900)\nvery_clear_data_collator(ds)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-02T07:14:55.992650Z","iopub.execute_input":"2023-08-02T07:14:55.993090Z","iopub.status.idle":"2023-08-02T07:14:56.027192Z","shell.execute_reply.started":"2023-08-02T07:14:55.993055Z","shell.execute_reply":"2023-08-02T07:14:56.025968Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  790, 30951,   517,  ..., 31155,    13,    13],\n         [  790, 30951,   517,  ...,     0,     0,     0],\n         [  790, 30951,   517,  ...,     0,     0,     0],\n         ...,\n         [  790, 30951,   517,  ...,     0,     0,     0],\n         [  790, 30951,   517,  ...,     0,     0,     0],\n         [  790, 30951,   517,  ...,     0,     0,     0]]),\n 'labels': tensor([[ -100,  -100,  -100,  ..., 31155,    13,    13],\n         [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n         [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n         ...,\n         [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n         [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n         [ -100,  -100,  -100,  ...,  -100,  -100,  -100]])}"},"metadata":{}}]},{"cell_type":"code","source":"logger.info(f\"tokenizer.pad_token_id={tokenizer.pad_token_id},tokenizer.pad_token={tokenizer.pad_token}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:01:16.539592Z","iopub.execute_input":"2023-07-31T11:01:16.539950Z","iopub.status.idle":"2023-07-31T11:01:16.546241Z","shell.execute_reply.started":"2023-07-31T11:01:16.539923Z","shell.execute_reply":"2023-07-31T11:01:16.545268Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"\u001b[32m2023-07-31 11:01:16.541\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mtokenizer.pad_token_id=0,tokenizer.pad_token=<unk>\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"logger.info(f\"tokenizer.pad_token_id={tokenizer.eos_token_id},tokenizer.pad_token={tokenizer.eos_token}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:01:19.594261Z","iopub.execute_input":"2023-07-31T11:01:19.594679Z","iopub.status.idle":"2023-07-31T11:01:19.601358Z","shell.execute_reply.started":"2023-07-31T11:01:19.594643Z","shell.execute_reply":"2023-07-31T11:01:19.600406Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"\u001b[32m2023-07-31 11:01:19.595\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mtokenizer.pad_token_id=2,tokenizer.pad_token=</s>\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"logger.info(f\"tokenizer.pad_token_id={tokenizer.bos_token_id},tokenizer.pad_token={tokenizer.bos_token}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:01:22.906285Z","iopub.execute_input":"2023-07-31T11:01:22.906669Z","iopub.status.idle":"2023-07-31T11:01:22.914228Z","shell.execute_reply.started":"2023-07-31T11:01:22.906638Z","shell.execute_reply":"2023-07-31T11:01:22.913187Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Using bos_token, but it is not set yet.\n\u001b[32m2023-07-31 11:01:22.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mtokenizer.pad_token_id=None,tokenizer.pad_token=None\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_dataset(data_path, tokenizer, global_args=None,max_samples=None):\n    \"\"\"读取本地包含json/jsonl文件的目录，将目录中所有文件作为dataset，并tokenize，shuffle，返回datasets.dataset\"\"\"\n    \n    if not (data_path is not None and os.path.exists(data_path)):\n        raise ValueError(\"data_path requires a directory pointing to  .txt files\")\n    \"\"\"https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py#L383\"\"\"\n    data_files_list = glob(f'{data_path}/**/*.txt', recursive=True) \n    logger.info(f\"data files: {', '.join(data_files_list)}\")\n          \n    extension = \"text\"\n    # sasmple_by=\"document\" 就是整个txt全部读取了\n    raw_datasets = load_dataset(\n        extension,\n        data_files=data_files_list,\n        sample_by=\"document\"\n    )\n#     logger.info(f\"在取样之前 data len ={len(data['train'])}\")\n#     if max_samples is not None and max_samples > 0:\n#             max_samples = min(len(data['train']), max_samples)  # \n#             data['train'] =  data['train'].select(range(max_samples))\n#     logger.info(f\"在取样之后 data len ={len(data['train'])}\")\n    \n#     column_names = data['train'].column_names  # remove_columns=column_names  ,remove all at once\n#     \"\"\"tokenize_func 中是单样本处理的写法 所以这里的batched只能设置为False\"\"\"\n#     logger.info(\"preprocessing dataset...\")\n#     dataset = data['train'].map(lambda example: tokenize_func(example, tokenizer, global_args),\n#                                 batched=False, \n#                                 remove_columns=column_names)\n#     dataset = dataset.shuffle(seed=global_args.seed)\n#     dataset = dataset.flatten_indices()\n    return raw_datasets","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:04:50.165158Z","iopub.execute_input":"2023-07-31T14:04:50.165619Z","iopub.status.idle":"2023-07-31T14:04:50.176925Z","shell.execute_reply.started":"2023-07-31T14:04:50.165573Z","shell.execute_reply":"2023-07-31T14:04:50.175681Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(examples,tokenizer):\n    # 这个函数必须返回dict \n    ## 不然raw_datasets.map(tokenize_function, ...)调用会提示报错\n    ## 要么就直接这么写  return tokenizer(examples[\"text\"])\n    return {\"input_ids\" :tokenizer(examples[\"text\"]).input_ids}","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:04:54.090024Z","iopub.execute_input":"2023-07-31T14:04:54.090461Z","iopub.status.idle":"2023-07-31T14:04:54.097487Z","shell.execute_reply.started":"2023-07-31T14:04:54.090425Z","shell.execute_reply":"2023-07-31T14:04:54.096326Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"block_size = 10","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:04:58.099095Z","iopub.execute_input":"2023-07-31T14:04:58.099530Z","iopub.status.idle":"2023-07-31T14:04:58.105562Z","shell.execute_reply.started":"2023-07-31T14:04:58.099469Z","shell.execute_reply":"2023-07-31T14:04:58.104376Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"def group_texts(examples,block_size):\n        # Concatenate all texts.\n        #concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n        total_length = len(examples[list(examples.keys())[0]])\n        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n        # customize this part to your needs.\n        print(f\"total_length={total_length}\")\n        \n        #ADD 20230731 原先shiming是去掉了最后的尾巴 但是我觉得可以保留 毕竟语料不多 每一部分都要用上\n        #if total_length >= block_size:\n        #    total_length = (total_length // block_size) * block_size\n        \n        # Split by chunks of max_len.\n        result = {\n            k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n            for k, t in examples.items()\n        }\n        #result[\"labels\"] = result[\"input_ids\"].copy()\n        return result","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:05:02.006246Z","iopub.execute_input":"2023-07-31T14:05:02.006682Z","iopub.status.idle":"2023-07-31T14:05:02.016789Z","shell.execute_reply.started":"2023-07-31T14:05:02.006645Z","shell.execute_reply":"2023-07-31T14:05:02.015579Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:05:05.266353Z","iopub.execute_input":"2023-07-31T14:05:05.267753Z","iopub.status.idle":"2023-07-31T14:05:05.531457Z","shell.execute_reply.started":"2023-07-31T14:05:05.267700Z","shell.execute_reply":"2023-07-31T14:05:05.530339Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"pad_token_id = tokenizer.pad_token_id\nprint(f\"pad_token_id = {pad_token_id}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:05:07.949958Z","iopub.execute_input":"2023-07-31T14:05:07.950378Z","iopub.status.idle":"2023-07-31T14:05:07.956106Z","shell.execute_reply.started":"2023-07-31T14:05:07.950341Z","shell.execute_reply":"2023-07-31T14:05:07.954937Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"pad_token_id = 0\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\ndef get_chained_lm_datasets(lm_datasets,\n                            pad_token_id: int ,\n                            global_args_max_length: int = 2048,\n                            num_samples=-1,\n                            tokenizer=None):\n    \"做padding和根据global.max_length截取\"\n    input_ids_list = list(chain(*lm_datasets['input_ids']))\n    # padding  with max_len_of_samples\n    max_len_ids = max([len(input_ids) for input_ids in input_ids_list] ) \n    print(f\"样本中ids 最大长度 max_len_ids={max_len_ids}\")\n    print(f\"全局训练参数的global_args_max_length={global_args_max_length}\")\n    print(f\"cutoff to len = {min(global_args_max_length,max_len_ids)}\")\n    \n    #截断\n    new_input_ids=[]\n    for ids in input_ids_list :\n        print(f\"len_sample_ids={len(ids)}\")\n        pad_len = max_len_ids - len(ids)\n        ids += [pad_token_id]* pad_len\n        # 截取 根据命令行输入的max len和 ids_list中最大长度 两值做比较取较小的来截取前面N个\n        ids =ids[: min(global_args_max_length,max_len_ids)]\n        new_input_ids.append(ids)\n    # 抽样\n    random_chosen_samples=new_input_ids\n    print(f\"验证查看 shuffle之前 第一个样本\\n{tokenizer.decode(random_chosen_samples[0])}\")\n    print(f\"验证查看 shuffle之前 最后一个样本\\n{tokenizer.decode(random_chosen_samples[-1])}\")\n    random.shuffle(new_input_ids)\n    if num_samples > -1: # num_samples  一般只是debug的时候取少量样本测试 ；\n        #random.sample 不重复抽样\n        random_chosen_samples = random.sample(new_input_ids,k=num_samples)\n    print(f\"样本数量={len(random_chosen_samples)}\")\n    #return {\"input_ids\":random_chosen_samples  ,\"labels\":random_chosen_samples.copy()}\n    return [{\"input_ids\": item ,\"labels\":item.copy() } for item in random_chosen_samples]","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:05:13.171433Z","iopub.execute_input":"2023-07-31T14:05:13.171883Z","iopub.status.idle":"2023-07-31T14:05:13.183137Z","shell.execute_reply.started":"2023-07-31T14:05:13.171848Z","shell.execute_reply":"2023-07-31T14:05:13.181919Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"def fault_tolerance_data_collator(features: List) -> Dict[str, Any]:\n    if not isinstance(features[0], Mapping):\n        features = [vars(f) for f in features]\n    first = features[0]\n    batch = {}\n\n    # Special handling for labels.\n    # Ensure that tensor is created with the correct type\n    if \"label\" in first and first[\"label\"] is not None:\n        label = first[\"label\"].item() if isinstance(first[\"label\"], torch.Tensor) else first[\"label\"]\n        dtype = torch.long if isinstance(label, int) else torch.float\n        batch[\"labels\"] = torch.tensor([f[\"label\"] for f in features], dtype=dtype)\n    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n        if isinstance(first[\"label_ids\"], torch.Tensor):\n            batch[\"labels\"] = torch.stack([f[\"label_ids\"] for f in features])\n        else:\n            dtype = torch.long if type(first[\"label_ids\"][0]) is int else torch.float\n            batch[\"labels\"] = torch.tensor([f[\"label_ids\"] for f in features], dtype=dtype)\n\n    # Handling of all other possible keys.\n    # Again, we will use the first element to figure out which key/values are not None for this model.\n    try:\n        for k, v in first.items():\n            if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n                if isinstance(v, torch.Tensor):\n                    batch[k] = torch.stack([f[k] for f in features])\n                elif isinstance(v, np.ndarray):\n                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))\n                else:\n                    batch[k] = torch.tensor([f[k] for f in features])\n    except ValueError:  # quick fix by simply take the first example\n        for k, v in first.items():\n            if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n                if isinstance(v, torch.Tensor):\n                    batch[k] = torch.stack([features[0][k]] * len(features))\n                elif isinstance(v, np.ndarray):\n                    batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))\n                else:\n                    batch[k] = torch.tensor([features[0][k]] * len(features))\n    #print(\"打印fault_tolerance_data_collator处理后的信息\")\n    #print(f\"batch.keys()={batch.keys()},len_batch_key_labels={len(batch['labels'])}    ,len_batch_key_k={len(batch[k])}\")\n    return batch","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:38:11.820321Z","iopub.execute_input":"2023-07-31T11:38:11.820722Z","iopub.status.idle":"2023-07-31T11:38:11.837468Z","shell.execute_reply.started":"2023-07-31T11:38:11.820689Z","shell.execute_reply":"2023-07-31T11:38:11.836108Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"a=[1,2,3]\nrandom.sample(a,k=2)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:38:15.694975Z","iopub.execute_input":"2023-07-31T11:38:15.695350Z","iopub.status.idle":"2023-07-31T11:38:15.703949Z","shell.execute_reply.started":"2023-07-31T11:38:15.695318Z","shell.execute_reply":"2023-07-31T11:38:15.701863Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"[3, 2]"},"metadata":{}}]},{"cell_type":"code","source":"# 是两段文本\nraw_datasets = get_dataset(data_path=\"/kaggle/working/test\",tokenizer=tokenizer,max_samples=10000)\nprint(raw_datasets)\nprint(raw_datasets['train']['text'])","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:05:20.252637Z","iopub.execute_input":"2023-07-31T14:05:20.253040Z","iopub.status.idle":"2023-07-31T14:05:20.522374Z","shell.execute_reply.started":"2023-07-31T14:05:20.253008Z","shell.execute_reply":"2023-07-31T14:05:20.521203Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stderr","text":"\u001b[32m2023-07-31 14:05:20.255\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_dataset\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mdata files: /kaggle/working/test/test/test0730.txt, /kaggle/working/test/test/test0730_2.txt\u001b[0m\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f3259d560b944d99dd4c9bf6f50affa"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 2\n    })\n})\n[\"1+2+3+466666 66666 hgsg sgs hshshs hsfh shsh sh dh fhh\\n4+3+2=1                     afghu wfsvjoer n gw jgohuth4odjf ajaddpjfwon sdjnfslf  \\n我很优秀  你呢？我不知道啊，哈哈。。。 vsoro nvlknlbh h rwlj flvnskd gb gd \\n吃放睡觉打豆豆哦，【】[];'lvsvd a  gg  \\\\n brh5p bph owgwk rwf s59jbg\\nhd g  ghr5 eg r hsghrwgrwhhwh hg  wfsvjoer\\ng shth h j efwe gb\\n\\nef g h hsr \\n\\n\\nge whreh \\n hgs ghhth\", \"1+2+3+466666 66666 hgsg sgs hshshs hsfh shsh sh dh fhh\\n4+3+2=1                     afghu wfsvjoer n gw jgohuth4odjf ajaddpjfwon sdjnfslf  \\n我很优秀  你呢？我不知道啊，哈哈。。。 vsoro nvlknlbh h rwlj flvnskd gb gd \\n吃放睡觉打豆豆哦，【】[];'lvsvd a  gg  \\\\n brh5p bph owgwk rwf s59jbg\\nhd g  ghr5 eg r hsghrwgrwhhwh hg  wfsvjoer\\ng shth h j efwe gb\\n\\nef g h hsr \\n\\n\\nge whreh \\n hgs ghhth\"]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenized_datasets = raw_datasets.map(\n                lambda examples:tokenize_function(examples,tokenizer),\n                batched=True,\n                num_proc=2,#data_args.preprocessing_num_workers,\n                remove_columns=['text'],#column_names,\n                #load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on dataset\",\n            )\nprint(tokenized_datasets)\nprint(tokenized_datasets['train']['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:05:28.258397Z","iopub.execute_input":"2023-07-31T14:05:28.259431Z","iopub.status.idle":"2023-07-31T14:05:28.286670Z","shell.execute_reply.started":"2023-07-31T14:05:28.259395Z","shell.execute_reply":"2023-07-31T14:05:28.285590Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids'],\n        num_rows: 2\n    })\n})\n[[64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972, 30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978, 30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917, 299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578, 13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296, 10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317, 30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953, 2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926, 5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514, 42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920, 16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933, 30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058, 54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995, 4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265, 30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927, 30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540, 13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299, 10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265, 30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299, 467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317, 299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366, 263, 30919, 30910, 13, 299, 6196, 317, 12578, 398], [64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972, 30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978, 30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917, 299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578, 13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296, 10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317, 30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953, 2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926, 5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514, 42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920, 16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933, 30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058, 54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995, 4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265, 30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927, 30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540, 13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299, 10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265, 30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299, 467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317, 299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366, 263, 30919, 30910, 13, 299, 6196, 317, 12578, 398]]\n","output_type":"stream"}]},{"cell_type":"code","source":"#raw_data['train']['text']","metadata":{"execution":{"iopub.status.busy":"2023-07-31T04:30:50.188040Z","iopub.execute_input":"2023-07-31T04:30:50.188504Z","iopub.status.idle":"2023-07-31T04:30:50.193872Z","shell.execute_reply.started":"2023-07-31T04:30:50.188465Z","shell.execute_reply":"2023-07-31T04:30:50.192680Z"},"trusted":true},"execution_count":243,"outputs":[]},{"cell_type":"code","source":"block_size=50","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:06:26.542621Z","iopub.execute_input":"2023-07-31T14:06:26.543257Z","iopub.status.idle":"2023-07-31T14:06:26.548926Z","shell.execute_reply.started":"2023-07-31T14:06:26.543209Z","shell.execute_reply":"2023-07-31T14:06:26.547903Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"lm_datasets = tokenized_datasets['train'].map(\n                lambda examples :group_texts(examples ,block_size) ,\n                batched=False,\n                num_proc=2,#data_args.preprocessing_num_workers,\n                #load_from_cache_file=not data_args.overwrite_cache,\n                desc=f\"Grouping texts in chunks of {block_size}\",\n                #remove_columns =['attention_mask', 'position_ids',]\n            )\nprint(lm_datasets)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:06:29.816465Z","iopub.execute_input":"2023-07-31T14:06:29.816973Z","iopub.status.idle":"2023-07-31T14:06:30.218954Z","shell.execute_reply.started":"2023-07-31T14:06:29.816931Z","shell.execute_reply":"2023-07-31T14:06:30.216996Z"},"trusted":true},"execution_count":129,"outputs":[{"output_type":"display_data","data":{"text/plain":"Grouping texts in chunks of 50 (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"total_length=219total_length=219\n\nDataset({\n    features: ['input_ids'],\n    num_rows: 2\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_train = get_chained_lm_datasets(lm_datasets,\n                        pad_token_id=tokenizer.pad_token_id,\n                        global_args_max_length=50000,\n                         num_samples=-1,\n                        tokenizer = tokenizer)\nprint(dataset_train)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:06:33.495448Z","iopub.execute_input":"2023-07-31T14:06:33.496730Z","iopub.status.idle":"2023-07-31T14:06:33.509114Z","shell.execute_reply.started":"2023-07-31T14:06:33.496685Z","shell.execute_reply":"2023-07-31T14:06:33.507814Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stdout","text":"样本中ids 最大长度 max_len_ids=50\n全局训练参数的global_args_max_length=50000\ncutoff to len = 50\nlen_sample_ids=50\nlen_sample_ids=50\nlen_sample_ids=50\nlen_sample_ids=50\nlen_sample_ids=19\nlen_sample_ids=50\nlen_sample_ids=50\nlen_sample_ids=50\nlen_sample_ids=50\nlen_sample_ids=19\nshuffle之前 第一个样本 =1+2+3+466666 66666 hgsg sgs hshshs hsfh shsh sh dh fhh\n4+3+2=1                    \nshuffle之前 最后一个样本 =h hsr \n\n\nge whreh \n hgs ghhth\n样本数量=10\n[{'input_ids': [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366, 263, 30919, 30910, 13, 299, 6196, 317, 12578, 398, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366, 263, 30919, 30910, 13, 299, 6196, 317, 12578, 398, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'input_ids': [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317, 30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953, 2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926, 5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514, 42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920], 'labels': [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317, 30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953, 2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926, 5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514, 42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920]}, {'input_ids': [64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972, 30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978, 30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917, 299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578, 13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296], 'labels': [64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972, 30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978, 30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917, 299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578, 13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296]}, {'input_ids': [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366, 263, 30919, 30910, 13, 299, 6196, 317, 12578, 398, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366, 263, 30919, 30910, 13, 299, 6196, 317, 12578, 398, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'input_ids': [64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972, 30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978, 30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917, 299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578, 13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296], 'labels': [64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972, 30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978, 30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917, 299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578, 13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296]}, {'input_ids': [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933, 30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058, 54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995, 4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265, 30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927], 'labels': [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933, 30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058, 54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995, 4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265, 30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927]}, {'input_ids': [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540, 13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299, 10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265, 30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299, 467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317], 'labels': [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540, 13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299, 10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265, 30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299, 467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317]}, {'input_ids': [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317, 30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953, 2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926, 5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514, 42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920], 'labels': [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317, 30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953, 2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926, 5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514, 42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920]}, {'input_ids': [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933, 30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058, 54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995, 4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265, 30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927], 'labels': [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933, 30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058, 54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995, 4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265, 30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927]}, {'input_ids': [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540, 13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299, 10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265, 30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299, 467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317], 'labels': [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540, 13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299, 10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265, 30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299, 467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317]}]\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_eval = get_chained_lm_datasets(lm_datasets,\n                        pad_token_id=tokenizer.pad_token_id,\n                        global_args_max_length=5,\n                                 num_samples=2)\nprint(dataset_eval)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:32:39.085970Z","iopub.execute_input":"2023-07-31T11:32:39.087034Z","iopub.status.idle":"2023-07-31T11:32:39.094084Z","shell.execute_reply.started":"2023-07-31T11:32:39.086998Z","shell.execute_reply":"2023-07-31T11:32:39.092447Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"样本中ids 最大长度 max_len_ids=219\n全局训练参数的global_args_max_length=5\ncutoff to len = 5\n样本数量=2\n[{'input_ids': [64790, 64792, 30910, 30939, 31011], 'labels': [64790, 64792, 30910, 30939, 31011]}, {'input_ids': [64790, 64792, 30910, 30939, 31011], 'labels': [64790, 64792, 30910, 30939, 31011]}]\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_datset_for_pretrain(data_path, tokenizer, block_size=10,global_args_max_length=0,max_samples=0):\n    \"\"\"读取本地包含json/jsonl文件的目录，将目录中所有文件作为dataset，并tokenize，shuffle，返回datasets.dataset\"\"\"\n    print(123)\n    if not (data_path is not None and os.path.exists(data_path)):\n        raise ValueError(\"data_path requires a directory pointing to  .txt files\")\n    \"\"\"https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py#L383\"\"\"\n    data_files_list = glob(f'{data_path}/**/*.txt', recursive=True) \n    logger.info(f\"data files: {', '.join(data_files_list)}\")\n          \n    extension = \"text\"\n    # 读取全部txt文档 有N篇文档 那raw_datasets这个list就包含N个篇章级别的文本结果\n    #sasmple_by=\"document\" 就是将一个txt当做一个sample全部读取\n    raw_datasets = load_dataset(\n        extension,\n        data_files=data_files_list,\n        sample_by=\"document\"\n    )\n    # 对全部N个文档str 做分词 分词结果也是N个长list 每个list都含有input_ids这个\n    tokenized_datasets = raw_datasets.map(\n                lambda examples : tokenize_function(examples ,tokenizer),\n                batched=True,\n                num_proc=2,#data_args.preprocessing_num_workers,\n                remove_columns=['text'],#column_names,\n                #load_from_cache_file=not data_args.overwrite_cache,\n                desc=\"Running tokenizer on dataset\",\n            )\n    # N个长list在这一步被按照block_size切成小段 原先是[长1],[长2] 现在仍然为2元素 但是每个里面都是小段[[短1-1],[短1-2]] ,[[短2-1],[短2-2]]\n    lm_datasets = tokenized_datasets['train'].map(\n                lambda examples :group_texts(examples , block_size),\n                batched=False,\n                num_proc=2,#data_args.preprocessing_num_workers,\n                #load_from_cache_file=not data_args.overwrite_cache,\n                #desc=f\"Grouping texts in chunks of {block_size}\",\n                #remove_columns =['attention_mask', 'position_ids',]\n            )\n    dataset_final = get_chained_lm_datasets(lm_datasets,\n                        pad_token_id=tokenizer.pad_token_id,\n                        global_args_max_length=global_args_max_length,\n                        num_samples=max_samples,\n                        tokenizer = tokenizer)\n#     logger.info(f\"在取样之前 data len ={len(data['train'])}\")\n#     if max_samples is not None and max_samples > 0:\n#             max_samples = min(len(data['train']), max_samples)  # \n#             data['train'] =  data['train'].select(range(max_samples))\n#     logger.info(f\"在取样之后 data len ={len(data['train'])}\")\n    \n#     column_names = data['train'].column_names  # remove_columns=column_names  ,remove all at once\n#     \"\"\"tokenize_func 中是单样本处理的写法 所以这里的batched只能设置为False\"\"\"\n#     logger.info(\"preprocessing dataset...\")\n#     dataset = data['train'].map(lambda example: tokenize_func(example, tokenizer, global_args),\n#                                 batched=False, \n#                                 remove_columns=column_names)\n#     dataset = dataset.shuffle(seed=global_args.seed)\n#     dataset = dataset.flatten_indices()\n    return dataset_final","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:08:33.681211Z","iopub.execute_input":"2023-07-31T11:08:33.681608Z","iopub.status.idle":"2023-07-31T11:08:33.694435Z","shell.execute_reply.started":"2023-07-31T11:08:33.681575Z","shell.execute_reply":"2023-07-31T11:08:33.693559Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"train_ds = get_datset_for_pretrain(data_path=\"/kaggle/working/test\",\n                                  tokenizer =  AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True),\n                                  block_size = 10,\n                                  global_args_max_length=6,\n                                  max_samples=-1)\nprint(train_ds)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:08:57.615461Z","iopub.execute_input":"2023-07-31T11:08:57.615871Z","iopub.status.idle":"2023-07-31T11:08:59.048973Z","shell.execute_reply.started":"2023-07-31T11:08:57.615840Z","shell.execute_reply":"2023-07-31T11:08:59.047973Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"\u001b[32m2023-07-31 11:08:57.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset_for_pretrain\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mdata files: /kaggle/working/test/test/test0730.txt, /kaggle/working/test/test/test0730_2.txt\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"123\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8741e7b022f441c9a974ea570cfb23dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"total_length=219total_length=219\n\n样本中ids 最大长度 max_len_ids=10\n全局训练参数的global_args_max_length=6\ncutoff to len = 6\n样本数量=42\n[{'input_ids': [13, 15939, 317, 265, 30927, 5323], 'labels': [13, 15939, 317, 265, 30927, 5323]}, {'input_ids': [30929, 30934, 416, 30929, 30926, 268], 'labels': [30929, 30934, 416, 30929, 30926, 268]}, {'input_ids': [30974, 30916, 822, 30919, 30970, 30925], 'labels': [30974, 30916, 822, 30919, 30970, 30925]}, {'input_ids': [2065, 30925, 30965, 30926, 23347, 268], 'labels': [2065, 30925, 30965, 30926, 23347, 268]}, {'input_ids': [10789, 30927, 11408, 271, 9563, 30933], 'labels': [10789, 30927, 11408, 271, 9563, 30933]}, {'input_ids': [30929, 9563, 30933, 5282, 266, 13], 'labels': [30929, 9563, 30933, 5282, 266, 13]}, {'input_ids': [10038, 5323, 30929, 689, 2277, 30919], 'labels': [10038, 5323, 30929, 689, 2277, 30919]}, {'input_ids': [54804, 35228, 54819, 55720, 55720, 56645], 'labels': [54804, 35228, 54819, 55720, 55720, 56645]}, {'input_ids': [2065, 30925, 30965, 30926, 23347, 268], 'labels': [2065, 30925, 30965, 30926, 23347, 268]}, {'input_ids': [30978, 299, 6196, 30927, 268, 6196], 'labels': [30978, 299, 6196, 30927, 268, 6196]}, {'input_ids': [30929, 30934, 416, 30929, 30926, 268], 'labels': [30929, 30934, 416, 30929, 30926, 268]}, {'input_ids': [299, 299, 30917, 30918, 30910, 13], 'labels': [299, 299, 30917, 30918, 30910, 13]}, {'input_ids': [30916, 2823, 30921, 317, 30931, 317], 'labels': [30916, 2823, 30921, 317, 30931, 317]}, {'input_ids': [16462, 26604, 30919, 299, 416, 30929], 'labels': [16462, 26604, 30919, 299, 416, 30929]}, {'input_ids': [30978, 299, 6196, 30927, 268, 6196], 'labels': [30978, 299, 6196, 30927, 268, 6196]}, {'input_ids': [30978, 30978, 30978, 30978, 30978, 30910], 'labels': [30978, 30978, 30978, 30978, 30978, 30910]}, {'input_ids': [64790, 64792, 30910, 30939, 31011, 30943], 'labels': [64790, 64792, 30910, 30939, 31011, 30943]}, {'input_ids': [13, 30972, 31011, 30966, 31011, 30943], 'labels': [13, 30972, 31011, 30966, 31011, 30943]}, {'input_ids': [467, 315, 30926, 734, 317, 30931], 'labels': [467, 315, 30926, 734, 317, 30931]}, {'input_ids': [10038, 5323, 30929, 689, 2277, 30919], 'labels': [10038, 5323, 30929, 689, 2277, 30919]}, {'input_ids': [299, 10240, 30919, 438, 1480, 438], 'labels': [299, 10240, 30919, 438, 1480, 438]}, {'input_ids': [13, 30972, 31011, 30966, 31011, 30943], 'labels': [13, 30972, 31011, 30966, 31011, 30943]}, {'input_ids': [13, 15939, 317, 265, 30927, 5323], 'labels': [13, 15939, 317, 265, 30927, 5323]}, {'input_ids': [30916, 2823, 30921, 317, 30931, 317], 'labels': [30916, 2823, 30921, 317, 30931, 317]}, {'input_ids': [42660, 55674, 31123, 34380, 39507, 5088], 'labels': [42660, 55674, 31123, 34380, 39507, 5088]}, {'input_ids': [467, 315, 30926, 734, 317, 30931], 'labels': [467, 315, 30926, 734, 317, 30931]}, {'input_ids': [30929, 9563, 30933, 5282, 266, 13], 'labels': [30929, 9563, 30933, 5282, 266, 13]}, {'input_ids': [30929, 467, 30927, 1063, 1056, 30972], 'labels': [30929, 467, 30927, 1063, 1056, 30972]}, {'input_ids': [16462, 26604, 30919, 299, 416, 30929], 'labels': [16462, 26604, 30919, 299, 416, 30929]}, {'input_ids': [42660, 55674, 31123, 34380, 39507, 5088], 'labels': [42660, 55674, 31123, 34380, 39507, 5088]}, {'input_ids': [64790, 64792, 30910, 30939, 31011, 30943], 'labels': [64790, 64792, 30910, 30939, 31011, 30943]}, {'input_ids': [54804, 35228, 54819, 55720, 55720, 56645], 'labels': [54804, 35228, 54819, 55720, 55720, 56645]}, {'input_ids': [10789, 30927, 11408, 271, 9563, 30933], 'labels': [10789, 30927, 11408, 271, 9563, 30933]}, {'input_ids': [5239, 30926, 265, 13, 36545, 31994], 'labels': [5239, 30926, 265, 13, 36545, 31994]}, {'input_ids': [4571, 30953, 30920, 30933, 13532, 30921], 'labels': [4571, 30953, 30920, 30933, 13532, 30921]}, {'input_ids': [30978, 30978, 30978, 30978, 30978, 30910], 'labels': [30978, 30978, 30978, 30978, 30978, 30910]}, {'input_ids': [30974, 30916, 822, 30919, 30970, 30925], 'labels': [30974, 30916, 822, 30919, 30970, 30925]}, {'input_ids': [299, 299, 30917, 30918, 30910, 13], 'labels': [299, 299, 30917, 30918, 30910, 13]}, {'input_ids': [5239, 30926, 265, 13, 36545, 31994], 'labels': [5239, 30926, 265, 13, 36545, 31994]}, {'input_ids': [4571, 30953, 30920, 30933, 13532, 30921], 'labels': [4571, 30953, 30920, 30933, 13532, 30921]}, {'input_ids': [299, 10240, 30919, 438, 1480, 438], 'labels': [299, 10240, 30919, 438, 1480, 438]}, {'input_ids': [30929, 467, 30927, 1063, 1056, 30972], 'labels': [30929, 467, 30927, 1063, 1056, 30972]}]\n","output_type":"stream"}]},{"cell_type":"code","source":"eval_ds = get_datset_for_pretrain(data_path=\"/kaggle/working/test\",\n                                  tokenizer =  AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True),\n                                  block_size = 10,\n                                  global_args_max_length=6,\n                                  max_samples=5)\nprint(eval_ds)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:09:06.648999Z","iopub.execute_input":"2023-07-31T11:09:06.650002Z","iopub.status.idle":"2023-07-31T11:09:07.033138Z","shell.execute_reply.started":"2023-07-31T11:09:06.649963Z","shell.execute_reply":"2023-07-31T11:09:07.032082Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"\u001b[32m2023-07-31 11:09:06.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_datset_for_pretrain\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mdata files: /kaggle/working/test/test/test0730.txt, /kaggle/working/test/test/test0730_2.txt\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"123\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a77d920f19104606960855bd30c560d9"}},"metadata":{}},{"name":"stdout","text":"样本中ids 最大长度 max_len_ids=10\n全局训练参数的global_args_max_length=6\ncutoff to len = 6\n样本数量=5\n[{'input_ids': [42660, 55674, 31123, 34380, 39507, 5088], 'labels': [42660, 55674, 31123, 34380, 39507, 5088]}, {'input_ids': [299, 10240, 30919, 438, 1480, 438], 'labels': [299, 10240, 30919, 438, 1480, 438]}, {'input_ids': [10789, 30927, 11408, 271, 9563, 30933], 'labels': [10789, 30927, 11408, 271, 9563, 30933]}, {'input_ids': [30929, 467, 30927, 1063, 1056, 30972], 'labels': [30929, 467, 30927, 1063, 1056, 30972]}, {'input_ids': [13, 30972, 31011, 30966, 31011, 30943], 'labels': [13, 30972, 31011, 30966, 31011, 30943]}]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport os\nfrom dataclasses import dataclass, field\nfrom glob import glob\nfrom itertools import chain\nfrom typing import Optional, List, Dict, Any, Mapping\ndef fault_tolerance_data_collator(features: List) -> Dict[str, Any]:\n    if not isinstance(features[0], Mapping):\n        features = [vars(f) for f in features]\n    first = features[0]\n    batch = {}\n\n    # Special handling for labels.\n    # Ensure that tensor is created with the correct type\n    if \"label\" in first and first[\"label\"] is not None:\n        label = first[\"label\"].item() if isinstance(first[\"label\"], torch.Tensor) else first[\"label\"]\n        dtype = torch.long if isinstance(label, int) else torch.float\n        batch[\"labels\"] = torch.tensor([f[\"label\"] for f in features], dtype=dtype)\n    elif \"label_ids\" in first and first[\"label_ids\"] is not None:\n        if isinstance(first[\"label_ids\"], torch.Tensor):\n            batch[\"labels\"] = torch.stack([f[\"label_ids\"] for f in features])\n        else:\n            dtype = torch.long if type(first[\"label_ids\"][0]) is int else torch.float\n            batch[\"labels\"] = torch.tensor([f[\"label_ids\"] for f in features], dtype=dtype)\n\n    # Handling of all other possible keys.\n    # Again, we will use the first element to figure out which key/values are not None for this model.\n    try:\n        for k, v in first.items():\n            if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n                if isinstance(v, torch.Tensor):\n                    batch[k] = torch.stack([f[k] for f in features])\n                elif isinstance(v, np.ndarray):\n                    batch[k] = torch.tensor(np.stack([f[k] for f in features]))\n                else:\n                    batch[k] = torch.tensor([f[k] for f in features])\n    except ValueError:  # quick fix by simply take the first example\n        for k, v in first.items():\n            if k not in (\"label\", \"label_ids\") and v is not None and not isinstance(v, str):\n                if isinstance(v, torch.Tensor):\n                    batch[k] = torch.stack([features[0][k]] * len(features))\n                elif isinstance(v, np.ndarray):\n                    batch[k] = torch.tensor(np.stack([features[0][k]] * len(features)))\n                else:\n                    batch[k] = torch.tensor([features[0][k]] * len(features))\n    #print(\"打印fault_tolerance_data_collator处理后的信息\")\n    #print(f\"batch.keys()={batch.keys()},len_batch_key_labels={len(batch['labels'])}    ,len_batch_key_k={len(batch[k])}\")\n    return batch","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:14:48.136277Z","iopub.execute_input":"2023-07-31T11:14:48.137287Z","iopub.status.idle":"2023-07-31T11:14:48.156430Z","shell.execute_reply.started":"2023-07-31T11:14:48.137216Z","shell.execute_reply":"2023-07-31T11:14:48.155473Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"fault_tolerance_data_collator(eval_ds)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:14:51.959136Z","iopub.execute_input":"2023-07-31T11:14:51.959516Z","iopub.status.idle":"2023-07-31T11:14:52.021243Z","shell.execute_reply.started":"2023-07-31T11:14:51.959469Z","shell.execute_reply":"2023-07-31T11:14:52.020202Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[42660, 55674, 31123, 34380, 39507,  5088],\n         [  299, 10240, 30919,   438,  1480,   438],\n         [10789, 30927, 11408,   271,  9563, 30933],\n         [30929,   467, 30927,  1063,  1056, 30972],\n         [   13, 30972, 31011, 30966, 31011, 30943]]),\n 'labels': tensor([[42660, 55674, 31123, 34380, 39507,  5088],\n         [  299, 10240, 30919,   438,  1480,   438],\n         [10789, 30927, 11408,   271,  9563, 30933],\n         [30929,   467, 30927,  1063,  1056, 30972],\n         [   13, 30972, 31011, 30966, 31011, 30943]])}"},"metadata":{}}]},{"cell_type":"code","source":"class DataCollatorForChatGLM:\n    \n    # 虽然这个函数中又做了pad和截断 但是实际在pt阶段没有效果 因为之前已经是统一长度而且把尾部短文本用\n    # pad token 补齐过了 这里代码留着是为了后面参考改多轮对话用\n    def __init__(self,\n                 pad_token_id: int,\n                 max_length: int = 2048,\n                 ignore_label_id: int = -100):\n        self.pad_token_id = pad_token_id\n        self.ignore_label_id = ignore_label_id\n        self.max_length = max_length\n\n    def __call__(self, batch_data: List[Dict[str, List]]) -> Dict[str, torch.Tensor]:\n        \"\"\"根据batch最大长度做padding\"\"\"\n        len_list = [len(d['input_ids']) for d in batch_data]\n        batch_max_len = max(len_list)\n        input_ids, labels = [], []\n        for len_of_d, d in sorted(zip(len_list, batch_data), key=lambda x: -x[0]):\n            #padding :虽然预训练之前的步骤做了 长度已经是统一的 但是这个可以继续pad\n            #注意 ids和label使用的pad token不同 \n            # 在预训练阶段 下面的6行实际没有任何作用 因为pad_len 此时为0 可以打印看看\n            pad_len = batch_max_len - len_of_d\n            print(f\"pad_len in data collator = {pad_len}\")\n            ids = d['input_ids'] + [self.pad_token_id] * pad_len\n            label = d['labels'] + [self.ignore_label_id] * pad_len\n            if batch_max_len > self.max_length:\n                ids = ids[: self.max_length]\n                label = label[: self.max_length]\n            input_ids.append(torch.LongTensor(ids))\n            labels.append(torch.LongTensor(label))\n        input_ids = torch.stack(input_ids)\n        labels = torch.stack(labels)\n        return {'input_ids': input_ids, 'labels': labels}\n\ndata_collator = DataCollatorForChatGLM(pad_token_id=tokenizer.pad_token_id,\n                                           max_length=7)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:16:55.699696Z","iopub.execute_input":"2023-07-31T11:16:55.700445Z","iopub.status.idle":"2023-07-31T11:16:55.711548Z","shell.execute_reply.started":"2023-07-31T11:16:55.700407Z","shell.execute_reply":"2023-07-31T11:16:55.710471Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"data_collator(train_ds)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T11:17:15.706497Z","iopub.execute_input":"2023-07-31T11:17:15.706925Z","iopub.status.idle":"2023-07-31T11:17:15.743203Z","shell.execute_reply.started":"2023-07-31T11:17:15.706894Z","shell.execute_reply":"2023-07-31T11:17:15.741985Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[   13, 15939,   317,   265, 30927,  5323],\n         [30929, 30934,   416, 30929, 30926,   268],\n         [30974, 30916,   822, 30919, 30970, 30925],\n         [ 2065, 30925, 30965, 30926, 23347,   268],\n         [10789, 30927, 11408,   271,  9563, 30933],\n         [30929,  9563, 30933,  5282,   266,    13],\n         [10038,  5323, 30929,   689,  2277, 30919],\n         [54804, 35228, 54819, 55720, 55720, 56645],\n         [ 2065, 30925, 30965, 30926, 23347,   268],\n         [30978,   299,  6196, 30927,   268,  6196],\n         [30929, 30934,   416, 30929, 30926,   268],\n         [  299,   299, 30917, 30918, 30910,    13],\n         [30916,  2823, 30921,   317, 30931,   317],\n         [16462, 26604, 30919,   299,   416, 30929],\n         [30978,   299,  6196, 30927,   268,  6196],\n         [30978, 30978, 30978, 30978, 30978, 30910],\n         [64790, 64792, 30910, 30939, 31011, 30943],\n         [   13, 30972, 31011, 30966, 31011, 30943],\n         [  467,   315, 30926,   734,   317, 30931],\n         [10038,  5323, 30929,   689,  2277, 30919],\n         [  299, 10240, 30919,   438,  1480,   438],\n         [   13, 30972, 31011, 30966, 31011, 30943],\n         [   13, 15939,   317,   265, 30927,  5323],\n         [30916,  2823, 30921,   317, 30931,   317],\n         [42660, 55674, 31123, 34380, 39507,  5088],\n         [  467,   315, 30926,   734,   317, 30931],\n         [30929,  9563, 30933,  5282,   266,    13],\n         [30929,   467, 30927,  1063,  1056, 30972],\n         [16462, 26604, 30919,   299,   416, 30929],\n         [42660, 55674, 31123, 34380, 39507,  5088],\n         [64790, 64792, 30910, 30939, 31011, 30943],\n         [54804, 35228, 54819, 55720, 55720, 56645],\n         [10789, 30927, 11408,   271,  9563, 30933],\n         [ 5239, 30926,   265,    13, 36545, 31994],\n         [ 4571, 30953, 30920, 30933, 13532, 30921],\n         [30978, 30978, 30978, 30978, 30978, 30910],\n         [30974, 30916,   822, 30919, 30970, 30925],\n         [  299,   299, 30917, 30918, 30910,    13],\n         [ 5239, 30926,   265,    13, 36545, 31994],\n         [ 4571, 30953, 30920, 30933, 13532, 30921],\n         [  299, 10240, 30919,   438,  1480,   438],\n         [30929,   467, 30927,  1063,  1056, 30972]]),\n 'labels': tensor([[   13, 15939,   317,   265, 30927,  5323],\n         [30929, 30934,   416, 30929, 30926,   268],\n         [30974, 30916,   822, 30919, 30970, 30925],\n         [ 2065, 30925, 30965, 30926, 23347,   268],\n         [10789, 30927, 11408,   271,  9563, 30933],\n         [30929,  9563, 30933,  5282,   266,    13],\n         [10038,  5323, 30929,   689,  2277, 30919],\n         [54804, 35228, 54819, 55720, 55720, 56645],\n         [ 2065, 30925, 30965, 30926, 23347,   268],\n         [30978,   299,  6196, 30927,   268,  6196],\n         [30929, 30934,   416, 30929, 30926,   268],\n         [  299,   299, 30917, 30918, 30910,    13],\n         [30916,  2823, 30921,   317, 30931,   317],\n         [16462, 26604, 30919,   299,   416, 30929],\n         [30978,   299,  6196, 30927,   268,  6196],\n         [30978, 30978, 30978, 30978, 30978, 30910],\n         [64790, 64792, 30910, 30939, 31011, 30943],\n         [   13, 30972, 31011, 30966, 31011, 30943],\n         [  467,   315, 30926,   734,   317, 30931],\n         [10038,  5323, 30929,   689,  2277, 30919],\n         [  299, 10240, 30919,   438,  1480,   438],\n         [   13, 30972, 31011, 30966, 31011, 30943],\n         [   13, 15939,   317,   265, 30927,  5323],\n         [30916,  2823, 30921,   317, 30931,   317],\n         [42660, 55674, 31123, 34380, 39507,  5088],\n         [  467,   315, 30926,   734,   317, 30931],\n         [30929,  9563, 30933,  5282,   266,    13],\n         [30929,   467, 30927,  1063,  1056, 30972],\n         [16462, 26604, 30919,   299,   416, 30929],\n         [42660, 55674, 31123, 34380, 39507,  5088],\n         [64790, 64792, 30910, 30939, 31011, 30943],\n         [54804, 35228, 54819, 55720, 55720, 56645],\n         [10789, 30927, 11408,   271,  9563, 30933],\n         [ 5239, 30926,   265,    13, 36545, 31994],\n         [ 4571, 30953, 30920, 30933, 13532, 30921],\n         [30978, 30978, 30978, 30978, 30978, 30910],\n         [30974, 30916,   822, 30919, 30970, 30925],\n         [  299,   299, 30917, 30918, 30910,    13],\n         [ 5239, 30926,   265,    13, 36545, 31994],\n         [ 4571, 30953, 30920, 30933, 13532, 30921],\n         [  299, 10240, 30919,   438,  1480,   438],\n         [30929,   467, 30927,  1063,  1056, 30972]])}"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# 注意加上*\na=lm_datasets['input_ids']\nlist(chain(*a))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-31T04:36:49.226020Z","iopub.execute_input":"2023-07-31T04:36:49.226464Z","iopub.status.idle":"2023-07-31T04:36:49.246300Z","shell.execute_reply.started":"2023-07-31T04:36:49.226426Z","shell.execute_reply":"2023-07-31T04:36:49.245138Z"},"trusted":true},"execution_count":250,"outputs":[{"execution_count":250,"output_type":"execute_result","data":{"text/plain":"[[64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972],\n [30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978],\n [30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917],\n [299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578],\n [13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296],\n [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317],\n [30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953],\n [2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926],\n [5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514],\n [42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920],\n [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933],\n [30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058],\n [54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995],\n [4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265],\n [30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927],\n [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540],\n [13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299],\n [10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265],\n [30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299],\n [467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317],\n [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366],\n [64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972],\n [30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978],\n [30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917],\n [299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578],\n [13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296],\n [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317],\n [30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953],\n [2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926],\n [5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514],\n [42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920],\n [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933],\n [30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058],\n [54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995],\n [4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265],\n [30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927],\n [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540],\n [13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299],\n [10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265],\n [30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299],\n [467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317],\n [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366]]"},"metadata":{}}]},{"cell_type":"code","source":"lm_datasets['train']['input_ids']","metadata":{"execution":{"iopub.status.busy":"2023-07-31T03:06:31.270348Z","iopub.execute_input":"2023-07-31T03:06:31.270792Z","iopub.status.idle":"2023-07-31T03:06:31.290551Z","shell.execute_reply.started":"2023-07-31T03:06:31.270759Z","shell.execute_reply":"2023-07-31T03:06:31.289556Z"},"trusted":true},"execution_count":138,"outputs":[{"execution_count":138,"output_type":"execute_result","data":{"text/plain":"[[[64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972],\n  [30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978],\n  [30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917],\n  [299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578],\n  [13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296],\n  [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317],\n  [30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953],\n  [2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926],\n  [5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514],\n  [42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920],\n  [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933],\n  [30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058],\n  [54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995],\n  [4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265],\n  [30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927],\n  [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540],\n  [13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299],\n  [10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265],\n  [30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299],\n  [467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317],\n  [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366]],\n [[64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972],\n  [30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978],\n  [30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917],\n  [299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578],\n  [13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296],\n  [10789, 30927, 11408, 271, 9563, 30933, 5282, 266, 310, 317],\n  [30929, 467, 30927, 1063, 1056, 30972, 374, 30965, 30926, 29953],\n  [2065, 30925, 30965, 30926, 23347, 268, 30921, 30965, 30916, 30926],\n  [5239, 30926, 265, 13, 36545, 31994, 265, 54622, 55282, 31514],\n  [42660, 55674, 31123, 34380, 39507, 5088, 14724, 310, 30933, 30920],\n  [16462, 26604, 30919, 299, 416, 30929, 30920, 30965, 795, 30933],\n  [30916, 2823, 30921, 317, 30931, 317, 30921, 30910, 13, 55058],\n  [54804, 35228, 54819, 55720, 55720, 56645, 31123, 31538, 31529, 30995],\n  [4571, 30953, 30920, 30933, 13532, 30921, 260, 265, 1298, 265],\n  [30974, 30916, 822, 30919, 30970, 30925, 280, 670, 20092, 30927],\n  [30929, 30934, 416, 30929, 30926, 268, 30970, 30969, 30965, 24540],\n  [13, 15939, 317, 265, 30927, 5323, 30970, 22130, 416, 299],\n  [10038, 5323, 30929, 689, 2277, 30919, 2277, 299, 30927, 265],\n  [30929, 9563, 30933, 5282, 266, 13, 30927, 438, 398, 299],\n  [467, 315, 30926, 734, 317, 30931, 13, 13, 5377, 317],\n  [299, 299, 30917, 30918, 30910, 13, 13, 13, 465, 366]]]"},"metadata":{}}]},{"cell_type":"code","source":"a=[[64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972],\n  [30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978]]\nb=[[30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917],\n  [299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578],\n  [13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296],]\nfrom itertools import chain\nlist(chain(a,b))","metadata":{"execution":{"iopub.status.busy":"2023-07-31T03:39:10.354886Z","iopub.execute_input":"2023-07-31T03:39:10.355335Z","iopub.status.idle":"2023-07-31T03:39:10.367623Z","shell.execute_reply.started":"2023-07-31T03:39:10.355301Z","shell.execute_reply":"2023-07-31T03:39:10.366689Z"},"trusted":true},"execution_count":173,"outputs":[{"execution_count":173,"output_type":"execute_result","data":{"text/plain":"[[64790, 64792, 30910, 30939, 31011, 30943, 31011, 30966, 31011, 30972],\n [30978, 30978, 30978, 30978, 30978, 30910, 30978, 30978, 30978, 30978],\n [30978, 299, 6196, 30927, 268, 6196, 299, 1480, 1480, 30917],\n [299, 10240, 30919, 438, 1480, 438, 292, 30919, 279, 12578],\n [13, 30972, 31011, 30966, 31011, 30943, 30980, 30939, 647, 296]]"},"metadata":{}}]},{"cell_type":"code","source":"# 可以验证\n#list(chain(lm_datasets['train']['input_ids'][0] ,lm_datasets['train']['input_ids'][1]))","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:10:33.423041Z","iopub.execute_input":"2023-07-31T14:10:33.423431Z","iopub.status.idle":"2023-07-31T14:10:33.428209Z","shell.execute_reply.started":"2023-07-31T14:10:33.423398Z","shell.execute_reply":"2023-07-31T14:10:33.427085Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"#可以验证\n#tokenized_dataset['train']['input_ids']","metadata":{"execution":{"iopub.status.busy":"2023-07-31T14:10:41.417770Z","iopub.execute_input":"2023-07-31T14:10:41.418163Z","iopub.status.idle":"2023-07-31T14:10:41.423007Z","shell.execute_reply.started":"2023-07-31T14:10:41.418128Z","shell.execute_reply":"2023-07-31T14:10:41.421539Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"a=[1,2]\na[:5]","metadata":{"execution":{"iopub.status.busy":"2023-07-31T07:20:33.711492Z","iopub.execute_input":"2023-07-31T07:20:33.711901Z","iopub.status.idle":"2023-07-31T07:20:33.720058Z","shell.execute_reply.started":"2023-07-31T07:20:33.711867Z","shell.execute_reply":"2023-07-31T07:20:33.718994Z"},"trusted":true},"execution_count":399,"outputs":[{"execution_count":399,"output_type":"execute_result","data":{"text/plain":"[1, 2]"},"metadata":{}}]},{"cell_type":"code","source":"!git clone https://github.com/valkryhx/chatGLM-6B-QLoRA","metadata":{"execution":{"iopub.status.busy":"2023-07-31T07:38:32.577090Z","iopub.execute_input":"2023-07-31T07:38:32.577986Z","iopub.status.idle":"2023-07-31T07:38:36.080069Z","shell.execute_reply.started":"2023-07-31T07:38:32.577951Z","shell.execute_reply":"2023-07-31T07:38:36.078925Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'chatGLM-6B-QLoRA'...\nremote: Enumerating objects: 941, done.\u001b[K\nremote: Counting objects: 100% (161/161), done.\u001b[K\nremote: Compressing objects: 100% (85/85), done.\u001b[K\nremote: Total 941 (delta 123), reused 73 (delta 73), pack-reused 780\u001b[K\nReceiving objects: 100% (941/941), 17.26 MiB | 15.56 MiB/s, done.\nResolving deltas: 100% (592/592), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"cd /kaggle/working/chatGLM-6B-QLoRA","metadata":{"execution":{"iopub.status.busy":"2023-07-31T12:08:07.005651Z","iopub.execute_input":"2023-07-31T12:08:07.006765Z","iopub.status.idle":"2023-07-31T12:08:07.013734Z","shell.execute_reply.started":"2023-07-31T12:08:07.006715Z","shell.execute_reply":"2023-07-31T12:08:07.012676Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"/kaggle/working/chatGLM-6B-QLoRA\n","output_type":"stream"}]},{"cell_type":"code","source":"!git pull --all --force\n#!pip install peft==0.4.0\n#!pip install  -U git+https://github.com/huggingface/peft.git\n%cd /kaggle/working/chatGLM-6B-QLoRA \n!ls\n!pip install -r requirements.txt\n#!pip install deepspeed==0.9.5  这个也是需要的 但是目前kaggle 的runtime自带了","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls data/pretrain/xuanyan","metadata":{"execution":{"iopub.status.busy":"2023-07-31T13:36:04.353352Z","iopub.execute_input":"2023-07-31T13:36:04.353941Z","iopub.status.idle":"2023-07-31T13:36:05.355775Z","shell.execute_reply.started":"2023-07-31T13:36:04.353898Z","shell.execute_reply":"2023-07-31T13:36:05.354598Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stdout","text":"2  xy.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"!git pull --all --force \n#!pip install  -U git+https://github.com/huggingface/peft.git   # 20230717 peft==0.4.0正式发布了 不用调版本了推理完后再训练需要重新升级到0.4.0dev 所以有这句\n!deepspeed --include localhost:0,1  pretrain_qlora_chatglm2.py \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-pt \\\n  --num_train_samples -1 \\\n  --num_eval_samples 20 \\\n  --block_size 1024 \\\n  --train_data_path ./data/pretrain/xuanyan/ \\\n  --eval_data_path  ./data/pretrain/xuanyan/ \\\n  --max_length 1024 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 1 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  5e-5 \\\n  --num_train_epochs  40  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"execution":{"iopub.status.busy":"2023-07-31T15:42:55.202053Z","iopub.execute_input":"2023-07-31T15:42:55.202476Z","iopub.status.idle":"2023-07-31T16:10:46.278003Z","shell.execute_reply.started":"2023-07-31T15:42:55.202439Z","shell.execute_reply":"2023-07-31T16:10:46.271671Z"},"trusted":true},"execution_count":149,"outputs":[{"name":"stdout","text":"Fetching origin\nremote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (3/3), 682 bytes | 227.00 KiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   5d7feba..78dd514  main       -> origin/main\nUpdating 5d7feba..78dd514\nFast-forward\n pretrain_qlora_chatglm2.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n 1 file changed, 1 insertion(+), 1 deletion(-)\n[2023-07-31 15:42:59,920] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-07-31 15:43:07,267] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n[2023-07-31 15:43:07,283] [INFO] [runner.py:555:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None pretrain_qlora_chatglm2.py --train_args_json luzi.json --model_name_or_path THUDM/chatglm2-6b --output_dir output-pt --num_train_samples -1 --num_eval_samples 20 --block_size 1024 --train_data_path ./data/pretrain/xuanyan/ --eval_data_path ./data/pretrain/xuanyan/ --max_length 1024 --lora_rank 64 --lora_dropout 0.05 --compute_dtype fp16 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --learning_rate 5e-5 --num_train_epochs 40 --save_total_limit 2 --load_in_4bit True --deepspeed ds_zero2_config.json\n[2023-07-31 15:43:09,144] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-07-31 15:43:15,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n[2023-07-31 15:43:15,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n[2023-07-31 15:43:15,415] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n[2023-07-31 15:43:15,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n[2023-07-31 15:43:15,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n[2023-07-31 15:43:15,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n[2023-07-31 15:43:15,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n[2023-07-31 15:43:15,415] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n[2023-07-31 15:43:15,415] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n[2023-07-31 15:43:15,416] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n[2023-07-31 15:43:15,416] [INFO] [launch.py:163:main] dist_world_size=2\n[2023-07-31 15:43:15,416] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n[2023-07-31 15:43:18,588] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-07-31 15:43:18,671] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/cuda/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n/kaggle/working/chatGLM-6B-QLoRA/pretrain_qlora_chatglm2.py:51: FutureWarning: set_caching_enabled is deprecated and will be removed in the next major version of datasets. Use datasets.enable_caching() or datasets.disable_caching() instead. This function will be removed in a future version of datasets.\n  set_caching_enabled(False)\nsave !!!!!!\n\u001b[32m2023-07-31 15:43:29.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m419\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n/kaggle/working/chatGLM-6B-QLoRA/pretrain_qlora_chatglm2.py:51: FutureWarning: set_caching_enabled is deprecated and will be removed in the next major version of datasets. Use datasets.enable_caching() or datasets.disable_caching() instead. This function will be removed in a future version of datasets.\n  set_caching_enabled(False)\nsave !!!!!!\n\u001b[32m2023-07-31 15:43:29.069\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m419\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n\u001b[32m2023-07-31 15:43:29.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m465\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-07-31 15:43:29.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_dataset_for_pretrain\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1mdata files: ./data/pretrain/xuanyan/xy.txt\u001b[0m\n\u001b[32m2023-07-31 15:43:29.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m465\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-07-31 15:43:29.331\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_dataset_for_pretrain\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1mdata files: ./data/pretrain/xuanyan/xy.txt\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 257.81it/s]\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 258.70it/s]\nMap:   0%|                                         | 0/1 [00:00<?, ? examples/s]\u001b[32m2023-07-31 15:43:29.947\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m201\u001b[0m - \u001b[31m\u001b[1mexamples.keys()=['input_ids']\u001b[0m\nMap:   0%|                                         | 0/1 [00:00<?, ? examples/s]\u001b[32m2023-07-31 15:43:29.952\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m201\u001b[0m - \u001b[31m\u001b[1mexamples.keys()=['input_ids']\u001b[0m\n每个txt文件对应的samples个数 total_length=40558\n\u001b[32m2023-07-31 15:43:29.997\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m206\u001b[0m - \u001b[31m\u001b[1m注意如果出现空白内容txt 则会出现异常短的单个sample 长度为4 类似[64790, 64792, 30910, 13, 0, 0, 0, 0, 0,0]，很难排查 所以数据集目录中不要出现空白内容txt。\u001b[0m\n每个txt文件对应的samples个数 total_length=40558\n\u001b[32m2023-07-31 15:43:30.020\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m206\u001b[0m - \u001b[31m\u001b[1m注意如果出现空白内容txt 则会出现异常短的单个sample 长度为4 类似[64790, 64792, 30910, 13, 0, 0, 0, 0, 0,0]，很难排查 所以数据集目录中不要出现空白内容txt。\u001b[0m\n样本中ids 最大长度 max_len_ids=1024                                                    \n全局训练参数的global_args_max_length=1024\ncutoff to len = 1024\n\u001b[32m2023-07-31 15:43:30.139\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m244\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 第一个样本】\n中文马克思主义文库 -> 马克思　恩格斯\n共产党宣言\n\n马克思 恩格斯\n\n（1848年）\n\n\n· 1872年德文版序言\n· 1882年俄文版序言\n· 1883年德文版序言\n· 1888年英文版序言\n· 1890年德文版序言\n· 1892年波兰文版序言\n· 1893年意大利文版序言\n\n共产党宣言\n一、资产者和无产者\n二、无产者和共产党人\n三、社会主义的和共产主义的文献\n四、共产党人对各种反对党派的态度\n人名索引\n\n\n　　〔来源〕原载中共中央马克思恩格斯列宁斯大林著作编译局《共产党宣言（纪念马克思诞辰200周年马克思恩格斯著作特辑》（人民出版社，2018-4-1）\n　　中文马克思主义文库2022年11月更新。\n\n\n1872年德文版序言[1]\n\n　　共产主义者同盟[2]这个在当时条件下自然只能是秘密团体的国际工人组织，1847年11月在伦敦举行的代表大会上委托我们两人起草一个准备公布的详细的理论和实践的党纲。结果就产生了这个《宣言》，《宣言》原稿在二月革命[3]前几星期送到伦敦付印。《宣言》最初用德文出版，它用这种文字在德国、英国和美国至少印过12种不同的版本。第一个英译本是由海伦·麦克法林女士翻译的，于1850年在伦敦《红色共和党人》[4]杂志上发表，1871年至少又有三种不同的英译本在美国出版。法译本于1848年六月起义[5]前不久第一次在巴黎印行，最近又有法译本在纽约《社会主义者报》[6]上发表；现在有人在准备新译本。波兰文译本在德文本初版问世后不久就在伦敦出现。俄译本是60年代在日内瓦出版的。丹麦文译本也是在原书问世后不久就出版了。\n\n　　不管最近25年来的情况发生了多大的变化，这个《宣言》中所阐述的一般原理整个说来直到现在还是完全正确的。某些地方本来可以作一些修改。这些原理的实际运用，正如《宣言》中所说的，随时随地都要以当时的历史条件为转移，所以第二章末尾提出的那些革命措施根本没有特别的意义。如果是在今天，这一段在许多方面都会有不同的写法了。由于最近25年来大工业有了巨大发展而工人阶级的政党组织也跟着发展起来，由于首先有了二月革命的实际经验而后来尤其是有了无产阶级第一次掌握政权达两月之久的巴黎公社[7]的实际经验，所以这个纲领现在有些地方已经过时了。特别是公社已经证明：“工人阶级不能简单地掌握现成的国家机器，并运用它来达到自己的目的。”（见《法兰西内战。国际工人协会总委员会宣言》德文版第19页，那里对这个思想作了更详细的阐述。）〔注：见《马克思恩格斯选集》第3版第3卷第95页。——编者注〕其次，很明显，对于社会主义文献所作的批判在今天看来是不完全的，因为这一批判只包括到1847年为止；同样也很明显，关于共产党人对待各种反对党派的态度的论述（第四章）虽然在原则上今天还是正确的，但是就其实际运用来说今天毕竟已经过时，因为政治形势已经完全改变，当时所列举的那些党派大部分已被历史的发展彻底扫除了。\n\n　　但是《宣言》是一个历史文件，我们已没有权利来加以修改。下次再版时也许能加上一篇论述1847年到现在这段时期的导言。这次再版太仓促了，我们来不及做这件工作。\n\n　　卡尔·马克思　弗里德里希·恩格斯\n　　1872年6月24日于伦敦\n\n\n卡·马克思和弗·恩格斯写于1872年6月24日\n载于1872年在莱比锡出版的德文版《共产主义宣言》一书\n\n原文是德文\n选自《马克思恩格斯选集》第3版第1卷第376-377页\n\n\n\n1882年俄文版序言[8]\n\n　　巴枯宁翻译的《共产党宣言》俄文第一版，60年代初〔注：应是1869年。——编者注〕由《钟声》印刷所出版。当时西方认为这件事（《宣言》译成俄文出版）不过是著作界的一件\u001b[0m\n样本中ids 最大长度 max_len_ids=1024\n全局训练参数的global_args_max_length=1024\ncutoff to len = 1024\n\u001b[32m2023-07-31 15:43:30.152\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m245\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 倒数第二个样本（长度满足block size）】\n政府主义理论的创始人，第二共和国时期是制宪议会议员（1848）。——10、18、19、59、109、118、124、129。\n\n　　普芬德，卡尔（Pfänder，Carl 1819-1876）——德国微型画画家，德国工人运动和国际工人运动的活动家，1845年起侨居伦敦，正义者同盟盟员，伦敦德意志工人共产主义教育协会会员；1849年巴登-普法尔茨起义的参加者，起义失败后流亡英国；共产主义者同盟中央委员会委员，1850年共产主义者同盟分裂后支持马克思和恩格斯；国际总委员会委员（1864-1867和1870-1872）；马克思和恩格斯的朋友和战友。——104。\n\nR\n\n　　茹柯夫斯基，尤利·加拉克季昂诺维奇（Жуковский, Юлий Галактионович 1822-1907）——俄国资产阶级庸俗经济学家和政论家；国家银行行长；曾撰写《卡尔·马克思和他的〈资本论〉一书》一文，攻击马克思主义。——134。\n\nS\n\n　　沙佩尔，卡尔（Schapper，Karl 1812-1870）——德国工人运动和国际工人运动的活动家，正义者同盟的领导者之一，伦敦德意志工人共产主义教育协会创建人之一，共产主义者同盟中央委员会委员；1848-1849年革命的参加者；民主主义者莱茵区域委员会委员，该委员会案件（1849年2月8日）的被告之一；1849年2-5月为科隆工人联合会主席，《新莱茵报》撰稿人；1850年共产主义者同盟分裂时为冒险主义宗派集团的领袖之一；1856年起重新同马克思和恩格斯接近；国际总委员会委员（1865），1865年伦敦代表会议的参加者。——75、95、96、102、107、110、113、114、144。\n\n　　圣西门，昂利（Saint-Simon，Henri 1760-1825）——法国空想社会主义者。——60。\n\n　　施梯伯，威廉（Stieber，Wilhelm 1818-1882）——普鲁士警官，普鲁士政治警察局局长（1852-1860），科隆共产党人案件（1852）的策划者之一和主要原告证人；同卡·维尔穆特合编《19世纪共产主义者的阴谋》一书；普奥战争（1866）和普法战争（1870-1871）时期为军事警察局局长，在法国境内的德国情报机关的首脑。——94、105。\n\n　　叔尔茨，卡尔（Schurz，Carl 1829-1906）——德国政论家，小资产阶级民主主义者，1849年巴登-普法尔茨起义的参加者，起义失败后流亡瑞士，加入秘密组织“革命集中”；1852年迁居美国，站在北部方面参加美国内战，美国共和党领袖之一，曾任美国驻西班牙公使，后为参议员和内政部长（1877-1881）。——111。\n\nW\n\n　　维尔穆特，卡尔·格奥尔格·路德维希（Wermuth，Carl Georg Ludwig 1803-1867）——德国警官，汉诺威警察局长，科隆共产党人案件（1852）的策划者之一和原告证人；同威·施梯伯合编《19世纪共产主义者的阴谋》一书。——94、105。\n\n　　维利希，奥古斯特（Willich，August 1810-1878）——普鲁士军官，1847年起为共产主义者同盟盟员，1849年巴登-普法尔茨起义中为志愿军团首领；1850年共产主义者同盟分裂时同卡·沙佩尔一起组成反对马克思的冒险主义宗派集团；1853年侨居美国，站在北部方面参加美国内战，任将军。——110、111、113、114。\n\n　　魏特林，克里斯蒂安·威廉（Weitling，\u001b[0m\n\u001b[32m2023-07-31 15:43:30.159\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m244\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 第一个样本】\n中文马克思主义文库 -> 马克思　恩格斯\n共产党宣言\n\n马克思 恩格斯\n\n（1848年）\n\n\n· 1872年德文版序言\n· 1882年俄文版序言\n· 1883年德文版序言\n· 1888年英文版序言\n· 1890年德文版序言\n· 1892年波兰文版序言\n· 1893年意大利文版序言\n\n共产党宣言\n一、资产者和无产者\n二、无产者和共产党人\n三、社会主义的和共产主义的文献\n四、共产党人对各种反对党派的态度\n人名索引\n\n\n　　〔来源〕原载中共中央马克思恩格斯列宁斯大林著作编译局《共产党宣言（纪念马克思诞辰200周年马克思恩格斯著作特辑》（人民出版社，2018-4-1）\n　　中文马克思主义文库2022年11月更新。\n\n\n1872年德文版序言[1]\n\n　　共产主义者同盟[2]这个在当时条件下自然只能是秘密团体的国际工人组织，1847年11月在伦敦举行的代表大会上委托我们两人起草一个准备公布的详细的理论和实践的党纲。结果就产生了这个《宣言》，《宣言》原稿在二月革命[3]前几星期送到伦敦付印。《宣言》最初用德文出版，它用这种文字在德国、英国和美国至少印过12种不同的版本。第一个英译本是由海伦·麦克法林女士翻译的，于1850年在伦敦《红色共和党人》[4]杂志上发表，1871年至少又有三种不同的英译本在美国出版。法译本于1848年六月起义[5]前不久第一次在巴黎印行，最近又有法译本在纽约《社会主义者报》[6]上发表；现在有人在准备新译本。波兰文译本在德文本初版问世后不久就在伦敦出现。俄译本是60年代在日内瓦出版的。丹麦文译本也是在原书问世后不久就出版了。\n\n　　不管最近25年来的情况发生了多大的变化，这个《宣言》中所阐述的一般原理整个说来直到现在还是完全正确的。某些地方本来可以作一些修改。这些原理的实际运用，正如《宣言》中所说的，随时随地都要以当时的历史条件为转移，所以第二章末尾提出的那些革命措施根本没有特别的意义。如果是在今天，这一段在许多方面都会有不同的写法了。由于最近25年来大工业有了巨大发展而工人阶级的政党组织也跟着发展起来，由于首先有了二月革命的实际经验而后来尤其是有了无产阶级第一次掌握政权达两月之久的巴黎公社[7]的实际经验，所以这个纲领现在有些地方已经过时了。特别是公社已经证明：“工人阶级不能简单地掌握现成的国家机器，并运用它来达到自己的目的。”（见《法兰西内战。国际工人协会总委员会宣言》德文版第19页，那里对这个思想作了更详细的阐述。）〔注：见《马克思恩格斯选集》第3版第3卷第95页。——编者注〕其次，很明显，对于社会主义文献所作的批判在今天看来是不完全的，因为这一批判只包括到1847年为止；同样也很明显，关于共产党人对待各种反对党派的态度的论述（第四章）虽然在原则上今天还是正确的，但是就其实际运用来说今天毕竟已经过时，因为政治形势已经完全改变，当时所列举的那些党派大部分已被历史的发展彻底扫除了。\n\n　　但是《宣言》是一个历史文件，我们已没有权利来加以修改。下次再版时也许能加上一篇论述1847年到现在这段时期的导言。这次再版太仓促了，我们来不及做这件工作。\n\n　　卡尔·马克思　弗里德里希·恩格斯\n　　1872年6月24日于伦敦\n\n\n卡·马克思和弗·恩格斯写于1872年6月24日\n载于1872年在莱比锡出版的德文版《共产主义宣言》一书\n\n原文是德文\n选自《马克思恩格斯选集》第3版第1卷第376-377页\n\n\n\n1882年俄文版序言[8]\n\n　　巴枯宁翻译的《共产党宣言》俄文第一版，60年代初〔注：应是1869年。——编者注〕由《钟声》印刷所出版。当时西方认为这件事（《宣言》译成俄文出版）不过是著作界的一件\u001b[0m\n\u001b[32m2023-07-31 15:43:30.172\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m245\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 倒数第二个样本（长度满足block size）】\n政府主义理论的创始人，第二共和国时期是制宪议会议员（1848）。——10、18、19、59、109、118、124、129。\n\n　　普芬德，卡尔（Pfänder，Carl 1819-1876）——德国微型画画家，德国工人运动和国际工人运动的活动家，1845年起侨居伦敦，正义者同盟盟员，伦敦德意志工人共产主义教育协会会员；1849年巴登-普法尔茨起义的参加者，起义失败后流亡英国；共产主义者同盟中央委员会委员，1850年共产主义者同盟分裂后支持马克思和恩格斯；国际总委员会委员（1864-1867和1870-1872）；马克思和恩格斯的朋友和战友。——104。\n\nR\n\n　　茹柯夫斯基，尤利·加拉克季昂诺维奇（Жуковский, Юлий Галактионович 1822-1907）——俄国资产阶级庸俗经济学家和政论家；国家银行行长；曾撰写《卡尔·马克思和他的〈资本论〉一书》一文，攻击马克思主义。——134。\n\nS\n\n　　沙佩尔，卡尔（Schapper，Karl 1812-1870）——德国工人运动和国际工人运动的活动家，正义者同盟的领导者之一，伦敦德意志工人共产主义教育协会创建人之一，共产主义者同盟中央委员会委员；1848-1849年革命的参加者；民主主义者莱茵区域委员会委员，该委员会案件（1849年2月8日）的被告之一；1849年2-5月为科隆工人联合会主席，《新莱茵报》撰稿人；1850年共产主义者同盟分裂时为冒险主义宗派集团的领袖之一；1856年起重新同马克思和恩格斯接近；国际总委员会委员（1865），1865年伦敦代表会议的参加者。——75、95、96、102、107、110、113、114、144。\n\n　　圣西门，昂利（Saint-Simon，Henri 1760-1825）——法国空想社会主义者。——60。\n\n　　施梯伯，威廉（Stieber，Wilhelm 1818-1882）——普鲁士警官，普鲁士政治警察局局长（1852-1860），科隆共产党人案件（1852）的策划者之一和主要原告证人；同卡·维尔穆特合编《19世纪共产主义者的阴谋》一书；普奥战争（1866）和普法战争（1870-1871）时期为军事警察局局长，在法国境内的德国情报机关的首脑。——94、105。\n\n　　叔尔茨，卡尔（Schurz，Carl 1829-1906）——德国政论家，小资产阶级民主主义者，1849年巴登-普法尔茨起义的参加者，起义失败后流亡瑞士，加入秘密组织“革命集中”；1852年迁居美国，站在北部方面参加美国内战，美国共和党领袖之一，曾任美国驻西班牙公使，后为参议员和内政部长（1877-1881）。——111。\n\nW\n\n　　维尔穆特，卡尔·格奥尔格·路德维希（Wermuth，Carl Georg Ludwig 1803-1867）——德国警官，汉诺威警察局长，科隆共产党人案件（1852）的策划者之一和原告证人；同威·施梯伯合编《19世纪共产主义者的阴谋》一书。——94、105。\n\n　　维利希，奥古斯特（Willich，August 1810-1878）——普鲁士军官，1847年起为共产主义者同盟盟员，1849年巴登-普法尔茨起义中为志愿军团首领；1850年共产主义者同盟分裂时同卡·沙佩尔一起组成反对马克思的冒险主义宗派集团；1853年侨居美国，站在北部方面参加美国内战，任将军。——110、111、113、114。\n\n　　魏特林，克里斯蒂安·威廉（Weitling，\u001b[0m\n\u001b[32m2023-07-31 15:43:30.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 最后一个样本（长度不足blcok size）】\n【tokens】=[23688, 27720, 30910, 30939, 30973, 30940, 30973, 30941, 30939, 30973, 30981, 30939, 53437, 32942, 33796, 31903, 31651, 54561, 31123, 36495, 54631, 43166, 35945, 31123, 32156, 54532, 55857, 56985, 54659, 54913, 54695, 32686, 41005, 32135, 45508, 55685, 54587, 54561, 54659, 33796, 43166, 54530, 35461, 32395, 33796, 32892, 54771, 54612, 54530, 32124, 54631, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 31730, 31123, 39435, 33810, 31721, 33796, 32302, 36905, 30939, 30943, 31201, 30939, 30969, 31201, 30969, 30981, 30941, 30939, 30940, 30940, 31201, 30939, 30940, 30966, 31201, 30939, 30939, 30939, 31201, 30939, 30939, 30966, 31201, 30939, 30939, 30969, 31201, 30939, 30966, 30943, 31155, 13, 13, 31478, 31478, 40069, 56642, 31123, 56642, 54662, 38963, 55211, 31065, 42224, 31301, 30959, 325, 486, 31123, 30960, 2161, 6611, 27720, 30910, 55867, 55186, 54927, 380, 455, 337, 30910, 30939, 30973, 30940, 30969, 30941, 30939, 30973, 30978, 30972, 53437, 32942, 45853, 32123, 45508, 54681, 54903, 54561, 31123, 32156, 54532, 54635, 54650, 31123, 54739, 54662, 34793, 32868, 42574, 54659, 30939, 30973, 30966, 30972, 30941, 30939, 30973, 30966, 30969, 39103, 54637, 54534, 55186, 55867, 54930, 38600, 54659, 30939, 30973, 30972, 30978, 30941, 30939, 30973, 30972, 30981, 52317, 41588, 39136, 41005, 34105, 38508, 31123, 41005, 54631, 43166, 35461, 31905, 54542, 43166, 31819, 38508, 31301, 30939, 30973, 30972, 30973, 54540, 30966, 41630, 31798, 54611, 54575, 56236, 58181, 54771, 54612, 32494, 31301, 30939, 30973, 30972, 30973, 30941, 30939, 30973, 30972, 30969, 31798, 32681, 45856, 56236, 58181, 32159, 32007, 54542, 54693, 55954, 31660, 38508, 54659, 45886, 54982, 55049, 33039, 54910, 31769, 54650, 31123, 32180, 55023, 55419, 55443, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 37533, 31123, 30939, 30973, 30970, 30939, 54540, 56081, 55118, 32463, 31123, 30939, 30973, 30970, 30966, 37017, 54534, 56115, 55678, 34602, 54673, 54635, 54650, 54659, 33929, 54542, 55609, 47734, 33695, 54542, 40493, 36905, 30981, 30970, 31201, 30939, 30940, 30970, 31201, 30939, 30940, 30981, 31201, 30939, 30940, 30969, 31201, 30939, 30943, 30940, 31155, 13, 13, 31017, 13, 13, 31478, 31478, 44466, 55717, 54616, 31123, 54772, 31065, 55404, 55075, 31065, 56236, 55532, 55428, 55075, 31065, 54739, 55717, 54954, 31065, 54954, 31301, 30937, 1082, 639, 30915, 31123, 30977, 14649, 30941, 12392, 773, 30941, 30957, 31008, 264, 446, 3019, 11437, 384, 30910, 30939, 30981, 30981, 30966, 30941, 30939, 30973, 30972, 30943, 53437, 37533, 50639, 47450, 34486, 31123, 31832, 35936, 54538, 34317, 54545, 55443, 38942, 32477, 36905, 30970, 30972, 31155, 13, 13, 31478, 31478, 55287, 55075, 31123, 44554, 16747, 54904, 55404, 56313, 55075, 31123, 44554, 31155, 13, 13, 30986, 13, 13, 31478, 31478, 55640, 54693, 54703, 31123, 55256, 55929, 55097, 57100, 31301, 30977, 338, 15373, 31123, 30938, 7078, 2521, 30910, 30939, 30973, 30966, 30940, 30941, 30939, 30969, 30939, 30969, 53437, 32942, 32718, 31123, 55336, 55609, 49413, 37556, 32991, 34136, 33972, 31301, 30939, 30973, 30970, 30940, 30941, 30939, 30973, 30970, 30939, 31798, 41005, 54631, 43166, 55653, 54650, 54542, 54717, 54783, 31123, 54693, 55954, 32555, 54533, 32722, 31301, 30939, 30973, 30970, 30943, 31300, 54530, 34148, 31905, 31123, 54732, 56024, 55225, 36720, 39069, 54716, 55512, 31123, 50678, 54732, 55034, 30989, 46332, 37553, 52603, 30991, 54617, 31913, 54732, 55153, 55630, 54659, 30939, 30973, 30970, 30966, 54540, 54805, 55737, 32463, 31123, 54585, 56081, 55118, 31730, 31123, 35777, 54530, 41203, 54547, 31924, 34768, 31885, 54659, 33348, 36144, 31687, 31836, 31730, 52790, 31123, 54585, 54541, 35152, 48949, 49286, 31301, 30939, 30973, 30973, 30970, 30941, 30939, 30973, 30973, 30969, 31798, 31784, 37396, 54530, 32037, 54542, 33283, 31123, 55172, 54536, 32834, 32924, 34355, 36905, 30939, 30939, 30972, 31155, 13, 13, 31478, 31478, 51329, 41305, 47358, 31301, 31069, 30737, 6856, 31018, 31000, 6866, 30910, 30939, 30973, 30972, 30970, 30941, 30939, 30973, 30969, 30972, 53437, 46161, 33690, 31301, 30939, 30973, 30973, 30939, 30941, 30939, 30973, 30969, 30972, 31966, 16747, 30978, 31201, 30939, 30978, 31155, 13, 13, 13, 32823, 30910, 57906, 56484, 31201, 55233, 56722, 57087, 30910, 52806, 13, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n【最后一个样本原文】=Christian Wilhelm 1808-1871）——德国工人运动活动家，正义者同盟领导人，职业是裁缝；空想平均共产主义理论家和鼓动家；工人同盟的创始人，《工人共和国报》的出版者；1849年流亡美国，晚年接近国际工人协会。——12、19、97-100、103、111、113、119、132。\n\n　　沃尔弗，弗里德里希·威廉（Wolff，Friedrich Wilhelm 鲁普斯 Lupus 1809-1864）——德国无产阶级革命家和政论家，职业是教员，西里西亚农民的儿子；1834-1839年被关在普鲁士监狱；1846-1847年为布鲁塞尔共产主义通讯委员会委员，共产主义者同盟创始人之一和同盟中央委员会委员（1848年3月起），《新莱茵报》编辑（1848-1849），民主主义者莱茵区域委员会和科隆安全委员会委员；法兰克福国民议会议员，属于极左派；1849年流亡瑞士，1851年迁居英国，1853年起在曼彻斯特当教员；马克思和恩格斯的朋友和战友。——75、105、107、109、120。\n\nX\n\n　　西斯蒙第，让·沙尔·莱奥纳尔·西蒙德·德（Sismondi，Jean-Charles-Léonard Simonde de 1773-1842）——瑞士经济学家和历史学家，政治经济学中浪漫学派的代表人物。——54。\n\n　　席尔，卡尔——见沙佩尔，卡尔。\n\nY\n\n　　雅科比，阿伯拉罕（Jacobi，Abraham 1830-1919）——德国医生，波恩体操联合会创建人和会长（1850-1851），共产主义者同盟盟员和特使，科隆共产党人案件（1852）的被告之一，被陪审法庭宣告无罪，但因被控“侮辱国王陛下”而继续被监禁；1853年流亡英国，后迁居美国，在美国的刊物上宣传马克思主义思想；站在北部方面参加美国内战，后为纽约医学科学院院长（1885-1889），一些医学院的教授和院长，写有医学方面的著作。——114。\n\n　　亚历山大三世（Александр III 1845-1894）——俄国皇帝（1881-1894）。——6、16。\n\n\n感谢 聂孟、卫玄鹤 校对\n\n\n【最后一个样本原文长度】=893\u001b[0m\n\u001b[32m2023-07-31 15:43:30.182\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m验证查看完毕\u001b[0m\n样本数量=40\n\u001b[32m2023-07-31 15:43:30.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_dataset_for_pretrain\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1mdata files: ./data/pretrain/xuanyan/xy.txt\u001b[0m\n\u001b[32m2023-07-31 15:43:30.193\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 最后一个样本（长度不足blcok size）】\n【tokens】=[23688, 27720, 30910, 30939, 30973, 30940, 30973, 30941, 30939, 30973, 30981, 30939, 53437, 32942, 33796, 31903, 31651, 54561, 31123, 36495, 54631, 43166, 35945, 31123, 32156, 54532, 55857, 56985, 54659, 54913, 54695, 32686, 41005, 32135, 45508, 55685, 54587, 54561, 54659, 33796, 43166, 54530, 35461, 32395, 33796, 32892, 54771, 54612, 54530, 32124, 54631, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 31730, 31123, 39435, 33810, 31721, 33796, 32302, 36905, 30939, 30943, 31201, 30939, 30969, 31201, 30969, 30981, 30941, 30939, 30940, 30940, 31201, 30939, 30940, 30966, 31201, 30939, 30939, 30939, 31201, 30939, 30939, 30966, 31201, 30939, 30939, 30969, 31201, 30939, 30966, 30943, 31155, 13, 13, 31478, 31478, 40069, 56642, 31123, 56642, 54662, 38963, 55211, 31065, 42224, 31301, 30959, 325, 486, 31123, 30960, 2161, 6611, 27720, 30910, 55867, 55186, 54927, 380, 455, 337, 30910, 30939, 30973, 30940, 30969, 30941, 30939, 30973, 30978, 30972, 53437, 32942, 45853, 32123, 45508, 54681, 54903, 54561, 31123, 32156, 54532, 54635, 54650, 31123, 54739, 54662, 34793, 32868, 42574, 54659, 30939, 30973, 30966, 30972, 30941, 30939, 30973, 30966, 30969, 39103, 54637, 54534, 55186, 55867, 54930, 38600, 54659, 30939, 30973, 30972, 30978, 30941, 30939, 30973, 30972, 30981, 52317, 41588, 39136, 41005, 34105, 38508, 31123, 41005, 54631, 43166, 35461, 31905, 54542, 43166, 31819, 38508, 31301, 30939, 30973, 30972, 30973, 54540, 30966, 41630, 31798, 54611, 54575, 56236, 58181, 54771, 54612, 32494, 31301, 30939, 30973, 30972, 30973, 30941, 30939, 30973, 30972, 30969, 31798, 32681, 45856, 56236, 58181, 32159, 32007, 54542, 54693, 55954, 31660, 38508, 54659, 45886, 54982, 55049, 33039, 54910, 31769, 54650, 31123, 32180, 55023, 55419, 55443, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 37533, 31123, 30939, 30973, 30970, 30939, 54540, 56081, 55118, 32463, 31123, 30939, 30973, 30970, 30966, 37017, 54534, 56115, 55678, 34602, 54673, 54635, 54650, 54659, 33929, 54542, 55609, 47734, 33695, 54542, 40493, 36905, 30981, 30970, 31201, 30939, 30940, 30970, 31201, 30939, 30940, 30981, 31201, 30939, 30940, 30969, 31201, 30939, 30943, 30940, 31155, 13, 13, 31017, 13, 13, 31478, 31478, 44466, 55717, 54616, 31123, 54772, 31065, 55404, 55075, 31065, 56236, 55532, 55428, 55075, 31065, 54739, 55717, 54954, 31065, 54954, 31301, 30937, 1082, 639, 30915, 31123, 30977, 14649, 30941, 12392, 773, 30941, 30957, 31008, 264, 446, 3019, 11437, 384, 30910, 30939, 30981, 30981, 30966, 30941, 30939, 30973, 30972, 30943, 53437, 37533, 50639, 47450, 34486, 31123, 31832, 35936, 54538, 34317, 54545, 55443, 38942, 32477, 36905, 30970, 30972, 31155, 13, 13, 31478, 31478, 55287, 55075, 31123, 44554, 16747, 54904, 55404, 56313, 55075, 31123, 44554, 31155, 13, 13, 30986, 13, 13, 31478, 31478, 55640, 54693, 54703, 31123, 55256, 55929, 55097, 57100, 31301, 30977, 338, 15373, 31123, 30938, 7078, 2521, 30910, 30939, 30973, 30966, 30940, 30941, 30939, 30969, 30939, 30969, 53437, 32942, 32718, 31123, 55336, 55609, 49413, 37556, 32991, 34136, 33972, 31301, 30939, 30973, 30970, 30940, 30941, 30939, 30973, 30970, 30939, 31798, 41005, 54631, 43166, 55653, 54650, 54542, 54717, 54783, 31123, 54693, 55954, 32555, 54533, 32722, 31301, 30939, 30973, 30970, 30943, 31300, 54530, 34148, 31905, 31123, 54732, 56024, 55225, 36720, 39069, 54716, 55512, 31123, 50678, 54732, 55034, 30989, 46332, 37553, 52603, 30991, 54617, 31913, 54732, 55153, 55630, 54659, 30939, 30973, 30970, 30966, 54540, 54805, 55737, 32463, 31123, 54585, 56081, 55118, 31730, 31123, 35777, 54530, 41203, 54547, 31924, 34768, 31885, 54659, 33348, 36144, 31687, 31836, 31730, 52790, 31123, 54585, 54541, 35152, 48949, 49286, 31301, 30939, 30973, 30973, 30970, 30941, 30939, 30973, 30973, 30969, 31798, 31784, 37396, 54530, 32037, 54542, 33283, 31123, 55172, 54536, 32834, 32924, 34355, 36905, 30939, 30939, 30972, 31155, 13, 13, 31478, 31478, 51329, 41305, 47358, 31301, 31069, 30737, 6856, 31018, 31000, 6866, 30910, 30939, 30973, 30972, 30970, 30941, 30939, 30973, 30969, 30972, 53437, 46161, 33690, 31301, 30939, 30973, 30973, 30939, 30941, 30939, 30973, 30969, 30972, 31966, 16747, 30978, 31201, 30939, 30978, 31155, 13, 13, 13, 32823, 30910, 57906, 56484, 31201, 55233, 56722, 57087, 30910, 52806, 13, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n【最后一个样本原文】=Christian Wilhelm 1808-1871）——德国工人运动活动家，正义者同盟领导人，职业是裁缝；空想平均共产主义理论家和鼓动家；工人同盟的创始人，《工人共和国报》的出版者；1849年流亡美国，晚年接近国际工人协会。——12、19、97-100、103、111、113、119、132。\n\n　　沃尔弗，弗里德里希·威廉（Wolff，Friedrich Wilhelm 鲁普斯 Lupus 1809-1864）——德国无产阶级革命家和政论家，职业是教员，西里西亚农民的儿子；1834-1839年被关在普鲁士监狱；1846-1847年为布鲁塞尔共产主义通讯委员会委员，共产主义者同盟创始人之一和同盟中央委员会委员（1848年3月起），《新莱茵报》编辑（1848-1849），民主主义者莱茵区域委员会和科隆安全委员会委员；法兰克福国民议会议员，属于极左派；1849年流亡瑞士，1851年迁居英国，1853年起在曼彻斯特当教员；马克思和恩格斯的朋友和战友。——75、105、107、109、120。\n\nX\n\n　　西斯蒙第，让·沙尔·莱奥纳尔·西蒙德·德（Sismondi，Jean-Charles-Léonard Simonde de 1773-1842）——瑞士经济学家和历史学家，政治经济学中浪漫学派的代表人物。——54。\n\n　　席尔，卡尔——见沙佩尔，卡尔。\n\nY\n\n　　雅科比，阿伯拉罕（Jacobi，Abraham 1830-1919）——德国医生，波恩体操联合会创建人和会长（1850-1851），共产主义者同盟盟员和特使，科隆共产党人案件（1852）的被告之一，被陪审法庭宣告无罪，但因被控“侮辱国王陛下”而继续被监禁；1853年流亡英国，后迁居美国，在美国的刊物上宣传马克思主义思想；站在北部方面参加美国内战，后为纽约医学科学院院长（1885-1889），一些医学院的教授和院长，写有医学方面的著作。——114。\n\n　　亚历山大三世（Александр III 1845-1894）——俄国皇帝（1881-1894）。——6、16。\n\n\n感谢 聂孟、卫玄鹤 校对\n\n\n【最后一个样本原文长度】=893\u001b[0m\n\u001b[32m2023-07-31 15:43:30.194\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m验证查看完毕\u001b[0m\n样本数量=40\n\u001b[32m2023-07-31 15:43:30.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_dataset_for_pretrain\u001b[0m:\u001b[36m265\u001b[0m - \u001b[1mdata files: ./data/pretrain/xuanyan/xy.txt\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 489.47it/s]\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 563.30it/s]\n\u001b[32m2023-07-31 15:43:30.730\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m201\u001b[0m - \u001b[31m\u001b[1mexamples.keys()=['input_ids']\u001b[0m\nMap:   0%|                                         | 0/1 [00:00<?, ? examples/s]\u001b[32m2023-07-31 15:43:30.745\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m201\u001b[0m - \u001b[31m\u001b[1mexamples.keys()=['input_ids']\u001b[0m\n每个txt文件对应的samples个数 total_length=40558\n\u001b[32m2023-07-31 15:43:30.774\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m206\u001b[0m - \u001b[31m\u001b[1m注意如果出现空白内容txt 则会出现异常短的单个sample 长度为4 类似[64790, 64792, 30910, 13, 0, 0, 0, 0, 0,0]，很难排查 所以数据集目录中不要出现空白内容txt。\u001b[0m\n每个txt文件对应的samples个数 total_length=40558\n\u001b[32m2023-07-31 15:43:30.787\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgroup_texts\u001b[0m:\u001b[36m206\u001b[0m - \u001b[31m\u001b[1m注意如果出现空白内容txt 则会出现异常短的单个sample 长度为4 类似[64790, 64792, 30910, 13, 0, 0, 0, 0, 0,0]，很难排查 所以数据集目录中不要出现空白内容txt。\u001b[0m\n样本中ids 最大长度 max_len_ids=1024                                                    \n全局训练参数的global_args_max_length=1024\ncutoff to len = 1024\n样本中ids 最大长度 max_len_ids=1024\n全局训练参数的global_args_max_length=1024\ncutoff to len = 1024\n\u001b[32m2023-07-31 15:43:30.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m244\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 第一个样本】\n中文马克思主义文库 -> 马克思　恩格斯\n共产党宣言\n\n马克思 恩格斯\n\n（1848年）\n\n\n· 1872年德文版序言\n· 1882年俄文版序言\n· 1883年德文版序言\n· 1888年英文版序言\n· 1890年德文版序言\n· 1892年波兰文版序言\n· 1893年意大利文版序言\n\n共产党宣言\n一、资产者和无产者\n二、无产者和共产党人\n三、社会主义的和共产主义的文献\n四、共产党人对各种反对党派的态度\n人名索引\n\n\n　　〔来源〕原载中共中央马克思恩格斯列宁斯大林著作编译局《共产党宣言（纪念马克思诞辰200周年马克思恩格斯著作特辑》（人民出版社，2018-4-1）\n　　中文马克思主义文库2022年11月更新。\n\n\n1872年德文版序言[1]\n\n　　共产主义者同盟[2]这个在当时条件下自然只能是秘密团体的国际工人组织，1847年11月在伦敦举行的代表大会上委托我们两人起草一个准备公布的详细的理论和实践的党纲。结果就产生了这个《宣言》，《宣言》原稿在二月革命[3]前几星期送到伦敦付印。《宣言》最初用德文出版，它用这种文字在德国、英国和美国至少印过12种不同的版本。第一个英译本是由海伦·麦克法林女士翻译的，于1850年在伦敦《红色共和党人》[4]杂志上发表，1871年至少又有三种不同的英译本在美国出版。法译本于1848年六月起义[5]前不久第一次在巴黎印行，最近又有法译本在纽约《社会主义者报》[6]上发表；现在有人在准备新译本。波兰文译本在德文本初版问世后不久就在伦敦出现。俄译本是60年代在日内瓦出版的。丹麦文译本也是在原书问世后不久就出版了。\n\n　　不管最近25年来的情况发生了多大的变化，这个《宣言》中所阐述的一般原理整个说来直到现在还是完全正确的。某些地方本来可以作一些修改。这些原理的实际运用，正如《宣言》中所说的，随时随地都要以当时的历史条件为转移，所以第二章末尾提出的那些革命措施根本没有特别的意义。如果是在今天，这一段在许多方面都会有不同的写法了。由于最近25年来大工业有了巨大发展而工人阶级的政党组织也跟着发展起来，由于首先有了二月革命的实际经验而后来尤其是有了无产阶级第一次掌握政权达两月之久的巴黎公社[7]的实际经验，所以这个纲领现在有些地方已经过时了。特别是公社已经证明：“工人阶级不能简单地掌握现成的国家机器，并运用它来达到自己的目的。”（见《法兰西内战。国际工人协会总委员会宣言》德文版第19页，那里对这个思想作了更详细的阐述。）〔注：见《马克思恩格斯选集》第3版第3卷第95页。——编者注〕其次，很明显，对于社会主义文献所作的批判在今天看来是不完全的，因为这一批判只包括到1847年为止；同样也很明显，关于共产党人对待各种反对党派的态度的论述（第四章）虽然在原则上今天还是正确的，但是就其实际运用来说今天毕竟已经过时，因为政治形势已经完全改变，当时所列举的那些党派大部分已被历史的发展彻底扫除了。\n\n　　但是《宣言》是一个历史文件，我们已没有权利来加以修改。下次再版时也许能加上一篇论述1847年到现在这段时期的导言。这次再版太仓促了，我们来不及做这件工作。\n\n　　卡尔·马克思　弗里德里希·恩格斯\n　　1872年6月24日于伦敦\n\n\n卡·马克思和弗·恩格斯写于1872年6月24日\n载于1872年在莱比锡出版的德文版《共产主义宣言》一书\n\n原文是德文\n选自《马克思恩格斯选集》第3版第1卷第376-377页\n\n\n\n1882年俄文版序言[8]\n\n　　巴枯宁翻译的《共产党宣言》俄文第一版，60年代初〔注：应是1869年。——编者注〕由《钟声》印刷所出版。当时西方认为这件事（《宣言》译成俄文出版）不过是著作界的一件\u001b[0m\n\u001b[32m2023-07-31 15:43:30.872\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m244\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 第一个样本】\n中文马克思主义文库 -> 马克思　恩格斯\n共产党宣言\n\n马克思 恩格斯\n\n（1848年）\n\n\n· 1872年德文版序言\n· 1882年俄文版序言\n· 1883年德文版序言\n· 1888年英文版序言\n· 1890年德文版序言\n· 1892年波兰文版序言\n· 1893年意大利文版序言\n\n共产党宣言\n一、资产者和无产者\n二、无产者和共产党人\n三、社会主义的和共产主义的文献\n四、共产党人对各种反对党派的态度\n人名索引\n\n\n　　〔来源〕原载中共中央马克思恩格斯列宁斯大林著作编译局《共产党宣言（纪念马克思诞辰200周年马克思恩格斯著作特辑》（人民出版社，2018-4-1）\n　　中文马克思主义文库2022年11月更新。\n\n\n1872年德文版序言[1]\n\n　　共产主义者同盟[2]这个在当时条件下自然只能是秘密团体的国际工人组织，1847年11月在伦敦举行的代表大会上委托我们两人起草一个准备公布的详细的理论和实践的党纲。结果就产生了这个《宣言》，《宣言》原稿在二月革命[3]前几星期送到伦敦付印。《宣言》最初用德文出版，它用这种文字在德国、英国和美国至少印过12种不同的版本。第一个英译本是由海伦·麦克法林女士翻译的，于1850年在伦敦《红色共和党人》[4]杂志上发表，1871年至少又有三种不同的英译本在美国出版。法译本于1848年六月起义[5]前不久第一次在巴黎印行，最近又有法译本在纽约《社会主义者报》[6]上发表；现在有人在准备新译本。波兰文译本在德文本初版问世后不久就在伦敦出现。俄译本是60年代在日内瓦出版的。丹麦文译本也是在原书问世后不久就出版了。\n\n　　不管最近25年来的情况发生了多大的变化，这个《宣言》中所阐述的一般原理整个说来直到现在还是完全正确的。某些地方本来可以作一些修改。这些原理的实际运用，正如《宣言》中所说的，随时随地都要以当时的历史条件为转移，所以第二章末尾提出的那些革命措施根本没有特别的意义。如果是在今天，这一段在许多方面都会有不同的写法了。由于最近25年来大工业有了巨大发展而工人阶级的政党组织也跟着发展起来，由于首先有了二月革命的实际经验而后来尤其是有了无产阶级第一次掌握政权达两月之久的巴黎公社[7]的实际经验，所以这个纲领现在有些地方已经过时了。特别是公社已经证明：“工人阶级不能简单地掌握现成的国家机器，并运用它来达到自己的目的。”（见《法兰西内战。国际工人协会总委员会宣言》德文版第19页，那里对这个思想作了更详细的阐述。）〔注：见《马克思恩格斯选集》第3版第3卷第95页。——编者注〕其次，很明显，对于社会主义文献所作的批判在今天看来是不完全的，因为这一批判只包括到1847年为止；同样也很明显，关于共产党人对待各种反对党派的态度的论述（第四章）虽然在原则上今天还是正确的，但是就其实际运用来说今天毕竟已经过时，因为政治形势已经完全改变，当时所列举的那些党派大部分已被历史的发展彻底扫除了。\n\n　　但是《宣言》是一个历史文件，我们已没有权利来加以修改。下次再版时也许能加上一篇论述1847年到现在这段时期的导言。这次再版太仓促了，我们来不及做这件工作。\n\n　　卡尔·马克思　弗里德里希·恩格斯\n　　1872年6月24日于伦敦\n\n\n卡·马克思和弗·恩格斯写于1872年6月24日\n载于1872年在莱比锡出版的德文版《共产主义宣言》一书\n\n原文是德文\n选自《马克思恩格斯选集》第3版第1卷第376-377页\n\n\n\n1882年俄文版序言[8]\n\n　　巴枯宁翻译的《共产党宣言》俄文第一版，60年代初〔注：应是1869年。——编者注〕由《钟声》印刷所出版。当时西方认为这件事（《宣言》译成俄文出版）不过是著作界的一件\u001b[0m\n\u001b[32m2023-07-31 15:43:30.876\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m245\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 倒数第二个样本（长度满足block size）】\n政府主义理论的创始人，第二共和国时期是制宪议会议员（1848）。——10、18、19、59、109、118、124、129。\n\n　　普芬德，卡尔（Pfänder，Carl 1819-1876）——德国微型画画家，德国工人运动和国际工人运动的活动家，1845年起侨居伦敦，正义者同盟盟员，伦敦德意志工人共产主义教育协会会员；1849年巴登-普法尔茨起义的参加者，起义失败后流亡英国；共产主义者同盟中央委员会委员，1850年共产主义者同盟分裂后支持马克思和恩格斯；国际总委员会委员（1864-1867和1870-1872）；马克思和恩格斯的朋友和战友。——104。\n\nR\n\n　　茹柯夫斯基，尤利·加拉克季昂诺维奇（Жуковский, Юлий Галактионович 1822-1907）——俄国资产阶级庸俗经济学家和政论家；国家银行行长；曾撰写《卡尔·马克思和他的〈资本论〉一书》一文，攻击马克思主义。——134。\n\nS\n\n　　沙佩尔，卡尔（Schapper，Karl 1812-1870）——德国工人运动和国际工人运动的活动家，正义者同盟的领导者之一，伦敦德意志工人共产主义教育协会创建人之一，共产主义者同盟中央委员会委员；1848-1849年革命的参加者；民主主义者莱茵区域委员会委员，该委员会案件（1849年2月8日）的被告之一；1849年2-5月为科隆工人联合会主席，《新莱茵报》撰稿人；1850年共产主义者同盟分裂时为冒险主义宗派集团的领袖之一；1856年起重新同马克思和恩格斯接近；国际总委员会委员（1865），1865年伦敦代表会议的参加者。——75、95、96、102、107、110、113、114、144。\n\n　　圣西门，昂利（Saint-Simon，Henri 1760-1825）——法国空想社会主义者。——60。\n\n　　施梯伯，威廉（Stieber，Wilhelm 1818-1882）——普鲁士警官，普鲁士政治警察局局长（1852-1860），科隆共产党人案件（1852）的策划者之一和主要原告证人；同卡·维尔穆特合编《19世纪共产主义者的阴谋》一书；普奥战争（1866）和普法战争（1870-1871）时期为军事警察局局长，在法国境内的德国情报机关的首脑。——94、105。\n\n　　叔尔茨，卡尔（Schurz，Carl 1829-1906）——德国政论家，小资产阶级民主主义者，1849年巴登-普法尔茨起义的参加者，起义失败后流亡瑞士，加入秘密组织“革命集中”；1852年迁居美国，站在北部方面参加美国内战，美国共和党领袖之一，曾任美国驻西班牙公使，后为参议员和内政部长（1877-1881）。——111。\n\nW\n\n　　维尔穆特，卡尔·格奥尔格·路德维希（Wermuth，Carl Georg Ludwig 1803-1867）——德国警官，汉诺威警察局长，科隆共产党人案件（1852）的策划者之一和原告证人；同威·施梯伯合编《19世纪共产主义者的阴谋》一书。——94、105。\n\n　　维利希，奥古斯特（Willich，August 1810-1878）——普鲁士军官，1847年起为共产主义者同盟盟员，1849年巴登-普法尔茨起义中为志愿军团首领；1850年共产主义者同盟分裂时同卡·沙佩尔一起组成反对马克思的冒险主义宗派集团；1853年侨居美国，站在北部方面参加美国内战，任将军。——110、111、113、114。\n\n　　魏特林，克里斯蒂安·威廉（Weitling，\u001b[0m\n\u001b[32m2023-07-31 15:43:30.886\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m245\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 倒数第二个样本（长度满足block size）】\n政府主义理论的创始人，第二共和国时期是制宪议会议员（1848）。——10、18、19、59、109、118、124、129。\n\n　　普芬德，卡尔（Pfänder，Carl 1819-1876）——德国微型画画家，德国工人运动和国际工人运动的活动家，1845年起侨居伦敦，正义者同盟盟员，伦敦德意志工人共产主义教育协会会员；1849年巴登-普法尔茨起义的参加者，起义失败后流亡英国；共产主义者同盟中央委员会委员，1850年共产主义者同盟分裂后支持马克思和恩格斯；国际总委员会委员（1864-1867和1870-1872）；马克思和恩格斯的朋友和战友。——104。\n\nR\n\n　　茹柯夫斯基，尤利·加拉克季昂诺维奇（Жуковский, Юлий Галактионович 1822-1907）——俄国资产阶级庸俗经济学家和政论家；国家银行行长；曾撰写《卡尔·马克思和他的〈资本论〉一书》一文，攻击马克思主义。——134。\n\nS\n\n　　沙佩尔，卡尔（Schapper，Karl 1812-1870）——德国工人运动和国际工人运动的活动家，正义者同盟的领导者之一，伦敦德意志工人共产主义教育协会创建人之一，共产主义者同盟中央委员会委员；1848-1849年革命的参加者；民主主义者莱茵区域委员会委员，该委员会案件（1849年2月8日）的被告之一；1849年2-5月为科隆工人联合会主席，《新莱茵报》撰稿人；1850年共产主义者同盟分裂时为冒险主义宗派集团的领袖之一；1856年起重新同马克思和恩格斯接近；国际总委员会委员（1865），1865年伦敦代表会议的参加者。——75、95、96、102、107、110、113、114、144。\n\n　　圣西门，昂利（Saint-Simon，Henri 1760-1825）——法国空想社会主义者。——60。\n\n　　施梯伯，威廉（Stieber，Wilhelm 1818-1882）——普鲁士警官，普鲁士政治警察局局长（1852-1860），科隆共产党人案件（1852）的策划者之一和主要原告证人；同卡·维尔穆特合编《19世纪共产主义者的阴谋》一书；普奥战争（1866）和普法战争（1870-1871）时期为军事警察局局长，在法国境内的德国情报机关的首脑。——94、105。\n\n　　叔尔茨，卡尔（Schurz，Carl 1829-1906）——德国政论家，小资产阶级民主主义者，1849年巴登-普法尔茨起义的参加者，起义失败后流亡瑞士，加入秘密组织“革命集中”；1852年迁居美国，站在北部方面参加美国内战，美国共和党领袖之一，曾任美国驻西班牙公使，后为参议员和内政部长（1877-1881）。——111。\n\nW\n\n　　维尔穆特，卡尔·格奥尔格·路德维希（Wermuth，Carl Georg Ludwig 1803-1867）——德国警官，汉诺威警察局长，科隆共产党人案件（1852）的策划者之一和原告证人；同威·施梯伯合编《19世纪共产主义者的阴谋》一书。——94、105。\n\n　　维利希，奥古斯特（Willich，August 1810-1878）——普鲁士军官，1847年起为共产主义者同盟盟员，1849年巴登-普法尔茨起义中为志愿军团首领；1850年共产主义者同盟分裂时同卡·沙佩尔一起组成反对马克思的冒险主义宗派集团；1853年侨居美国，站在北部方面参加美国内战，任将军。——110、111、113、114。\n\n　　魏特林，克里斯蒂安·威廉（Weitling，\u001b[0m\n\u001b[32m2023-07-31 15:43:30.905\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 最后一个样本（长度不足blcok size）】\n【tokens】=[23688, 27720, 30910, 30939, 30973, 30940, 30973, 30941, 30939, 30973, 30981, 30939, 53437, 32942, 33796, 31903, 31651, 54561, 31123, 36495, 54631, 43166, 35945, 31123, 32156, 54532, 55857, 56985, 54659, 54913, 54695, 32686, 41005, 32135, 45508, 55685, 54587, 54561, 54659, 33796, 43166, 54530, 35461, 32395, 33796, 32892, 54771, 54612, 54530, 32124, 54631, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 31730, 31123, 39435, 33810, 31721, 33796, 32302, 36905, 30939, 30943, 31201, 30939, 30969, 31201, 30969, 30981, 30941, 30939, 30940, 30940, 31201, 30939, 30940, 30966, 31201, 30939, 30939, 30939, 31201, 30939, 30939, 30966, 31201, 30939, 30939, 30969, 31201, 30939, 30966, 30943, 31155, 13, 13, 31478, 31478, 40069, 56642, 31123, 56642, 54662, 38963, 55211, 31065, 42224, 31301, 30959, 325, 486, 31123, 30960, 2161, 6611, 27720, 30910, 55867, 55186, 54927, 380, 455, 337, 30910, 30939, 30973, 30940, 30969, 30941, 30939, 30973, 30978, 30972, 53437, 32942, 45853, 32123, 45508, 54681, 54903, 54561, 31123, 32156, 54532, 54635, 54650, 31123, 54739, 54662, 34793, 32868, 42574, 54659, 30939, 30973, 30966, 30972, 30941, 30939, 30973, 30966, 30969, 39103, 54637, 54534, 55186, 55867, 54930, 38600, 54659, 30939, 30973, 30972, 30978, 30941, 30939, 30973, 30972, 30981, 52317, 41588, 39136, 41005, 34105, 38508, 31123, 41005, 54631, 43166, 35461, 31905, 54542, 43166, 31819, 38508, 31301, 30939, 30973, 30972, 30973, 54540, 30966, 41630, 31798, 54611, 54575, 56236, 58181, 54771, 54612, 32494, 31301, 30939, 30973, 30972, 30973, 30941, 30939, 30973, 30972, 30969, 31798, 32681, 45856, 56236, 58181, 32159, 32007, 54542, 54693, 55954, 31660, 38508, 54659, 45886, 54982, 55049, 33039, 54910, 31769, 54650, 31123, 32180, 55023, 55419, 55443, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 37533, 31123, 30939, 30973, 30970, 30939, 54540, 56081, 55118, 32463, 31123, 30939, 30973, 30970, 30966, 37017, 54534, 56115, 55678, 34602, 54673, 54635, 54650, 54659, 33929, 54542, 55609, 47734, 33695, 54542, 40493, 36905, 30981, 30970, 31201, 30939, 30940, 30970, 31201, 30939, 30940, 30981, 31201, 30939, 30940, 30969, 31201, 30939, 30943, 30940, 31155, 13, 13, 31017, 13, 13, 31478, 31478, 44466, 55717, 54616, 31123, 54772, 31065, 55404, 55075, 31065, 56236, 55532, 55428, 55075, 31065, 54739, 55717, 54954, 31065, 54954, 31301, 30937, 1082, 639, 30915, 31123, 30977, 14649, 30941, 12392, 773, 30941, 30957, 31008, 264, 446, 3019, 11437, 384, 30910, 30939, 30981, 30981, 30966, 30941, 30939, 30973, 30972, 30943, 53437, 37533, 50639, 47450, 34486, 31123, 31832, 35936, 54538, 34317, 54545, 55443, 38942, 32477, 36905, 30970, 30972, 31155, 13, 13, 31478, 31478, 55287, 55075, 31123, 44554, 16747, 54904, 55404, 56313, 55075, 31123, 44554, 31155, 13, 13, 30986, 13, 13, 31478, 31478, 55640, 54693, 54703, 31123, 55256, 55929, 55097, 57100, 31301, 30977, 338, 15373, 31123, 30938, 7078, 2521, 30910, 30939, 30973, 30966, 30940, 30941, 30939, 30969, 30939, 30969, 53437, 32942, 32718, 31123, 55336, 55609, 49413, 37556, 32991, 34136, 33972, 31301, 30939, 30973, 30970, 30940, 30941, 30939, 30973, 30970, 30939, 31798, 41005, 54631, 43166, 55653, 54650, 54542, 54717, 54783, 31123, 54693, 55954, 32555, 54533, 32722, 31301, 30939, 30973, 30970, 30943, 31300, 54530, 34148, 31905, 31123, 54732, 56024, 55225, 36720, 39069, 54716, 55512, 31123, 50678, 54732, 55034, 30989, 46332, 37553, 52603, 30991, 54617, 31913, 54732, 55153, 55630, 54659, 30939, 30973, 30970, 30966, 54540, 54805, 55737, 32463, 31123, 54585, 56081, 55118, 31730, 31123, 35777, 54530, 41203, 54547, 31924, 34768, 31885, 54659, 33348, 36144, 31687, 31836, 31730, 52790, 31123, 54585, 54541, 35152, 48949, 49286, 31301, 30939, 30973, 30973, 30970, 30941, 30939, 30973, 30973, 30969, 31798, 31784, 37396, 54530, 32037, 54542, 33283, 31123, 55172, 54536, 32834, 32924, 34355, 36905, 30939, 30939, 30972, 31155, 13, 13, 31478, 31478, 51329, 41305, 47358, 31301, 31069, 30737, 6856, 31018, 31000, 6866, 30910, 30939, 30973, 30972, 30970, 30941, 30939, 30973, 30969, 30972, 53437, 46161, 33690, 31301, 30939, 30973, 30973, 30939, 30941, 30939, 30973, 30969, 30972, 31966, 16747, 30978, 31201, 30939, 30978, 31155, 13, 13, 13, 32823, 30910, 57906, 56484, 31201, 55233, 56722, 57087, 30910, 52806, 13, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n【最后一个样本原文】=Christian Wilhelm 1808-1871）——德国工人运动活动家，正义者同盟领导人，职业是裁缝；空想平均共产主义理论家和鼓动家；工人同盟的创始人，《工人共和国报》的出版者；1849年流亡美国，晚年接近国际工人协会。——12、19、97-100、103、111、113、119、132。\n\n　　沃尔弗，弗里德里希·威廉（Wolff，Friedrich Wilhelm 鲁普斯 Lupus 1809-1864）——德国无产阶级革命家和政论家，职业是教员，西里西亚农民的儿子；1834-1839年被关在普鲁士监狱；1846-1847年为布鲁塞尔共产主义通讯委员会委员，共产主义者同盟创始人之一和同盟中央委员会委员（1848年3月起），《新莱茵报》编辑（1848-1849），民主主义者莱茵区域委员会和科隆安全委员会委员；法兰克福国民议会议员，属于极左派；1849年流亡瑞士，1851年迁居英国，1853年起在曼彻斯特当教员；马克思和恩格斯的朋友和战友。——75、105、107、109、120。\n\nX\n\n　　西斯蒙第，让·沙尔·莱奥纳尔·西蒙德·德（Sismondi，Jean-Charles-Léonard Simonde de 1773-1842）——瑞士经济学家和历史学家，政治经济学中浪漫学派的代表人物。——54。\n\n　　席尔，卡尔——见沙佩尔，卡尔。\n\nY\n\n　　雅科比，阿伯拉罕（Jacobi，Abraham 1830-1919）——德国医生，波恩体操联合会创建人和会长（1850-1851），共产主义者同盟盟员和特使，科隆共产党人案件（1852）的被告之一，被陪审法庭宣告无罪，但因被控“侮辱国王陛下”而继续被监禁；1853年流亡英国，后迁居美国，在美国的刊物上宣传马克思主义思想；站在北部方面参加美国内战，后为纽约医学科学院院长（1885-1889），一些医学院的教授和院长，写有医学方面的著作。——114。\n\n　　亚历山大三世（Александр III 1845-1894）——俄国皇帝（1881-1894）。——6、16。\n\n\n感谢 聂孟、卫玄鹤 校对\n\n\n【最后一个样本原文长度】=893\u001b[0m\n\u001b[32m2023-07-31 15:43:30.906\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m验证查看完毕\u001b[0m\n样本数量=20\n\u001b[32m2023-07-31 15:43:30.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m246\u001b[0m - \u001b[1m【验证查看 shuffle和采样之前 最后一个样本（长度不足blcok size）】\n【tokens】=[23688, 27720, 30910, 30939, 30973, 30940, 30973, 30941, 30939, 30973, 30981, 30939, 53437, 32942, 33796, 31903, 31651, 54561, 31123, 36495, 54631, 43166, 35945, 31123, 32156, 54532, 55857, 56985, 54659, 54913, 54695, 32686, 41005, 32135, 45508, 55685, 54587, 54561, 54659, 33796, 43166, 54530, 35461, 32395, 33796, 32892, 54771, 54612, 54530, 32124, 54631, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 31730, 31123, 39435, 33810, 31721, 33796, 32302, 36905, 30939, 30943, 31201, 30939, 30969, 31201, 30969, 30981, 30941, 30939, 30940, 30940, 31201, 30939, 30940, 30966, 31201, 30939, 30939, 30939, 31201, 30939, 30939, 30966, 31201, 30939, 30939, 30969, 31201, 30939, 30966, 30943, 31155, 13, 13, 31478, 31478, 40069, 56642, 31123, 56642, 54662, 38963, 55211, 31065, 42224, 31301, 30959, 325, 486, 31123, 30960, 2161, 6611, 27720, 30910, 55867, 55186, 54927, 380, 455, 337, 30910, 30939, 30973, 30940, 30969, 30941, 30939, 30973, 30978, 30972, 53437, 32942, 45853, 32123, 45508, 54681, 54903, 54561, 31123, 32156, 54532, 54635, 54650, 31123, 54739, 54662, 34793, 32868, 42574, 54659, 30939, 30973, 30966, 30972, 30941, 30939, 30973, 30966, 30969, 39103, 54637, 54534, 55186, 55867, 54930, 38600, 54659, 30939, 30973, 30972, 30978, 30941, 30939, 30973, 30972, 30981, 52317, 41588, 39136, 41005, 34105, 38508, 31123, 41005, 54631, 43166, 35461, 31905, 54542, 43166, 31819, 38508, 31301, 30939, 30973, 30972, 30973, 54540, 30966, 41630, 31798, 54611, 54575, 56236, 58181, 54771, 54612, 32494, 31301, 30939, 30973, 30972, 30973, 30941, 30939, 30973, 30972, 30969, 31798, 32681, 45856, 56236, 58181, 32159, 32007, 54542, 54693, 55954, 31660, 38508, 54659, 45886, 54982, 55049, 33039, 54910, 31769, 54650, 31123, 32180, 55023, 55419, 55443, 54659, 30939, 30973, 30972, 30969, 54540, 54805, 55737, 37533, 31123, 30939, 30973, 30970, 30939, 54540, 56081, 55118, 32463, 31123, 30939, 30973, 30970, 30966, 37017, 54534, 56115, 55678, 34602, 54673, 54635, 54650, 54659, 33929, 54542, 55609, 47734, 33695, 54542, 40493, 36905, 30981, 30970, 31201, 30939, 30940, 30970, 31201, 30939, 30940, 30981, 31201, 30939, 30940, 30969, 31201, 30939, 30943, 30940, 31155, 13, 13, 31017, 13, 13, 31478, 31478, 44466, 55717, 54616, 31123, 54772, 31065, 55404, 55075, 31065, 56236, 55532, 55428, 55075, 31065, 54739, 55717, 54954, 31065, 54954, 31301, 30937, 1082, 639, 30915, 31123, 30977, 14649, 30941, 12392, 773, 30941, 30957, 31008, 264, 446, 3019, 11437, 384, 30910, 30939, 30981, 30981, 30966, 30941, 30939, 30973, 30972, 30943, 53437, 37533, 50639, 47450, 34486, 31123, 31832, 35936, 54538, 34317, 54545, 55443, 38942, 32477, 36905, 30970, 30972, 31155, 13, 13, 31478, 31478, 55287, 55075, 31123, 44554, 16747, 54904, 55404, 56313, 55075, 31123, 44554, 31155, 13, 13, 30986, 13, 13, 31478, 31478, 55640, 54693, 54703, 31123, 55256, 55929, 55097, 57100, 31301, 30977, 338, 15373, 31123, 30938, 7078, 2521, 30910, 30939, 30973, 30966, 30940, 30941, 30939, 30969, 30939, 30969, 53437, 32942, 32718, 31123, 55336, 55609, 49413, 37556, 32991, 34136, 33972, 31301, 30939, 30973, 30970, 30940, 30941, 30939, 30973, 30970, 30939, 31798, 41005, 54631, 43166, 55653, 54650, 54542, 54717, 54783, 31123, 54693, 55954, 32555, 54533, 32722, 31301, 30939, 30973, 30970, 30943, 31300, 54530, 34148, 31905, 31123, 54732, 56024, 55225, 36720, 39069, 54716, 55512, 31123, 50678, 54732, 55034, 30989, 46332, 37553, 52603, 30991, 54617, 31913, 54732, 55153, 55630, 54659, 30939, 30973, 30970, 30966, 54540, 54805, 55737, 32463, 31123, 54585, 56081, 55118, 31730, 31123, 35777, 54530, 41203, 54547, 31924, 34768, 31885, 54659, 33348, 36144, 31687, 31836, 31730, 52790, 31123, 54585, 54541, 35152, 48949, 49286, 31301, 30939, 30973, 30973, 30970, 30941, 30939, 30973, 30973, 30969, 31798, 31784, 37396, 54530, 32037, 54542, 33283, 31123, 55172, 54536, 32834, 32924, 34355, 36905, 30939, 30939, 30972, 31155, 13, 13, 31478, 31478, 51329, 41305, 47358, 31301, 31069, 30737, 6856, 31018, 31000, 6866, 30910, 30939, 30973, 30972, 30970, 30941, 30939, 30973, 30969, 30972, 53437, 46161, 33690, 31301, 30939, 30973, 30973, 30939, 30941, 30939, 30973, 30969, 30972, 31966, 16747, 30978, 31201, 30939, 30978, 31155, 13, 13, 13, 32823, 30910, 57906, 56484, 31201, 55233, 56722, 57087, 30910, 52806, 13, 13, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n【最后一个样本原文】=Christian Wilhelm 1808-1871）——德国工人运动活动家，正义者同盟领导人，职业是裁缝；空想平均共产主义理论家和鼓动家；工人同盟的创始人，《工人共和国报》的出版者；1849年流亡美国，晚年接近国际工人协会。——12、19、97-100、103、111、113、119、132。\n\n　　沃尔弗，弗里德里希·威廉（Wolff，Friedrich Wilhelm 鲁普斯 Lupus 1809-1864）——德国无产阶级革命家和政论家，职业是教员，西里西亚农民的儿子；1834-1839年被关在普鲁士监狱；1846-1847年为布鲁塞尔共产主义通讯委员会委员，共产主义者同盟创始人之一和同盟中央委员会委员（1848年3月起），《新莱茵报》编辑（1848-1849），民主主义者莱茵区域委员会和科隆安全委员会委员；法兰克福国民议会议员，属于极左派；1849年流亡瑞士，1851年迁居英国，1853年起在曼彻斯特当教员；马克思和恩格斯的朋友和战友。——75、105、107、109、120。\n\nX\n\n　　西斯蒙第，让·沙尔·莱奥纳尔·西蒙德·德（Sismondi，Jean-Charles-Léonard Simonde de 1773-1842）——瑞士经济学家和历史学家，政治经济学中浪漫学派的代表人物。——54。\n\n　　席尔，卡尔——见沙佩尔，卡尔。\n\nY\n\n　　雅科比，阿伯拉罕（Jacobi，Abraham 1830-1919）——德国医生，波恩体操联合会创建人和会长（1850-1851），共产主义者同盟盟员和特使，科隆共产党人案件（1852）的被告之一，被陪审法庭宣告无罪，但因被控“侮辱国王陛下”而继续被监禁；1853年流亡英国，后迁居美国，在美国的刊物上宣传马克思主义思想；站在北部方面参加美国内战，后为纽约医学科学院院长（1885-1889），一些医学院的教授和院长，写有医学方面的著作。——114。\n\n　　亚历山大三世（Александр III 1845-1894）——俄国皇帝（1881-1894）。——6、16。\n\n\n感谢 聂孟、卫玄鹤 校对\n\n\n【最后一个样本原文长度】=893\u001b[0m\n\u001b[32m2023-07-31 15:43:30.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_chained_lm_datasets\u001b[0m:\u001b[36m247\u001b[0m - \u001b[1m验证查看完毕\u001b[0m\n样本数量=20\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:44<00:00, 14.89s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:44<00:00, 14.89s/it]\nmemory footprint of model: 3.6520424485206604 GB\n\u001b[32m2023-07-31 15:45:16.223\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m568\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\nmemory footprint of model: 3.6520424485206604 GB\n\u001b[32m2023-07-31 15:45:16.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m568\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nnumber_train_samples=40\nnumber_of_eval_numbers=20\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=5e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=1,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-pt,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=40.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-pt,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=1,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.0,\nxpu_backend=None,\n)\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nnumber_train_samples=40\nnumber_of_eval_numbers=20\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=5e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-pt,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=40.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-pt,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=1,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.0,\nxpu_backend=None,\n)\n{'loss': 9.3749, 'learning_rate': 4.375e-06, 'epoch': 0.5}                      \n  1%|▌                                         | 10/800 [00:34<42:28,  3.23s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:04,  1.70it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:05,  1.17it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:05,  1.01it/s]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:04<00:05,  1.06s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.11s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.15s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.17s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:09<00:01,  1.19s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 9.158706665039062, 'eval_runtime': 13.4102, 'eval_samples_per_second': 1.491, 'eval_steps_per_second': 0.746, 'epoch': 0.5}\n  1%|▌                                         | 10/800 [00:48<42:28,  3.23s/it]\n100%|███████████████████████████████████████████| 10/10 [00:11<00:00,  1.20s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 8.3408, 'learning_rate': 1e-05, 'epoch': 1.0}                          \n  2%|█                                         | 20/800 [01:23<46:12,  3.55s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.56it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.12it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.04s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.11s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.16s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.20s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.22s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.24s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 8.151468276977539, 'eval_runtime': 12.6858, 'eval_samples_per_second': 1.577, 'eval_steps_per_second': 0.788, 'epoch': 1.0}\n  2%|█                                         | 20/800 [01:36<46:12,  3.55s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.25s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 7.3967, 'learning_rate': 1.6250000000000002e-05, 'epoch': 1.5}         \n  4%|█▌                                        | 30/800 [02:13<47:19,  3.69s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 6.320302486419678, 'eval_runtime': 12.821, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.78, 'epoch': 1.5}\n  4%|█▌                                        | 30/800 [02:26<47:19,  3.69s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 5.3765, 'learning_rate': 2.1875e-05, 'epoch': 2.0}                     \n  5%|██                                        | 40/800 [03:04<47:10,  3.72s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.56it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.11it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.04s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 4.975358963012695, 'eval_runtime': 12.7857, 'eval_samples_per_second': 1.564, 'eval_steps_per_second': 0.782, 'epoch': 2.0}\n  5%|██                                        | 40/800 [03:17<47:10,  3.72s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 4.1276, 'learning_rate': 2.7500000000000004e-05, 'epoch': 2.5}         \n  6%|██▋                                       | 50/800 [03:55<46:57,  3.76s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 3.8995280265808105, 'eval_runtime': 12.8126, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.78, 'epoch': 2.5}\n  6%|██▋                                       | 50/800 [04:07<46:57,  3.76s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 3.6702, 'learning_rate': 3.375000000000001e-05, 'epoch': 3.0}          \n  8%|███▏                                      | 60/800 [04:45<46:05,  3.74s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 3.1182117462158203, 'eval_runtime': 12.8171, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.78, 'epoch': 3.0}\n  8%|███▏                                      | 60/800 [04:58<46:05,  3.74s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 2.9281, 'learning_rate': 4e-05, 'epoch': 3.5}                          \n  9%|███▋                                      | 70/800 [05:36<45:26,  3.74s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.688452959060669, 'eval_runtime': 12.8141, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.78, 'epoch': 3.5}\n  9%|███▋                                      | 70/800 [05:49<45:26,  3.74s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 2.5833, 'learning_rate': 4.6250000000000006e-05, 'epoch': 4.0}         \n 10%|████▏                                     | 80/800 [06:26<44:55,  3.74s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.53it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.3532204627990723, 'eval_runtime': 12.8229, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.78, 'epoch': 4.0}\n 10%|████▏                                     | 80/800 [06:39<44:55,  3.74s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 2.1908, 'learning_rate': 4.972222222222223e-05, 'epoch': 4.5}          \n 11%|████▋                                     | 90/800 [07:17<44:06,  3.73s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.0095479488372803, 'eval_runtime': 12.8107, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.781, 'epoch': 4.5}\n 11%|████▋                                     | 90/800 [07:29<44:06,  3.73s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 2.0675, 'learning_rate': 4.902777777777778e-05, 'epoch': 5.0}          \n 12%|█████▏                                   | 100/800 [08:07<43:43,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.777112603187561, 'eval_runtime': 12.813, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.78, 'epoch': 5.0}\n 12%|█████▏                                   | 100/800 [08:20<43:43,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 1.6654, 'learning_rate': 4.8333333333333334e-05, 'epoch': 5.5}         \n 14%|█████▋                                   | 110/800 [08:58<42:55,  3.73s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.3937040567398071, 'eval_runtime': 12.8147, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.78, 'epoch': 5.5}\n 14%|█████▋                                   | 110/800 [09:10<42:55,  3.73s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 1.4235, 'learning_rate': 4.7638888888888887e-05, 'epoch': 6.0}         \n 15%|██████▏                                  | 120/800 [09:49<42:43,  3.77s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.2182728052139282, 'eval_runtime': 12.835, 'eval_samples_per_second': 1.558, 'eval_steps_per_second': 0.779, 'epoch': 6.0}\n 15%|██████▏                                  | 120/800 [10:02<42:43,  3.77s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 1.1643, 'learning_rate': 4.6944444444444446e-05, 'epoch': 6.5}         \n 16%|██████▋                                  | 130/800 [10:39<41:39,  3.73s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.54it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.10it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 1.056727409362793, 'eval_runtime': 12.7906, 'eval_samples_per_second': 1.564, 'eval_steps_per_second': 0.782, 'epoch': 6.5}\n 16%|██████▋                                  | 130/800 [10:52<41:39,  3.73s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 1.0491, 'learning_rate': 4.6250000000000006e-05, 'epoch': 7.0}         \n 18%|███████▏                                 | 140/800 [11:30<41:12,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.8522399663925171, 'eval_runtime': 12.8126, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.78, 'epoch': 7.0}\n 18%|███████▏                                 | 140/800 [11:43<41:12,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.25s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.772, 'learning_rate': 4.555555555555556e-05, 'epoch': 7.5}           \n 19%|███████▋                                 | 150/800 [12:20<40:37,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.53it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.6952500939369202, 'eval_runtime': 12.8113, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.781, 'epoch': 7.5}\n 19%|███████▋                                 | 150/800 [12:33<40:37,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.7081, 'learning_rate': 4.486111111111111e-05, 'epoch': 8.0}          \n 20%|████████▏                                | 160/800 [13:11<39:47,  3.73s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.53it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.10it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.5874060988426208, 'eval_runtime': 12.799, 'eval_samples_per_second': 1.563, 'eval_steps_per_second': 0.781, 'epoch': 8.0}\n 20%|████████▏                                | 160/800 [13:24<39:47,  3.73s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.391, 'learning_rate': 4.4166666666666665e-05, 'epoch': 8.5}          \n 21%|████████▋                                | 170/800 [14:01<39:22,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.08it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.4233517646789551, 'eval_runtime': 12.8297, 'eval_samples_per_second': 1.559, 'eval_steps_per_second': 0.779, 'epoch': 8.5}\n 21%|████████▋                                | 170/800 [14:14<39:22,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.5461, 'learning_rate': 4.3472222222222225e-05, 'epoch': 9.0}         \n 22%|█████████▏                               | 180/800 [14:52<38:45,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.54it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.10it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.3296318054199219, 'eval_runtime': 12.7891, 'eval_samples_per_second': 1.564, 'eval_steps_per_second': 0.782, 'epoch': 9.0}\n 22%|█████████▏                               | 180/800 [15:05<38:45,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!save 123 !!!!!!\n\n{'loss': 0.3341, 'learning_rate': 4.277777777777778e-05, 'epoch': 9.5}          \n 24%|█████████▋                               | 190/800 [15:43<38:03,  3.74s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.52it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.3400401175022125, 'eval_runtime': 12.8221, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.78, 'epoch': 9.5}\n 24%|█████████▋                               | 190/800 [15:55<38:03,  3.74s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.2921, 'learning_rate': 4.208333333333334e-05, 'epoch': 10.0}         \n 25%|██████████▎                              | 200/800 [16:33<37:28,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.2524644732475281, 'eval_runtime': 12.8372, 'eval_samples_per_second': 1.558, 'eval_steps_per_second': 0.779, 'epoch': 10.0}\n 25%|██████████▎                              | 200/800 [16:46<37:28,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.159, 'learning_rate': 4.138888888888889e-05, 'epoch': 10.5}          \n 26%|██████████▊                              | 210/800 [17:24<36:51,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.23511140048503876, 'eval_runtime': 12.8174, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.78, 'epoch': 10.5}\n 26%|██████████▊                              | 210/800 [17:37<36:51,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.2787, 'learning_rate': 4.0694444444444444e-05, 'epoch': 11.0}        \n 28%|███████████▎                             | 220/800 [18:15<36:06,  3.73s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.2081928700208664, 'eval_runtime': 12.8193, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.78, 'epoch': 11.0}\n 28%|███████████▎                             | 220/800 [18:27<36:06,  3.73s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.107, 'learning_rate': 4e-05, 'epoch': 11.5}                          \n 29%|███████████▊                             | 230/800 [19:05<35:35,  3.75s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.53it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.10it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.24s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.1859593242406845, 'eval_runtime': 12.789, 'eval_samples_per_second': 1.564, 'eval_steps_per_second': 0.782, 'epoch': 11.5}\n 29%|███████████▊                             | 230/800 [19:18<35:35,  3.75s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.25s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.2416, 'learning_rate': 3.9305555555555556e-05, 'epoch': 12.0}        \n 30%|████████████▎                            | 240/800 [19:56<34:52,  3.74s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.17775551974773407, 'eval_runtime': 12.8272, 'eval_samples_per_second': 1.559, 'eval_steps_per_second': 0.78, 'epoch': 12.0}\n 30%|████████████▎                            | 240/800 [20:09<34:52,  3.74s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.0817, 'learning_rate': 3.8611111111111116e-05, 'epoch': 12.5}        \n 31%|████████████▊                            | 250/800 [20:46<34:16,  3.74s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.54it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.10it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.24s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.17295053601264954, 'eval_runtime': 12.7821, 'eval_samples_per_second': 1.565, 'eval_steps_per_second': 0.782, 'epoch': 12.5}\n 31%|████████████▊                            | 250/800 [20:59<34:16,  3.74s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.1949, 'learning_rate': 3.791666666666667e-05, 'epoch': 13.0}         \n 32%|█████████████▎                           | 260/800 [21:37<33:21,  3.71s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.53it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.15463270246982574, 'eval_runtime': 12.8101, 'eval_samples_per_second': 1.561, 'eval_steps_per_second': 0.781, 'epoch': 13.0}\n 32%|█████████████▎                           | 260/800 [21:50<33:21,  3.71s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.25s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.1764, 'learning_rate': 3.722222222222222e-05, 'epoch': 13.5}         \n 34%|█████████████▊                           | 270/800 [22:27<32:58,  3.73s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.53it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.1672443002462387, 'eval_runtime': 12.8193, 'eval_samples_per_second': 1.56, 'eval_steps_per_second': 0.78, 'epoch': 13.5}\n 34%|█████████████▊                           | 270/800 [22:40<32:58,  3.73s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n{'loss': 0.0695, 'learning_rate': 3.6527777777777775e-05, 'epoch': 14.0}        \n 35%|██████████████▎                          | 280/800 [23:18<32:27,  3.74s/it]\n  0%|                                                    | 0/10 [00:00<?, ?it/s]\u001b[A\n 20%|████████▊                                   | 2/10 [00:01<00:05,  1.51it/s]\u001b[A\n 30%|█████████████▏                              | 3/10 [00:02<00:06,  1.09it/s]\u001b[A\n 40%|█████████████████▌                          | 4/10 [00:03<00:06,  1.05s/it]\u001b[A\n 50%|██████████████████████                      | 5/10 [00:05<00:05,  1.13s/it]\u001b[A\n 60%|██████████████████████████▍                 | 6/10 [00:06<00:04,  1.18s/it]\u001b[A\n 70%|██████████████████████████████▊             | 7/10 [00:07<00:03,  1.21s/it]\u001b[A\n 80%|███████████████████████████████████▏        | 8/10 [00:08<00:02,  1.23s/it]\u001b[A\n 90%|███████████████████████████████████████▌    | 9/10 [00:10<00:01,  1.25s/it]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.14790131151676178, 'eval_runtime': 12.8342, 'eval_samples_per_second': 1.558, 'eval_steps_per_second': 0.779, 'epoch': 14.0}\n 35%|██████████████▎                          | 280/800 [23:31<32:27,  3.74s/it]\n100%|███████████████████████████████████████████| 10/10 [00:12<00:00,  1.26s/it]\u001b[A\n                                                                                \u001b[Asave 123 !!!!!!\nsave 123 !!!!!!\n 35%|██████████████▌                          | 283/800 [23:44<50:14,  5.83s/it]^C\n[2023-07-31 16:10:45,747] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4066\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 多轮对话sft ","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/valkryhx/chatGLM-6B-QLoRA","metadata":{"execution":{"iopub.status.busy":"2023-08-02T08:05:25.501780Z","iopub.execute_input":"2023-08-02T08:05:25.502124Z","iopub.status.idle":"2023-08-02T08:05:26.466358Z","shell.execute_reply.started":"2023-08-02T08:05:25.502095Z","shell.execute_reply":"2023-08-02T08:05:26.464910Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"fatal: destination path 'chatGLM-6B-QLoRA' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"!git pull --all --force\n#!pip install peft==0.4.0\n#!pip install  -U git+https://github.com/huggingface/peft.git\n%cd /kaggle/working/chatGLM-6B-QLoRA \n!ls\n!pip install -r requirements.txt\n#!pip install deepspeed==0.9.5  这个也是需要的 但是目前kaggle 的runtime自带了","metadata":{"execution":{"iopub.status.busy":"2023-08-02T08:06:43.941690Z","iopub.execute_input":"2023-08-02T08:06:43.942103Z","iopub.status.idle":"2023-08-02T08:07:24.140768Z","shell.execute_reply.started":"2023-08-02T08:06:43.942069Z","shell.execute_reply":"2023-08-02T08:07:24.139447Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"fatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n/kaggle/working/chatGLM-6B-QLoRA\nREADME.md\t\t    pretrain_qlora_chatglm2.py\nchatGLM_6B_QLoRA.json\t    qa_aug.json\ndata\t\t\t    qlora_param.json\nds_zero2_config.json\t    remote_scripts\nds_zero3_config.json\t    requirements.txt\ninference_qlora.py\t    state.db\ninference_test.py\t    train_ds_zero2_test.py——错误的\nluzi.json\t\t    train_normal_lora.py\nmerge_lora_and_quantize.py  train_qlora.py\noutput-pt\t\t    train_qlora_deepspeed_zero.py\npics\t\t\t    参考与经验\nCollecting peft==0.4.0 (from -r requirements.txt (line 1))\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers==4.30.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.30.2)\nCollecting datasets==2.12.0 (from -r requirements.txt (line 3))\n  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm==4.65.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.65.0)\nCollecting loguru==0.7.0 (from -r requirements.txt (line 5))\n  Downloading loguru-0.7.0-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting fire==0.5.0 (from -r requirements.txt (line 6))\n  Downloading fire-0.5.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting bitsandbytes==0.39.0 (from -r requirements.txt (line 7))\n  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting wandb==0.15.3 (from -r requirements.txt (line 8))\n  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting cpm_kernels==1.0.11 (from -r requirements.txt (line 9))\n  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate==0.20.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.20.3)\nRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.1.99)\nCollecting deepspeed==0.9.5 (from -r requirements.txt (line 12))\n  Downloading deepspeed-0.9.5.tar.gz (809 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.9/809.9 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (6.0)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (2.0.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 1)) (0.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.16.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements.txt (line 2)) (0.13.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (11.0.0)\nRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (1.5.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (3.8.4)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0->-r requirements.txt (line 3)) (0.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 6)) (2.3.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.1.31)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.3->-r requirements.txt (line 8)) (3.20.3)\nCollecting hjson (from deepspeed==0.9.5->-r requirements.txt (line 12))\n  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.11.1)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (9.0.0)\nRequirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.9.5->-r requirements.txt (line 12)) (1.10.10)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0->-r requirements.txt (line 3)) (1.3.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (4.0.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2->-r requirements.txt (line 2)) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0->-r requirements.txt (line 1)) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements.txt (line 2)) (2023.5.7)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0->-r requirements.txt (line 3)) (2023.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 8)) (5.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 1)) (1.3.0)\nBuilding wheels for collected packages: fire, deepspeed\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116949 sha256=fe3f432b7c1a001c0223c16ca5fab579c98035081979acf08e1aa4b407996b21\n  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.9.5-py3-none-any.whl size=844544 sha256=aad8386ef2426d6e4849f16b20ec5b5a442236f8f7e2a1ae522070928afb283c\n  Stored in directory: /root/.cache/pip/wheels/7e/a9/bb/a00d383521da14dc91b65ae2d0062401b750d968a548401b2a\nSuccessfully built fire deepspeed\nInstalling collected packages: hjson, cpm_kernels, bitsandbytes, loguru, fire, wandb, deepspeed, peft, datasets\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.5\n    Uninstalling wandb-0.15.5:\n      Successfully uninstalled wandb-0.15.5\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\nSuccessfully installed bitsandbytes-0.39.0 cpm_kernels-1.0.11 datasets-2.12.0 deepspeed-0.9.5 fire-0.5.0 hjson-3.1.0 loguru-0.7.0 peft-0.4.0 wandb-0.15.3\n","output_type":"stream"}]},{"cell_type":"code","source":"!git pull --all --force \n#!pip install  -U git+https://github.com/huggingface/peft.git   # 20230717 peft==0.4.0正式发布了 不用调版本了推理完后再训练需要重新升级到0.4.0dev 所以有这句\n!deepspeed --include localhost:0,1  sft_qlora_chatglm2.py  \\\n  --train_args_json luzi.json \\\n  --model_name_or_path THUDM/chatglm2-6b \\\n  --output_dir output-multi-turn-sft-0802-v1 \\\n  --num_train_samples -1 \\\n  --num_eval_samples 5 \\\n  --train_data_path ./data/multi_turn_conversations   \\\n  --eval_data_path  ./data/multi_turn_conversations     \\\n  --max_length 1024 \\\n  --lora_rank 64 \\\n  --lora_dropout 0.05 \\\n  --compute_dtype fp16 \\\n  --per_device_train_batch_size 1 \\\n  --per_device_eval_batch_size 1  \\\n  --gradient_accumulation_steps 1 \\\n  --learning_rate  5e-5 \\\n  --num_train_epochs  40  \\\n  --save_total_limit 2 \\\n  --load_in_4bit True \\\n--deepspeed ds_zero2_config.json","metadata":{"execution":{"iopub.status.busy":"2023-08-02T08:13:38.302841Z","iopub.execute_input":"2023-08-02T08:13:38.303804Z","iopub.status.idle":"2023-08-02T08:22:45.871535Z","shell.execute_reply.started":"2023-08-02T08:13:38.303758Z","shell.execute_reply":"2023-08-02T08:22:45.870257Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Fetching origin\nremote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (3/3), done.\u001b[K\nremote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (3/3), 675 bytes | 337.00 KiB/s, done.\nFrom https://github.com/valkryhx/chatGLM-6B-QLoRA\n   3d29fa8..290f0bd  main       -> origin/main\nUpdating 3d29fa8..290f0bd\nFast-forward\n sft_qlora_chatglm2.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n 1 file changed, 1 insertion(+), 1 deletion(-)\n[2023-08-02 08:13:42,194] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-02 08:13:48,367] [WARNING] [runner.py:196:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n[2023-08-02 08:13:48,383] [INFO] [runner.py:555:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None sft_qlora_chatglm2.py --train_args_json luzi.json --model_name_or_path THUDM/chatglm2-6b --output_dir output-multi-turn-sft-0802-v1 --num_train_samples -1 --num_eval_samples 5 --train_data_path ./data/multi_turn_conversations --eval_data_path ./data/multi_turn_conversations --max_length 1024 --lora_rank 64 --lora_dropout 0.05 --compute_dtype fp16 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --learning_rate 5e-5 --num_train_epochs 40 --save_total_limit 2 --load_in_4bit True --deepspeed ds_zero2_config.json\n[2023-08-02 08:13:50,096] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n[2023-08-02 08:13:55,819] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.15.5-1+cuda11.8\n[2023-08-02 08:13:55,820] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.15.5-1\n[2023-08-02 08:13:55,820] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.15.5-1\n[2023-08-02 08:13:55,820] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n[2023-08-02 08:13:55,820] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.15.5-1+cuda11.8\n[2023-08-02 08:13:55,820] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n[2023-08-02 08:13:55,820] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.15.5-1\n[2023-08-02 08:13:55,820] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}\n[2023-08-02 08:13:55,820] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0\n[2023-08-02 08:13:55,820] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})\n[2023-08-02 08:13:55,820] [INFO] [launch.py:163:main] dist_world_size=2\n[2023-08-02 08:13:55,820] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1\n[2023-08-02 08:13:58,755] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-08-02 08:13:58,819] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\n\n===================================BUG REPORT===================================\nWelcome to bitsandbytes. For bug reports, please run\n\npython -m bitsandbytes\n\n and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n================================================================================\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/cuda/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 118\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\nbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/lib/x86_64-linux-gnu'), PosixPath('/usr/local/cuda/lib'), PosixPath('/usr/local/nvidia/lib')}\n  warn(msg)\nCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\nCUDA SETUP: Highest compute capability among GPUs detected: 7.5\nCUDA SETUP: Detected CUDA version 117\nCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n/kaggle/working/chatGLM-6B-QLoRA/sft_qlora_chatglm2.py:51: FutureWarning: set_caching_enabled is deprecated and will be removed in the next major version of datasets. Use datasets.enable_caching() or datasets.disable_caching() instead. This function will be removed in a future version of datasets.\n  set_caching_enabled(False)\ninto lora trainer !!!\n\u001b[32m2023-08-02 08:14:07.692\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m328\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n/kaggle/working/chatGLM-6B-QLoRA/sft_qlora_chatglm2.py:51: FutureWarning: set_caching_enabled is deprecated and will be removed in the next major version of datasets. Use datasets.enable_caching() or datasets.disable_caching() instead. This function will be removed in a future version of datasets.\n  set_caching_enabled(False)\ninto lora trainer !!!\n\u001b[32m2023-08-02 08:14:07.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m328\u001b[0m - \u001b[1mloading parameters...\u001b[0m\n\u001b[32m2023-08-02 08:14:08.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m374\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-02 08:14:08.931\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m156\u001b[0m - \u001b[1mdata files: ./data/multi_turn_conversations/10_turns.json\u001b[0m\n\u001b[32m2023-08-02 08:14:08.939\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m374\u001b[0m - \u001b[1mloading dataset...\u001b[0m\n\u001b[32m2023-08-02 08:14:08.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m156\u001b[0m - \u001b[1mdata files: ./data/multi_turn_conversations/10_turns.json\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 377.25it/s]\n\u001b[32m2023-08-02 08:14:09.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1m在取样之前 data len =10\u001b[0m\n\u001b[32m2023-08-02 08:14:09.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m166\u001b[0m - \u001b[1m在取样之后 data len =10\u001b[0m\n\u001b[32m2023-08-02 08:14:09.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 285.50it/s]\n\u001b[32m2023-08-02 08:14:09.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1m在取样之前 data len =10\u001b[0m\n\u001b[32m2023-08-02 08:14:09.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m166\u001b[0m - \u001b[1m在取样之后 data len =10\u001b[0m\n\u001b[32m2023-08-02 08:14:09.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\ntokenized_dataset=Dataset({                                                     \n    features: ['input_ids', 'labels'],\n    num_rows: 10\n})\ntokenized_dataset=Dataset({                                                     \n    features: ['input_ids', 'labels'],\n    num_rows: 10\n})\ntokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)=\n[Round 1]\n\n问：我今天想去玩术士。 \n\n答：你要打toc吗？\n\n [Round 2]\n\n问：是的，我要买饰品。 \n\n答：老四的裁决？\n\n [Round 3]\n\n问：怎么可能？我要买小强的亡者君临 \n\n答：哦哦对 裁决是物理毕业 不是SS的\n\n\ntokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)=\n[Round 1]\n\n问：我今天想去玩术士。 \n\n答：你要打toc吗？\n\n [Round 2]\n\n问：是的，我要买饰品。 \n\n答：老四的裁决？\n\n [Round 3]\n\n问：怎么可能？我要买小强的亡者君临 \n\n答：哦哦对 裁决是物理毕业 不是SS的\n\n\ntokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)=\n\n\n答：你要打toc吗？\n\n \n\n答：老四的裁决？\n\n \n\n答：哦哦对 裁决是物理毕业 不是SS的\n\n\ntokenized_dataset[0]['input_ids']=\n[790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 54546, 31869, 37580, 55409, 54780, 54930, 31155, 30910, 13, 13, 55437, 31211, 34526, 54819, 30912, 420, 55398, 31514, 13, 13, 790, 30951, 517, 30910, 30943, 30996, 13, 13, 54761, 31211, 38586, 31123, 34034, 55304, 55739, 54667, 31155, 30910, 13, 13, 55437, 31211, 54729, 54795, 54530, 48568, 31514, 13, 13, 790, 30951, 517, 30910, 30966, 30996, 13, 13, 54761, 31211, 52687, 31514, 34034, 55304, 54603, 33026, 55737, 54631, 55835, 55290, 30910, 13, 13, 55437, 31211, 56645, 56645, 54570, 30910, 48568, 54532, 33176, 32121, 30910, 31663, 4800, 54530, 13, 13]\ntokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)=\n\n\n答：你要打toc吗？\n\n \n\n答：老四的裁决？\n\n \n\n答：哦哦对 裁决是物理毕业 不是SS的\n\ntokenized_dataset[0]['labels']=\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 34526, 54819, 30912, 420, 55398, 31514, 13, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 54729, 54795, 54530, 48568, 31514, 13, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 56645, 56645, 54570, 30910, 48568, 54532, 33176, 32121, 30910, 31663, 4800, 54530, 13, 13]\n\nlen(tokenized_dataset[0]['input_ids']=98\ntokenized_dataset[0]['input_ids']=\n[790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 54546, 31869, 37580, 55409, 54780, 54930, 31155, 30910, 13, 13, 55437, 31211, 34526, 54819, 30912, 420, 55398, 31514, 13, 13, 790, 30951, 517, 30910, 30943, 30996, 13, 13, 54761, 31211, 38586, 31123, 34034, 55304, 55739, 54667, 31155, 30910, 13, 13, 55437, 31211, 54729, 54795, 54530, 48568, 31514, 13, 13, 790, 30951, 517, 30910, 30966, 30996, 13, 13, 54761, 31211, 52687, 31514, 34034, 55304, 54603, 33026, 55737, 54631, 55835, 55290, 30910, 13, 13, 55437, 31211, 56645, 56645, 54570, 30910, 48568, 54532, 33176, 32121, 30910, 31663, 4800, 54530, 13, 13]\nlen(tokenized_dataset[0]['labels']=98\ntokenized_dataset[0]['labels']=\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 34526, 54819, 30912, 420, 55398, 31514, 13, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 54729, 54795, 54530, 48568, 31514, 13, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 56645, 56645, 54570, 30910, 48568, 54532, 33176, 32121, 30910, 31663, 4800, 54530, 13, 13]\nlen(tokenized_dataset[0]['input_ids']=98\nlen(tokenized_dataset[0]['labels']=98\n\u001b[32m2023-08-02 08:14:09.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m156\u001b[0m - \u001b[1mdata files: ./data/multi_turn_conversations/10_turns.json\u001b[0m\n\u001b[32m2023-08-02 08:14:09.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m156\u001b[0m - \u001b[1mdata files: ./data/multi_turn_conversations/10_turns.json\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 577.81it/s]\n\u001b[32m2023-08-02 08:14:09.388\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1m在取样之前 data len =10\u001b[0m\n  0%|                                                     | 0/1 [00:00<?, ?it/s]\u001b[32m2023-08-02 08:14:09.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m166\u001b[0m - \u001b[1m在取样之后 data len =5\u001b[0m\n\u001b[32m2023-08-02 08:14:09.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\n100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 509.33it/s]\n\u001b[32m2023-08-02 08:14:09.393\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1m在取样之前 data len =10\u001b[0m\n\u001b[32m2023-08-02 08:14:09.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m166\u001b[0m - \u001b[1m在取样之后 data len =5\u001b[0m\n\u001b[32m2023-08-02 08:14:09.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_multi_turn_conversations_datset\u001b[0m:\u001b[36m170\u001b[0m - \u001b[1mpreprocessing dataset...\u001b[0m\ntokenized_dataset=Dataset({                                                     \n    features: ['input_ids', 'labels'],\n    num_rows: 5\n})\ntokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)=\n[Round 1]\n\n问：我今天想去玩术士。 \n\n答：你要打toc吗？\n\n [Round 2]\n\n问：是的，我要买饰品。 \n\n答：老四的裁决？\n\n [Round 3]\n\n问：怎么可能？我要买小强的亡者君临 \n\n答：哦哦对 裁决是物理毕业 不是SS的\n\n\ntokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)=\n\n\n答：你要打toc吗？\n\n \n\n答：老四的裁决？\n\n \n\n答：哦哦对 裁决是物理毕业 不是SS的\n\n\ntokenized_dataset[0]['input_ids']=\n[790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 54546, 31869, 37580, 55409, 54780, 54930, 31155, 30910, 13, 13, 55437, 31211, 34526, 54819, 30912, 420, 55398, 31514, 13, 13, 790, 30951, 517, 30910, 30943, 30996, 13, 13, 54761, 31211, 38586, 31123, 34034, 55304, 55739, 54667, 31155, 30910, 13, 13, 55437, 31211, 54729, 54795, 54530, 48568, 31514, 13, 13, 790, 30951, 517, 30910, 30966, 30996, 13, 13, 54761, 31211, 52687, 31514, 34034, 55304, 54603, 33026, 55737, 54631, 55835, 55290, 30910, 13, 13, 55437, 31211, 56645, 56645, 54570, 30910, 48568, 54532, 33176, 32121, 30910, 31663, 4800, 54530, 13, 13]\ntokenized_dataset[0]['labels']=\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 34526, 54819, 30912, 420, 55398, 31514, 13, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 54729, 54795, 54530, 48568, 31514, 13, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 56645, 56645, 54570, 30910, 48568, 54532, 33176, 32121, 30910, 31663, 4800, 54530, 13, 13]\nlen(tokenized_dataset[0]['input_ids']=98\nlen(tokenized_dataset[0]['labels']=98\ntokenized_dataset=Dataset({                                                     \n    features: ['input_ids', 'labels'],\n    num_rows: 5\n})\ntokenizer.decode(tokenized_dataset[0]['input_ids'],skip_special_tokens=False)=\n[Round 1]\n\n问：我今天想去玩术士。 \n\n答：你要打toc吗？\n\n [Round 2]\n\n问：是的，我要买饰品。 \n\n答：老四的裁决？\n\n [Round 3]\n\n问：怎么可能？我要买小强的亡者君临 \n\n答：哦哦对 裁决是物理毕业 不是SS的\n\n\nFlattening the indices:   0%|                      | 0/5 [00:00<?, ? examples/s]tokenizer.decode(tokenized_dataset[0]['labels'],skip_special_tokens=False)=\n\n\n答：你要打toc吗？\n\n \n\n答：老四的裁决？\n\n \n\n答：哦哦对 裁决是物理毕业 不是SS的\n\n\ntokenized_dataset[0]['input_ids']=\n[790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 54546, 31869, 37580, 55409, 54780, 54930, 31155, 30910, 13, 13, 55437, 31211, 34526, 54819, 30912, 420, 55398, 31514, 13, 13, 790, 30951, 517, 30910, 30943, 30996, 13, 13, 54761, 31211, 38586, 31123, 34034, 55304, 55739, 54667, 31155, 30910, 13, 13, 55437, 31211, 54729, 54795, 54530, 48568, 31514, 13, 13, 790, 30951, 517, 30910, 30966, 30996, 13, 13, 54761, 31211, 52687, 31514, 34034, 55304, 54603, 33026, 55737, 54631, 55835, 55290, 30910, 13, 13, 55437, 31211, 56645, 56645, 54570, 30910, 48568, 54532, 33176, 32121, 30910, 31663, 4800, 54530, 13, 13]\ntokenized_dataset[0]['labels']=\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 34526, 54819, 30912, 420, 55398, 31514, 13, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 54729, 54795, 54530, 48568, 31514, 13, 13, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 13, 13, 55437, 31211, 56645, 56645, 54570, 30910, 48568, 54532, 33176, 32121, 30910, 31663, 4800, 54530, 13, 13]\nlen(tokenized_dataset[0]['input_ids']=98\nlen(tokenized_dataset[0]['labels']=98\nDownloading (…)lve/main/config.json: 100%|█| 1.22k/1.22k [00:00<00:00, 7.34MB/s]\nDownloading (…)iguration_chatglm.py: 100%|█| 2.25k/2.25k [00:00<00:00, 15.1MB/s]\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- configuration_chatglm.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nDownloading (…)/modeling_chatglm.py: 100%|█| 50.7k/50.7k [00:00<00:00, 2.46MB/s]\nDownloading (…)main/quantization.py: 100%|█| 14.7k/14.7k [00:00<00:00, 73.2MB/s]\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- modeling_chatglm.py\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n- modeling_chatglm.py\n- quantization.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nDownloading (…)model.bin.index.json: 100%|█| 20.4k/20.4k [00:00<00:00, 85.0MB/s]\nDownloading shards:   0%|                                 | 0/7 [00:00<?, ?it/s]\nDownloading (…)l-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:   1%| | 10.5M/1.83G [00:00<00:18, 97.7MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:   2%|  | 41.9M/1.83G [00:00<00:10, 175MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:   3%|  | 62.9M/1.83G [00:00<00:09, 180MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:   5%|  | 83.9M/1.83G [00:00<00:09, 185MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:   6%|▏  | 105M/1.83G [00:00<00:09, 187MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:   7%|▏  | 136M/1.83G [00:00<00:08, 201MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:   9%|▎  | 168M/1.83G [00:00<00:08, 207MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  11%|▎  | 199M/1.83G [00:01<00:07, 207MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  13%|▍  | 231M/1.83G [00:01<00:07, 214MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  14%|▍  | 262M/1.83G [00:01<00:07, 214MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  16%|▍  | 294M/1.83G [00:01<00:07, 216MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  18%|▌  | 325M/1.83G [00:01<00:07, 206MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  20%|▌  | 357M/1.83G [00:01<00:06, 211MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  21%|▋  | 388M/1.83G [00:01<00:06, 214MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  23%|▋  | 419M/1.83G [00:02<00:06, 217MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  25%|▋  | 451M/1.83G [00:02<00:06, 217MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  26%|▊  | 482M/1.83G [00:02<00:06, 220MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  28%|▊  | 514M/1.83G [00:02<00:05, 223MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  30%|▉  | 545M/1.83G [00:02<00:05, 222MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  32%|▉  | 577M/1.83G [00:02<00:05, 223MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  33%|▉  | 608M/1.83G [00:02<00:05, 222MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  35%|█  | 640M/1.83G [00:03<00:05, 223MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  37%|█  | 671M/1.83G [00:03<00:05, 222MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  38%|█▏ | 703M/1.83G [00:03<00:05, 222MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  40%|█▏ | 734M/1.83G [00:03<00:05, 216MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  42%|█▎ | 765M/1.83G [00:03<00:04, 218MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  44%|█▎ | 797M/1.83G [00:03<00:04, 219MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  45%|█▎ | 828M/1.83G [00:03<00:04, 221MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  47%|█▍ | 860M/1.83G [00:04<00:04, 224MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  49%|█▍ | 891M/1.83G [00:04<00:04, 222MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  50%|█▌ | 923M/1.83G [00:04<00:04, 224MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  52%|█▌ | 954M/1.83G [00:04<00:03, 222MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  54%|█▌ | 986M/1.83G [00:04<00:03, 226MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  56%|█ | 1.02G/1.83G [00:04<00:03, 226MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  57%|█▏| 1.05G/1.83G [00:04<00:03, 226MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  59%|█▏| 1.08G/1.83G [00:05<00:03, 227MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  61%|█▏| 1.11G/1.83G [00:05<00:03, 225MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  63%|█▎| 1.14G/1.83G [00:05<00:03, 226MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  64%|█▎| 1.17G/1.83G [00:05<00:02, 227MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  66%|█▎| 1.21G/1.83G [00:05<00:02, 225MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  68%|█▎| 1.24G/1.83G [00:05<00:02, 225MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  69%|█▍| 1.27G/1.83G [00:05<00:02, 222MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  71%|█▍| 1.30G/1.83G [00:06<00:02, 214MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  73%|█▍| 1.33G/1.83G [00:06<00:02, 175MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  74%|█▍| 1.35G/1.83G [00:06<00:02, 168MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  75%|█▌| 1.37G/1.83G [00:06<00:03, 141MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  76%|█▌| 1.39G/1.83G [00:06<00:03, 125MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  77%|█▌| 1.42G/1.83G [00:07<00:03, 124MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  79%|█▌| 1.44G/1.83G [00:07<00:03, 128MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  80%|█▌| 1.46G/1.83G [00:07<00:02, 131MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  81%|█▌| 1.48G/1.83G [00:07<00:02, 127MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  82%|█▋| 1.50G/1.83G [00:07<00:02, 128MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  83%|█▋| 1.52G/1.83G [00:07<00:02, 120MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  84%|█▋| 1.54G/1.83G [00:08<00:02, 120MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  85%|█▋| 1.56G/1.83G [00:08<00:02, 114MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  87%|█▋| 1.58G/1.83G [00:08<00:01, 131MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  88%|█▊| 1.61G/1.83G [00:08<00:01, 154MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  89%|█▊| 1.64G/1.83G [00:08<00:01, 166MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  91%|█▊| 1.66G/1.83G [00:08<00:00, 174MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  92%|█▊| 1.68G/1.83G [00:08<00:00, 181MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  93%|█▊| 1.70G/1.83G [00:08<00:00, 185MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  94%|█▉| 1.72G/1.83G [00:09<00:00, 183MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  96%|█▉| 1.75G/1.83G [00:09<00:00, 193MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  97%|█▉| 1.77G/1.83G [00:09<00:00, 196MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin:  98%|█▉| 1.79G/1.83G [00:09<00:00, 198MB/s]\u001b[A\nDownloading (…)l-00001-of-00007.bin: 100%|██| 1.83G/1.83G [00:09<00:00, 191MB/s]\u001b[A\nDownloading shards:  14%|███▌                     | 1/7 [00:09<00:58,  9.72s/it]\nDownloading (…)l-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:   2%|  | 31.5M/1.97G [00:00<00:10, 190MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:   3%|  | 52.4M/1.97G [00:00<00:09, 196MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:   4%|  | 73.4M/1.97G [00:00<00:09, 194MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:   5%|  | 94.4M/1.97G [00:00<00:16, 112MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:   6%|▏  | 115M/1.97G [00:00<00:14, 131MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:   7%|▏  | 136M/1.97G [00:00<00:12, 148MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:   8%|▏  | 157M/1.97G [00:01<00:11, 162MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  10%|▎  | 189M/1.97G [00:01<00:09, 182MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  11%|▎  | 220M/1.97G [00:01<00:09, 194MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  13%|▍  | 252M/1.97G [00:01<00:08, 203MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  14%|▍  | 283M/1.97G [00:01<00:08, 209MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  16%|▍  | 315M/1.97G [00:01<00:07, 211MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  18%|▌  | 346M/1.97G [00:01<00:07, 207MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  19%|▌  | 367M/1.97G [00:01<00:07, 205MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  20%|▌  | 388M/1.97G [00:02<00:07, 206MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  21%|▋  | 419M/1.97G [00:02<00:07, 211MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  23%|▋  | 451M/1.97G [00:02<00:07, 216MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  25%|▋  | 482M/1.97G [00:02<00:06, 218MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  26%|▊  | 514M/1.97G [00:02<00:06, 218MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  28%|▊  | 545M/1.97G [00:02<00:06, 221MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  29%|▉  | 577M/1.97G [00:02<00:06, 223MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  31%|▉  | 608M/1.97G [00:03<00:06, 219MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  32%|▉  | 640M/1.97G [00:03<00:06, 220MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  34%|█  | 671M/1.97G [00:03<00:05, 221MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  36%|█  | 703M/1.97G [00:03<00:05, 218MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  37%|█  | 734M/1.97G [00:03<00:05, 218MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  39%|█▏ | 765M/1.97G [00:03<00:05, 213MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  40%|█▏ | 797M/1.97G [00:03<00:05, 211MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  42%|█▎ | 828M/1.97G [00:04<00:05, 211MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  44%|█▎ | 860M/1.97G [00:04<00:05, 216MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  45%|█▎ | 891M/1.97G [00:04<00:04, 220MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  47%|█▍ | 923M/1.97G [00:04<00:04, 220MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  48%|█▍ | 954M/1.97G [00:04<00:04, 220MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  50%|█▌ | 986M/1.97G [00:04<00:04, 218MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  52%|█ | 1.02G/1.97G [00:04<00:04, 218MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  53%|█ | 1.05G/1.97G [00:05<00:04, 217MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  55%|█ | 1.08G/1.97G [00:05<00:04, 218MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  56%|█▏| 1.11G/1.97G [00:05<00:03, 219MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  58%|█▏| 1.14G/1.97G [00:05<00:03, 219MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  60%|█▏| 1.17G/1.97G [00:05<00:03, 219MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  61%|█▏| 1.21G/1.97G [00:05<00:03, 221MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  63%|█▎| 1.24G/1.97G [00:05<00:03, 218MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  64%|█▎| 1.27G/1.97G [00:06<00:03, 220MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  66%|█▎| 1.30G/1.97G [00:06<00:03, 219MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  68%|█▎| 1.33G/1.97G [00:06<00:02, 217MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  69%|█▍| 1.36G/1.97G [00:06<00:02, 217MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  71%|█▍| 1.39G/1.97G [00:06<00:02, 218MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  72%|█▍| 1.43G/1.97G [00:06<00:02, 217MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  74%|█▍| 1.46G/1.97G [00:06<00:02, 218MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  76%|█▌| 1.49G/1.97G [00:07<00:02, 220MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  77%|█▌| 1.52G/1.97G [00:07<00:02, 221MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  79%|█▌| 1.55G/1.97G [00:07<00:01, 216MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  80%|█▌| 1.58G/1.97G [00:07<00:01, 211MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  82%|█▋| 1.61G/1.97G [00:07<00:01, 204MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  83%|█▋| 1.64G/1.97G [00:07<00:01, 197MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  84%|█▋| 1.66G/1.97G [00:07<00:01, 197MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  85%|█▋| 1.68G/1.97G [00:08<00:01, 192MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  86%|█▋| 1.70G/1.97G [00:08<00:01, 188MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  87%|█▋| 1.72G/1.97G [00:08<00:01, 191MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  88%|█▊| 1.74G/1.97G [00:08<00:01, 193MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  89%|█▊| 1.76G/1.97G [00:08<00:01, 193MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  91%|█▊| 1.78G/1.97G [00:08<00:00, 192MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  92%|█▊| 1.80G/1.97G [00:08<00:00, 195MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  93%|█▊| 1.82G/1.97G [00:08<00:00, 196MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  94%|█▉| 1.85G/1.97G [00:08<00:00, 196MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  95%|█▉| 1.87G/1.97G [00:09<00:00, 195MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  96%|█▉| 1.89G/1.97G [00:09<00:00, 196MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  97%|█▉| 1.91G/1.97G [00:09<00:00, 197MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin:  98%|█▉| 1.93G/1.97G [00:09<00:00, 200MB/s]\u001b[A\nDownloading (…)l-00002-of-00007.bin: 100%|██| 1.97G/1.97G [00:09<00:00, 205MB/s]\u001b[A\nDownloading shards:  29%|███████▏                 | 2/7 [00:19<00:48,  9.68s/it]\nDownloading (…)l-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   1%|  | 21.0M/1.93G [00:00<00:09, 207MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   2%|  | 41.9M/1.93G [00:00<00:09, 206MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   3%|  | 62.9M/1.93G [00:00<00:09, 200MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   4%|  | 83.9M/1.93G [00:00<00:09, 199MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   5%|▏  | 105M/1.93G [00:00<00:09, 201MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   7%|▏  | 126M/1.93G [00:00<00:08, 201MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   8%|▏  | 147M/1.93G [00:00<00:08, 201MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:   9%|▎  | 168M/1.93G [00:00<00:09, 192MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  10%|▎  | 189M/1.93G [00:00<00:08, 194MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  11%|▎  | 210M/1.93G [00:01<00:09, 191MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  12%|▎  | 231M/1.93G [00:01<00:08, 190MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  13%|▍  | 252M/1.93G [00:01<00:08, 190MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  14%|▍  | 273M/1.93G [00:01<00:08, 192MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  15%|▍  | 294M/1.93G [00:01<00:08, 187MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  16%|▍  | 315M/1.93G [00:01<00:08, 189MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  17%|▌  | 336M/1.93G [00:01<00:08, 188MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  18%|▌  | 357M/1.93G [00:01<00:08, 184MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  20%|▌  | 377M/1.93G [00:01<00:08, 183MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  21%|▌  | 398M/1.93G [00:02<00:08, 184MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  22%|▋  | 419M/1.93G [00:02<00:08, 186MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  23%|▋  | 440M/1.93G [00:02<00:07, 187MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  24%|▋  | 461M/1.93G [00:02<00:07, 187MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  25%|▊  | 482M/1.93G [00:02<00:07, 191MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  26%|▊  | 503M/1.93G [00:02<00:07, 193MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  27%|▊  | 524M/1.93G [00:02<00:07, 197MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  29%|▊  | 556M/1.93G [00:02<00:06, 204MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  30%|▉  | 587M/1.93G [00:03<00:06, 210MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  32%|▉  | 619M/1.93G [00:03<00:06, 211MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  34%|█  | 650M/1.93G [00:03<00:06, 212MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  35%|█  | 682M/1.93G [00:03<00:05, 214MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  37%|█  | 713M/1.93G [00:03<00:05, 218MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  39%|█▏ | 744M/1.93G [00:03<00:05, 221MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  40%|█▏ | 776M/1.93G [00:03<00:05, 218MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  42%|█▎ | 807M/1.93G [00:04<00:05, 221MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  44%|█▎ | 839M/1.93G [00:04<00:05, 218MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  45%|█▎ | 870M/1.93G [00:04<00:04, 218MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  47%|█▍ | 902M/1.93G [00:04<00:04, 218MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  48%|█▍ | 933M/1.93G [00:04<00:04, 219MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  50%|█▌ | 965M/1.93G [00:04<00:04, 219MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  52%|█▌ | 996M/1.93G [00:04<00:04, 220MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  53%|█ | 1.03G/1.93G [00:05<00:04, 222MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  55%|█ | 1.06G/1.93G [00:05<00:03, 221MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  57%|█▏| 1.09G/1.93G [00:05<00:03, 221MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  58%|█▏| 1.12G/1.93G [00:05<00:03, 216MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  60%|█▏| 1.15G/1.93G [00:05<00:03, 217MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  61%|█▏| 1.18G/1.93G [00:05<00:03, 214MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  63%|█▎| 1.22G/1.93G [00:05<00:03, 214MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  65%|█▎| 1.25G/1.93G [00:06<00:03, 215MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  66%|█▎| 1.28G/1.93G [00:06<00:03, 212MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  68%|█▎| 1.31G/1.93G [00:06<00:02, 206MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  70%|█▍| 1.34G/1.93G [00:06<00:02, 211MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  71%|█▍| 1.37G/1.93G [00:06<00:02, 213MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  73%|█▍| 1.41G/1.93G [00:06<00:02, 215MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  75%|█▍| 1.44G/1.93G [00:06<00:02, 216MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  76%|█▌| 1.47G/1.93G [00:07<00:02, 217MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  78%|█▌| 1.50G/1.93G [00:07<00:01, 220MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  79%|█▌| 1.53G/1.93G [00:07<00:01, 222MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  81%|█▌| 1.56G/1.93G [00:07<00:01, 223MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  83%|█▋| 1.59G/1.93G [00:07<00:01, 222MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  84%|█▋| 1.63G/1.93G [00:07<00:01, 222MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  86%|█▋| 1.66G/1.93G [00:07<00:01, 224MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  88%|█▊| 1.69G/1.93G [00:08<00:01, 223MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  89%|█▊| 1.72G/1.93G [00:08<00:00, 224MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  91%|█▊| 1.75G/1.93G [00:08<00:00, 223MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  92%|█▊| 1.78G/1.93G [00:08<00:00, 221MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  94%|█▉| 1.81G/1.93G [00:08<00:00, 220MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  96%|█▉| 1.85G/1.93G [00:08<00:00, 223MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin:  97%|█▉| 1.88G/1.93G [00:08<00:00, 219MB/s]\u001b[A\nDownloading (…)l-00003-of-00007.bin: 100%|██| 1.93G/1.93G [00:09<00:00, 210MB/s]\u001b[A\nDownloading shards:  43%|██████████▋              | 3/7 [00:28<00:37,  9.50s/it]\nDownloading (…)l-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:   1%|  | 21.0M/1.82G [00:00<00:08, 208MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:   2%|  | 41.9M/1.82G [00:00<00:10, 164MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:   3%|  | 62.9M/1.82G [00:00<00:11, 159MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:   5%|  | 83.9M/1.82G [00:00<00:10, 172MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:   6%|▏  | 105M/1.82G [00:00<00:09, 182MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:   7%|▏  | 126M/1.82G [00:00<00:09, 184MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:   9%|▎  | 157M/1.82G [00:00<00:08, 195MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  10%|▎  | 178M/1.82G [00:00<00:08, 188MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  11%|▎  | 199M/1.82G [00:01<00:08, 192MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  12%|▎  | 220M/1.82G [00:01<00:08, 191MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  13%|▍  | 241M/1.82G [00:01<00:08, 194MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  15%|▍  | 273M/1.82G [00:01<00:07, 202MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  17%|▌  | 304M/1.82G [00:01<00:07, 206MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  18%|▌  | 325M/1.82G [00:01<00:07, 207MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  19%|▌  | 346M/1.82G [00:01<00:07, 202MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  20%|▌  | 367M/1.82G [00:01<00:07, 196MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  21%|▋  | 388M/1.82G [00:02<00:07, 182MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  23%|▋  | 419M/1.82G [00:02<00:07, 193MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  24%|▋  | 440M/1.82G [00:02<00:07, 194MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  26%|▊  | 472M/1.82G [00:02<00:06, 197MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  27%|▊  | 493M/1.82G [00:02<00:06, 200MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  28%|▊  | 514M/1.82G [00:02<00:06, 194MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  29%|▉  | 535M/1.82G [00:02<00:06, 188MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  31%|▉  | 556M/1.82G [00:02<00:06, 189MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  32%|▉  | 577M/1.82G [00:03<00:06, 191MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  33%|▉  | 598M/1.82G [00:03<00:06, 194MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  35%|█  | 629M/1.82G [00:03<00:05, 198MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  36%|█  | 650M/1.82G [00:03<00:05, 196MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  37%|█  | 671M/1.82G [00:03<00:05, 196MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  38%|█▏ | 692M/1.82G [00:03<00:05, 196MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  39%|█▏ | 713M/1.82G [00:03<00:05, 198MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  40%|█▏ | 734M/1.82G [00:03<00:05, 192MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  42%|█▏ | 755M/1.82G [00:03<00:05, 197MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  43%|█▎ | 776M/1.82G [00:04<00:05, 196MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  44%|█▎ | 797M/1.82G [00:04<00:05, 193MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  45%|█▎ | 818M/1.82G [00:04<00:05, 183MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  46%|█▍ | 839M/1.82G [00:04<00:05, 186MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  47%|█▍ | 860M/1.82G [00:04<00:05, 187MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  49%|█▍ | 881M/1.82G [00:04<00:05, 178MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  50%|█▍ | 902M/1.82G [00:04<00:04, 184MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  51%|█▌ | 923M/1.82G [00:04<00:04, 189MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  52%|█▌ | 944M/1.82G [00:04<00:04, 191MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  54%|█▌ | 975M/1.82G [00:05<00:04, 199MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  55%|█ | 1.01G/1.82G [00:05<00:03, 209MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  57%|█▏| 1.04G/1.82G [00:05<00:03, 215MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  59%|█▏| 1.07G/1.82G [00:05<00:03, 212MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  61%|█▏| 1.10G/1.82G [00:05<00:03, 214MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  62%|█▏| 1.13G/1.82G [00:05<00:03, 218MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  64%|█▎| 1.16G/1.82G [00:05<00:02, 218MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  66%|█▎| 1.20G/1.82G [00:06<00:02, 219MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  68%|█▎| 1.23G/1.82G [00:06<00:02, 218MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  69%|█▍| 1.26G/1.82G [00:06<00:02, 220MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  71%|█▍| 1.29G/1.82G [00:06<00:02, 221MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  73%|█▍| 1.32G/1.82G [00:06<00:02, 221MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  75%|█▍| 1.35G/1.82G [00:06<00:02, 221MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  76%|█▌| 1.38G/1.82G [00:06<00:01, 217MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  78%|█▌| 1.42G/1.82G [00:07<00:01, 216MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  80%|█▌| 1.45G/1.82G [00:07<00:01, 219MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  81%|█▋| 1.48G/1.82G [00:07<00:01, 220MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  83%|█▋| 1.51G/1.82G [00:07<00:01, 219MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  85%|█▋| 1.54G/1.82G [00:07<00:01, 219MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  87%|█▋| 1.57G/1.82G [00:07<00:01, 222MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  88%|█▊| 1.60G/1.82G [00:07<00:00, 221MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  90%|█▊| 1.64G/1.82G [00:08<00:00, 220MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  92%|█▊| 1.67G/1.82G [00:08<00:00, 219MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  94%|█▊| 1.70G/1.82G [00:08<00:00, 220MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  95%|█▉| 1.73G/1.82G [00:08<00:00, 217MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  97%|█▉| 1.76G/1.82G [00:08<00:00, 218MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin:  99%|█▉| 1.79G/1.82G [00:08<00:00, 220MB/s]\u001b[A\nDownloading (…)l-00004-of-00007.bin: 100%|██| 1.82G/1.82G [00:08<00:00, 204MB/s]\u001b[A\nDownloading shards:  57%|██████████████▎          | 4/7 [00:37<00:27,  9.29s/it]\nDownloading (…)l-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:   2%|  | 31.5M/1.97G [00:00<00:08, 239MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:   3%|  | 62.9M/1.97G [00:00<00:08, 231MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:   5%|  | 94.4M/1.97G [00:00<00:14, 133MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:   6%|▏  | 126M/1.97G [00:00<00:11, 158MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:   8%|▏  | 157M/1.97G [00:00<00:10, 176MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:   9%|▎  | 178M/1.97G [00:01<00:09, 183MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  10%|▎  | 199M/1.97G [00:01<00:09, 187MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  11%|▎  | 220M/1.97G [00:01<00:10, 168MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  12%|▎  | 241M/1.97G [00:01<00:11, 148MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  13%|▍  | 262M/1.97G [00:01<00:12, 140MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  14%|▍  | 283M/1.97G [00:01<00:12, 131MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  15%|▍  | 304M/1.97G [00:01<00:12, 137MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  17%|▍  | 325M/1.97G [00:02<00:12, 135MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  18%|▌  | 346M/1.97G [00:02<00:12, 134MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  19%|▌  | 367M/1.97G [00:02<00:12, 128MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  20%|▌  | 388M/1.97G [00:02<00:12, 126MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  21%|▌  | 409M/1.97G [00:02<00:12, 125MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  22%|▋  | 430M/1.97G [00:02<00:12, 124MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  23%|▋  | 451M/1.97G [00:03<00:12, 125MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  24%|▋  | 472M/1.97G [00:03<00:11, 125MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  25%|▊  | 493M/1.97G [00:03<00:11, 126MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  26%|▊  | 514M/1.97G [00:03<00:11, 126MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  27%|▊  | 535M/1.97G [00:03<00:11, 126MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  28%|▊  | 556M/1.97G [00:03<00:11, 124MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  29%|▉  | 577M/1.97G [00:04<00:11, 121MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  30%|▉  | 598M/1.97G [00:04<00:11, 120MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  31%|▉  | 619M/1.97G [00:04<00:10, 124MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  32%|▉  | 640M/1.97G [00:04<00:09, 140MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  34%|█  | 661M/1.97G [00:04<00:08, 151MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  35%|█  | 682M/1.97G [00:04<00:07, 163MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  36%|█  | 703M/1.97G [00:04<00:07, 165MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  37%|█  | 724M/1.97G [00:05<00:07, 165MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  38%|█▏ | 744M/1.97G [00:05<00:07, 167MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  39%|█▏ | 765M/1.97G [00:05<00:07, 169MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  40%|█▏ | 786M/1.97G [00:05<00:06, 172MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  41%|█▏ | 807M/1.97G [00:05<00:06, 177MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  42%|█▎ | 828M/1.97G [00:05<00:06, 185MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  44%|█▎ | 860M/1.97G [00:05<00:05, 194MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  45%|█▎ | 881M/1.97G [00:05<00:06, 181MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  46%|█▎ | 902M/1.97G [00:06<00:05, 187MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  47%|█▍ | 923M/1.97G [00:06<00:05, 191MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  48%|█▍ | 944M/1.97G [00:06<00:05, 193MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  50%|█▍ | 975M/1.97G [00:06<00:04, 202MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  51%|█ | 1.01G/1.97G [00:06<00:04, 210MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  53%|█ | 1.04G/1.97G [00:06<00:04, 211MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  54%|█ | 1.07G/1.97G [00:06<00:04, 214MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  56%|█ | 1.10G/1.97G [00:06<00:03, 217MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  58%|█▏| 1.13G/1.97G [00:07<00:03, 218MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  59%|█▏| 1.16G/1.97G [00:07<00:03, 221MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  61%|█▏| 1.20G/1.97G [00:07<00:03, 221MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  62%|█▏| 1.23G/1.97G [00:07<00:03, 222MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  64%|█▎| 1.26G/1.97G [00:07<00:03, 221MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  66%|█▎| 1.29G/1.97G [00:07<00:03, 221MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  67%|█▎| 1.32G/1.97G [00:07<00:02, 219MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  69%|█▎| 1.35G/1.97G [00:08<00:02, 217MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  70%|█▍| 1.38G/1.97G [00:08<00:02, 219MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  72%|█▍| 1.42G/1.97G [00:08<00:02, 218MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  74%|█▍| 1.45G/1.97G [00:08<00:02, 219MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  75%|█▌| 1.48G/1.97G [00:08<00:02, 221MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  77%|█▌| 1.51G/1.97G [00:08<00:02, 220MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  78%|█▌| 1.54G/1.97G [00:08<00:01, 221MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  80%|█▌| 1.57G/1.97G [00:09<00:01, 218MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  82%|█▋| 1.60G/1.97G [00:09<00:01, 218MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  83%|█▋| 1.64G/1.97G [00:09<00:01, 220MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  85%|█▋| 1.67G/1.97G [00:09<00:01, 221MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  86%|█▋| 1.70G/1.97G [00:09<00:01, 220MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  88%|█▊| 1.73G/1.97G [00:09<00:01, 222MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  89%|█▊| 1.76G/1.97G [00:09<00:00, 223MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  91%|█▊| 1.79G/1.97G [00:10<00:00, 223MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  93%|█▊| 1.82G/1.97G [00:10<00:00, 220MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  94%|█▉| 1.86G/1.97G [00:10<00:00, 222MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  96%|█▉| 1.89G/1.97G [00:10<00:00, 221MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin:  97%|█▉| 1.92G/1.97G [00:10<00:00, 220MB/s]\u001b[A\nDownloading (…)l-00005-of-00007.bin: 100%|██| 1.97G/1.97G [00:10<00:00, 181MB/s]\u001b[A\nDownloading shards:  71%|█████████████████▊       | 5/7 [00:48<00:19,  9.90s/it]\nDownloading (…)l-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:   1%|  | 21.0M/1.93G [00:00<00:09, 193MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:   2%|  | 41.9M/1.93G [00:00<00:09, 190MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:   4%|  | 73.4M/1.93G [00:00<00:09, 203MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:   5%|▏  | 105M/1.93G [00:00<00:08, 211MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:   7%|▏  | 136M/1.93G [00:00<00:08, 217MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:   9%|▎  | 168M/1.93G [00:00<00:08, 217MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  10%|▎  | 199M/1.93G [00:00<00:07, 219MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  12%|▎  | 231M/1.93G [00:01<00:07, 221MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  14%|▍  | 262M/1.93G [00:01<00:07, 220MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  15%|▍  | 294M/1.93G [00:01<00:07, 221MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  17%|▌  | 325M/1.93G [00:01<00:07, 220MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  18%|▌  | 357M/1.93G [00:01<00:07, 222MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  20%|▌  | 388M/1.93G [00:01<00:06, 222MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  22%|▋  | 419M/1.93G [00:01<00:06, 222MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  23%|▋  | 451M/1.93G [00:02<00:06, 220MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  25%|▊  | 482M/1.93G [00:02<00:06, 223MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  27%|▊  | 514M/1.93G [00:02<00:06, 218MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  28%|▊  | 545M/1.93G [00:02<00:06, 206MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  29%|▉  | 566M/1.93G [00:02<00:06, 200MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  30%|▉  | 587M/1.93G [00:02<00:07, 175MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  32%|▉  | 608M/1.93G [00:02<00:08, 154MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  33%|▉  | 629M/1.93G [00:03<00:08, 154MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  34%|█  | 650M/1.93G [00:03<00:08, 158MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  35%|█  | 671M/1.93G [00:03<00:07, 165MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  36%|█  | 692M/1.93G [00:03<00:07, 176MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  37%|█  | 713M/1.93G [00:03<00:06, 183MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  38%|█▏ | 734M/1.93G [00:03<00:06, 189MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  39%|█▏ | 755M/1.93G [00:03<00:06, 195MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  40%|█▏ | 776M/1.93G [00:03<00:05, 193MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  41%|█▏ | 797M/1.93G [00:03<00:05, 194MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  42%|█▎ | 818M/1.93G [00:04<00:05, 193MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  44%|█▎ | 839M/1.93G [00:04<00:05, 191MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  45%|█▎ | 860M/1.93G [00:04<00:05, 195MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  46%|█▎ | 881M/1.93G [00:04<00:05, 199MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  47%|█▍ | 902M/1.93G [00:04<00:05, 194MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  48%|█▍ | 923M/1.93G [00:04<00:05, 183MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  49%|█▍ | 944M/1.93G [00:04<00:05, 186MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  50%|█▌ | 965M/1.93G [00:04<00:05, 182MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  51%|█▌ | 986M/1.93G [00:05<00:06, 154MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  52%|█ | 1.01G/1.93G [00:05<00:05, 161MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  53%|█ | 1.03G/1.93G [00:05<00:05, 168MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  54%|█ | 1.05G/1.93G [00:05<00:05, 172MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  55%|█ | 1.07G/1.93G [00:05<00:04, 177MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  57%|█▏| 1.09G/1.93G [00:05<00:04, 182MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  58%|█▏| 1.11G/1.93G [00:05<00:04, 186MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  59%|█▏| 1.13G/1.93G [00:05<00:04, 184MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  60%|█▏| 1.15G/1.93G [00:05<00:04, 189MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  61%|█▏| 1.17G/1.93G [00:06<00:03, 193MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  62%|█▏| 1.20G/1.93G [00:06<00:03, 197MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  63%|█▎| 1.22G/1.93G [00:06<00:03, 199MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  64%|█▎| 1.24G/1.93G [00:06<00:03, 193MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  65%|█▎| 1.26G/1.93G [00:06<00:03, 190MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  66%|█▎| 1.28G/1.93G [00:06<00:03, 193MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  67%|█▎| 1.30G/1.93G [00:06<00:03, 194MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  69%|█▎| 1.32G/1.93G [00:06<00:03, 196MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  70%|█▍| 1.34G/1.93G [00:06<00:03, 194MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  71%|█▍| 1.36G/1.93G [00:07<00:02, 198MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  72%|█▍| 1.38G/1.93G [00:07<00:02, 201MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  73%|█▍| 1.42G/1.93G [00:07<00:02, 203MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  75%|█▌| 1.45G/1.93G [00:07<00:02, 209MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  77%|█▌| 1.48G/1.93G [00:07<00:02, 215MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  78%|█▌| 1.51G/1.93G [00:07<00:01, 215MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  80%|█▌| 1.54G/1.93G [00:07<00:01, 218MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  82%|█▋| 1.57G/1.93G [00:07<00:01, 219MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  83%|█▋| 1.60G/1.93G [00:08<00:01, 217MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  85%|█▋| 1.64G/1.93G [00:08<00:01, 218MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  87%|█▋| 1.67G/1.93G [00:08<00:01, 220MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  88%|█▊| 1.70G/1.93G [00:08<00:01, 222MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  90%|█▊| 1.73G/1.93G [00:08<00:00, 222MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  91%|█▊| 1.76G/1.93G [00:08<00:00, 224MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  93%|█▊| 1.79G/1.93G [00:08<00:00, 223MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  95%|█▉| 1.82G/1.93G [00:09<00:00, 222MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  96%|█▉| 1.86G/1.93G [00:09<00:00, 224MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin:  98%|█▉| 1.89G/1.93G [00:09<00:00, 221MB/s]\u001b[A\nDownloading (…)l-00006-of-00007.bin: 100%|██| 1.93G/1.93G [00:09<00:00, 201MB/s]\u001b[A\nDownloading shards:  86%|█████████████████████▍   | 6/7 [00:58<00:09,  9.82s/it]\nDownloading (…)l-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:   3%|  | 31.5M/1.05G [00:00<00:04, 244MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:   6%|  | 62.9M/1.05G [00:00<00:04, 233MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:   9%|▏ | 94.4M/1.05G [00:00<00:04, 225MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  12%|▎  | 126M/1.05G [00:00<00:04, 225MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  15%|▍  | 157M/1.05G [00:00<00:03, 225MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  18%|▌  | 189M/1.05G [00:00<00:03, 226MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  21%|▋  | 220M/1.05G [00:00<00:03, 224MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  24%|▋  | 252M/1.05G [00:01<00:03, 224MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  27%|▊  | 283M/1.05G [00:01<00:03, 225MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  30%|▉  | 315M/1.05G [00:01<00:03, 223MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  33%|▉  | 346M/1.05G [00:01<00:03, 224MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  36%|█  | 377M/1.05G [00:01<00:03, 222MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  39%|█▏ | 409M/1.05G [00:01<00:02, 225MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  42%|█▎ | 440M/1.05G [00:01<00:02, 220MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  45%|█▎ | 472M/1.05G [00:02<00:02, 220MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  48%|█▍ | 503M/1.05G [00:02<00:02, 218MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  51%|█▌ | 535M/1.05G [00:02<00:02, 219MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  54%|█▌ | 566M/1.05G [00:02<00:02, 221MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  57%|█▋ | 598M/1.05G [00:02<00:02, 223MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  60%|█▊ | 629M/1.05G [00:02<00:01, 222MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  63%|█▉ | 661M/1.05G [00:02<00:01, 223MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  66%|█▉ | 692M/1.05G [00:03<00:01, 217MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  69%|██ | 724M/1.05G [00:03<00:01, 214MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  72%|██▏| 755M/1.05G [00:03<00:01, 202MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  74%|██▏| 776M/1.05G [00:03<00:01, 195MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  76%|██▎| 797M/1.05G [00:03<00:01, 198MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  79%|██▎| 828M/1.05G [00:03<00:01, 200MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  81%|██▍| 849M/1.05G [00:03<00:01, 198MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  83%|██▍| 870M/1.05G [00:04<00:00, 199MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  85%|██▌| 891M/1.05G [00:04<00:00, 200MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  87%|██▌| 912M/1.05G [00:04<00:00, 201MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  89%|██▋| 933M/1.05G [00:04<00:00, 202MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  91%|██▋| 954M/1.05G [00:04<00:00, 200MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  93%|██▊| 975M/1.05G [00:04<00:00, 202MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  95%|██▊| 996M/1.05G [00:04<00:00, 197MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin:  97%|█▉| 1.02G/1.05G [00:04<00:00, 200MB/s]\u001b[A\nDownloading (…)l-00007-of-00007.bin: 100%|██| 1.05G/1.05G [00:04<00:00, 211MB/s]\u001b[A\nDownloading shards: 100%|█████████████████████████| 7/7 [01:03<00:00,  9.05s/it]\nDownloading shards: 100%|█████████████████████████| 7/7 [01:03<00:00,  9.05s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:37<00:00, 13.96s/it]\nLoading checkpoint shards: 100%|██████████████████| 7/7 [01:37<00:00, 13.97s/it]\nmemory footprint of model: 3.6520424485206604 GB\nmemory footprint of model: 3.6520424485206604 GB\n\u001b[32m2023-08-02 08:16:53.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m485\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\n\u001b[32m2023-08-02 08:16:53.132\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m485\u001b[0m - \u001b[1mprepare_model_for_kbit_training...\u001b[0m\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nnumber_train_samples=10\nnumber_of_eval_numbers=5\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=5e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=1,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-multi-turn-sft-0802-v1,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=40.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-multi-turn-sft-0802-v1,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=1,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.0,\nxpu_backend=None,\n)\ntrainable params: 118,587,392 || all params: 3,506,898,944 || trainable%: 3.381545744364626\nnumber_train_samples=10\nnumber_of_eval_numbers=5\nTrainingArguments(\n_n_gpu=1,\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_pin_memory=True,\nddp_backend=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=False,\nddp_timeout=1800,\ndebug=[],\ndeepspeed={'fp16': {'enabled': True, 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': False}, 'optimizer': {'type': 'AdamW', 'params': {'lr': 'auto', 'weight_decay': 'auto', 'torch_adam': True, 'adam_w_mode': True}}, 'scheduler': {'type': 'WarmupDecayLR', 'params': {'warmup_min_lr': 'auto', 'warmup_max_lr': 'auto', 'warmup_num_steps': 'auto', 'total_num_steps': 'auto'}}, 'zero_optimization': {'stage': 2, 'offload_optimizer': {'device': 'cpu', 'pin_memory': False}, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000.0, 'reduce_bucket_size': 'auto', 'stage3_prefetch_bucket_size': 'auto', 'stage3_param_persistence_threshold': 'auto', 'stage3_max_live_parameters': 1000000.0, 'stage3_max_reuse_distance': 1000000.0, 'stage3_gather_16bit_weights_on_model_save': False}, 'train_batch_size': 8, 'train_micro_batch_size_per_gpu': 4},\ndisable_tqdm=False,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=10,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngreater_is_better=False,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=5e-05,\nlength_column_name=length,\nload_best_model_at_end=True,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=output-multi-turn-sft-0802-v1,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=10,\nlogging_strategy=steps,\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=loss,\nmp_parameters=,\nno_cuda=False,\nnum_train_epochs=40.0,\noptim=paged_adamw_8bit,\noptim_args=None,\noutput_dir=output-multi-turn-sft-0802-v1,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=1,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=False,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=out/1,\nsave_on_each_node=False,\nsave_safetensors=False,\nsave_steps=10,\nsave_strategy=steps,\nsave_total_limit=2,\nseed=42,\nsharded_ddp=[],\nskip_memory_metrics=True,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.1,\nwarmup_steps=0,\nweight_decay=0.0,\nxpu_backend=None,\n)\n{'loss': 9.6928, 'learning_rate': 1.25e-05, 'epoch': 2.0}                       \n  5%|██                                        | 10/200 [00:11<02:43,  1.17it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 9.145455360412598, 'eval_runtime': 1.3005, 'eval_samples_per_second': 3.845, 'eval_steps_per_second': 2.307, 'epoch': 2.0}\n  5%|██                                        | 10/200 [00:12<02:43,  1.17it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 10.08it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 7.1208, 'learning_rate': 3.7500000000000003e-05, 'epoch': 4.0}         \n 10%|████▏                                     | 20/200 [00:22<02:35,  1.16it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  9.65it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 4.856997013092041, 'eval_runtime': 0.6594, 'eval_samples_per_second': 7.583, 'eval_steps_per_second': 4.55, 'epoch': 4.0}\n 10%|████▏                                     | 20/200 [00:23<02:35,  1.16it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.90it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 4.1598, 'learning_rate': 4.8611111111111115e-05, 'epoch': 6.0}         \n 15%|██████▎                                   | 30/200 [00:34<02:31,  1.12it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  9.54it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 2.8180716037750244, 'eval_runtime': 0.6749, 'eval_samples_per_second': 7.409, 'eval_steps_per_second': 4.445, 'epoch': 6.0}\n 15%|██████▎                                   | 30/200 [00:35<02:31,  1.12it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.69it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 1.9819, 'learning_rate': 4.5833333333333334e-05, 'epoch': 8.0}         \n 20%|████████▍                                 | 40/200 [00:45<02:19,  1.15it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  9.35it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.37353378534317017, 'eval_runtime': 0.6873, 'eval_samples_per_second': 7.275, 'eval_steps_per_second': 4.365, 'epoch': 8.0}\n 20%|████████▍                                 | 40/200 [00:46<02:19,  1.15it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.56it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.5319, 'learning_rate': 4.305555555555556e-05, 'epoch': 10.0}         \n 25%|██████████▌                               | 50/200 [00:57<02:11,  1.14it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  9.46it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.20311495661735535, 'eval_runtime': 0.6909, 'eval_samples_per_second': 7.237, 'eval_steps_per_second': 4.342, 'epoch': 10.0}\n 25%|██████████▌                               | 50/200 [00:58<02:11,  1.14it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.58it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.2306, 'learning_rate': 4.027777777777778e-05, 'epoch': 12.0}         \n 30%|████████████▌                             | 60/200 [01:09<02:04,  1.13it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  9.24it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.10414393246173859, 'eval_runtime': 0.7244, 'eval_samples_per_second': 6.903, 'eval_steps_per_second': 4.142, 'epoch': 12.0}\n 30%|████████████▌                             | 60/200 [01:10<02:04,  1.13it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.35it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.3476, 'learning_rate': 3.7500000000000003e-05, 'epoch': 14.0}        \n 35%|██████████████▋                           | 70/200 [01:20<01:56,  1.11it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  8.86it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.42670148611068726, 'eval_runtime': 0.733, 'eval_samples_per_second': 6.821, 'eval_steps_per_second': 4.093, 'epoch': 14.0}\n 35%|██████████████▋                           | 70/200 [01:21<01:56,  1.11it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.07it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.1236, 'learning_rate': 3.472222222222222e-05, 'epoch': 16.0}         \n 40%|████████████████▊                         | 80/200 [01:32<01:48,  1.11it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  9.08it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.31223228573799133, 'eval_runtime': 0.7444, 'eval_samples_per_second': 6.717, 'eval_steps_per_second': 4.03, 'epoch': 16.0}\n 40%|████████████████▊                         | 80/200 [01:33<01:48,  1.11it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.17it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.1607, 'learning_rate': 3.194444444444444e-05, 'epoch': 18.0}         \n 45%|██████████████████▉                       | 90/200 [01:45<01:41,  1.08it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  9.31it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.03633572533726692, 'eval_runtime': 0.7264, 'eval_samples_per_second': 6.883, 'eval_steps_per_second': 4.13, 'epoch': 18.0}\n 45%|██████████████████▉                       | 90/200 [01:45<01:41,  1.08it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.36it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.125, 'learning_rate': 2.916666666666667e-05, 'epoch': 20.0}          \n 50%|████████████████████▌                    | 100/200 [01:56<01:29,  1.12it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  9.32it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.15400442481040955, 'eval_runtime': 0.706, 'eval_samples_per_second': 7.082, 'eval_steps_per_second': 4.249, 'epoch': 20.0}\n 50%|████████████████████▌                    | 100/200 [01:57<01:29,  1.12it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.08it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!save done !!!\n\n{'loss': 0.0934, 'learning_rate': 2.6388888888888892e-05, 'epoch': 22.0}        \n 55%|██████████████████████▌                  | 110/200 [02:08<01:24,  1.07it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  9.54it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.18147076666355133, 'eval_runtime': 0.7538, 'eval_samples_per_second': 6.633, 'eval_steps_per_second': 3.98, 'epoch': 22.0}\n 55%|██████████████████████▌                  | 110/200 [02:09<01:24,  1.07it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  5.14it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.1187, 'learning_rate': 2.361111111111111e-05, 'epoch': 24.0}         \n 60%|████████████████████████▌                | 120/200 [02:20<01:09,  1.15it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  8.93it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.11258673667907715, 'eval_runtime': 0.7118, 'eval_samples_per_second': 7.024, 'eval_steps_per_second': 4.215, 'epoch': 24.0}\n 60%|████████████████████████▌                | 120/200 [02:20<01:09,  1.15it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.45it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.1114, 'learning_rate': 2.0833333333333336e-05, 'epoch': 26.0}        \n 65%|██████████████████████████▋              | 130/200 [02:31<01:02,  1.11it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  9.14it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.04599953815340996, 'eval_runtime': 0.7136, 'eval_samples_per_second': 7.007, 'eval_steps_per_second': 4.204, 'epoch': 26.0}\n 65%|██████████████████████████▋              | 130/200 [02:32<01:02,  1.11it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.51it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.0346, 'learning_rate': 1.8055555555555555e-05, 'epoch': 28.0}        \n 70%|████████████████████████████▋            | 140/200 [02:43<00:54,  1.10it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  8.96it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.06494708359241486, 'eval_runtime': 0.7169, 'eval_samples_per_second': 6.974, 'eval_steps_per_second': 4.185, 'epoch': 28.0}\n 70%|████████████████████████████▋            | 140/200 [02:44<00:54,  1.10it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.49it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.0787, 'learning_rate': 1.527777777777778e-05, 'epoch': 30.0}         \n 75%|██████████████████████████████▊          | 150/200 [02:55<00:45,  1.11it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  9.35it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.04928788170218468, 'eval_runtime': 0.7133, 'eval_samples_per_second': 7.009, 'eval_steps_per_second': 4.206, 'epoch': 30.0}\n 75%|██████████████████████████████▊          | 150/200 [02:55<00:45,  1.11it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.51it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.0268, 'learning_rate': 1.2777777777777777e-05, 'epoch': 32.0}        \n 80%|████████████████████████████████▊        | 160/200 [03:06<00:35,  1.11it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  8.97it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.3443671762943268, 'eval_runtime': 0.7147, 'eval_samples_per_second': 6.996, 'eval_steps_per_second': 4.198, 'epoch': 32.0}\n 80%|████████████████████████████████▊        | 160/200 [03:07<00:35,  1.11it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  5.61it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.1571, 'learning_rate': 1e-05, 'epoch': 34.0}                         \n 85%|██████████████████████████████████▊      | 170/200 [03:18<00:27,  1.11it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  9.80it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.12428184598684311, 'eval_runtime': 0.7114, 'eval_samples_per_second': 7.028, 'eval_steps_per_second': 4.217, 'epoch': 34.0}\n 85%|██████████████████████████████████▊      | 170/200 [03:19<00:27,  1.11it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.56it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.025, 'learning_rate': 7.222222222222222e-06, 'epoch': 36.0}          \n 90%|████████████████████████████████████▉    | 180/200 [03:30<00:18,  1.11it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  9.26it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.06888707727193832, 'eval_runtime': 0.7132, 'eval_samples_per_second': 7.011, 'eval_steps_per_second': 4.206, 'epoch': 36.0}\n 90%|████████████████████████████████████▉    | 180/200 [03:31<00:18,  1.11it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.45it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.053, 'learning_rate': 4.444444444444445e-06, 'epoch': 38.0}          \n 95%|██████████████████████████████████████▉  | 190/200 [03:42<00:09,  1.05it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  8.48it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.0011943158460780978, 'eval_runtime': 0.8043, 'eval_samples_per_second': 6.217, 'eval_steps_per_second': 3.73, 'epoch': 38.0}\n 95%|██████████████████████████████████████▉  | 190/200 [03:43<00:09,  1.05it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.83it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'loss': 0.0201, 'learning_rate': 1.6666666666666667e-06, 'epoch': 40.0}        \n100%|█████████████████████████████████████████| 200/200 [03:54<00:00,  1.12it/s]\n  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[A\n 67%|██████████████████████████████               | 2/3 [00:00<00:00,  9.51it/s]\u001b[A\n                                                                                \u001b[A\n\u001b[A{'eval_loss': 0.001105221570469439, 'eval_runtime': 0.7151, 'eval_samples_per_second': 6.992, 'eval_steps_per_second': 4.195, 'epoch': 40.0}\n100%|█████████████████████████████████████████| 200/200 [03:54<00:00,  1.12it/s]\n100%|█████████████████████████████████████████████| 3/3 [00:00<00:00,  6.53it/s]\u001b[A\n                                                                                \u001b[Abegin to save  !!!\nbegin to save  !!!\nsave done !!!\nsave done !!!\n{'train_runtime': 238.3396, 'train_samples_per_second': 1.678, 'train_steps_per_second': 0.839, 'train_loss': 1.2596728940308095, 'epoch': 40.0}\n100%|█████████████████████████████████████████| 200/200 [03:57<00:00,  1.19s/it]\n[2023-08-02 08:22:42,401] [INFO] [launch.py:347:main] Process 368 exits successfully.\n[2023-08-02 08:22:42,401] [INFO] [launch.py:347:main] Process 367 exits successfully.\n","output_type":"stream"}]}]}